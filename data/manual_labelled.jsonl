{"id": "WOS:000395896500089", "keywords": "Restricted boltzmann machine; Deep Boltzmann machine; Deep belief network; Dropout; Weight uncertainty; Feature selection", "title": "Research on Point-wise Gated Deep Networks", "abstract": "Stacking Restricted Boltzmann Machines (RBM) to create deep networks, such as Deep Belief Networks( DBN) and Deep Boltzmann Machines (DBM), has become one of the most important research fields indeep learning. DBM and DBN provide state-of-the-art results in many fields such as image recognition, but they don't show better learning abilities than RBM when dealing with data containing irrelevant patterns. Point-wise Gated Restricted Boltzmann Machines (pgRBM) can effectively find the task-relevant patterns from data containing irrelevant patterns and thus achieve satisfied classification results. For the limitations of the DBN and the DBM in the processing of data containing irrelevant patterns, we introduce the pgRBM into the DBN and the DBM and present Point-wise Gated Deep Belief Networks (pgDBN) and Pointwise Gated Deep Boltzmann Machines (pgDBM). The pgDBN and the pgDBM both utilize the pgRBM instead of the RBM to pre-train the weights connecting the networks' the visible layer and the hidden layer, and apply the pgRBM learning task-relevant data subset for traditional networks. Then, this paper discusses the validity that dropout and weight uncertainty methods are developed to prevent over fitting in pgRBMs, pgDBNs, and pgDBMs networks. Experimental results on MNIST variation datasets show that the pgDBN and the pgDBM are effective deep neural networks learning (C) 2016 Elsevier B.V. All rights reserved.", "journal": "APPLIED SOFT COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": ["neural net", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000384863900011", "keywords": "Hidden Markov model deep neural network (HMM-DNN); Neural networks; Acoustic modeling; Speech recognition; Large vocabulary continuous speech recognition (LVCSR)", "title": "Building DNN acoustic models for large vocabulary speech recognition", "abstract": "Understanding architectural choices for deep neural networks (DNNs) is crucial to improving state-of-the-art speech recognition systems. We investigate which aspects of DNN acoustic model design are most important for speech recognition system performance, focusing on feed-forward networks. We study the effects of parameters like model size (number of layers, total parameters), architecture (convolutional networks), and training details (loss function, regularization methods) on DNN classifier performance and speech recognizer word error rates. On the Switchboard benchmark corpus we compare standard DNNs to convolutional networks, and present the first experiments using locally-connected, untied neural networks for acoustic modeling. Using a much larger 2100-hour training corpus (combining Switchboard and Fisher) we examine the performance of very large DNN models with up to ten times more parameters than those typically used in speech recognition systems. The results suggest that a relatively simple DNN architecture and optimization technique give strong performance, and we offer intuitions about architectural choices like network depth over breadth. Our findings extend previous works to help establish a set of best practices for building DNN hybrid speech recognition systems and constitute an important first step toward analyzing more complex recurrent, sequence-discriminative, and HMM-free architectures. (C) 2016 Elsevier Ltd. All rights reserved.", "journal": "COMPUTER SPEECH AND LANGUAGE", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000560557000011", "keywords": "Pedestrian detectors; Computer vision; LeNet convolutional networks; Image classification", "title": "Pedestrian detection with LeNet-like convolutional networks", "abstract": "We present a detection method that is able to detect a learned target and is valid for both static and moving cameras. As an application, we detect pedestrians, but could be anything if there is a large set of images of it. The data set is fed into a number of deep convolutional networks, and then, two of these models are set in cascade in order to filter the cutouts of a multi-resolution window that scans the frames in a video sequence. We demonstrate that the excellent performance of deep convolutional networks is very difficult to match when dealing with real problems, and yet we obtain competitive results.", "journal": "NEURAL COMPUTING & APPLICATIONS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000504825900007", "keywords": "ANN; MLP; RBF; fixed-bed column; lactose; breakthrough curves modeling", "title": "MODELING OF MILK LACTOSE REMOVAL BY COLUMN ADSORPTION USING ARTIFICIAL NEURAL NETWORKS: MLP AND RBF", "abstract": "Artificial neural network (ANN) techniques are effective in modeling nonlinear processes, are simple to implement and require low computational time. In this work, the lactose adsorption process for continuous flow in a fixed-bed column with a molecularly imprinted polymer (MIP) adsorbent was modeled using an ANN technique. The neural models allowed predicting the relative lactose concentration (C/C-0) from the interactions between the variables of contact time (min), temperature (degrees C), granulometly (mesh), bed height (cm) and flow rate (mL min(-1)). The ANN models were developed in MATLAB using multilayer perceptrons (MLP) and a radial basis function network (RBF). The MLP model was developed using a three-layer feed forward backpropagation network with 5, 8 and 4 neurons in the first, second and third layer, respectively. The function (RBF) network is also proposed and its performance is compared to a traditional network type. The best architecture configuration RBF model was developed using 5, 14 and 1 neurons in the first, second and third layer, respectively. The proposal of development of mathematical models applied to multi-component adsorption system for milk using these approaches is innovative. The resulting breakthrough curve models for lactose adsorption were in good agreement with the experimental results. Performance indices, such as R-2, MSE, RMSE, SSE, MAE and RME were used to evaluate the reliabilities and accuracies of the models. A comparison between the ANN models shows the ability to predict the breakthrough curves of lactose removal in the milk adsorption process. Though, the MLP network model shows more accurately a higher correlation coefficient (R-2 = 0.9751) and lower values for the obtained error indices. The accuracy of the model is confirmed by the comparison between the predicted and experimental data. The results showed that both neural models efficiently described the non-linear process of lactose adsorption in a fixed-bed column.", "journal": "CHEMICAL INDUSTRY & CHEMICAL ENGINEERING QUARTERLY", "category": "Chemistry, Applied; Engineering, Chemical", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000507606200003", "keywords": "video smoke detection; deep learning; object detection; convolutional neural networks", "title": "Video Smoke Detection Method Based on Change-Cumulative Image and Fusion Deep Network", "abstract": "Smoke detection technology based on computer vision is a popular research direction in fire detection. This technology is widely used in outdoor fire detection fields (e.g., forest fire detection). Smoke detection is often based on features such as color, shape, texture, and motion to distinguish between smoke and non-smoke objects. However, the salience and robustness of these features are insufficiently strong, resulting in low smoke detection performance under complex environment. Deep learning technology has improved smoke detection performance to a certain degree, but extracting smoke detail features is difficult when the number of network layers is small. With no effective use of smoke motion characteristics, indicators such as false alarm rate are high in video smoke detection. To enhance the detection performance of smoke objects in videos, this paper proposes a concept of change-cumulative image by converting the YUV color space of multi-frame video images into a change-cumulative image, which can represent the motion and color-change characteristics of smoke. Then, a fusion deep network is designed, which increases the depth of the VGG16 network by arranging two convolutional layers after each of its convolutional layer. The VGG16 and Resnet50 (Deep residual network) network models are also arranged using the fusion deep network to improve feature expression ability while increasing the depth of the whole network. Doing so can help extract additional discriminating characteristics of smoke. Experimental results show that by using the change-cumulative image as the input image of the deep network model, smoke detection performance is superior to the classic RGB input image; the smoke detection performance of the fusion deep network model is better than that of the single VGG16 and Resnet50 network models; the smoke detection accuracy, false positive rate, and false alarm rate of this method are better than those of the current popular methods of video smoke detection.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000704005800002", "keywords": "Graph convolutional network; Band selection; Hyperspectral image classification; Attention mechanism", "title": "Dual-graph convolutional network based on band attention and sparse constraint for hyperspectral band selection", "abstract": "Band selection is a research hotspot in hyperspectral image processing. The continuity of the spectral bands causes the adjacent bands to be highly correlated, and correlation among long-range bands is possible with hundreds of spectral bands. Most existing deep learning methods fail to make full use of the inter-band correlation for band selection. In this paper, a novel dual-graph convolutional network based on band attention and a sparse constraint is proposed for band selection. The network consists of two branches. In the attention branch, band-based dual graphs are constructed to encode the contextual correlation of adjacent bands and the structural correlation of long-range bands into non-Euclidean space. Subsequently, the graph convolution-based band attention mechanism is devised to aggregate the band information in the band-based dual graphs and to generate the attention map for all bands. The band attention map is sparsely constrained and embedded as a mask into the trunk branch. In the trunk branch, sample-based dual graphs are constructed to represent the topological information of the samples in the spectral and spatial domains. Furthermore, a dense graph convolutional network is designed to extract and fuse the spatial-spectral and topological features from the shallow to deep layers for classification. A soft-shifting optimization strategy is implemented by defining a new loss from full bands and selected bands to solve the optimization problem caused by the sparse constraint. In this manner, band selection, feature extraction, and classification can be combined into an end-to-end trainable network. The experimental results on representative hyperspectral image datasets demonstrate the superiority of the proposed method over current state-of-the-art band selection methods. (C) 2021 Elsevier B.V. All rights reserved.", "journal": "KNOWLEDGE-BASED SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000655830300017", "keywords": "Feature extraction; Correlation; Task analysis; Semantics; Deep learning; Visualization; Benchmark testing; Multi-label image classification; label-specific feature; label correlations; graph convolutional network; deep learning", "title": "Joint Input and Output Space Learning for Multi-Label Image Classification", "abstract": "Multi-label image classification aims to predict the labels associated with a given image. While most existing methods utilize unified image representations, extracting label-specific features through input space learning would improve the discriminative power of the learned features. On the other hand, most feature learning studies often ignore the learning in the output label space, although taking advantage of label correlations can boost the classification performance. In this paper, we propose a deep learning framework that incorporates flexible modules which can learn from both input and output spaces for multi-label image classification. For the input space learning, we devise a label-specific feature pooling method to refine convolutional features for obtaining features specific to each label. For the output space learning, we design a Two-Stream Graph Convolutional Network (TSGCN) to learn multi-label classifiers by mapping spatial object relationships and semantic label correlations. More specifically, we build object spatial graphs to characterize the spatial relationships among objects in an image, which supplements the label semantic graphs modelling the semantic label correlations. Experimental results on two popular benchmark datasets (i.e., Pascal VOC and MS-COCO) show that our proposed method achieves superior performance over the state-of-the-arts.", "journal": "IEEE TRANSACTIONS ON MULTIMEDIA", "category": "Computer Science, Information Systems; Computer Science, Software Engineering; Telecommunications", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000456542000007", "keywords": "Hashing; image retrieval; ranking structure; fully convolutional network; convolutional neural network; local spatial; global semantic information", "title": "Deep Ordinal Hashing With Spatial Attention", "abstract": "Hashing has attracted increasing research attention in recent years due to its high efficiency of computation and storage in image retrieval. Recent works have demonstrated the superiority of simultaneous feature representations and hash functions learning with deep neural networks. However, most existing deep hashing methods directly learn the hash functions by encoding the global semantic information, while ignoring the local spatial information of images. The loss of local spatial structure makes the performance bottleneck of hash functions, therefore limiting its application for accurate similarity retrieval. In this paper, we propose a novel deep ordinal hashing (DOH) method, which learns ordinal representations to generate ranking-based hash codes by leveraging the ranking structure of feature space from both local and global views. In particular, to effectively build the ranking structure, we propose to learn the rank correlation space by exploiting the local spatial information from fully convolutional network and the global semantic information from the convolutional neural network simultaneously. More specifically, an effective spatial attention model is designed to capture the local spatial information by selectively learning well-specified locations closely related to target objects. In such hashing framework, the local spatial and global semantic nature of images is captured in an end-to-end ranking-to-hashing manner. Experimental results conducted on three widely used datasets demonstrate that the proposed DOH method significantly outperforms the state-of-the-art hashing methods.", "journal": "IEEE TRANSACTIONS ON IMAGE PROCESSING", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000674954800001", "keywords": "noise reduction; convolutional neural network; signal to noise ratio; seismic signal; residual dense blocks", "title": "Research on Deep Convolutional Neural Network Time-Frequency Domain Seismic Signal Denoising Combined With Residual Dense Blocks", "abstract": "Deep Convolutional Neural Networks (DCNN) have the ability to learn complex features and are thus widely used in the field of seismic signal denoising with low signal-to-noise ratio (SNR). However, the current convolutional deep network used for seismic signal noise reduction does not make full use of the feature information extracted from all convolution layers in the network, and thus cannot fit the seismic signal with high SNR. To deal with this problem, this paper proposes the DnRDB model, a convolutional deep network time-frequency domain seismic signal denoising model combined with residual dense blocks (RDB). The model is mainly composed of several RDB in series. The input of each convolution layer in each RDB module is formed by the output of all the previous convolution layers. Meanwhile, even if the number of layers is increased, the fusion of the seismic signal features learned by the RDB modules can still achieve full extraction of seismic signals. Furthermore, deepening the model structure by concatenating multiple RDB modules enables further useful feature information to be extracted, which improves the SNR of seismic signals. The DnRDB model was trained and tested using the Stanford Global Seismic Dataset. The experimental results show that the DnRDB model can effectively recover seismic signals and remove various forms of noise. Even in the case of high noise, the denoised signal still has a high SNR. When the DnRDB model is compared with other denoising approaches such as wavelet threshold, empirical mode decomposition, and different deep learning methods, the results indicate that it performs best overall in denoising the same segment of the noisy seismic signal; the denoised signal also has less waveform distortion. Use of the DnRDB model in subsequent seismic signal processing work indicates that it can help the phase recognition algorithm improve the accuracy of seismic recognition through noise reduction.", "journal": "FRONTIERS IN EARTH SCIENCE", "category": "Geosciences, Multidisciplinary", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000353845100016", "keywords": "Heating consumption prediction; Neural networks ensemble", "title": "Ensemble of various neural networks for prediction of heating energy consumption", "abstract": "For prediction of heating energy consumption-of a university campus, various artificial neural networks are used: feed forward backpropagation neural network (FFNN), radial basis function network (RBFN) and adaptive neuro-fuzzy interference system (ANFIS). Actual measured data are used for training and testing the models. For each neural networks type, three models (using different number of input parameters) are analyzed. In order to improve prediction accuracy, ensemble of neural networks is examined. Three different combinations of output are analyzed. It is shown that all proposed neural networks can predict heating consumption with great accuracy, and that using ensemble achieves even better results. (C) 2015 Elsevier B.V. All rights reserved.", "journal": "ENERGY AND BUILDINGS", "category": "Construction & Building Technology; Energy & Fuels; Engineering, Civil", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000615551200001", "keywords": "Image captioning; Masked convolution; Dense fusion connection; Improved stacked attention module", "title": "Image Captioning with Dense Fusion Connection and Improved Stacked Attention Module", "abstract": "In the existing image captioning methods, masked convolution is usually used to generate language description, and traditional residual network (ResNets) methods used for masked convolution bring about the vanishing gradient problem. To address this issue, we propose a new image captioning framework that combines dense fusion connection (DFC) and improved stacked attention module. DFC uses dense convolutional networks (DenseNets) architecture to connect each layer to any other layer in a feed-forward fashion, then adopts ResNets method to combine features through summation. The improved stacked attention module can capture more fine-grained visual information highly relevant to the word prediction. Finally, we employ the Transformer to the image encoder to sufficiently obtain the attended image representation. The experimental results on MS-COCO dataset demonstrate the proposed model can increase CIDEr score from 91.2% to 106.1%, which has higher performance than the comparable models and verifies the effectiveness of the proposed model.", "journal": "NEURAL PROCESSING LETTERS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000419346900019", "keywords": "Molecular and cellular imaging; gastrointestinal tract; segmentation", "title": "Structure Prediction for Gland Segmentation With Hand-Crafted and Deep Convolutional Features", "abstract": "We present a novel method to segment instances of glandular structures from colon histopathology images. We use a structure learning approach which represents local spatial configurations of class labels, capturing structural information normally ignored by sliding-window methods. This allows us to reveal different spatial structures of pixel labels (e.g., locations between adjacent glands, or far from glands), and to identify correctly neighboring glandular structures as separate instances. Exemplars of label structures are obtained via clustering and used to train support vector machine classifiers. The label structures predicted are then combined and post-processed to obtain segmentation maps. We combine handcrafted, multi-scale image features with features computed by a deep convolutional network trained to map images to segmentation maps. We evaluate the proposed method on the public domain GlaS data set, which allows extensive comparisons with recent, alternative methods. Using the GlaS contest protocol, our method achieves the overall best performance.", "journal": "IEEE TRANSACTIONS ON MEDICAL IMAGING", "category": "Computer Science, Interdisciplinary Applications; Engineering, Biomedical; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000506678200137", "keywords": "gesture recognition; motion representation; 2D CNN; 3D DenseNet; information fusion", "title": "Fusion of 2D CNN and 3D DenseNet for Dynamic Gesture Recognition", "abstract": "Gesture recognition has been applied in many fields as it is a natural human-computer communication method. However, recognition of dynamic gesture is still a challenging topic because of complex disturbance information and motion information. In this paper, we propose an effective dynamic gesture recognition method by fusing the prediction results of a two-dimensional (2D) motion representation convolution neural network (CNN) model and three-dimensional (3D) dense convolutional network (DenseNet) model. Firstly, to obtain a compact and discriminative gesture motion representation, the motion history image (MHI) and pseudo-coloring technique were employed to integrate the spatiotemporal motion sequences into a frame image, before being fed into a 2D CNN model for gesture classification. Next, the proposed 3D DenseNet model was used to extract spatiotemporal features directly from Red, Green, Blue (RGB) gesture videos. Finally, the prediction results of the proposed 2D and 3D deep models were blended together to boost recognition performance. The experimental results on two public datasets demonstrate the effectiveness of our proposed method.", "journal": "ELECTRONICS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Physics, Applied", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000521971700062", "keywords": "Remote sensing; Cross-modal retrieval; Deep learning; Panchromatic; Multispectral; Audio samples", "title": "CMIR-NET : A deep learning based model for cross-modal retrieval in remote sensing", "abstract": "We address the problem of cross-modal information retrieval in the domain of remote sensing. In particular, we are interested in two application scenarios: i) cross-modal retrieval between panchromatic (PAN) and multispectral imagery, and ii) multi-label image retrieval between very high resolution (VHR) images and speech-based label annotations. These multi-modal retrieval scenarios are more challenging than the traditional uni-modal retrieval approaches given the inherent differences in distributions between the modalities. However, with the increasing availability of multi-source remote sensing data and the scarcity of enough semantic annotations, the task of multi-modal retrieval has recently become extremely important. In this regard, we propose a novel deep neural network-based architecture that is considered to learn a discriminative shared feature space for all the input modalities, suitable for semantically coherent information retrieval. Extensive experiments are carried out on the benchmark large-scale PAN - multispectral DSRSID dataset and the multi-label UC-Merced dataset. Together with the Merced dataset, we generate a corpus of speech signals corresponding to the labels. Superior performance with respect to the current state-of-the-art is observed in all the cases. (C) 2020 Elsevier B.V. All rights reserved.", "journal": "PATTERN RECOGNITION LETTERS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000378756500043", "keywords": "tool condition monitoring (TCM); remaining useful life (RUL); wireless sensor; wavelet analysis; wavelet packet transform (WPT); neuro-fuzzy network (NFN)", "title": "Tool Condition Monitoring and Remaining Useful Life Prognostic Based on a Wireless Sensor in Dry Milling Operations", "abstract": "Tool breakage causes losses of surface polishing and dimensional accuracy for machined part, or possible damage to a workpiece or machine. Tool Condition Monitoring (TCM) is considerably vital in the manufacturing industry. In this paper, an indirect TCM approach is introduced with a wireless triaxial accelerometer. The vibrations in the three vertical directions (x, y and z) are acquired during milling operations, and the raw signals are de-noised by wavelet analysis. These features of de-noised signals are extracted in the time, frequency and time-frequency domains. The key features are selected based on Pearson's Correlation Coefficient (PCC). The Neuro-Fuzzy Network (NFN) is adopted to predict the tool wear and Remaining Useful Life (RUL). In comparison with Back Propagation Neural Network (BPNN) and Radial Basis Function Network (RBFN), the results show that the NFN has the best performance in the prediction of tool wear and RUL.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000708365800002", "keywords": "LSTM; dynamic systems; time-variant reliability; deep learning; Gaussian Process", "title": "LSTM-augmented deep networks for time-variant reliability assessment of dynamic systems", "abstract": "This paper presents a long short-term memory (LSTM)-augmented deep learning framework for time-dependent reliability analysis of dynamic systems. To capture the behavior of dynamic systems under time-dependent uncertainties, multiple LSTMs are trained to generate local surrogate models of dynamic systems in the time-independent system input space. With these local surrogate models, the time-dependent responses of dynamic systems at specific input configurations can be predicted as an augmented dataset accordingly. Then feedforward neural networks (FNN) can be trained as global surrogate models of dynamic systems based on the augmented data. To further enhance the performance of the global surrogate models, the Gaussian process regression technique is utilized to optimize the architecture of the FNNs by minimizing a validation loss. With the global surrogates, the time-dependent system reliability can be directly approximated by the Monte Carlo simulation (MCS). Three case studies are used to demonstrate the effectiveness of the proposed approach.", "journal": "RELIABILITY ENGINEERING & SYSTEM SAFETY", "category": "Engineering, Industrial; Operations Research & Management Science", "annotated_keywords": ["neural net", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000460696500102", "keywords": "space-time characteristics; feature fusion; 3D skeleton", "title": "3D Behavior Recognition Based on Multi-Modal Deep Space-Time Learning", "abstract": "This paper proposes a dual-stream 3D space-time convolutional neural network action recognition framework. The original depth map sequence data is set as the input in order to study the global space-time characteristics of each action category. The high correlation within the human action itself is considered in the time domain, and then the deep motion map sequence is introduced as the input to another stream of the 3D space-time convolutional network. Furthermore, the corresponding 3D skeleton sequence data is set as the third input of the whole recognition framework. Although the skeleton sequence data has the advantage of including 3D information, it is also confronted with the problems of the existence of rate change, temporal mismatch and noise. Thus, specially designed space-time features are applied to cope with these problems. The proposed methods allow the whole recognition system to fully exploit and utilize the discriminatory space-time features from different perspectives, and ultimately improve the classification accuracy of the system. Experimental results on different public 3D data sets illustrate the effectiveness of the proposed method.", "journal": "APPLIED SCIENCES-BASEL", "category": "Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials Science, Multidisciplinary; Physics, Applied", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000663417700010", "keywords": "Person re-identification; Higher-order pooling; Attention mechanism; Video re-identification", "title": "Video person re-identification with global statistic pooling and self-attention distillation", "abstract": "Most existing methods for video person re-identification apply spatial-temporal global average or attention pooling to aggregate frame-level feature and get video-level feature. The obtained video-level feature models only the first-order statistics of the appearance feature from holistic video, resulting in limited representation capability of the feature network. In this paper, we propose a novel Global Statistic Pooling network (GSPnet) which takes full advantage of the second-order information for enhancing modeling capability. Firstly, a novel global statistic pooling module is proposed to summarize both the first-and second-order statistics across frame-level feature, and then transfer them into a compact and robust video-level feature embedding. Secondly, a statistic-based attention block is incorporated into multiple stages of convolutional networks to fully explore the second-order representations from low-to high-level features. To enhance the representation learning ability and further boost re-identification (re ID) performance, we also propose a multi-level self-attention distillation training scheme, which squeezes the knowledge learned in the deeper portion of the networks into the shallow ones. Extensive experimental results have demonstrated the effectiveness and superiority of our approach on four popular video person re-ID datasets. (c) 2020 Elsevier B.V. All rights reserved.", "journal": "NEUROCOMPUTING", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000611057000010", "keywords": "Low-dose CT; Image enhancement; Anatomical prior information; Attribute augmentation; Weight prediction", "title": "Considering anatomical prior information for low-dose CT image enhancement using attribute-augmented Wasserstein generative adversarial networks", "abstract": "Currently, many deep learning (DL)-based low-dose CT image postprocessing technologies fail to consider the anatomical differences in training data among different human body sites, such as the cranium, lung and pelvis. In addition, we can observe evident anatomical similarities at the same site among individuals. However, these anatomical differences and similarities are ignored in the current DL-based methods during the network training process. In this paper, we propose a deep network trained by introducing anatomical site labels, termed attributes for training data. Then, the network can adaptively learn to obtain the optimal weight for each anatomical site. By doing so, the proposed network can take full advantage of anatomical prior information to estimate high-resolution CT images. Furthermore, we employ a Wasserstein generative adversarial network (WGAN) augmented with attributes to preserve more structural details. Compared with the traditional networks that do not consider the anatomical prior and whose weights are consequently the same for each anatomical site, the proposed network achieves better performance by adaptively adjusting to the anatomical prior information. (c) 2020 Elsevier B.V. All rights reserved.", "journal": "NEUROCOMPUTING", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000392682400036", "keywords": "Semantic video segmentation; Deconvolutional neural network; Coarse-to-fine training; Spatio-temporal consistence", "title": "Hierarchically Supervised Deconvolutional Network for Semantic Video Segmentation", "abstract": "Semantic video segmentation is a challenging task of fine-grained semantic understanding of video data. In this paper, we present a jointly trained deep learning framework to make the best use of spatial and temporal information for semantic video segmentation. Along the spatial dimension, a hierarchically supervised deconvolutional neural network (HDCNN) is proposed to conduct pixel-wise semantic interpretation for single video frames. HDCNN is constructed with convolutional layers in VGG-net and their mirrored deconvolutional structure, where all fully connected layers are removed. And hierarchical classification layers are added to multi scale deconvolutional features to introduce more contextual information for pixel-wise semantic interpretation. Besides, a coarse-to-fine training strategy is adopted to enhance the performance of foreground object segmentation in videos. Along the temporal dimension, we introduce Transition Layers upon the structure of HDCNN to make the pixel-wise label prediction consist with adjacent, pixels across space and time domains. The learning process of the Transition Layers can be implemented as a set of extra convolutional calculations connected with HDCNN. These two parts are jointly trained as a unified deep network in our approach. Thorough evaluations are performed on two challenging video datasets, i.e., CamVid and GATECH. Our approach achieves state-of-the-art performance on both of the two datasets.", "journal": "PATTERN RECOGNITION", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": ["neural net", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000725271300005", "keywords": "retinal vessel segmentation; generative; adversarial networks (GANs); generative adversarial networks (GANs); attention; attention module; module", "title": "Deep Att-ResGAN: A Retinal Vessel Segmentation Network for Color Fundus Images", "abstract": "Retinal vessel segmentation plays a significant role in the diagnosis and treatment of ophthalmological diseases. Recent studies have proved that deep learning can effectively segment the retinal vessel structure. However, the existing methods have difficulty in segmenting thin vessels, especially when the original image contains lesions. Based on generative adversarial network (GAN), this paper proposes a deep network with residual module and attention module (Deep Att-ResGAN). The network consists of four identical subnetworks. The output of each subnetwork is imported to the next subnetwork as contextual features that guide the segmentation. Firstly, the problems of the original image, namely, low contrast, uneven illumination, and data insufficiency, were solved through image enhancement and preprocessing. Next, an improved U-Net was adopted to serve as the generator, which stacks the residual and attention modules. These modules optimize the weight of the generator, and enhance the generalizability of the network. Further, the segmentation was refined iteratively by the discriminator, which contributes to the performance of vessel segmentation. Finally, comparative experiments were carried out on two public datasets: Digital Retinal Images for Vessel Extraction (DRIVE) and Structured Analysis of the Retina (STARE). The experimental results show that Deep Att-ResGAN outperformed the equivalent models like U-Net and GAN in most metrics. Our network achieved accuracy of 0.9565 and F1 of 0.829 on DRIVE, and accuracy of 0.9690 and F1 of 0.841 on STARE.", "journal": "TRAITEMENT DU SIGNAL", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000435217500018", "keywords": "Gas condensate; CVD test; Model; Neural network; LSSVM; ANFIS", "title": "Reliable modeling of constant volume depletion (CVD) behaviors in gas condensate reservoirs", "abstract": "It is important to access a clear understanding about the phase behavior of gas condensate reservoirs in order to forecast the future performance of such reservoirs. In this communication, different models based on multilayer perceptron network (MLP NN), least square support vector machine (LSSVM), adaptive neuro inference system (ANFIS) and radial basis function networks optimized by genetic algorithm (GA-RBF NN) were developed for estimation of amount of produced gas using constant volume depletion (CVD) tests of retrograde gas condensate reservoirs. Results show that the developed models are capable of accurately estimating the cumulative produced gas (G(p)) as an output parameter by utilizing various input parameters including temperature, pressure, composition of gas, and properties of plus fraction. The analysis of results reveals that the GA-RBF NN presents more accurate results in comparison with MLP NN, ANFIS and LSSVM models. Moreover, comparison between GA-RBF NN model as the most accurate model developed in the present work and two literature models shows the superiority of GA-RBF NN. Results of this study can be used in PVT softwares to enhance the accuracy and precision of CVD modeling of gas condensate reservoirs.", "journal": "FUEL", "category": "Energy & Fuels; Engineering, Chemical", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000615045000047", "keywords": "Spatiotemporal phenomena; Visualization; Cameras; Feature extraction; Interference; Three-dimensional displays; Tensors; Distance-based global and partial multi-regional network; global similarity; distance-based classification; vehicle re-identification", "title": "Vehicle Re-Identification Using Distance-Based Global and Partial Multi-Regional Feature Learning", "abstract": "Vehicle re-identification supports cross-camera tracking and the location of specific vehicles in a smart city. The gallery images of vehicles are ranked based on the similarities in the appearance of objects to a vehicle query image. Previous work on vehicle re-identification has mainly focused on global or local analyses of predefined regions of vehicles to classify the vehicle images with a softmax loss function. On the one hand, separate global or predefined local regions of vehicles are often sensitive to perspective and occlusions. On the other hand, the embedding space supervised by the softmax loss function is not sufficiently compact for the object class. To solve these problems, we propose an end-to-end distance-based global and partial multi-regional deep network (DGPM) that combines multi-regional features to identify global and local differences. We exploit a three-branch architecture to learn the global and partial features from coarsely partitioned regions. A global similarity module is introduced to reduce the background information interference in the local branches. Unlike general classification, we design a distance-based classification layer that maintains consistency among criteria for similarity evaluation. Furthermore, we use spatiotemporal vehicle information to improve the vehicle re-identification results when the camera and shooting time are available. Systematic comparative evaluations performed on the large-scale VeRi and VehicleID datasets showed that our approach robustly achieved state-of-the-art performance. For instance, for the VeRi dataset, we achieve (79.39 + 2.78)% mAP and (96.19 + 2.26)% Rank-1 accuracy.", "journal": "IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS", "category": "Engineering, Civil; Engineering, Electrical & Electronic; Transportation Science & Technology", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000849646700002", "keywords": "smoke detection; convolutional neural network; two-stream; spatio-temporal attention", "title": "Deep Hybrid Convolutional Neural Network for Segmentation of Melanoma Skin Lesion", "abstract": "Melanoma is a type of skin cancer that often leads to poor prognostic responses and survival rates. Melanoma usually develops in the limbs, including in fingers, palms, and the margins of the nails. When melanoma is detected early, surgical treatment may achieve a higher cure rate. The early diagnosis of melanoma depends on the manual segmentation of suspected lesions. However, manual segmentation can lead to problems, including misclassification and low efficiency. Therefore, it is essential to devise a method for automatic image segmentation that overcomes the aforementioned issues. In this study, an improved algorithm is proposed, termed EfficientUNet++, which is developed from the U-Net model. In EfficientUNet++, the pretrained EfficientNet model is added to the UNet++ model to accelerate segmentation process, leading to more reliable and precise results in skin cancer image segmentation. Two skin lesion datasets were used to compare the performance of the proposed EfficientUNet++ algorithm with other common models. In the PH2 dataset, EfficientUNet++ achieved a better Dice coefficient (93% vs. 76%-91%), Intersection over Union (IoU, 96% vs. 74%-95%), and loss value (30% vs. 44%-32%) compared with other models. In the International Skin Imaging Collaboration dataset, EfficientUNet++ obtained a similar Dice coefficient (96% vs. 94%-96%) but a better IoU (94% vs. 89%-93%) and loss value (11% vs. 13%-11%) than other models. In conclusion, the EfficientUNet++ model efficiently detects skin lesions by improving composite coefficients and structurally expanding the size of the convolution network. Moreover, the use of residual units deepens the network to further improve performance.", "journal": "COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE", "category": "Mathematical & Computational Biology; Neurosciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000325590200013", "keywords": "Indoor positioning; Smart building; RFID; Artificial neural networks; Particle filter", "title": "An indoor localization system based on artificial neural networks and particle filters applied to intelligent buildings", "abstract": "Smart Buildings aim to provide users with seamless, invisible and proactive services adapted to their preferences and needs. These services can be offered intelligently by means of considering the static and dynamical status of the building and the location of its occupants. Furthermore, gathering data about the identity and location of users enables to provide more personalized services, while wasted energy in overuse is reduced. But to cope with these objectives, it is necessary to acquire contextual information, both from users and the environment, using nonintrusive, ubiquitous and cheap technologies. In this work, we propose a low-cost and nonintrusive solution to solve the indoor localization problem focused on satisfying the requirements, in terms of accuracy in localization data, to provide customized comfort services in buildings, such as climate and lighting control, or security, with the goal of ensuring users comfort while saving energy. The proposed localization system is based on RFID (Radio-Frequency Identification) and IR (Infra-Red) data. The solution implements a Radial Basis Function Network to estimate the location of occupants, and a Particle Filter to track their next positions. This mechanism has been tested in a reference building where an automation system for collecting data and controlling devices has been setup. Results obtained from experimental assessments reveal that, despite our localization system uses a relative low number of sensors, estimated positions are really accurate considering the requirements of precision to provide user-oriented pervasive services in buildings. (C) 2013 Elsevier B.V. All rights reserved.", "journal": "NEUROCOMPUTING", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000687404200003", "keywords": "recurrent networks; neuronal dynamics; predictive coding; rate codes; temporal codes", "title": "Recurrent dynamics in the cerebral cortex: Integration of sensory evidence with stored knowledge", "abstract": "Current concepts of sensory processing in the cerebral cortex emphasize serial extraction and recombination of features in hierarchically structured feed-forward networks in order to capture the relations among the components of perceptual objects. These concepts are implemented in convolutional deep learning networks and have been validated by the astounding similarities between the functional properties of artificial systems and their natural counterparts. However, cortical architectures also display an abundance of recurrent coupling within and between the layers of the processing hierarchy. This massive recurrence gives rise to highly complex dynamics whose putative function is poorly understood. Here a concept is proposed that assigns specific functions to the dynamics of cortical networks and combines, in a unifying approach, the respective advantages of recurrent and feed-forward processing. It is proposed that the priors about regularities of the world are stored in the weight distributions of feed-forward and recurrent connections and that the high-dimensional, dynamic space provided by recurrent interactions is exploited for computations. These comprise the ultrafast matching of sensory evidence with the priors covertly represented in the correlation structure of spontaneous activity and the context-dependent grouping of feature constellations characterizing natural objects. The concept posits that information is encoded not only in the discharge frequency of neurons but also in the precise timing relations among the discharges. Results of experiments designed to test the predictions derived from this concept support the hypothesis that cerebral cortex exploits the high-dimensional recurrent dynamics for computations serving predictive coding.", "journal": "PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA", "category": "Multidisciplinary Sciences", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000554495704073", "keywords": "NOx; SCR; V2O5/TiO2; Response surface methodology; Artificial neural network", "title": "Code-free Mobile Automated Deep Learning Model for Ophthalmic Image Classification and Deployment as an App for Smartphones", "abstract": "Machine learning is transforming many industries through self-improving models that are fueled by big data and high computing power. The field of metabolic engineering, which uses cellular biochemical network to manufacture useful small molecules, has also witnessed the first wave of machine learning applications in the past five years, covering reaction route design, enzyme selection, pathway engineering and process optimization. This review focuses on pathway engineering, and uses a few recent studies to illustrate (1) how machine learning models can be useful in overcoming an evident rate-limiting step, and (2) how the models may be used to exhaustively search - or guide optimization algorithms to search -a large design space when the cellular regulation of the reaction network is more convoluted.", "journal": "INVESTIGATIVE OPHTHALMOLOGY & VISUAL SCIENCE", "category": "Ophthalmology", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000424738000006", "keywords": "Deep learning; convolutional neural network; software-generated composite sketches; face photos; morphological model; augmentation; database", "title": "Matching Software-Generated Sketches to Face Photographs With a Very Deep CNN, Morphed Faces, and Transfer Learning", "abstract": "Sketches obtained from eyewitness descriptions of criminals have proven to be useful in apprehending criminals, particularly when there is a lack of evidence. Automated methods to identify subjects depicted in sketches have been proposed in the literature, but their performance is still unsatisfactory when using software-generated sketches and when tested using extensive galleries with a large amount of subjects. Despite the success of deep learning in several applications including face recognition, little work has been done in applying it for face photograph-sketch recognition. This is mainly a consequence of the need to ensure robust training of deep networks by using a large number of images, yet limited quantities are publicly available. Moreover, most algorithms have not been designed to operate on software-generated face composite sketches which are used by numerous law enforcement agencies worldwide. This paper aims to tackle these issues with the following contributions: 1) a very deep convolutional neural network is utilised to determine the identity of a subject in a composite sketch by comparing it to face photographs and is trained by applying transfer learning to a state-of-the-art model pretrained for face photograph recognition; 2) a 3-D morphable model is used to synthesise both photographs and sketches to augment the available training data, an approach that is shown to significantly aid performance; and 3) the UoM-SGFS database is extended to contain twice the number of subjects, now having 1200 sketches of 600 subjects. An extensive evaluation of popular and state-of-the-art algorithms is also performed due to the lack of such information in the literature, where it is demonstrated that the proposed approach comprehensively outperforms state-of-the-art methods on all publicly available composite sketch datasets.", "journal": "IEEE TRANSACTIONS ON INFORMATION FORENSICS AND SECURITY", "category": "Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": ["neural net", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000460468800011", "keywords": "Age-related macular degeneration; Multimodal deep learning; OCT; Fundus photograph", "title": "The possibility of the combination of OCT and fundus images for improving the diagnostic accuracy of deep learning for age-related macular degeneration: a preliminary experiment", "abstract": "Recently, researchers have built new deep learning (DL) models using a single image modality to diagnose age-related macular degeneration (AMD). Retinal fundus and optical coherence tomography (OCT) images in clinical settings are the most important modalities investigating AMD. Whether concomitant use of fundus and OCT data in DL technique is beneficial has not been so clearly identified. This experimental analysis used OCT and fundus image data of postmortems from the Project Macula. The DL based on OCT, fundus, and combination of OCT and fundus were invented to diagnose AMD. These models consisted of pre-trained VGG-19 and transfer learning using random forest. Following the data augmentation and training process, the DL using OCT alone showed diagnostic efficiency with area under the curve (AUC) of 0.906 (95% confidence interval, 0.891-0.921) and 82.6% (81.0-84.3%) accuracy rate. The DL using fundus alone exhibited AUC of 0.914 (0.900-0.928) and 83.5% (81.8-85.0%) accuracy rate. Combined usage of the fundus with OCT increased the diagnostic power with AUC of 0.969 (0.956-0.979) and 90.5% (89.2-91.8%) accuracy rate. The Delong test showed that the DL using both OCT and fundus data outperformed the DL using OCT alone (P value <0.001) and fundus image alone (P value <0.001). This multimodal random forest model showed even better performance than a restricted Boltzmann machine (P value=0.002) and deep belief network algorithms (P value=0.042). According to Duncan's multiple range test, the multimodal methods significantly improved the performance obtained by the single-modal methods. In this preliminary study, a multimodal DL algorithm based on the combination of OCT and fundus image raised the diagnostic accuracy compared to this data alone. Future diagnostic DL needs to adopt the multimodal process to combine various types of imaging for a more precise AMD diagnosis.", "journal": "MEDICAL & BIOLOGICAL ENGINEERING & COMPUTING", "category": "Computer Science, Interdisciplinary Applications; Engineering, Biomedical; Mathematical & Computational Biology; Medical Informatics", "annotated_keywords": ["deep learning", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000600277000004", "keywords": "Bridge pier; prediction models; local scour; machine learning", "title": "Artificial Intelligence for Predicting Local Scour Depth around Piers Based on Dimensional Analysis", "abstract": "Accurate and reliable prediction of scour depth around bridge piers is essential for bridge engineering. The nondimensional parameters and artificial intelligence algorithms are combined to predict local scour depth. Based on the results of field observation and laboratory tests, five machine-learning models are applied and compared with the Hydraulic Engineering Circular No. 18 (HEC-18) formula, which is widely used in the United States. The results show that the machine-learning models are more accurate than the traditional HEC-18 formula and that the neural network models are more suitable for the prediction of bridge pier erosion than the linear regression model.", "journal": "JOURNAL OF COASTAL RESEARCH", "category": "Environmental Sciences; Geography, Physical; Geosciences, Multidisciplinary", "annotated_keywords": ["artificial intelligen", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000611969200034", "keywords": "m-learning; Immersive virtual reality; Immersive learning; Education; Personalized learning; Evolutionary learning; Reinforcement learning", "title": "A comparison of machine learning models versus clinical evaluation for mortality prediction in patients with sepsis", "abstract": "Introduction Patients with sepsis who present to an emergency department (ED) have highly variable underlying disease severity, and can be categorized from low to high risk. Development of a risk stratification tool for these patients is important for appropriate triage and early treatment. The aim of this study was to develop machine learning models predicting 31-day mortality in patients presenting to the ED with sepsis and to compare these to internal medicine physicians and clinical risk scores. Methods A single-center, retrospective cohort study was conducted amongst 1,344 emergency department patients fulfilling sepsis criteria. Laboratory and clinical data that was available in the first two hours of presentation from these patients were randomly partitioned into a development (n = 1,244) and validation dataset (n = 100). Machine learning models were trained and evaluated on the development dataset and compared to internal medicine physicians and risk scores in the independent validation dataset. The primary outcome was 31-day mortality. Results A number of 1,344 patients were included of whom 174 (13.0%) died. Machine learning models trained with laboratory or a combination of laboratory + clinical data achieved an area-under-the ROC curve of 0.82 (95% CI: 0.80-0.84) and 0.84 (95% CI: 0.81-0.87) for predicting 31-day mortality, respectively. In the validation set, models outperformed internal medicine physicians and clinical risk scores in sensitivity (92% vs. 72% vs. 78%;p<0.001,all comparisons) while retaining comparable specificity (78% vs. 74% vs. 72%;p>0.02). The model had higher diagnostic accuracy with an area-under-the-ROC curve of 0.85 (95%CI: 0.78-0.92) compared to abbMEDS (0.63,0.54-0.73), mREMS (0.63,0.54-0.72) and internal medicine physicians (0.74,0.65-0.82). Conclusion Machine learning models outperformed internal medicine physicians and clinical risk scores in predicting 31-day mortality. These models are a promising tool to aid in risk stratification of patients presenting to the ED with sepsis.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": ["machine learning", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000424168100026", "keywords": "Pluvial flooding; Emergency response point; GIS; Maximum coverage location problem", "title": "Neural networks modeling of Aspergillus flavus growth in tomato paste containing microencapsulated olive leaf extract", "abstract": "Our aim was to incorporate olive leaf extract in both microencapsulated (ME) and nonencapsulated (NE) forms into tomato paste to get benefit from antimicrobial properties of the extract. Response variables were diametrical growth of Aspergillus flavus, total soluble solids (TSS), pH, and color indices of inoculated tomato paste. NE was more successful than ME to restrict growth of the fungus at both temperatures of 25 and 30 degrees C. Although the rate of TSS decreasing over the storage time was higher for ME samples than NE ones, but it remained constant for ME samples at the end of storage; TSS index of ME samples at 25 degrees C was kept unchanged for four final days of storage. Also, ME olive leaf extract could stabilize pH index equal to 4.75 (at 10th day of storage at 30 degrees C) for the rest of storage. ME samples enjoyed higher a* values than NE samples during storage at 30 degrees C. Considering fitting indices, Feed-Forward-Back-Propagation network, Levenberg-Marquardt training algorithm, hyperbolic tangent sigmoid transfer function with 3-5-6 and 3-6-6 topologies represented the best artificial neural network models to predict both physicochemical and microbial properties of ME and NE samples when inoculated by A.flavus, respectively. Practical applicationsAntimicrobial properties of herbs and spices have been recognized for several years. Natural herbs or spices tend to lose their efficiency and fail to release their beneficial effects evenly over a longtime period. Simultaneously, it has been proved that microencapsulation is able to build a barrier between components of particles and their environment. Thus, it might be applied for olive leaf extract to keep favorable properties of tomato paste. Our aim was to incorporate microencapsulated (ME) olive leaf extract into tomato paste to get benefit from antimicrobial properties of the extract (against Aspergillus flavus) over short and longtime storage. Also, our another objective was to model the physicochemical and fungal changes within tomato paste containing nonencapsulated and ME olive leaf extract for the first time.", "journal": "JOURNAL OF FOOD SAFETY", "category": "Biotechnology & Applied Microbiology; Food Science & Technology", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000438015400006", "keywords": "blind source separation; functional magnetic resonance imaging; restricted Boltzmann machine", "title": "Latent source mining in FMRI via restricted Boltzmann machine", "abstract": "Blind source separation (BSS) is commonly used in functional magnetic resonance imaging (fMRI) data analysis. Recently, BSS models based on restricted Boltzmann machine (RBM), one of the building blocks of deep learning models, have been shown to improve brain network identification compared to conventional single matrix factorization models such as independent component analysis (ICA). These models, however, trained RBM on fMRI volumes, and are hence challenged by model complexity and limited training set. In this article, we propose to apply RBM to fMRI time courses instead of volumes for BSS. The proposed method not only interprets fMRI time courses explicitly to take advantages of deep learning models in latent feature learning but also substantially reduces model complexity and increases the scale of training set to improve training efficiency. Our experimental results based on Human Connectome Project (HCP) datasets demonstrated the superiority of the proposed method over ICA and the one that applied RBM to fMRI volumes in identifying task-related components, resulted in more accurate and specific representations of task-related activations. Moreover, our method separated out components representing intermixed effects between task events, which could reflect inherent interactions among functionally connected brain regions. Our study demonstrates the value of RBM in mining complex structures embedded in large-scale fMRI data and its potential as a building block for deeper models in fMRI data analysis.", "journal": "HUMAN BRAIN MAPPING", "category": "Neurosciences; Neuroimaging; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000548455300033", "keywords": "Assay Central; bayesian; drug-induced liver injury; machine learning; MegaTox", "title": "Comparing Machine Learning Algorithms for Predicting Drug-Induced Liver Injury (DILI)", "abstract": "Drug-induced liver injury (DILI) is one the most unpredictable adverse reactions to xenobiotics in humans and the leading cause of postmarketing withdrawals of approved drugs. To date, these drugs have been collated by the FDA to form the DILIRank database, which classifies DILI severity and potential. These classifications have been used by various research groups in generating computational predictions for this type of liver injury. Recently, groups from Pfizer and AstraZeneca have collated DILI in vitro data and physicochemical properties for compounds that can be used along with data from the FDA to build machine learning models for DILI. In this study, we have used these data sets, as well as the Biopharmaceutics Drug Disposition Classification System data set, to generate Bayesian machine learning models with our inhouse software, Assay Central. The performance of all machine learning models was assessed through both the internal 5-fold cross-validation metrics and prediction accuracy of an external test set of compounds with known hepatotoxicity. The best-performing Bayesian model was based on the DILI-concern category from the DILIRank database with an ROC of 0.814, a sensitivity of 0.741, a specificity of 0.755, and an accuracy of 0.746. A comparison of alternative machine learning algorithms, such as k-nearest neighbors, support vector classification, AdaBoosted decision trees, and deep learning methods, produced similar statistics to those generated with the Bayesian algorithm in Assay Central. This study demonstrates machine learning models grouped in a tool called MegaTox that can be used to predict early-stage clinical compounds, as well as recent FDA-approved drugs, to identify potential DILI.", "journal": "MOLECULAR PHARMACEUTICS", "category": "Medicine, Research & Experimental; Pharmacology & Pharmacy", "annotated_keywords": ["machine learning", "deep learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000645567000030", "keywords": "cancer incidence rates; machine learning; Europe; linear regression; support vector regression", "title": "Prediction of cancer incidence rates for the European continent using machine learning models", "abstract": "Cancer is one of the most important and common public health problems on Earth that can occur in many different types. Treatments and precautions are aimed at minimizing the deaths caused by cancer; however, incidence rates continue to rise. Thus, it is important to analyze and estimate incidence rates to support the determination of more effective precautions. In this research, 2018 Cancer Datasheet of World Health Organization (WHO), is used and all countries on the European Continent are considered to analyze and predict the incidence rates until 2020, for Lung cancer, Breast cancer, Colorectal cancer, Prostate cancer and All types of cancer, which have highest incidence and mortality rates. Each cancer type is trained by six machine learning models namely, Linear Regression, Support Vector Regression, Decision Tree, Long-Short Term Memory neural network, Backpropagation neural network, and Radial Basis Function neural network according to gender types separately. Linear regression and support vector regression outperformed the other models with the R 2 scores 0.99 and 0.98, respectively, in initial experiments, and then used for prediction of incidence rates of the considered cancer types. The ML models estimated that the maximum rise of incidence rates would be in colorectal cancer for females by 6%.", "journal": "HEALTH INFORMATICS JOURNAL", "category": "Health Care Sciences & Services; Medical Informatics", "annotated_keywords": ["neural net", "machine learning", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000583983300001", "keywords": "autism; diagnosis; autoencoder; convolution neural network; machine learning", "title": "An Autoencoder-Based Deep Learning Classifier for Efficient Diagnosis of Autism", "abstract": "Autism spectrum disorder (ASD) is a neurodevelopmental disorder characterized by a lack of social communication and social interaction. Autism is a mental disorder investigated by social and computational intelligence scientists utilizing advanced technologies such as machine learning models to enhance clinicians' ability to provide robust diagnosis and prognosis of autism. However, with dynamic changes in autism behaviour patterns, these models' quality and accuracy have become a great challenge for clinical practitioners. We applied a deep neural network learning on a large brain image dataset obtained from ABIDE (autism brain imaging data exchange) to provide an efficient diagnosis of ASD, especially for children. Our deep learning model combines unsupervised neural network learning, an autoencoder, and supervised deep learning using convolutional neural networks. Our proposed algorithm outperforms individual-based classifiers measured by various validations and assessment measures. Experimental results indicate that the autoencoder combined with the convolution neural networks provides the best performance by achieving 84.05% accuracy and Area under the Curve (AUC) value of 0.78.", "journal": "CHILDREN-BASEL", "category": "Pediatrics", "annotated_keywords": ["neural net", "machine learning", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000534811900005", "keywords": "Human activity recognition; RGB-D; CNN; VGG; Multi-stream CNN models; Transfer learning", "title": "Combining CNN streams of dynamic image and depth data for action recognition", "abstract": "RGB-D sensors have been in great demand due to its capability of producing large amount of multimodal data like RGB images and depth maps, useful for better training of deep learning models. In this paper, a deep learning model for recognizing human activities in a video sequence by combining multiple CNN streams has been proposed. The proposed work comprises the use of dynamic images generated from RGB images and depth map for three different dimensions. The proposed model is trained using these four streams on VGG Net for action recognition purpose. Further, it is evaluated and compared with the other state-of-the-art methods available in literature, on three challenging datasets, namely MSR daily Activity, UTD MHAD and CAD 60, in terms of accuracy, error, recall, specificity, precision and f-score. From obtained results, it has been observed that the proposed method outperforms other methods.", "journal": "MULTIMEDIA SYSTEMS", "category": "Computer Science, Information Systems; Computer Science, Theory & Methods", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000458875200001", "keywords": "health risk appraisal; risk; hypertension; chronic disease; clustering and classification; decision support systems", "title": "Patient-Level Prediction of Cardio-Cerebrovascular Events in Hypertension Using Nationwide Claims Data", "abstract": "Background: Prevention and management of chronic diseases are the main goals of national health maintenance programs. Previously widely used screening tools, such as Health Risk Appraisal, are restricted in their achievement this goal due to their limitations, such as static characteristics, accessibility, and generalizability. Hypertension is one of the most important chronic diseases requiring management via the nationwide health maintenance program, and health care providers should inform patients about their risks of a complication caused by hypertension. Objective: Our goal was to develop and compare machine learning models predicting high-risk vascular diseases for hypertensive patients so that they can manage their blood pressure based on their risk level. Methods: We used a 12-year longitudinal dataset of the nationwide sample cohort, which contains the data of 514,866 patients and allows tracking of patients' medical history across all health care providers in Korea (N= 51,920). To ensure the generalizability of our models, we conducted an external validation using another national sample cohort dataset, comprising one million different patients, published by the National Health Insurance Service. From each dataset, we obtained the data of 74,535 and 59,738 patients with essential hypertension and developed machine learning models for predicting cardiovascular and cerebrovascular events. Six machine learning models were developed and compared for evaluating performances based on validation metrics. Results: Machine learning algorithms enabled us to detect high-risk patients based on their medical history. The long short-term memory-based algorithm outperformed in the within test (F1-score=. 772, external test F1-score=. 613), and the random forest-based algorithm of risk prediction showed better performance over other machine learning algorithms concerning generalization (within test F1-score=.757, external test F1-score=.705). Concerning the number of features, in the within test, the long short-term memory-based algorithms outperformed regardless of the number of features. However, in the external test, the random forest-based algorithm was the best, irrespective of the number of features it encountered. Conclusions: We developed and compared machine learning models predicting high-risk vascular diseases in hypertensive patients so that they may manage their blood pressure based on their risk level. By relying on the prediction model, a government can predict high-risk patients at the nationwide level and establish health care policies in advance.", "journal": "JOURNAL OF MEDICAL INTERNET RESEARCH", "category": "Health Care Sciences & Services; Medical Informatics", "annotated_keywords": ["machine learning", "learning algorithm"], "label": "1", "title_label": "0"}
{"id": "WOS:000681768900001", "keywords": "deep learning; multiple pathological types; multi-resolution method; pulmonary nodules", "title": "3D multi-resolution deep learning model for diagnosis of multiple pathological types on pulmonary nodules", "abstract": "To accurately diagnose multiple pathological types of pulmonary nodules based on lung computed tomography (CT) images, a multi-resolution three-dimensional (3D) multi-classification deep learning model (Mr-Mc) was proposed. The Mr-Mc model was constructed by using our own constructed lung CT image dataset of pulmonary nodules with clinical pathological information (LCID-CPI), which can accurately diagnose inflammation, squamous cell carcinoma, adenocarcinoma, and other benign diseases. In order to process nodules with different sizes, a multi-resolution extraction method was proposed to extract 3D volume data with different resolutions from lung CT images. The Mr-Mc was composed of three different resolution networks, each of which has input volume data of a specific resolution. Experiments showed that the constructed Mr-Mc model can achieve an average accuracy of 0.81 on LCID-CPI. Besides, the Mr-Mc model can also achieve a high accuracy of 0.87 on the Lung Image Database Consortium and Image Database Resource Initiative dataset.", "journal": "INTERNATIONAL JOURNAL OF IMAGING SYSTEMS AND TECHNOLOGY", "category": "Engineering, Electrical & Electronic; Optics; Imaging Science & Photographic Technology", "annotated_keywords": ["deep learning", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000612192600001", "keywords": "IMRT QA; machine learning; quality assurance; radiomics", "title": "Detecting MLC modeling errors using radiomics-based machine learning in patient-specific QA with an EPID for intensity-modulated radiation therapy", "abstract": "Purpose We sought to develop machine learning models to detect multileaf collimator (MLC) modeling errors with the use of radiomic features of fluence maps measured in patient-specific quality assurance (QA) for intensity-modulated radiation therapy (IMRT) with an electric portal imaging device (EPID). Methods Fluence maps measured with EPID for 38 beams from 19 clinical IMRT plans were assessed. Plans with various degrees of error in MLC modeling parameters [i.e., MLC transmission factor (TF) and dosimetric leaf gap (DLG)] and plans with an MLC positional error for comparison were created. For a total of 152 error plans for each type of error, we calculated fluence difference maps for each beam by subtracting the calculated maps from the measured maps. A total of 837 radiomic features were extracted from each fluence difference map, and we determined the number of features used for the training dataset in the machine learning models by using random forest regression. Machine learning models using the five typical algorithms [decision tree, k-nearest neighbor (kNN), support vector machine (SVM), logistic regression, and random forest] for binary classification between the error-free plan and the plan with the corresponding error for each type of error were developed. We used part of the total dataset to perform fourfold cross-validation to tune the models, and we used the remaining test dataset to evaluate the performance of the developed models. A gamma analysis was also performed between the measured and calculated fluence maps with the criteria of 3%/2 and 2%/2 mm for all of the types of error. Results The radiomic features and its optimal number were similar for the models for the TF and the DLG error detection, which was different from the MLC positional error. The highest sensitivity was obtained as 0.913 for the TF error with SVM and logistic regression, 0.978 for the DLG error with kNN and SVM, and 1.000 for the MLC positional error with kNN, SVM, and random forest. The highest specificity was obtained as 1.000 for the TF error with a decision tree, SVM, and logistic regression, 1.000 for the DLG error with a decision tree, logistic regression, and random forest, and 0.909 for the MLC positional error with a decision tree and logistic regression. The gamma analysis showed the poorest performance in which sensitivities were 0.737 for the TF error and the DLG error and 0.882 for the MLC positional error for 3%/2 mm. The addition of another type of error to fluence maps significantly reduced the sensitivity for the TF and the DLG error, whereas no effect was observed for the MLC positional error detection. Conclusions Compared to the conventional gamma analysis, the radiomics-based machine learning models showed higher sensitivity and specificity in detecting a single type of the MLC modeling error and the MLC positional error. Although the developed models need further improvement for detecting multiple types of error, radiomics-based IMRT QA was shown to be a promising approach for detecting the MLC modeling error.", "journal": "MEDICAL PHYSICS", "category": "Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": ["machine learning", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000711734500001", "keywords": "artificial intelligence; deep learning; neural network; predictive analytics", "title": "Machine learning in the prediction of medical inpatient length of stay", "abstract": "Length of stay (LOS) estimates are important for patients, doctors and hospital administrators. However, making accurate estimates of LOS can be difficult for medical patients. This review was conducted with the aim of identifying and assessing previous studies on the application of machine learning to the prediction of total hospital inpatient LOS for medical patients. A review of machine learning in the prediction of total hospital LOS for medical inpatients was conducted using the databases PubMed, EMBASE and Web of Science. Of the 673 publications returned by the initial search, 21 articles met inclusion criteria. Of these articles the most commonly represented medical specialty was cardiology. Studies were also identified that had specifically evaluated machine learning LOS prediction in patients with diabetes and tuberculosis. The performance of the machine learning models in the identified studies varied significantly depending on factors including differing input datasets and different LOS thresholds and outcome metrics. Common methodological shortcomings included a lack of reporting of patient demographics and lack of reporting of clinical details of included patients. The variable performance reported by the studies identified in this review supports the need for further research of the utility of machine learning in the prediction of total inpatient LOS in medical patients. Future studies should follow and report a more standardised methodology to better assess performance and to allow replication and validation. In particular, prospective validation studies and studies assessing the clinical impact of such machine learning models would be beneficial.", "journal": "INTERNAL MEDICINE JOURNAL", "category": "Medicine, General & Internal", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000451355500008", "keywords": "Machine learning; Supervised models; Predictive analytics; Penalized regression; Taxi-out operations", "title": "Can machines learn how to forecast taxi-out time? A comparison of predictive models applied to the case of Seattle/Tacoma International Airport", "abstract": "This study compares the performance of ensemble machine learning, ordinary least-squared and penalized algorithms to predict taxi-out time at two different periods of NextGen capability implementation. In the pre-sample, ordinary least-squared and ridge models performed better than other ensemble learning models. However, the gradient boosting model provided the lowest root mean squared errors in the post-sample. No algorithm fits data better in all cases. This paper recommends selecting the model that provides the best balance between bias and variance.", "journal": "TRANSPORTATION RESEARCH PART E-LOGISTICS AND TRANSPORTATION REVIEW", "category": "Economics; Engineering, Civil; Operations Research & Management Science; Transportation; Transportation Science & Technology", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000685931200003", "keywords": "Hybrid framework; Parametric design; CFD (Computational Fluid Dynamics); simulation; Image processing; Machine learning model", "title": "Hybrid framework for rapid evaluation of wind environment around buildings through parametric design, CFD simulation, image processing and machine learning", "abstract": "High-efficient evaluations of building performance are often required for comparisons of different design alternatives in architectural sustainable design processes. General Computational Fluid Dynamics (CFD) simulations are usually complicated and time-consuming for wind environment investigation and evaluation. A hybrid framework for rapid evaluation of pedestrian-level wind environment will be proposed in the present work. This framework will then be formulated by integrating parametric design, CFD simulation, image processing, and machine learning, and it could immediately predict the Low-Velocity Areas (LVAs) around rectangular-form buildings. A large amount of data of 300 building cases generated by parametric design, CFD simulation, and image processing to train a Machine Learning Model (MLM) could be applied for the prediction of LVAs. In the case investigations, MLM was tested in the prediction of the other new 24 building cases with random geometric parameters. The comparison of MLM and CFD results showed that their solutions were close to each other. Efficiency and accuracy of the hybrid framework were further demonstrated through quantitative analysis of statistical discrepancies of MLM and CFD results. Hybrid framework was an original attempt to integrate multiple emerging computational tools, and it could provide high-efficient quantitative analysis of wind environment and give practical design optimization information in the early stage.", "journal": "SUSTAINABLE CITIES AND SOCIETY", "category": "Construction & Building Technology; Green & Sustainable Science & Technology; Energy & Fuels", "annotated_keywords": ["machine learning", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000743136000002", "keywords": "Hydrogen-enriched rotary engine; Machine learning models; Combustion phases prediction; Generalization ability", "title": "Comparison and implementation of machine learning models for predicting the combustion phases of hydrogen-enriched Wankel rotary engines", "abstract": "Combustion phases, such as the development period (CA0-10) and flame propagation period (CA10-90), are the critical parameters for hydrogen-enriched Wankel rotary engines. An accurate simulation model and a suitable engine management system are required to control combustion phases. In this paper, five machine learning (ML) models, including the linear regression (LR), regression tree (TR), ensembles of trees (EnTR), support vector machine (SVM), and Gaussian process regression (GPR), are initially applied to predict combustion phases. Experiments were performed with variations of the main fuel types (gasoline and n-butanol), loads (idle and part load), ignition timing, hydrogen volume fraction, and excess air ratio. The sample data were divided into training and testing data set, and the normalization method, 5-fold cross-validation, and Bayesian optimization algorithm were used for data processing and model optimization. Among five ML models, the training speed of the LR model was the fastest; the generalization ability of the TR model was the worst. The minimum leaf size of the TR model significantly influenced regression and generalization ability. On this basis, the EnTR model improved the regression ability, but required more training time. The GPR model showed the best generalization ability among the above model, while SVM performed well in a certain data set. For CA0-10, the coefficient of determination (R2) of the best LR, TR, EnTR, SVM and GPR models was 0.9910, 0.9912, 0.9985, 0.9984 and 0.9994, respectively; for CA10-90, the R2 was 0.9348, 0.8974, 0.9873, 0.9916 and 0.9975, respectively. It is highly recommended to apply the GPR model to the combustion phases prediction and control system modeling.", "journal": "FUEL", "category": "Energy & Fuels; Engineering, Chemical", "annotated_keywords": ["machine learning", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000365446600025", "keywords": "Image retargeting; Nonparametric semantic segmentation; Seam carving", "title": "Image retargeting using nonparametric semantic segmentation", "abstract": "In this paper, a new full-automatic approach to content aware image retargeting is proposed. Most image retargeting approaches does not incorporate content information and only use local appearance information. However, there are some approaches which use high level information such as saliency regions, objects mask and depth information. Such methods do not use semantic labelling for each object. In this paper, object masks as well as their semantic class labels are used to propose a new approach to image retargeting. To do so, semantic segmentation of image is provided. Hence, a nonparametric approach to semantic segmentation is employed which is fast with no need to any learning model. This makes it simple and applicable to any dataset. To evaluate the proposed approach, besides presenting visual examples, we performed a set of subjective evaluations too. The obtained results show that our method outperforms other retargeting approaches.", "journal": "MULTIMEDIA TOOLS AND APPLICATIONS", "category": "Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000536066300045", "keywords": "Cloud computing; load balancing; microservices", "title": "Adaptive Microservice Scaling for Elastic Applications", "abstract": "Today, Internet users expect Web applications to be fast, performant, and always available. With the emergence of Internet of Things (IoT), data collection and the analysis of streams have become more and more challenging. Behind the scenes, application owners and cloud service providers work to meet these expectations, yet, the problem of how to most effectively and efficiently auto-scale a Web application to optimize for performance while reducing costs and energy usage is still a challenge. In particular, this problem has new relevance due to the continued rise of IoT and microservice-based architectures. A key concern, that is often not addressed by current auto-scaling systems, is the decision on which microservice to scale in order to increase performance. Our aim is to design a prototype auto-scaling system for microservice-based Web applications that can learn from the past service experience. The contributions of the work can be divided into two parts: 1) developing a pipeline for microservice auto-scaling and 2) evaluating a hybrid sequence and supervised learning model for recommending scaling actions. The pipeline has proven to be an effective platform for exploring auto-scaling solutions, as we will demonstrate through the evaluation of our proposed hybrid model. The results of the hybrid model show the merit of using a supervised model to identify which microservices should be scaled up more.", "journal": "IEEE INTERNET OF THINGS JOURNAL", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": ["supervised learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000540408500004", "keywords": "Generative art; Depth generation model; Abstract painting CoCBs", "title": "CHANGE-seq reveals genetic and epigenetic effects on CRISPR-Cas9 genome-wide activity", "abstract": "Current methods can illuminate the genome-wide activity of CRISPR-Cas9 nucleases, but are not easily scalable to the throughput needed to fully understand the principles that govern Cas9 specificity. Here we describe 'circularization for high-throughput analysis of nuclease genome-wide effects by sequencing' (CHANGE-seq), a scalable, automatable tagmentation-based method for measuring the genome-wide activity of Cas9 in vitro. We applied CHANGE-seq to 110 single guide RNA targets across 13 therapeutically relevant loci in human primary T cells and identified 201,934 off-target sites, enabling the training of a machine learning model to predict off-target activity. Comparing matched genome-wide off-target, chromatin modification and accessibility, and transcriptional data, we found that cellular off-target activity was two to four times more likely to occur near active promoters, enhancers and transcribed regions. Finally, CHANGE-seq analysis of six targets across eight individual genomes revealed that human single-nucleotide variation had significant effects on activity at similar to 15.2% of off-target sites analyzed. CHANGE-seq is a simplified, sensitive and scalable approach to understanding the specificity of genome editors.", "journal": "NATURE BIOTECHNOLOGY", "category": "Biotechnology & Applied Microbiology", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000704824300032", "keywords": "Computational modeling; Modulation; Feature extraction; Training; Solid modeling; Neural networks; Data models; Automatic modulation recognition; deep learning; low-complexity; lightweight; network pruning", "title": "An Efficient Deep Learning Model for Automatic Modulation Recognition Based on Parameter Estimation and Transformation", "abstract": "Automatic modulation recognition (AMR) is a promising technology for intelligent communication receivers to detect signal modulation schemes. Recently, the emerging deep learning (DL) research has facilitated high-performance DL-AMR approaches. However, most DL-AMR models only focus on recognition accuracy, leading to huge model sizes and high computational complexity, while some lightweight and low-complexity models struggle to meet the accuracy requirements. This letter proposes an efficient DL-AMR model based on phase parameter estimation and transformation, with convolutional neural network (CNN) and gated recurrent unit (GRU) as the feature extraction layers, which can achieve high recognition accuracy equivalent to the existing state-of-the-art models but reduces more than a third of the volume of their parameters. Meanwhile, our model is more competitive in training time and test time than the benchmark models with similar recognition accuracy. Moreover, we further propose to compress our model by pruning, which maintains the recognition accuracy higher than 90% while has less than 1/8 of the number of parameters comparing with state-of-the-art models.", "journal": "IEEE COMMUNICATIONS LETTERS", "category": "Telecommunications", "annotated_keywords": ["neural net", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000395561200002", "keywords": "Bilingual information; code-switching; emotion analysis; factor graph model", "title": "Emotion Analysis in Code-Switching Text With Joint Factor Graph Model", "abstract": "Previous research on emotions analysis has placed much emphasis in monolingual instead of bilingual text. However, emotions on social media platforms are often found in bilingual or code-switching posts. Different from monolingual text, emotions in code-switching text can be expressed in both monolingual and bilingual forms. Moreover, more than one emotion can be expressed within a single post; yet they tend to be related in some ways which offers some implications. It is thus necessary to consider the correlation between different emotions. In this paper, a joint factor graph model is proposed to address this issue. In particular, attribute functions of the factor graph model are utilized to learn both monolingual and bilingual information from each post, factor functions are used to explore the relationship among different emotions, and a belief propagation algorithm is employed to learn and predict the model. Empirical studies demonstrate the importance of emotion analysis in code-switching text and the effectiveness of our proposed joint learning model.", "journal": "IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING", "category": "Acoustics; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000532870000001", "keywords": "lncRNA; Protein; Interaction; Deep learning; Prediction; Graph attention", "title": "A deep learning model for plant lncRNA-protein interaction prediction with graph attention", "abstract": "Long non-coding RNAs (lncRNAs) play a broad spectrum of distinctive regulatory roles through interactions with proteins. However, only a few plant lncRNAs have been experimentally characterized. We propose GPLPI, a graph representation learning method, to predict plant lncRNA-protein interaction (LPI) from sequence and structural information. GPLPI employs a generative model using long short-term memory (LSTM) with graph attention. Evolutionary features are extracted using frequency chaos game representation (FCGR). Manifold regularization and l(2)-norm are adopted to obtain discriminant feature representations and mitigate overfitting. The model captures locality preserving and reconstruction constraints that lead to better generalization ability. Finally, potential interactions between lncRNAs and proteins are predicted by integrating catboost and regularized Logistic regression based on L-BFGS optimization algorithm. The method is trained and tested on Arabidopsis thaliana and Zea mays datasets. GPLPI achieves accuracies of 85.76% and 91.97% respectively. The results show that our method consistently outperforms other state-of-the-art methods.", "journal": "MOLECULAR GENETICS AND GENOMICS", "category": "Biochemistry & Molecular Biology; Genetics & Heredity", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000735270000001", "keywords": "Differential privacy; Artificial intelligence; Edge computing; IoT streaming data; Feature extraction", "title": "PrivStream: A privacy-preserving inference framework on IoT streaming data at the edge", "abstract": "Edge computing combining with artificial intelligence (AI) has enabled the timely processing and analysis of streaming data produced by IoT intelligent applications. However, it causes privacy risk due to the data exchanges between local devices and untrusted edge servers. The powerful analytical capability of AI further exacerbates the risks because it can even infer private information from insensitive data. In this paper, we propose a privacy-preserving IoT streaming data analytical framework based on edge computing, called PrivStream, to prevent the untrusted edge server from making sensitive inferences from the IoT streaming data. It utilizes a well-designed deep learning model to filter the sensitive information and combines with differential privacy to protect against the untrusted edge server. The noise is also injected into the framework in the training phase to increase the robustness of PrivStream to differential privacy noise. Taking into account the dynamic and real-time characteristics of streaming data, we realize PrivStream with two types of models to process data segment with fixed length and variable length, respectively, and implement it on a distributed streaming platform to achieve real-time streaming data transmission. We theoretically prove that Privstream satisfies -differential privacy and experimentally demonstrate that PrivStream has better performance than the state-of-the-art and has acceptable computation and storage overheads.", "journal": "INFORMATION FUSION", "category": "Computer Science, Artificial Intelligence; Computer Science, Theory & Methods", "annotated_keywords": ["artificial intelligen", "deep learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000685993500006", "keywords": "Multi-view learning; Consensus and complementarity information; Asymmetric LINEX loss function; Support vector machine", "title": "Multi-view learning methods with the LINEX loss for pattern classification", "abstract": "Multi-view learning concentrates on leveraging the consensus and complementarity information among multiple distinct feature representations to improve the performance. Most multi-view learning models deal with two main issues. Firstly, how to fully exploit the view-agreement and view-discrepancy poses a major challenge. Secondly, how to design a general multi-view model is indispensable. By inheriting the asymmetric merit of LINEX loss, we propose a general multi-view LINEX SVM framework, which includes two models called MVLSVM-CO and MVLSVM-SIM. They can not only use LINEX loss function to flexibly distinguish the error-prone samples of both classes, but also take advantage of the consistency and the complementarity of distinct views in multi-view scenario. An iterative two-step strategy is adopted to solve the optimization problems efficiently. Furthermore, we theoretically analyze the view-consistency and generalization capability of the proposed models by using Rademacher complexity. The extensive experiments confirm the effectiveness of MVLSVM-CO and MVLSVM-SIM. (C) 2021 Elsevier B.V. All rights reserved.", "journal": "KNOWLEDGE-BASED SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000611222400013", "keywords": "Satellites; Training; Semantics; Image segmentation; Adaptation models; Remote sensing; Standardization; Convolutional neural networks (CNNs); dense labeling; domain adaptation; generative adversarial networks (GANs); life-long adaption; multisource adaption; multitarget adaption; semantic segmentation", "title": "DAugNet: Unsupervised, Multisource, Multitarget, and Life-Long Domain Adaptation for Semantic Segmentation of Satellite Images", "abstract": "The domain adaptation of satellite images has recently gained increasing attention to overcome the limited generalization abilities of machine learning models when segmenting large-scale satellite images. Most of the existing approaches seek for adapting the model from one domain to another. However, such single-source and single-target setting prevents the methods from being scalable solutions since, nowadays, multiple sources and target domains having different data distributions are usually available. Besides, the continuous proliferation of satellite images necessitates the classifiers to adapt to continuously increasing data. We propose a novel approach, coined DAugNet, for unsupervised, multisource, multitarget, and life-long domain adaptation of satellite images. It consists of a classifier and a data augmentor. The data augmentor, which is a shallow network, is able to perform style transfer between multiple satellite images in an unsupervised manner, even when new data are added over time. In each training iteration, it provides the classifier with diversified data, which makes the classifier robust to large data distribution difference between the domains. Our extensive experiments prove that DAugNet significantly better generalizes to new geographic locations than the existing approaches.", "journal": "IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING", "category": "Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000725585200001", "keywords": "plankton camera; deep learning; plankton classification; transfer learning; Greifensee; ensemble learning; fresh water; lake plankton images", "title": "Deep Learning Classification of Lake Zooplankton", "abstract": "Plankton are effective indicators of environmental change and ecosystem health in freshwater habitats, but collection of plankton data using manual microscopic methods is extremely labor-intensive and expensive. Automated plankton imaging offers a promising way forward to monitor plankton communities with high frequency and accuracy in real-time. Yet, manual annotation of millions of images proposes a serious challenge to taxonomists. Deep learning classifiers have been successfully applied in various fields and provided encouraging results when used to categorize marine plankton images. Here, we present a set of deep learning models developed for the identification of lake plankton, and study several strategies to obtain optimal performances, which lead to operational prescriptions for users. To this aim, we annotated into 35 classes over 17900 images of zooplankton and large phytoplankton colonies, detected in Lake Greifensee (Switzerland) with the Dual Scripps Plankton Camera. Our best models were based on transfer learning and ensembling, which classified plankton images with 98% accuracy and 93% F1 score. When tested on freely available plankton datasets produced by other automated imaging tools (ZooScan, Imaging FlowCytobot, and ISIIS), our models performed better than previously used models. Our annotated data, code and classification models are freely available online.", "journal": "FRONTIERS IN MICROBIOLOGY", "category": "Microbiology", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000321408200018", "keywords": "Self-organizing map; Spatial data mining; Spatial outlier detection; Iterative learning; Robust distance", "title": "Spatial outlier detection based on iterative self-organizing learning model", "abstract": "In this paper, we propose an iterative self-organizing map (SOM) approach with robust distance estimation (ISOMRD) for spatial outlier detection. Generally speaking, spatial outliers are irregular data instances which have significantly distinct non-spatial attribute values compared to their spatial neighbors. In our proposed approach, we adopt SOM to preserve the intrinsic topological and metric relationships of the data distribution to seek reasonable spatial clusters for outlier detection. The proposed iterative learning process with robust distance estimation can address the high dimensional problems of spatial attributes and accurately detect spatial outliers with irregular features. To verify the efficiency and robustness of our proposed algorithm, comparative study of ISOMRD and several existing approaches are presented in detail. Specifically, we test the performance of our method based on four real-world spatial datasets. Various simulation results demonstrate the effectiveness of the proposed approach. (C) 2013 Elsevier B.V. All rights reserved.", "journal": "NEUROCOMPUTING", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000642744500022", "keywords": "Citation classification; Machine learning; In-text citations; Scholarly search systems; Bibliometric-enhanced information retrieval", "title": "Machine learning guided aptamer refinement and discovery", "abstract": "Aptamers are single-stranded nucleic acid ligands that bind to target molecules with high affinity and specificity. They are typically discovered by searching large libraries for sequences with desirable binding properties. These libraries, however, are practically constrained to a fraction of the theoretical sequence space. Machine learning provides an opportunity to intelligently navigate this space to identify high-performing aptamers. Here, we propose an approach that employs particle display (PD) to partition a library of aptamers by affinity, and uses such data to train machine learning models to predict affinity in silico. Our model predicted high-affinity DNA aptamers from experimental candidates at a rate 11-fold higher than random perturbation and generated novel, high-affinity aptamers at a greater rate than observed by PD alone. Our approach also facilitated the design of truncated aptamers 70% shorter and with higher binding affinity (1.5 nM) than the best experimental candidate. This work demonstrates how combining machine learning and physical approaches can be used to expedite the discovery of better diagnostic and therapeutic agents. Current aptamer discovery approaches are unable to probe the complete space of possible sequences. Here, the authors use machine learning to facilitate the development of DNA aptamers with improved binding affinities, and truncate them without significantly compromising binding affinity.", "journal": "NATURE COMMUNICATIONS", "category": "Multidisciplinary Sciences", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000717765400001", "keywords": "Codes; Hardware; Support vector machines; Machine learning algorithms; Registers; Prediction algorithms; Logistics; Embedded code classifier; multi-label; tag-correlated; text classification", "title": "Machine Learning Based Embedded Code Multi-Label Classification", "abstract": "With the development of Internet of Things (IoT) technology, embedded based electronic devices have penetrated every corner of our daily lives. As the brain of IoT devices, embedded based micro controller unit (MCU) plays an irreplaceable role. The functions of the MCUs are becoming more and more powerful and complicated, which brings huge challenges to embedded programmers. Embedded code, which is highly related to the hardware resources, differs from other popular programming code. The hardware configuration may be a big challenge to the programmers, who may only be good at software development and algorithm design. Online code searching can be time consuming and cannot guarantee an optimal approach. To solve this problem, in this paper, an embedded code classifier, which is designed to help embedded programmers to search for the most efficient code with precise tags, is demonstrated. A high quality embedded code dataset is built. A tag correlated multi-label machine learning model is developed for the embedded code dataset. The experimental results show that the proposed code dataset structure is proved to be more efficient on embedded code classification. The proposed embedded classifier algorithm shows a promising result on embedded code dataset. And it outperforms the traditional machine learning text classification models.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000671281200001", "keywords": "active learning; landslide modelling; support vector machine; machine learning", "title": "Active-Learning Approaches for Landslide Mapping Using Support Vector Machines", "abstract": "Ex post landslide mapping for emergency response and ex ante landslide susceptibility modelling for hazard mitigation are two important application scenarios that require the development of accurate, yet cost-effective spatial landslide models. However, the manual labelling of instances for training machine learning models is time-consuming given the data requirements of flexible data-driven algorithms and the small percentage of area covered by landslides. Active learning aims to reduce labelling costs by selecting more informative instances. In this study, two common active-learning strategies, uncertainty sampling and query by committee, are combined with the support vector machine (SVM), a state-of-the-art machine-learning technique, in a landslide mapping case study in order to assess their possible benefits compared to simple random sampling of training locations. By selecting more \"informative\" instances, the SVMs with active learning based on uncertainty sampling outperformed both random sampling and query-by-committee strategies when considering mean AUROC (area under the receiver operating characteristic curve) as performance measure. Uncertainty sampling also produced more stable performances with a smaller AUROC standard deviation across repetitions. In conclusion, under limited data conditions, uncertainty sampling reduces the amount of expert time needed by selecting more informative instances for SVM training. We therefore recommend incorporating active learning with uncertainty sampling into interactive landslide modelling workflows, especially in emergency response settings, but also in landslide susceptibility modelling.", "journal": "REMOTE SENSING", "category": "Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000399086500002", "keywords": "Social networks; image analysis; privacy; machine learning", "title": "Toward Automated Online Photo Privacy", "abstract": "Online photo sharing is an increasingly popular activity for Internet users. More and more users are now constantly sharing their images in various social media, from social networking sites to online communities, blogs, and content sharing sites. In this article, we present an extensive study exploring privacy and sharing needs of users' uploaded images. We develop learning models to estimate adequate privacy settings for newly uploaded images, based on carefully selected image-specific features. Our study investigates both visual and textual features of images for privacy classification. We consider both basic image-specific features, commonly used for image processing, as well as more sophisticated and abstract visual features. Additionally, we include a visual representation of the sentiment evoked by images. To our knowledge, sentiment has never been used in the context of image classification for privacy purposes. We identify the smallest set of features, that by themselves or combined together with others, can perform well in properly predicting the degree of sensitivity of users' images. We consider both the case of binary privacy settings (i.e., public, private), as well as the case of more complex privacy options, characterized by multiple sharing options. Our results show that with few carefully selected features, one may achieve high accuracy, especially when high-quality tags are available.", "journal": "ACM TRANSACTIONS ON THE WEB", "category": "Computer Science, Information Systems; Computer Science, Software Engineering", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000701708700003", "keywords": "cyber-security; IoT; drones; machine learning", "title": "Determinants of brain swelling in pediatric and adult cerebral malaria", "abstract": "Cerebral malaria (CM) affects children and adults, but brain swelling is more severe in children. To investigate features associated with brain swelling in malaria, we performed blood profiling and brain MRI in a cohort of pediatric and adult patients with CM in Rourkela, India, and compared them with an African pediatric CM cohort in Malawi. We determined that higher plasma Plasmodium falciparum histidine rich protein 2 (PfHRP2) levels and elevated var transcripts that encode for binding to endothelial protein C receptor (EPCR) were linked to CM at both sites. Machine learning models trained on the African pediatric cohort could classify brain swelling in Indian children CM cases but had weaker performance for adult classification, due to overall lower parasite var transcript levels in this age group and more severe thrombocytopenia in Rourkela adults. Subgrouping of patients with CM revealed higher parasite biomass linked to severe thrombocytopenia and higher Group A-EPCR var transcripts in mild thrombocytopenia. Overall, these findings provide evidence that higher parasite biomass and a subset of Group A-EPCR binding variants are common features in children and adult CM cases, despite age differences in brain swelling.", "journal": "JCI INSIGHT", "category": "Medicine, Research & Experimental", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000682201600001", "keywords": "dam siting; multi-criteria decision-making; geographic information systems; machine learning; siting factors", "title": "Dam Siting: A Review", "abstract": "Dams can effectively regulate the spatial and temporal distribution of water resources, where the rationality of dam siting determines whether the role of dams can be effectively performed. This paper reviews the research literature on dam siting in the past 20 years, discusses the methods used for dam siting, focuses on the factors influencing dam siting, and assesses the impact of different dam functions on siting factors. The results show the following: (1) Existing siting methods can be categorized into three types-namely, GIS/RS-based siting, MCDM- and MCDM-GIS-based siting, and machine learning-based siting. GIS/RS emphasizes the ability to capture and analyze data, MCDM has the advantage of weighing the importance of the relationship between multiple factors, and machine learning methods have a strong ability to learn and process complex data. (2) Site selection factors vary greatly, depending on the function of the dam. For dams with irrigation and water supply as the main purpose, the site selection is more focused on the evaluation of water quality. For dams with power generation as the main purpose, the hydrological factors characterizing the power generation potential are the most important. For dams with flood control as the main purpose, the topography and geological conditions are more important. (3) The integration of different siting methods and the siting of new functional dams in the existing research is not sufficient. Future research should focus on the integration of different methods and disciplines, in order to explore the siting of new types of dams.", "journal": "WATER", "category": "Environmental Sciences; Water Resources", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000428719000015", "keywords": "Cloud computing; Health services; Parallel particle swarm optimization; Linear regression; Neural network; Chronic kidney disease", "title": "A machine learning model for improving healthcare services on cloud computing environment", "abstract": "Recently, cloud computing gained an important role in healthcare services (HCS) due to its ability to improve the HCS performance. However, the optimal selection of virtual machines (VMs) to process a medical request represents a big challenge. Optimal selection of VMs performs a significant enhancement of the performance through reducing the execution time of medical requests (tasks) coming from stakeholders (patients, doctors, etc.) and maximizing utilization of cloud resources. For that, this paper proposes a new model for HCS based on cloud environment using Parallel Particle Swarm Optimization (PPSO) to optimize the VMs selection. In addition, a new model for chronic kidney disease (CKD) diagnosis and prediction is proposed to measure the performance of our VMs model. The prediction model of CKD is implemented using two consecutive techniques, which are linear regression (LR) and neural network (NN). LR is used to determine critical factors that influence on CKD. NN is used to predict of CKD. The results show that, the proposed model outperforms the state-of-the art models in total execution time the rate of 50%. In addition, the system efficiency regarding real-time data retrieval is greatly improved by 5.2%. In addition, the accuracy of hybrid intelligent model in predicting of CKD is 97.8%. The proposed model is superior to most of the referred models in the related works by 64%.", "journal": "MEASUREMENT", "category": "Engineering, Multidisciplinary; Instruments & Instrumentation", "annotated_keywords": ["neural net", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000708875900001", "keywords": "Cis-regulatory element; Enhancer prediction; Developmental gene regulation", "title": "Identification and prediction of developmental enhancers in sea urchin embryos", "abstract": "Background The transcription of developmental regulatory genes is often controlled by multiple cis-regulatory elements. The identification and functional characterization of distal regulatory elements remains challenging, even in tractable model organisms like sea urchins. Results We evaluate the use of chromatin accessibility, transcription and RNA Polymerase II for their ability to predict enhancer activity of genomic regions in sea urchin embryos. ATAC-seq, PRO-seq, and Pol II ChIP-seq from early and late blastula embryos are manually contrasted with experimental cis-regulatory analyses available in sea urchin embryos, with particular attention to common developmental regulatory elements known to have enhancer and silencer functions differentially deployed among embryonic territories. Using the three functional genomic data types, machine learning models are trained and tested to classify and quantitatively predict the enhancer activity of several hundred genomic regions previously validated with reporter constructs in vivo. Conclusions Overall, chromatin accessibility and transcription have substantial power for predicting enhancer activity. For promoter-overlapping cis-regulatory elements in particular, the distribution of Pol II is the best predictor of enhancer activity in blastula embryos. Furthermore, ATAC- and PRO-seq predictive value is stage dependent for the promoter-overlapping subset. This suggests that the sequence of regulatory mechanisms leading to transcriptional activation have distinct relevance at different levels of the developmental gene regulatory hierarchy deployed during embryogenesis.", "journal": "BMC GENOMICS", "category": "Biotechnology & Applied Microbiology; Genetics & Heredity", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000524314600002", "keywords": "alzheimer's disease; magnetic resonance imaging; multi-view; CNN", "title": "Multi-View Based Multi-Model Learning for MCI Diagnosis", "abstract": "Mild cognitive impairment (MCI) is the early stage of Alzheimer's disease (AD). Automatic diagnosis of MCI by magnetic resonance imaging (MRI) images has been the focus of research in recent years. Furthermore, deep learning models based on 2D view and 3D view have been widely used in the diagnosis of MCI. The deep learning architecture can capture anatomical changes in the brain from MRI scans to extract the underlying features of brain disease. In this paper, we propose a multi-view based multi-model (MVMM) learning framework, which effectively combines the local information of 2D images with the global information of 3D images. First, we select some 2D slices from MRI images and extract the features representing 2D local information. Then, we combine them with the features representing 3D global information learned from 3D images to train the MVMM learning framework. We evaluate our model on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. The experimental results show that our proposed model can effectively recognize MCI through MRI images (accuracy of 87.50% for MCI/HC and accuracy of 83.18% for MCI/AD).", "journal": "BRAIN SCIENCES", "category": "Neurosciences", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000529337400167", "keywords": "negative selection; central tolerance; self-nonself discrimination; T cell repertoires; artificial immune system; learning by example", "title": "Is T Cell Negative Selection a Learning Algorithm?", "abstract": "Our immune system can destroy most cells in our body, an ability that needs to be tightly controlled. To prevent autoimmunity, the thymic medulla exposes developing T cells to normal \"self\" peptides and prevents any responders from entering the bloodstream. However, a substantial number of self-reactive T cells nevertheless reaches the periphery, implying that T cells do not encounter all self peptides during this negative selection process. It is unclear if T cells can still discriminate foreign peptides from self peptides they haven't encountered during negative selection. We use an \"artificial immune system\"-a machine learning model of the T cell repertoire-to investigate how negative selection could alter the recognition of self peptides that are absent from the thymus. Our model reveals a surprising new role for T cell cross-reactivity in this context: moderate T cell cross-reactivity should skew the post-selection repertoire towards peptides that differ systematically from self. Moreover, even some self-like foreign peptides can be distinguished provided that the peptides presented in the thymus are not too similar to each other. Thus, our model predicts that negative selection on a well-chosen subset of self peptides would generate a repertoire that tolerates even \"unseen\" self peptides better than foreign peptides. This effect would resemble a \"generalization\" process as it is found in learning systems. We discuss potential experimental approaches to test our theory.", "journal": "CELLS", "category": "Cell Biology", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000523465500001", "keywords": "image classification; telemedicine; medical image processing; wavelet transforms; learning (artificial intelligence); cancer; skin; biomedical optical imaging; wavelet-based deep learning; skin lesion classification; skin lesions; malignant forms; benign forms; benign skin lesion types; malignant types; skin cancer; malignant melanoma; seborrhoeic keratosis lesions; skin images; vertical wavelet coefficients; deep learning models; approximate coefficients; sequential wavelet transformation; approximation coefficients; transfer learning-based ResNet-18; model images; skin lesion detection", "title": "Estimating Hourly Traffic Volumes using Artificial Neural Network with Additional Inputs from Automatic Traffic Recorders", "abstract": "Traffic volumes are an essential input to many highway planning and design models; however, collecting this data for all road network segments is neither practical nor cost-effective. Accordingly, transportation agencies must find ways to leverage limited ground truth volume data to obtain reasonable estimates at scale on the statewide network. This paper aims to investigate the impact of selecting a subset of available automatic traffic recorders (ATRs) (i.e., the ground truth volume data source) and incorporating their data as explanatory variables into a previously developed machine learning regression model for estimating hourly traffic volumes. The study introduces a handful of strategies for selecting this subset of ATRs and walks through the process of choosing them and training models using their data as additional inputs using the New Hampshire road network as a case study. The results reveal that the overall performance of the artificial neural network (ANN) machine learning model improves with the additional inputs of selected ATRs. However, this improvement is more significant if the ATRs are selected based on their spatial distribution over the traffic message channel (TMC) network. For instance, selecting eight ATR stations according to the TMC coverage-based strategy and training the ANN with their inputs leads to average relative reductions of 35.39% and 13.44% in the mean absolute percentage error (MAPE) and error to maximum flow ratio (EMFR), respectively. The results achieved by this study can be further expanded to create a practical strategy for optimizing the number and location of ATRs through transportation networks in a state.", "journal": "TRANSPORTATION RESEARCH RECORD", "category": "Engineering, Civil; Transportation; Transportation Science & Technology", "annotated_keywords": ["neural net", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000709807000013", "keywords": "machine learning; human coronavirus; polymerase chain reaction; viral pneumonia", "title": "3D Deep Learning Model for the Pretreatment Evaluation of Treatment Response in Esophageal Carcinoma: A Prospective Study (ChiCTR2000039279)", "abstract": "Purpose: To develop and validate a pretreatment computed tomography (CT)-based deep-learning (DL) model for predicting the treatment response to concurrent chemoradiation therapy (CCRT) among patients with locally advanced thoracic esophageal squamous cell carcinoma (TESCC). Methods and Materials: We conducted a prospective, multicenter study on the therapeutic efficacy of CCRT among TESCC patients across 9 hospitals in China (ChiCTR2000039279). A total of 306 patients with locally advanced TESCC diagnosed by histopathology from August 2015 to May 2020 were included in this study. A 3-dimensional DL radiomics model (3DDLRM) was developed and validated based on pretreatment CT images to predict the response to CCRT. Furthermore, the prediction performance of the newly developed 3D-DLRM was analyzed according to 3 categories: radiation therapy plan, radiation field, and prescription dose used. Results: The 3D-DLRM achieved good prediction performance, with areas under the receiver operating characteristic curve of 0.897 (95% confidence interval, 0.840-0.959) for the training cohort and 0.833 (95% confidence interval, 0.654-1.000) for the validation cohort. Specifically, the 3D-DLRM accurately predicted patients who would not respond to CCRT, with a positive predictive value (PPV) of 100% for the validation cohort. Moreover, the 3D-DLRM performed well in all 3 categories, each with areas under the receiver operating characteristic curve of >0.8 and positive predictive values of approximately 100%. Conclusion: The proposed pretreatment CT-based 3D-DLRM provides a potential tool for predicting the response to CCRT among patients with locally advanced TESCC. With the help of precise pretreatment prediction, we may guide the individualized treatment of patients and improve survival. (C) 2021 The Author(s). Published by Elsevier Inc.", "journal": "INTERNATIONAL JOURNAL OF RADIATION ONCOLOGY BIOLOGY PHYSICS", "category": "Oncology; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000593837400001", "keywords": "Gene ontology; Representation learning; HIN2Vec; Multi-label hierarchical classification", "title": "DeepciRGO: functional prediction of circular RNAs through hierarchical deep neural networks using heterogeneous network features", "abstract": "BackgroundCircular RNAs (circRNAs) are special noncoding RNA molecules with closed loop structures. Compared with the traditional linear RNA, circRNA is more stable and not easily degraded. Many studies have shown that circRNAs are involved in the regulation of various diseases and cancers. Determining the functions of circRNAs in mammalian cells is of great significance for revealing their mechanism of action in physiological and pathological processes, diagnosis and treatment of diseases. However, determining the functions of circRNAs on a large scale is a challenging task because of the high experimental costs.ResultsIn this paper, we present a hierarchical deep learning model, DeepciRGO, which can effectively predict gene ontology functions of circRNAs. We build a heterogeneous network containing circRNA co-expressions, protein-protein interactions and protein-circRNA interactions. The topology features of proteins and circRNAs are calculated using a novel representation learning approach HIN2Vec across the heterogeneous network. Then, a deep multi-label hierarchical classification model is trained with the topology features to predict the biological process function in the gene ontology for each circRNA. In particular, we manually curated a benchmark dataset containing 185 GO annotations for 62 circRNAs, namely, circRNA2GO-62. The DeepciRGO achieves promising performance on the circRNA2GO-62 dataset with a maximum F-measure of 0.412, a recall score of 0.400, and an accuracy of 0.425, which are significantly better than other state-of-the-art RNA function prediction methods. In addition, we demonstrate the considerable potential of integrating multiple interactions and association networks.ConclusionsDeepciRGO will be a useful tool for accurately annotating circRNAs. The experimental results show that integrating multi-source data can help to improve the predictive performance of DeepciRGO. Moreover, The model also can combine RNA structure and sequence information to further optimize predictive performance.", "journal": "BMC BIOINFORMATICS", "category": "Biochemical Research Methods; Biotechnology & Applied Microbiology; Mathematical & Computational Biology", "annotated_keywords": ["neural net", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000701801400007", "keywords": "Hydraulic piston pump; Intelligent fault diagnosis; Adapting learning rate; Convolutional neural network; Continuous wavelet transform", "title": "An improved convolutional neural network with an adaptable learning rate towards multi-signal fault diagnosis of hydraulic piston pump", "abstract": "Hydraulic piston pump is a vital component of hydraulic transmission system and plays a critical role in some modern industrials. On account of the deficiencies of traditional fault diagnosis in preprocessing of original data and feature extraction, the intelligent methods based on deep learning accomplish the automatic learning of fault information by integrating feature extraction and classification. As a popular deep learning model, convolutional neural network (CNN) has been demonstrated to be potent and effective in image classification. In this research, an improved intelligent method based on CNN with adapting learning rate is constructed for fault diagnosis of a hydraulic piston pump. Firstly, three raw signals are converted into two dimensional time-frequency images by continuous wavelet transform, including vibration signal, pressure signal and sound signal. Secondly, an improved deep CNN model is built with an adaptive learning rate strategy for identifying the different fault types. Moreover, t-distributed stochastic neighbor embedding is employed to visualize the distribution of features learned by the main layers of CNN model. Confusion matrix is used to analyze the classification accuracy of each fault type. Compared with the CNN model without adapting learning rate, the improved model achieves a higher accuracy based on the selected three kinds of signals. Experiments indicate that the improved CNN model can effectively and accurately identify various faults for a hydraulic piston pump.", "journal": "ADVANCED ENGINEERING INFORMATICS", "category": "Computer Science, Artificial Intelligence; Engineering, Multidisciplinary", "annotated_keywords": ["neural net", "neural net", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000523403600001", "keywords": "two-dimensional semiconductors; machine learning", "title": "Fundamental band gap and alignment of two-dimensional semiconductors explored by machine learning", "abstract": "Two-dimensional (2D) semiconductors isoelectronic to phosphorene have been drawing much attention recently due to their promising applications for next-generation (opt)electronics. This family of 2D materials contains more than 400 members, including (a) elemental group-V materials, (b) binary III-VII and IV-VI compounds, (c) ternary III-VI-VII and IV-V-VII compounds, making materials design with targeted functionality unprecedentedly rich and extremely challenging. To shed light on rational functionality design with this family of materials, we systemically explore their fundamental band gaps and alignments using hybrid density functional theory (DFT) in combination with machine learning. First, calculations are performed using both the Perdew-Burke-Ernzerhof exchange-correlation functional within the general-gradient-density approximation (GGA-PBE) and Heyd-Scuseria-Ernzerhof hybrid functional (HSE) as a reference. We find this family of materials share similar crystalline structures, but possess largely distributed band-gap values ranging approximately from 0 eV to 8 eV. Then, we apply machine learning methods, including linear regression (LR), random forest regression (RFR), and support vector machine regression (SVR), to build models for the prediction of electronic properties. Among these models, SVR is found to have the best performance, yielding the root mean square error (RMSE) less than 0.15 eV for the predicted band gaps, valence-band maximums (VBMs), and conduction-band minimums (CBMs) when both PBE results and elemental information are used as features. Thus, we demonstrate that the machine learning models are universally suitable for screening 2D isoelectronic systems with targeted functionality, and especially valuable for the design of alloys and heterogeneous systems.", "journal": "CHINESE PHYSICS B", "category": "Physics, Multidisciplinary", "annotated_keywords": ["machine learning", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000629917800004", "keywords": "Parallel genetic algorithm; Machine learning; Feature selection; Intrusion detection systems", "title": "Feature selection using cloud-based parallel genetic algorithm for intrusion detection data classification", "abstract": "With the exponential growth of the amount of data being generated, stored and processed on a daily basis in the machine learning, data analytics and decision-making systems, the data preprocessing established itself as the key factor for building reliable high-performance machine learning models. One of the roles in preprocessing is variable reduction using feature selection methods; however, the processing time needed for these methods is a major drawback. This study aims at mitigating this problem by migrating the algorithm to a MapReduce implementation suitable for parallelization on a high number of commodity hardware units. The genetic algorithm-based methods were put in the focus of this study. Hadoop, an open-source MapReduce library, was used as a framework for implementing parallel genetic algorithms within our research. The representative machine learning methods, SVM (support vector machine), ANN (artificial neural network), RT (random tree), logistic regression and Naive Bayes, were embedded into implementation for feature selection. The feature selection methods were applied to four NSL-KDD data sets, and the number of features is reduced from cca 40 to cca 10 data sets with the accuracy of 90.45%. These results have both significant practical and theoretical impact. On the one hand, the genetic algorithm has been parallelized in the MapReduce manner, which has been considered unachievable in a strict sense. Furthermore, the genetic algorithm allows randomness-enhanced feature selection and its parallelization reduces overall data preprocessing and allows larger population count which in turn leads to better feature selection. On the practical side, it has been shown that this implementation outperforms the existing feature selection methods.", "journal": "NEURAL COMPUTING & APPLICATIONS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000551568300001", "keywords": "gene prioritisation; machine learning; gene discovery; amyotrophic lateral sclerosis; motor neurone disease; knowledge graph", "title": "A Knowledge-Based Machine Learning Approach to Gene Prioritisation in Amyotrophic Lateral Sclerosis", "abstract": "Amyotrophic lateral sclerosis is a neurodegenerative disease of the upper and lower motor neurons resulting in death from neuromuscular respiratory failure, typically within two to five years of first symptoms. Several rare disruptive gene variants have been associated with ALS and are responsible for about 15% of all cases. Although our knowledge of the genetic landscape of this disease is improving, it remains limited. Machine learning models trained on the available protein-protein interaction and phenotype-genotype association data can use our current knowledge of the disease genetics for the prediction of novel candidate genes. Here, we describe a knowledge-based machine learning method for this purpose. We trained our model on protein-protein interaction data from IntAct, gene function annotation from Gene Ontology, and known disease-gene associations from DisGeNet. Using several sets of known ALS genes from public databases and a manual review as input, we generated a list of new candidate genes for each input set. We investigated the relevance of the predicted genes in ALS by using the available summary statistics from the largest ALS genome-wide association study and by performing functional and phenotype enrichment analysis. The predicted sets were enriched for genes associated with other neurodegenerative diseases known to overlap with ALS genetically and phenotypically, as well as for biological processes associated with the disease. Moreover, using ALS genes from ClinVar and our manual review as input, the predicted sets were enriched for ALS-associated genes (ClinVarp= 0.038 and manual reviewp= 0.060) when used for gene prioritisation in a genome-wide association study.", "journal": "GENES", "category": "Genetics & Heredity", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000679541000047", "keywords": "Driving safety; high-speed rail drivers; vigilance; reaction time; stacking; electroencephalogram (EEG)", "title": "Estimating the Vigilance of High-Speed Rail Drivers Using a Stacking Ensemble Learning Method", "abstract": "High-speed rail (HSR) drivers are the key part of operation safety, and their vigilance is the main factor affecting accidentoccurrence. Hence, an effective and reliable method to estimate HSR drivers' vigilance is needed to ensure driving safety. Given that drivers' reaction time can objectively and effectively reflect their vigilance, this paper proposed a two-layer stacking ensemble learning model to predict HSR drivers' reaction time to sudden stimuli based on electroencephalogram(EEG) signals. Three individual regression models were stacked together in the first layer to predict drivers' reaction time separately based on the inputted power spectral density features of EEG signals. Random forest was then used as the regression model in the second layer to negotiate the outputs from the first layer for a more accurate prediction of drivers' response time. The proposed model was trained and tested with the EEG data collected from 40 HSR drivers in a simulated driving experiment. The results show that the mean absolute error (MAE), root mean square error (RMSE), and goodness of fit ( R-2) of the estimated reaction time when using our proposedmodel were 70.14(+/- 13.02) ms, 102.19(+/- 22.18) ms, and 0.74(+/- 0.09), respectively, better than the corresponding results when using any of the regressionmodels individually or comparing to six other popular methods. The impacts of EEG features from different brain regions and the individual differences between HSR drivers on vigilance estimation were also analyzed to further examine the performance of our proposed model.", "journal": "IEEE SENSORS JOURNAL", "category": "Engineering, Electrical & Electronic; Instruments & Instrumentation; Physics, Applied", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000566668400022", "keywords": "Risk; Process monitoring; Neural network; Pandemic; Non-pharmaceutical interventions; Layers of protection", "title": "Machine Learning to Reveal Nanoparticle Dynamics from Liquid-Phase TEM Videos", "abstract": "Liquid-phase transmission electron microscopy (TEM) has been recently applied to materials chemistry to gain fundamental understanding of various reaction and phase transition dynamics at nanometer resolution. However, quantitative extraction of physical and chemical parameters from the liquid-phase TEM videos remains bottlenecked by the lack of automated analysis methods compatible with the videos' high noisiness and spatial heterogeneity. Here, we integrate, for the first time, liquid-phase TEM imaging with our customized analysis framework based on a machine learning model called U-Net neural network. This combination is made possible by our workflow to generate simulated TEM images as the training data with well-defined ground truth. We apply this framework to three typical systems of colloidal nanoparticles, concerning their diffusion and interaction, reaction kinetics, and assembly dynamics, all resolved in real-time and real-space by liquid-phase TEM. A diversity of properties for differently shaped anisotropic nanoparticles are mapped, including the anisotropic interaction landscape of nanoprisms, curvature-dependent and staged etching profiles of nanorods, and an unexpected kinetic law of first-order chaining assembly of concave nanocubes. These systems representing properties at the nanoscale are otherwise experimentally inaccessible. Compared to the prevalent image segmentation methods, U-Net shows a superior capability to predict the position and shape boundary of nanoparticles from highly noisy and fluctuating background-a challenge common and sometimes inevitable in liquid-phase TEM videos. We expect our framework to push the potency of liquid-phase TEM to its full quantitative level and to shed insights, in high-throughput and statistically significant fashion, on the nanoscale dynamics of synthetic and biological nanomaterials.", "journal": "ACS CENTRAL SCIENCE", "category": "Chemistry, Multidisciplinary", "annotated_keywords": ["neural net", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000730514000025", "keywords": "TinyML; continual learning; deep neural networks; parallel ultra-low-power; microcontrollers", "title": "A TinyML Platform for On-Device Continual Learning With Quantized Latent Replays", "abstract": "In the last few years, research and development on Deep Learning models & techniques for ultra-low-power devices - in a word, TinyML - has mainly focused on a train-then-deploy assumption, with static models that cannot he adapted to newly collected data without cloud-based data collection and fine-tuning. Latent Replay-based Continual Learning (CL) techniques (Pellegrini et al, 2020) enable online, serverless adaptation in principle, but so far they have still been too computation- and memory-hungry for ultra-low-power TinyML devices, which are typically based on microcontrollers. In this work, we introduce a HW/SW platform for end-to-end CL based on a 10-core FP32-enabled parallel ultra-low-power (PULP) processor. We rethink the baseline Latent Replay CL algorithm, leveraging quantization of the frozen stage of the model and Latent Replays (LRs) to reduce their memory cost with minimal impact on accuracy. In particular, 8-bit compression of the LR memory proves to be almost lossless (-0.26% with 3000LR) compared to the full-precision baseline implementation, but requires 4x less memory, while 7-bit can also be used with an additional minimal accuracy degradation (up to 5%). We also introduce optimized primitives for forward and backward propagation on the PULP processor, together with data tiling strategies to fully exploit its memory hierarchy, while maximizing efficiency. Our results show that by combining these techniques, continual learning can be achieved in practice using less than 64MB of memory - an amount compatible with embedding in TinyML devices. On an advanced 22nm prototype of our platform, called VEGA, the proposed solution performs on average 65 x faster than a low-power STM32 L4 microcontroller, being 37 x more energy efficient - enough for a lifetime of 535h when learning a new mini-batch of data once every minute.", "journal": "IEEE JOURNAL ON EMERGING AND SELECTED TOPICS IN CIRCUITS AND SYSTEMS", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000435187500140", "keywords": "fog detection; climatology; remote sensing; Meteosat Second Generation; machine learning", "title": "A Hybrid Approach for Fog Retrieval Based on a Combination of Satellite and Ground Truth Data", "abstract": "Fog has a substantial influence on various ecosystems and it impacts economy, traffic systems and human life in many ways. In order to be able to deal with the large number of influence factors, a spatially explicit high-resoluted data set of fog frequency distribution is needed. In this study, a hybrid approach for fog retrieval based on Meteosat Second Generation (MSG) data and ground truth data is presented. The method is based on a random forest (RF) machine learning model that is trained with cloud base altitude (CBA) observations from Meteorological Aviation Routine Weather Reports (METAR) as well as synoptic weather observations (SYNOP). Fog is assumed where the model predicts CBA values below a dynamically derived threshold above the terrain elevation. Cross validation results show good accordance with observation data with a mean absolute error of 298 m in CBA values and an average Heidke Skill Score of 0.58 for fog occurrence. Using this technique, a 10 year fog baseline climatology with a temporal resolution of 15 min was derived for Europe for the period from 2006 to 2015. Spatial and temporal variations in fog frequency are analyzed. Highest average fog occurrences are observed in mountainous regions with maxima in spring and summer. Plains and lowlands show less overall fog occurrence but strong positive anomalies in autumn and winter.", "journal": "REMOTE SENSING", "category": "Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000693584500006", "keywords": "Diagnostic classification; Deep transfer learning; Multiomics data; Lung cancer", "title": "Diagnostic Classification of Lung Cancer Using Deep Transfer Learning Technology and Multi-Omics Data", "abstract": "In recent years, with the increasing application of highthroughput sequencing technology, researchers have obtained and accumulated a large amount of multi-omics data, making it possible to diagnose cancer at the gene expression level. The proliferation of various omics data can provide a large amount of biological information, which brings new opportunities and great challenges as well to cancer classification and diagnosis. Machine learning algorithms for early diagnosis of lung cancer have emerged that distinguish cancers of the early and late stages by using genomic features. Omics data are generally characterized with low sample size, high dimensionality and high noise. Therefore, simple direct application of common classification methods cannot achieve better performance and must be improved in a targeted manner. This paper puts forward a combined convolutional neural network and convolutional auto-encoders approach to construct a deep migratory learning classification model for early lung cancer diagnosis. First, the convolutional auto-encoders algorithm is used to reduce the dimensionality of the dataset in order to make it better meet the requirements of migration learning. Second, a neural network model is constructed with the original dataset and the existing labeled dataset, and the model migration rules are set as well. Finally, a small number of labeled target datasets are used in the training to complete the construction of the classification model. The proposed convolutional neural network method based on model migration and five other popular machine learning models are used to classify and predict the three lung cancer gene datasets and the integrated dataset. The experimental results show that such four evaluation metrics as accuracy, precision, recall, and f1-score with our proposed method have obtained better prediction performance, and the average area under curve result also shows our proposed method is optimal.", "journal": "CHINESE JOURNAL OF ELECTRONICS", "category": "Engineering, Electrical & Electronic", "annotated_keywords": ["neural net", "machine learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000476538300013", "keywords": "neural nets; carbon steel; construction industry; learning (artificial intelligence); metallurgy; steel; image segmentation; deep learning approach; plain carbon steel microstructure images; grade; quality customised; construction industry; transportation; quality; grade; specific heat treatment procedures; specific desired properties; computer-based simulations; metallurgy industry; manual experimentation errors; metal heat treatment processes; digital microstructure images; suitable forms; optimal digital forms; simulation models; raw metal microstructure image; Generative Adversarial Network architecture; steel microstructure image segmentation; authors; GAN model; conventional deep learning models; annotated ground truth segmentation masks; sufficient segmented steel microstructure images; sufficient ground truths generation; segmentation network training; related metal microstructure image processing researches; experiments", "title": "Deep learning approach for segmentation of plain carbon steel microstructure images", "abstract": "To bring about variation in the physical and structural properties or grade of a metal, it is made to undergo specific heat treatment procedures; which can be customized to make the metal microstructure evolve desirably, to obtain specific targeted properties. Recently, computer-based simulations of such heat treatment procedures have become popular, however, such simulations are feasible only if the digital microstructure images are available in suitable forms (optimal digital forms of the microstructure images means the distinct grains identified and the grain boundaries demarcated, i.e., segmentation of microstructure images). To this end, the authors propose a deep learning based Generative Adversarial Network (GAN) architecture for steel microstructure image segmentation. The authors' experimental results prove the performance efficiency of the proposed GAN model, as compared to the state-of-the-art. However, the proposed network architecture requires large volumes of training data, in the form of annotated ground truth segmentation masks. The current literature lacks sufficient segmented steel microstructure images for this training, to the best of their knowledge. Hence, their second contribution in this study is the development of a Convolutional Neural Network-based framework for sufficient ground truths generation, to aid in the proposed segmentation network training.", "journal": "IET IMAGE PROCESSING", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000686717900008", "keywords": "PM2.5; random forest; air pollution; exposure assessment; satellite remote sensing", "title": "Discovery of novel Li SSE and anode coatings using interpretable machine learning and high-throughput multi-property screening", "abstract": "All-solid-state batteries with Li metal anode can address the safety issues surrounding traditional Li-ion batteries as well as the demand for higher energy densities. However, the development of solid electrolytes and protective anode coatings possessing high ionic conductivity and good stability with Li metal has proven to be a challenge. Here, we present our informatics approach to explore the Li compound space for promising electrolytes and anode coatings using high-throughput multi-property screening and interpretable machine learning. To do this, we generate a database of battery-related materials properties by computing Li+ migration barriers and stability windows for over 15,000 Li-containing compounds from Materials Project. We screen through the database for candidates with good thermodynamic and electrochemical stabilities, and low Li+ migration barriers, identifying promising new candidates such as Li9S3N, LiAlB2O5, LiYO2, LiSbF4, and Sr4Li(BN2)(3), among others. We train machine learning models, using ensemble methods, to predict migration barriers and oxidation and reduction potentials of these compounds by engineering input features that ensure accuracy and interpretability. Using only a small number of features, our gradient boosting regression models achieve R-2 values of 0.95 and 0.92 on the oxidation and reduction potential prediction tasks, respectively, and 0.86 on the migration barrier prediction task. Finally, we use Shapley additive explanations and permutation feature importance analyses to interpret our machine learning predictions and identify materials properties with the largest impact on predictions in our models. We show that our approach has the potential to enable rapid discovery and design of novel solid electrolytes and anode coatings.", "journal": "SCIENTIFIC REPORTS", "category": "Multidisciplinary Sciences", "annotated_keywords": ["machine learning", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000568663000007", "keywords": "Functional magnetic resonance imaging; Brain modeling; Dictionaries; Data models; Task analysis; Machine learning; Sparse matrices; Deep learning; functional brain networks; functional magnetic resonance imaging (fMRI)", "title": "Four-Dimensional Modeling of fMRI Data via Spatio-Temporal Convolutional Neural Networks (ST-CNNs)", "abstract": "Since the human brain functional mechanism has been enabled for investigation by the functional magnetic resonance imaging (fMRI) technology, simultaneous modeling of both the spatial and temporal patterns of brain functional networks from 4-D fMRI data has been a fundamental but still challenging research topic for neuroimaging and medical image analysis fields. Currently, general linear model (GLM), independent component analysis (ICA), sparse dictionary learning, and recently deep learning models, are major methods for fMRI data analysis in either spatial or temporal domains, but there are few joint spatial-temporal methods proposed, as far as we know. As a result, the 4-D nature of fMRI data has not been effectively investigated due to this methodological gap. The recent success of deep learning applications for functional brain decoding and encoding greatly inspired us in this paper to propose a novel framework called spatio-temporal convolutional neural network (ST-CNN) to extract both spatial and temporal characteristics from targeted networks jointly and automatically identify of functional networks. The identification of default mode network (DMN) from fMRI data was used for evaluation of the proposed framework. Results show that only training the framework on one fMRI data set is sufficiently generalizable to identify the DMN from different data sets of different cognitive tasks and resting state. Further investigation of the results shows that the joint-learning scheme can capture the intrinsic relationship between the spatial and temporal characteristics of DMN and thus it ensures the accurate identification of DMN from independent data sets. The ST-CNN model brings new tools and insights for fMRI analysis in cognitive and clinical neuroscience studies.", "journal": "IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS", "category": "Computer Science, Artificial Intelligence; Robotics; Neurosciences", "annotated_keywords": ["neural net", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000689535800001", "keywords": "convolutional neural network (CNN); generative adversarial network (GAN); medical image synthesis; radiotherapy planning; magnetic resonance imaging (MRI); computed tomography (CT)", "title": "CT-Based Pelvic T-1-Weighted MR Image Synthesis Using UNet, UNet plus plus and Cycle-Consistent Generative Adversarial Network (Cycle-GAN)", "abstract": "Background: Computed tomography (CT) and magnetic resonance imaging (MRI) are the mainstay imaging modalities in radiotherapy planning. In MR-Linac treatment, manual annotation of organs-at-risk (OARs) and clinical volumes requires a significant clinician interaction and is a major challenge. Currently, there is a lack of available pre-annotated MRI data for training supervised segmentation algorithms. This study aimed to develop a deep learning (DL)-based framework to synthesize pelvic T-1-weighted MRI from a pre-existing repository of clinical planning CTs. Methods: MRI synthesis was performed using UNet++ and cycle-consistent generative adversarial network (Cycle-GAN), and the predictions were compared qualitatively and quantitatively against a baseline UNet model using pixel-wise and perceptual loss functions. Additionally, the Cycle-GAN predictions were evaluated through qualitative expert testing (4 radiologists), and a pelvic bone segmentation routine based on a UNet architecture was trained on synthetic MRI using CT-propagated contours and subsequently tested on real pelvic T-1 weighted MRI scans. Results: In our experiments, Cycle-GAN generated sharp images for all pelvic slices whilst UNet and UNet++ predictions suffered from poorer spatial resolution within deformable soft-tissues (e.g. bladder, bowel). Qualitative radiologist assessment showed inter-expert variabilities in the test scores; each of the four radiologists correctly identified images as acquired/synthetic with 67%, 100%, 86% and 94% accuracy. Unsupervised segmentation of pelvic bone on T1-weighted images was successful in a number of test cases Conclusion: Pelvic MRI synthesis is a challenging task due to the absence of soft-tissue contrast on CT. Our study showed the potential of deep learning models for synthesizing realistic MR images from CT, and transferring cross-domain knowledge which may help to expand training datasets for 21 development of MR-only segmentation models.", "journal": "FRONTIERS IN ONCOLOGY", "category": "Oncology", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000637364200001", "keywords": "self-supervised learning; representation learning; machine learning; electroencephalography; sleep staging; pathology detection; clinical neuroscience", "title": "Uncovering the structure of clinical EEG signals with self-supervised learning", "abstract": "Objective. Supervised learning paradigms are often limited by the amount of labeled data that is available. This phenomenon is particularly problematic in clinically-relevant data, such as electroencephalography (EEG), where labeling can be costly in terms of specialized expertise and human processing time. Consequently, deep learning architectures designed to learn on EEG data have yielded relatively shallow models and performances at best similar to those of traditional feature-based approaches. However, in most situations, unlabeled data is available in abundance. By extracting information from this unlabeled data, it might be possible to reach competitive performance with deep neural networks despite limited access to labels. Approach. We investigated self-supervised learning (SSL), a promising technique for discovering structure in unlabeled data, to learn representations of EEG signals. Specifically, we explored two tasks based on temporal context prediction as well as contrastive predictive coding on two clinically-relevant problems: EEG-based sleep staging and pathology detection. We conducted experiments on two large public datasets with thousands of recordings and performed baseline comparisons with purely supervised and hand-engineered approaches. Main results. Linear classifiers trained on SSL-learned features consistently outperformed purely supervised deep neural networks in low-labeled data regimes while reaching competitive performance when all labels were available. Additionally, the embeddings learned with each method revealed clear latent structures related to physiological and clinical phenomena, such as age effects. Significance. We demonstrate the benefit of SSL approaches on EEG data. Our results suggest that self-supervision may pave the way to a wider use of deep learning models on EEG data.", "journal": "JOURNAL OF NEURAL ENGINEERING", "category": "Engineering, Biomedical; Neurosciences", "annotated_keywords": ["neural net", "deep learning", "supervised learning", "supervised learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000623420300015", "keywords": "Three-dimensional displays; Solid modeling; Two dimensional displays; Computational modeling; Shape; Feature extraction; Adaptation models; Characteristic View; pose estimation; reinforcement learning; 2D image; 3D object", "title": "3D Pose Estimation Based on Reinforce Learning for 2D Image-Based 3D Model Retrieval", "abstract": "In this paper, we propose a novel characteristic view selection model (CVSM) to address the 2D image-based 3D object retrieval problem. This work includes two key contributions: 1) we propose a novel reinforcement learning model to estimate the 3D pose based on a 2D image; and 2) we render the pose-specific model to generate a representative angle view for retrieval applications. First, we define state, policy, action and reward functions to train an agent with the reinforcement learning framework, by which the agent can effectively reduce the computational cost of the characteristic view selection and directly obtain the 3D model pose. Second, to resolve the problem of computing similarity in the cross-domain between the virtual 3D model view and the real query image, we project them into the skeleton domain, and the skeleton information can effectively bridge the gap between the image and 3D model view for cross-media retrieval. To demonstrate the performance of our approach, we compare with some classic 3D pose estimation methods using the popular Pascal3D dataset. To demonstrate the performance of our approach in model retrieval, we collect a new dataset that includes pairs of 2D images and 3D objects, where 3D objects are based on the ModelNet40 dataset and 2D images are based on the ImageNet dataset, and we experiment with our method using the SHREC 2018 and SHREC 2019 databases. The experimental results demonstrate the superiority of our method.", "journal": "IEEE TRANSACTIONS ON MULTIMEDIA", "category": "Computer Science, Information Systems; Computer Science, Software Engineering; Telecommunications", "annotated_keywords": ["reinforcement learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000343210300011", "keywords": "Novelty detection; System change; Outlier detection; Concept drift; Learning algorithms; Adaptive filter; Streaming data", "title": "Dual Reward Prediction Components Yield Pavlovian Sign- and Goal-Tracking", "abstract": "Reinforcement learning (RL) has become a dominant paradigm for understanding animal behaviors and neural correlates of decision-making, in part because of its ability to explain Pavlovian conditioned behaviors and the role of midbrain dopamine activity as reward prediction error (RPE). However, recent experimental findings indicate that dopamine activity, contrary to the RL hypothesis, may not signal RPE and differs based on the type of Pavlovian response (e. g. sign-and goal-tracking responses). In this study, we address this discrepancy by introducing a new neural correlate for learning reward predictions; the correlate is called \"cue-evoked reward''. It refers to a recall of reward evoked by the cue that is learned through simple cue-reward associations. We introduce a temporal difference learning model, in which neural correlates of the cue itself and cue-evoked reward underlie learning of reward predictions. The animal's reward prediction supported by these two correlates is divided into sign and goal components respectively. We relate the sign and goal components to approach responses towards the cue (i.e. sign-tracking) and the food-tray (i.e. goal-tracking) respectively. We found a number of correspondences between simulated models and the experimental findings (i.e. behavior and neural responses). First, the development of modeled responses is consistent with those observed in the experimental task. Second, the model's RPEs were similar to dopamine activity in respective response groups. Finally, goal-tracking, but not sign-tracking, responses rapidly emerged when RPE was restored in the simulated models, similar to experiments with recovery from dopamine-antagonist. These results suggest two complementary neural correlates, corresponding to the cue and its evoked reward, form the basis for learning reward predictions in the sign-and goal-tracking rats.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000502284300010", "keywords": "RFR; SVR; GBRT; MLPR; Lars; soil salt content; hyperspectral", "title": "Performance Comparison of Machine Learning Algorithms for Estimating the Soil Salinity of Salt-Affected Soil Using Field Spectral Data", "abstract": "Salt-affected soil is a prominent ecological and environmental problem in dry farming areas throughout the world. China has nearly 9.9 million km(2) of salt-affected land. The identification, monitoring, and utilization of soil salinization have become important research topics for promoting sustainable progress. In this paper, using field-measured spectral data and soil salinity parameter data, through analysis and transformation of spectral data, five machine learning models, namely, random forest regression (RFR), support vector regression (SVR), gradient-boosted regression tree (GBRT), multilayer perceptron regression (MLPR), and least angle regression (Lars) are compared. The following performance measures of each model were evaluated: the collinear problems, handling data noise, stability, and the accuracy. In terms of these four aspects, the performance of each model on estimating soil salinity is evaluated. The results demonstrate that among the five models, RFR has the best performance in dealing with collinearity, RFR and MLPR have the best performance in dealing with data noise, and the SVR model is the most stable. The Lars model has the highest accuracy, with a determination coefficient (R-2) of 0.87, ratio of performance to deviation (RPD) of 2.67, root mean square error (RMSE) of 0.18, and mean absolute percentage error (MAPE) of 0.11. Then, the comprehensive comparison and analysis of the five models are carried out, and it is found that the comprehensive performance of RFR model is the best; hence, this method is most suitable for estimating soil salinity using hyperspectral data. This study can provide a reference for the selection of regression methods in subsequent studies on estimating soil salinity using hyperspectral data.", "journal": "REMOTE SENSING", "category": "Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000621197700004", "keywords": "Deep neural network; Machine learning; Model interpretation; HO center dot radical; QSARs; XGBoost", "title": "Shedding light on \"Black Box\" machine learning models for predicting the reactivity of HO center dot radicals toward organic compounds", "abstract": "Developing quantitative structure-activity relationships (QSARs) is an important approach to predicting the reactivity of HO radicals toward newly emerged organic compounds. As compared with molecular descriptorsbased and the group contribution method-based QSARs, a combined molecular fingerprint-machine learning (ML) method can more quickly and accurately develop such models for a growing number of contaminants. However, it is yet unknown whether this method makes predictions by choosing meaningful structural features rather than spurious ones, which is vital for trusting the models. In this study, we developed QSAR models for the logk(HO center dot) values of 1089 organic compounds in the aqueous phase by two ML algorithms-deep neural networks (DNN) and eXtreme Gradient Boosting (XGBoost), and interpreted the built models by the SHapley Additive exPlanations (SHAP) method. The results showed that for the contribution of a given structural feature to logk(HO center dot) for different compounds, DNN and XGBoost treated it as a fixed and variable value, respectively. We then developed an ensemble model combining the DNN with XGBoost, which achieved satisfactory predictive performance for all three datasets: Training dataset: R-square (R-2) 0.89-0.91, root-mean-squared-error (RMSE) 0.21-0.23, and mean absolute error (MAE) 0.15-0.17; Validation dataset: R-2 0.63-0.78, RMSE 0.29-0.32, and MAE 0.21-0.25; and Test dataset: R-2 0.60-0.71, RMSE 0.30-0.35, and MAE 0.23-0.25. The SHAP method was further used to unveil that this ensemble model made predictions on logk(HO center dot) based on a correct `understanding' of the impact of electron-withdrawing and -donating groups and of the reactive sites in the compounds that can be attacked by HO center dot. This study offered some much-needed mechanistic insights into a ML-assisted environmental task, which are important for evaluating the trustworthiness of the ML-based models, further improving the models for specific applications, and leveraging the implicit knowledge the models carry.", "journal": "CHEMICAL ENGINEERING JOURNAL", "category": "Engineering, Environmental; Engineering, Chemical", "annotated_keywords": ["neural net", "machine learning", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000749736400001", "keywords": "temporal bone; automated segmentation; atlas; data set curation", "title": "Optimization Research on Deep Learning and Temporal Segmentation Algorithm of Video Shot in Basketball Games", "abstract": "The analysis of the video shot in basketball games and the edge detection of the video shot are the most active and rapid development topics in the field of multimedia research in the world. Video shots' temporal segmentation is based on video image frame extraction. It is the precondition for video application. Studying the temporal segmentation of basketball game video shots has great practical significance and application prospects. In view of the fact that the current algorithm has long segmentation time for the video shot of basketball games, the deep learning model and temporal segmentation algorithm based on the histogram for the video shot of the basketball game are proposed. The video data is converted from the RGB space to the HSV space by the boundary detection of the video shot of the basketball game using deep learning and processing of the image frames, in which the histogram statistics are used to reduce the dimension of the video image, and the three-color components in the video are combined into a one-dimensional feature vector to obtain the quantization level of the video. The one-dimensional vector is used as the variable to perform histogram statistics and analysis on the video shot and to calculate the continuous frame difference, the accumulated frame difference, the window frame difference, the adaptive window's mean, and the superaverage ratio of the basketball game video. The calculation results are combined with the set dynamic threshold to optimize the temporal segmentation of the video shot in the basketball game. It can be seen from the comparison results that the effectiveness of the proposed algorithm is verified by the test of the missed detection rate of the video shots. According to the test result of the split time, the optimization algorithm for temporal segmentation of the video shot in the basketball game is efficiently implemented.", "journal": "COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE", "category": "Mathematical & Computational Biology; Neurosciences", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000467585500001", "keywords": "suicide identification; crisis management; machine learning; microblog direct message; social network; Chinese young people", "title": "Proactive Suicide Prevention Online (PSPO): Machine Identification and Crisis Management for Chinese Social Media Users With Suicidal Thoughts and Behaviors", "abstract": "Background: Suicide is a great public health challenge. Two hundred million people attempt suicide in China annually Existing suicide prevention programs require the help-seeking initiative of suicidal individuals, but many of them have a low motivation to seek the required help. We propose that a proactive and targeted suicide prevention strategy can prompt more people with suicidal thoughts and behaviors to seek help. Objective: The goal of the research was to test the feasibility and acceptability of Proactive Suicide Prevention Online (PSPO), a new approach based on social media that combines proactive identification of suicide-prone individuals with specialized crisis management. Methods: We first located a microblog group online. Their comments on a suicide note were analyzed by experts to provide a training set for the machine learning models for suicide identification. The best-performing model was used to automatically identify posts that suggested suicidal thoughts and behaviors. Next, a microblog direct message containing crisis management information, including measures that covered suicide-related issues, depression, help-seeking behavior and an acceptability test, was sent to users who had been identified by the model to be at risk of suicide. For those who replied to the message, trained counselors provided tailored crisis management. The Simplified Chinese Linguistic Inquiry and Word Count was also used to analyze the users' psycholinguistic texts in 1-month time slots prior to and postconsultation. Results: A total of 27,007 comments made in April 2017 were analyzed. Among these, 2786 (10.32%) were classified as indicative of suicidal thoughts and behaviors. The performance of the detection model was good, with high precision (.86), recall (.78), F-measure (.86), and accuracy (.88). Between July 3, 2017, and July 3, 2018, we sent out a total of 24,727 direct messages to 12,486 social media users, and 5542 (44.39%) responded. Over one-third of the users who were contacted completed the questionnaires included in the direct message. Of the valid responses, 89.73% (1259/1403) reported suicidal ideation, but more than half (725/1403, 51.67%) reported that they had not sought help. The 9-Item Patient Health Questionnaire (PHQ-9) mean score was 17.40 (SD 5.98). More than two-thirds of the participants (968/1403, 69.00%) thought the PSPO approach was acceptable. Moreover, 2321 users replied to the direct message. In a comparison of the frequency of word usage in their microblog posts 1-month before and after the consultation, we found that the frequency of death-oriented words significantly declined while the frequency of future-oriented words significantly increased. Conclusions: The PSPO model is suitable for identifying populations that are at risk of suicide. When followed up with proactive crisis management, it may be a useful supplement to existing prevention programs because it has the potential to increase the accessibility of antisuicide information to people with suicidal thoughts and behaviors but a low motivation to seek help.", "journal": "JOURNAL OF MEDICAL INTERNET RESEARCH", "category": "Health Care Sciences & Services; Medical Informatics", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000540259800066", "keywords": "Neural network ensemble; Backpropagation algorithm; Negative correlation learning; Constructive algorithms; Pruning algorithms", "title": "A dynamic ensemble learning algorithm for neural networks", "abstract": "This paper presents a novel dynamic ensemble learning (DEL) algorithm for designing ensemble of neural networks (NNs). DEL algorithm determines the size of ensemble, the number of individual NNs employing a constructive strategy, the number of hidden nodes of individual NNs employing a constructive-pruning strategy, and different training samples for individual NN's learning. For diversity, negative correlation learning has been introduced and also variation of training samples has been made for individual NNs that provide better learning from the whole training samples. The major benefits of the proposed DEL compared to existing ensemble algorithms are (1) automatic design of ensemble; (2) maintaining accuracy and diversity of NNs at the same time; and (3) minimum number of parameters to be defined by user. DEL algorithm is applied to a set of real-world classification problems such as the cancer, diabetes, heart disease, thyroid, credit card, glass, gene, horse, letter recognition, mushroom, and soybean datasets. It has been confirmed by experimental results that DEL produces dynamic NN ensembles of appropriate architecture and diversity that demonstrate good generalization ability.", "journal": "NEURAL COMPUTING & APPLICATIONS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net", "neural net", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000551782500002", "keywords": "Laminated composite plates; Buckling; Free vibration; Cutout; Artificial neural networks", "title": "Free vibration and buckling analyses of laminated composite plates with cutout", "abstract": "This study aims to examine the effect of the diameter, number, and location of the circular cutout on the free vibration response and buckling loads of the laminated composites. Eigen-buckling and free vibrations analyses are performed for the laminated composite plates by using the finite element software ANSYS. Numerical results obtained by the finite element method are compared to the experimental ones. In the numerical analyses, the effect of the delamination around the cutout on the buckling load and the natural frequency is also examined. The critical buckling load and first natural frequency values obtained by both numerical and experimental studies are used to create a prediction model using the artificial neural networks. The Levenberg-Marquardt backpropagation algorithm is used as the training method. It is observed that both the number and location of the cutout affect the critical buckling load and first natural frequency values. Numerical and experimental results are presented together with the ANN prediction results.", "journal": "ARCHIVE OF APPLIED MECHANICS", "category": "Mechanics", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000460129200002", "keywords": "ANN; Water discharge; Suspended sediment concentration prediction; Ramganga River; Himalayas", "title": "Artificial neural network simulation for prediction of suspended sediment concentration in the River Ramganga, Ganges Basin, India", "abstract": "The relation between the water discharge (Q) and suspended sediment concentration (SSC) of the River Ramganga at Bareilly, Uttar Pradesh, in the Himalayas, has been modeled using Artificial Neural Networks (ANNs). The current study validates the practical capability and usefulness of this tool for simulating complex nonlinear, real world, river system processes in the Himalayan scenario. The modeling approach is based on the time series data collected from January to December (2008-2010) for Q and SSC. Three ANNs (T1-T3) with different network configurations have been developed and trained using the Levenberg Marquardt Back Propagation Algorithm in the Matlab routines. Networks were optimized using the enumeration technique, and, finally, the best network is used to predict the SSC values for the year 2011. The values thus obtained through the ANN model are compared with the observed values of SSC. The coefficient of determination (R-2), for the optimal network was found to be 0.99. The study not only provides insight into ANN modeling in the Himalayan river scenario, but it also focuses on the importance of understanding a river basin and the factors that affect the SSC, before attempting to model it. Despite the temporal variations in the study area, it is possible to model and successfully predict the SSC values with very simplistic ANN models. (C) 2018 International Research and Training Centre on Erosion and Sedimentation/the World Association for Sedimentation and Erosion Research. Published by Elsevier B.V. All rights reserved.", "journal": "INTERNATIONAL JOURNAL OF SEDIMENT RESEARCH", "category": "Environmental Sciences; Water Resources", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000354135700004", "keywords": "Facial point detection; Restricted Boltzmann Machine; Deep learning", "title": "Discriminative Deep Face Shape Model for Facial Point Detection", "abstract": "Facial point detection is an active area in computer vision due to its relevance to many applications. It is a nontrivial task, since facial shapes vary significantly with facial expressions, poses or occlusion. In this paper, we address this problem by proposing a discriminative deep face shape model that is constructed based on an augmented factorized three-way Restricted Boltzmann Machines model. Specifically, the discriminative deep model combines the top-down information from the embedded face shape patterns and the bottom up measurements from local point detectors in a unified framework. In addition, along with the model, effective algorithms are proposed to perform model learning and to infer the true facial point locations from their measurements. Based on the discriminative deep face shape model, 68 facial points are detected on facial images in both controlled and \"in-the-wild\" conditions. Experiments on benchmark data sets show the effectiveness of the proposed facial point detection algorithm against state-of-the-art methods.", "journal": "INTERNATIONAL JOURNAL OF COMPUTER VISION", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000537319700016", "keywords": "Personalized search; Interactive Estimation of Distribution Algorithm; Restricted Boltzmann Machine; Surrogate", "title": "Restricted Boltzmann Machine-driven Interactive Estimation of Distribution Algorithm for personalized search", "abstract": "Effective and efficient personalized search is one of the most pursued objectives in the era of big data. The challenge of this problem lies in its complex quantifying evaluations and dynamic user preferences. A user-involved interactive evolutionary algorithm is a good choice if it has reliable preference surrogate and powerful evolutionary strategies. A Restricted Boltzmann Machine (RBM) assisted Interactive Estimation of Distribution Algorithm (IEDA) is presented to enhance the IEDA in solving the personalized search. Specifically, a dual-RBM module is developed to simultaneously provide a preference surrogate and a probability model for conducting the individual selection and generation of the IEDA. Firstly, the positive and negative preferences of the currently involved user in IEDA are distinguished and combined to achieve a dual-RBM, and then the weighted energy functions of the RBM model together with social group information from users with similar preferences are designed as the preference surrogate. The probability of the trained positive RBM on the visible units is fetched as the reproduction model of EDA since it reflects the attribute distributions of more preferred items. Some benchmarks from the Movielens and Amazon datasets are applied to experimentally demonstrate the superiority of the proposed algorithm in improving the efficiency and effectiveness of the interactive evolutionary computations served personalized search. (C) 2020 Elsevier B.V. All rights reserved.", "journal": "KNOWLEDGE-BASED SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000319372400009", "keywords": "Estrogen; Estrogen receptor; Uterus; Menopause", "title": "Lessons from the dissection of the activation functions (AF-1 and AF-2) of the estrogen receptor alpha in vivo", "abstract": "Estrogens influence most of the physiological processes in mammals, including but not limited to reproduction, cognition, behavior, vascular system, metabolism and bone integrity. Given this widespread role for estrogen in human physiology, it is not surprising that estrogen influence the pathophysiology of numerous diseases, including cancer (of the reproductive tract as breast, endometrial but also colorectal, prostate, ... ), as well as neurodegenerative, inflammatory-immune, cardiovascular and metabolic diseases, and osteoporosis. These actions are mediated by the activation of estrogen receptors (ER) alpha (ER alpha) and beta (ER beta), which regulate target gene transcription (genomic action) through two independent activation functions (AF)-1 and AF-2, but can also elicit rapid membrane initiated steroid signals (MISS). Targeted ER gene inactivation has shown that although ER beta plays an important role in the central nervous system and in the heart, ER alpha appears to play a prominent role in most of the other tissues. Pharmacological activation or inhibition of ER alpha and/or ER beta provides already the basis for many therapeutic interventions, from hormone replacement at menopause to prevention of the recurrence of breast cancer. However, the use of these estrogens or selective estrogen receptors modulators (SERMs) have also induced undesired effects. Thus, an important challenge consists now to uncouple the beneficial actions from other deleterious ones. The in vivo molecular \"dissection\" of ER alpha represents both a molecular and integrated approach that already allowed to delineate in mouse the role of the main \"subfunctions\" of the receptor and that could pave the way to an optimization of the ER modulation. (C) 2012 Elsevier Inc. All rights reserved.", "journal": "STEROIDS", "category": "Biochemistry & Molecular Biology; Endocrinology & Metabolism", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000574435900001", "keywords": "Electrohysterogram signals; Multi-kernel support vector machine; Opposition based ant lion optimization; Wiener filter", "title": "Prediction of pre-term groups from EHG signals using optimal multi-kernel SVM", "abstract": "Pre-term birth is the birth that carries out before the baby's expected date. Sometimes, it augments the possibility of health problems or death. In this research, the abdominal EHG signal incorporates both the maternal heart beat signal and Fetal ECG signal. The removal of fetal ECG signal from the heart beat signal is difficult in pre-term detection. In this work, the EHG signal is pre-processed using wiener filter that is applied to enhance the signal quality. Then, the attributes are removed from the pre-processed signal to find the distinctive class. In addition, Opposition based Ant Lion Optimization is used to select the multi-kernel Support Vector Machine and training algorithms for predicting the pre-term birth. The proposed methodology is simulated by using MATLAB software and the results are investigated to verify the classification accuracy. From the experimental study, the proposed work enhanced the classification accuracy upto 3-19% related to the existing works.", "journal": "JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems; Telecommunications", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000576772800010", "keywords": "Heart sound segmentation; Long-Short Term Memory neural network; Sequence tagging", "title": "Heart sound segmentation via Duration Long-Short Term Memory neural network", "abstract": "Heart sound segmentation, which aims at detecting the first and second heart sound in phonocardiogram, is an essential step to automatically analyze heart valve diseases. Recently, the neural network-based methods have demonstrated their promising performance in segmenting the heart sound data. However, the methods also suffer from serious limitations due to the used envelope features. The reason is largely due to that the envelope features cannot effectively model the intrinsic sequential characteristic, resulting in the poor utilization of the duration information of heart cycles. In this paper, we propose a Duration Long-Short Term Memory network (Duration LSTM) to effectively address this problem by incorporating the duration features. The proposed method is investigated in the real-world phonocardiogram dataset (Massachusetts Institute of Technology heart sounds database) and compared with the two representatives of the existing state-of-the-art methods, the experimental results demonstrate that the proposed method has the promising performance on different tolerance windows. In addition, the proposed model also has some advantages in the impact of recording length and the phenomenon of the end effect. (C) 2020 Elsevier B.V. All rights reserved.", "journal": "APPLIED SOFT COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000567807900001", "keywords": "Recommendation; Sequence Modeling; Time-Aware; Long Short-Term Memory", "title": "Time-aware sequence model for next-item recommendation", "abstract": "The sequences of users' behaviors generally indicate their preferences, and they can be used to improve next-item prediction in sequential recommendation. Unfortunately, users' behaviors may change over time, making it difficult to capture users' dynamic preferences directly from recent sequences of behaviors. Traditional methods such as Markov Chains (MC), Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM) networks only consider the relative order of items in a sequence and ignore important time information such as the time interval and duration in the sequence. In this paper, we propose a novel sequential recommendation model, named Interval- and Duration-aware LSTM with Embedding layer and Coupled input and forget gate (IDLSTM-EC), which leverages time interval and duration information to accurately capture users' long-term and short-term preferences. In particular, the model incorporates global context information about sequences in the input layer to make better use of long-term memory. Furthermore, the model introduces the coupled input and forget gate and embedding layer to further improve efficiency and effectiveness. Experiments on real-world datasets show that the proposed approaches outperform the state-of-the-art baselines and can handle the problem of data sparsity effectively.", "journal": "APPLIED INTELLIGENCE", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000440807600018", "keywords": "Adaptive neuro-fuzzy inference system (ANFIS); recurrent neural network (RNN); long-short-term memory (LSTM); energy management strategy (EMS); hybrid auxiliary power unit (APU); more-electrical aircrafts (MEAs); microgrid", "title": "Intelligent Soft Computing-Based Security Control for Energy Management Architecture of Hybrid Emergency Power System for More-Electric Aircrafts", "abstract": "This paper proposes an attack-resilient intelligent soft computing-based security control for energy management architecture for a hybrid emergency power system of more-electric aircrafts (MEAs). Our proposed architecture develops an adaptive neuro-fuzzy inference system (ANFIS)-based method in conjunction with a specific recurrent neural network architecture called long-short-term memory (LSTM) to evaluate the integrity of the power output of the critical components, which have higher vulnerability to the potential cyber-attacks and critical for the effective energy management and emergency control. We evaluate the integrity of the critical measurements that have high vulnerability to the potential cyber-attacks by using LSTM techniques. After identifying the compromised measurements, we continue to activate the closed-loop ANFIS mechanism in which the structure of ANFIS is controlled according to the accuracy of its current estimations by probing the physical couplings in the system. In our simulation, we evaluate the performance of our proposed LSTM-ANFIS -based method to support the integrity of the energy management strategies used in hybrid emergency power system for MEAs by using TensorFlow and MATLAB/Simulink co-simulation environment. Our simulation results illustrate the effectiveness of our proposed method in effectively evaluating the integrity of critical data and achieving resilient control.", "journal": "IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING", "category": "Engineering, Electrical & Electronic", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000537169000005", "keywords": "Multi-step-ahead; Fine particulate matter; Long-term forecasting; Neural network; Least square support vector regression; Long short-term memory", "title": "An ensemble multi-step-ahead forecasting system for fine particulate matter in urban areas", "abstract": "In recent years, growing air pollution has become a significant issue due to its detrimental effects on the environment and different living organisms. Providing accurate and reliable forecasts of air quality over a long future horizon is an effective way to mitigate health risks. In this paper, the problem of urban PM2.5 forecasts for several days ahead is considered. An ensemble multi-step-ahead forecasting system is introduced for this problem, which combines different multi-step-ahead strategies (including single-output and multi-output approaches). The proposed hybrid framework consists of three parts. In the first part, the Ensemble Empirical Mode Decomposition (EEMD) technique is combined with a prediction tool and multi-step-ahead strategies. Boosting idea is considered in the second part of the algorithm. Finally, the stacked ensemble of boosted hybrid structures is developed to provide the final multi-step-ahead forecasts. Least Square Support Vector Regression (LSSVR), and Long Short-Term Memory neural network (LSTM) are employed as the prediction tools in the proposed hybrid framework. Through real PM2.5 data examples from Mashhad, Iran, the proposed ensemble model is investigated for 1-day-ahead to 10-days-ahead. The results reveal the effectiveness of the ensemble model in comparison with the multi-step-ahead strategies in all time-steps. The proposed model with LSSVR prediction tool shows the smallest mean RMSE, MAE, and MAPE values of 7.810, 5.562, and 18.104% over all time-steps, and RMSE improvement rates of more than 35% compared to simply combining different multi-step-ahead strategies with LSSVR approach. (C) 2020 Elsevier Ltd. All rights reserved.", "journal": "JOURNAL OF CLEANER PRODUCTION", "category": "Green & Sustainable Science & Technology; Engineering, Environmental; Environmental Sciences", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000540138000010", "keywords": "CryptoMining malware; Deep learning; Static analysis; Dynamic analysis", "title": "Detecting Cryptomining Malware: a Deep Learning Approach for Static and Dynamic Analysis", "abstract": "Cryptomining malware (also referred to as cryptojacking) has changed the cyber threat landscape. Such malware exploits the victim's CPU or GPU resources with the aim of generating cryptocurrency. In this paper, we study the potential of using deep learning techniques to detect cryptomining malware by utilizing both static and dynamic analysis approaches. To facilitate dynamic analysis, we establish an environment to capture the system call events of 1500 Portable Executable (PE) samples of the cryptomining malware. We also demonstrate how one can perform static analysis of PE files' opcode sequences. In our study, we evaluate the performance of using Long Short-Term Memory (LSTM), Attention-based LSTM (ATT-LSTM), and Convolutional Neural Networks (CNN) on our sequential data (opcodes and system call invocations) for classification by a Softmax function. We achieve an accuracy rate of 95% in the static analysis and an accuracy rate of 99% in the dynamic analysis.", "journal": "JOURNAL OF GRID COMPUTING", "category": "Computer Science, Information Systems; Computer Science, Theory & Methods", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000470082400008", "keywords": "Socially responsible investment; Portfolio optimization; Multivariate analytics; Deep reinforcement learning; Decision support systems", "title": "Detection of DNA base modifications by deep recurrent neural network on Oxford Nanopore sequencing data", "abstract": "DNA base modifications, such as C5-methylcytosine (5mC) and N6-methyldeoxyadenosine (6mA), are important types of epigenetic regulations. Short-read bisulfite sequencing and long-read PacBio sequencing have inherent limitations to detect DNA modifications. Here, using raw electric signals of Oxford Nanopore long-read sequencing data, we design Deep-Mod, a bidirectional recurrent neural network (RNN) with long short-term memory (LSTM) to detect DNA modifications. We sequence a human genome HX1 and a Chlamydomonas reinhardtii genome using Nanopore sequencing, and then evaluate DeepMod on three types of genomes (Escherichia coli, Chlamydomonas reinhardtii and human genomes). For 5mC detection, DeepMod achieves average precision up to 0.99 for both synthetically introduced and naturally occurring modifications. For 6mA detection, DeepMod achieves similar to 0.9 average precision on Escherichia coli data, and have improved performance than existing methods on Chlamydomonas reinhardtii data. In conclusion, DeepMod performs well for genome-scale detection of DNA modifications and will facilitate epigenetic analysis on diverse species.", "journal": "NATURE COMMUNICATIONS", "category": "Multidisciplinary Sciences", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000554868600001", "keywords": "incompressible flow; machine learning; model reduction; proper orthogonal decomposition; reduced order modeling", "title": "Modeling transient fluid simulations with proper orthogonal decomposition and machine learning", "abstract": "In this work, we present the results obtained from integrating several machine learning (ML) models with projection-based reduced order model for modeling the canonical case of flow past a stationary cylinder. We demonstrate how ML models can be used to model the time-varying characteristics of the proper orthogonal decomposition (POD) coefficients, and that the locally interpolating models such as regression trees and k-nearest neighbors seem to be better for such models than other models like support vector regression or long-short term memory networks. In addition, our numerical experiments also show that these POD coefficients are most effectively modeled by using their own previous time values, as opposed to the inclusion of high energy POD modes. Last but not least, we demonstrate that these models, although trained on inlet velocities of 0.8, 1.0, and 1.2 m/s, can still predict the POD coefficients of flow fields for inlet velocities of 0.9 and 1.25 m/s, with root mean squared error of under 10%.", "journal": "INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN FLUIDS", "category": "Computer Science, Interdisciplinary Applications; Mathematics, Interdisciplinary Applications; Mechanics; Physics, Fluids & Plasmas", "annotated_keywords": ["machine learning", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000601157900009", "keywords": "Stereoscopic saliency analysis; 3D images; Multi-modal context fusion; Context-dependent deconvolution", "title": "Context-aware network for RGB-D salient object detection", "abstract": "Convolutional neural networks (CNNs) have shown unprecedented success in object representation and detection. Nevertheless, CNNs lack the capability to model context dependencies among objects, which are crucial for salient object detection. As the long short-term memory (LSTM) is advantageous in propagating information, in this paper, we propose two variant LSTM units for the exploration of contextual dependencies. By incorporating these units, we present a context-aware network (CAN) to detect salient objects in RGB-D images. The proposed model consists of three components: feature extraction, context fusion of multiple modalities and context-dependent deconvolution. The first component is responsible for extracting hierarchical features in color and depth images using CNNs, respectively. The second component fuses high-level features by a variant LSTM to model multi-modal spatial dependencies in contexts. The third component, embedded with another variant LSTM, models local hierarchical context dependencies of the fused features at multi-scales. Experimental results on two public benchmark datasets show that the proposed CAN can achieve state-of-the-art performance for RGB-D stereoscopic salient object detection. (C) 2020 Elsevier Ltd. All rights reserved.", "journal": "PATTERN RECOGNITION", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000585606300001", "keywords": "flowmeter; LSTM; vibration response; mass flow rate; external flow", "title": "Development of a Flowmeter Using Vibration Interaction between Gauge Plate and External Flow Analyzed by LSTM", "abstract": "(1) Background: This study is aimed at the development of a precise and inexpensive device for flow information measurement for external flow. This novel flowmeter uses an LSTM (long short-term memory) neural network algorithm to analyze the vibration responses of the gauge plate. (2) Methods: A signal processing method using an LSTM neural network is proposed for the development of mass flow rate estimation by sensing the vibration responses of a gauge plate. An FFT (fast Fourier transform) and an STFT (short-time Fourier transform) were used to analyze the vibration characteristics of the gauge plate depending on the mass flow rate. For precise measurements, the vibration level and roughness were computed and used as input features. The actual mass flow rate measured by using a weight transducer was employed as the output features for the LSTM prediction model. (3) Results: The estimated flow rate matched the actual measured mass flow rate very closely. The deviations in measurements for the total mass flow were less than 6%. (4) Conclusions: The estimation of the mass flow rate for external flow through the proposed flowmeter by use of vibration responses analyzed by the LSTM neural network was proposed and verified.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000599508000005", "keywords": "Deep learning; Coated maize kernels; Feature extraction; Feature fusion; Near-infrared hyperspectral imaging", "title": "Application of near-infrared hyperspectral imaging for variety identification of coated maize kernels with deep learning", "abstract": "Tracing the varieties of seeds is of great importance for the seed industry. Maize kernels for planting are generally coated to protect kernels from fungi and insects. In this study, near-infrared hyperspectral imaging ranging from 874 nm to 1734 nm was used to identify the varieties of coated maize kernels. Spectral data were extracted and preprocessed. Logistic regression (LR) and support vector machine (SVM), convolutional neural network (CNN), recurrent neural network (RNN) and Long Short-Term Memory (LSTM) were used to build classification models. Furthermore, principal component analysis (PCA), CNN, RNN and LSTM were adopted to extract features. The extracted features were fused as the inputs of the classification models. Classification models using full spectra, extracted features and fused features obtained performances with the classification accuracy over 90% in the calibration, validation and prediction sets of most models. Models using extracted features obtained equivalently or slightly worse results than those using full spectra. The models using fused features all obtained good performances, with the classification accuracy over 90% in all sets. The overall results illustrated that near-infrared hyperspectral imaging with deep learning methods was a useful alternative for identifying coated maize varieties.", "journal": "INFRARED PHYSICS & TECHNOLOGY", "category": "Instruments & Instrumentation; Optics; Physics, Applied", "annotated_keywords": ["neural net", "deep learning", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000531824100110", "keywords": "hybrid analysis and modeling; Galerkin projection; proper orthogonal decomposition; long short-term memory; error correction", "title": "An Evolve-Then-Correct Reduced Order Model for Hidden Fluid Dynamics", "abstract": "In this paper, we put forth an evolve-then-correct reduced order modeling approach that combines intrusive and nonintrusive models to take hidden physical processes into account. Specifically, we split the underlying dynamics into known and unknown components. In the known part, we first utilize an intrusive Galerkin method projected on a set of basis functions obtained by proper orthogonal decomposition. We then present two variants of correction formula based on the assumption that the observed data are a manifestation of all relevant processes. The first method uses a standard least-squares regression with a quadratic approximation and requires solving a rank-deficient linear system, while the second approach employs a recurrent neural network emulator to account for the correction term. We further enhance our approach by using an orthonormality conforming basis interpolation approach on a Grassmannian manifold to address off-design conditions. The proposed framework is illustrated here with the application of two-dimensional co-rotating vortex simulations under modeling uncertainty. The results demonstrate highly accurate predictions underlining the effectiveness of the evolve-then-correct approach toward real-time simulations, where the full process model is not known a priori.", "journal": "MATHEMATICS", "category": "Mathematics", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000624567900017", "keywords": "Stomach infections; deep features; features optimization; fusion; classification", "title": "Multiclass Stomach Diseases Classification Using Deep Learning Features Optimization", "abstract": "In the area of medical image processing, stomach cancer is one of the most important cancers which need to be diagnose at the early stage. In this paper, an optimized deep learning method is presented for multiple stomach disease classification. The proposed method work in few important steps-preprocessing using the fusion of filtering images along with Ant Colony Optimization (ACO), deep transfer learning-based features extraction, optimization of deep extracted features using nature-inspired algorithms, and finally fusion of optimal vectors and classification using Multi-Layered Perceptron Neural Network (MLNN). In the feature extraction step, pre trained Inception V3 is utilized and retrained on selected stomach infection classes using the deep transfer learning step. Later on, the activation function is applied to Global Average Pool (GAP) for feature extraction. However, the extracted features are optimized through two different nature-inspired algorithms-Particle Swarm Optimization (PSO) with dynamic fitness function and Crow Search Algorithm (CSA). Hence, both methods' output is fused by a maximal value approach and classified the fused feature vector by MLNN. Two datasets are used to evaluate the proposed method-CUI WahStomach Diseases and Combined dataset and achieved an average accuracy of 99.5%. The comparison with existing techniques, it is shown that the proposed method shows significant performance.", "journal": "CMC-COMPUTERS MATERIALS & CONTINUA", "category": "Computer Science, Information Systems; Materials Science, Multidisciplinary", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000447864400001", "keywords": "Cognitive radio; Interference alignment; Spectrum sensing; Q-learning", "title": "Q-learning-based dynamic joint control of interference and transmission opportunities for cognitive radio", "abstract": "In cognitive radio (CR) system, secondary user (SU) should use available channels opportunistically when the primary user (PU) does not exist. In CR network, SUs have to detect the PU signal with sufficient sensing time to guarantee the detection probability and minimize the interference to the PU, while the CR system should have enough data transmission time to maximize the transmission opportunity of the SU. Therefore, the sensing time and data transmission time of the SU are generally considered as main optimization parameters to maximize the throughput of the CR system. In this paper, a separate sensing node is designated and the sensing is continuously performed using the interference alignment (IA) technique. In this paper, the designated sensing node estimates the interference ratio and transmission opportunity loss ratio. To satisfy the primary user's interference requirement and maximize secondary throughput we proposed dynamic adjustment mechanism for sensing slot time and sensing report interval using reinforcement learning in time-varying communication environment. The experimental results show that the proposed approach can minimize the interference on PU and enhance the transmission opportunity of SUs.", "journal": "EURASIP JOURNAL ON WIRELESS COMMUNICATIONS AND NETWORKING", "category": "Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": ["reinforcement learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000458439500001", "keywords": "Attribute extraction; natural language processing (NLP); long short-term memory (LSTM) network; Siamese network", "title": "Fault Diagnosis of Induction Motors Using Recurrence Quantification Analysis and LSTM with Weighted BN", "abstract": "Motor fault diagnosis has gained much attention from academic research and industry to guarantee motor reliability. Generally, there exist two major approaches in the feature engineering for motor fault diagnosis: (1) traditional feature learning, which heavily depends on manual feature extraction, is often unable to discover the important underlying representations of faulty motors; (2) state-of-the-art deep learning techniques, which have somewhat improved diagnostic performance, while the intrinsic characteristics of black box and the lack of domain expertise have limited the further improvement. To cover those shortcomings, in this paper, two manual feature learning approaches are embedded into a deep learning algorithm, and thus, a novel fault diagnosis framework is proposed for three-phase induction motors with a hybrid feature learning method, which combines empirical statistical parameters, recurrence quantification analysis (RQA) and long short-term memory (LSTM) neural network. In addition, weighted batch normalization (BN), a modification of BN, is designed to evaluate the contributions of the three feature learning approaches. The proposed method was experimentally demonstrated by carrying out the tests of 8 induction motors with 8 different faulty types. Results show that compared with other popular intelligent diagnosis methods, the proposed method achieves the highest diagnostic accuracy in both the original dataset and the noised dataset. It also verifies that RQA can play a bigger role in real-world applications for its excellent performance in dealing with the noised signals.", "journal": "SHOCK AND VIBRATION", "category": "Acoustics; Engineering, Mechanical; Mechanics", "annotated_keywords": ["neural net", "deep learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000338612800010", "keywords": "Modulation transfer function; Spatial frequency; Optical system; Neuro-fuzzy; ANFIS", "title": "RETRACTED: Adaptive neuro-fuzzy prediction of modulation transfer function of optical lens system (Retracted article. See vol.87, pg.153, 2017)", "abstract": "The quantitative assessment of image quality is an important consideration in any type of imaging system. The modulation transfer function (MTF) is a graphical description of the sharpness and contrast of an imaging system or of its individual components. The MTF is also known and spatial frequency response. The MTF curve has different meanings according to the corresponding frequency. The MTF of an optical system specifies the contrast transmitted by the system as a function of image size, and is determined by the inherent optical properties of the system. In this study, the adaptive neuro-fuzzy (ANFIS) estimator is designed and adapted to predict MTF value of the actual optical system. Neural network in ANFIS adjusts parameters of membership function in the fuzzy logic of the fuzzy inference system. The back propagation learning algorithm is used for training this network. This intelligent estimator is implemented using MATLAB/Simulink and the performances are investigated. The simulation results presented in this paper show the effectiveness of the developed method. (C) 2014 Elsevier B.V. All rights reserved.", "journal": "INFRARED PHYSICS & TECHNOLOGY", "category": "Instruments & Instrumentation; Optics; Physics, Applied", "annotated_keywords": ["learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000325237800014", "keywords": "Multi-agent systems; Machine learning; Modular reinforcement learning; Q-learning", "title": "A novel modular Q-learning architecture to improve performance under incomplete learning in a grid soccer game", "abstract": "Multi-agent reinforcement learning methods suffer from several deficiencies that are rooted in the large state space of multi-agent environments. This paper tackles two deficiencies of multi-agent reinforcement learning methods: their slow learning rate, and low quality decision-making in early stages of learning. The proposed methods are applied in a grid-world soccer game. In the proposed approach, modular reinforcement learning is applied to reduce the state space of the learning agents from exponential to linear in terms of the number of agents. The modular model proposed here includes two new modules, a partial-module and a single-module. These two new modules are effective for increasing the speed of learning in a soccer game. We also apply the instance-based learning concepts, to choose proper actions in states that are not experienced adequately during learning. The key idea is to use neighbouring states that have been explored sufficiently during the learning phase. The results of experiments in a grid-soccer game environment show that our proposed methods produce a higher average reward compared to the situation where the proposed method is not applied to the modular structure. (C) 2013 Elsevier Ltd. All rights reserved.", "journal": "ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE", "category": "Automation & Control Systems; Computer Science, Artificial Intelligence; Engineering, Multidisciplinary; Engineering, Electrical & Electronic", "annotated_keywords": ["reinforcement learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000425397500010", "keywords": "Action recognition; 3D convolution neural networks", "title": "Two-Stream 3-D convNet Fusion for Action Recognition in Videos With Arbitrary Size and Length", "abstract": "3-D convolutional neural networks (3-D-convNets) have been very recently proposed for action recognition in videos, and promising results are achieved. However, existing 3-D-convNets has two \"artificial\" requirements that may reduce the quality of video analysis: 1) It requires a fixed-sized (e.g., 112 x 112) input video; and 2) most of the 3-D-convNets require a fixed-length input (i.e., video shots with fixed number of frames). To tackle these issues, we propose an end-to-end pipeline named Two-stream 3-D-convNet Fusion, which can recognize human actions in videos of arbitrary size and length using multiple features. Specifically, we decompose a video into spatial and temporal shots. By taking a sequence of shots as input, each stream is implemented using a spatial temporal pyramid pooling (STPP) convNet with a long short-term memory (LSTM) or CNN-E model, softmax scores of which are combined by a late fusion. We devise the STPP convNet to extract equal-dimensional descriptions for each variable-size shot, and we adopt the LSTM/CNN-E model to learn a global description for the input video using these time-varying descriptions. With these advantages, our method should improve all 3-D CNN-based video analysis methods. We empirically evaluate our method for action recognition in videos and the experimental results show that our method outperforms the state-of-the-art methods (both 2-D and 3-D based) on three standard benchmark datasets (UCF101, HMDB51 and ACT datasets).", "journal": "IEEE TRANSACTIONS ON MULTIMEDIA", "category": "Computer Science, Information Systems; Computer Science, Software Engineering; Telecommunications", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000718847200008", "keywords": "Cough detection; Deep learning; BiLSTM; C-BiLSTM; Boundary regression", "title": "Automatic cough detection from realistic audio recordings using C-BiLSTM with boundary regression", "abstract": "Automatic cough detection in the patients' realistic audio recordings is of great significance to diagnose and monitor respiratory diseases, such as COVID-19. Many detection methods have been developed so far, but they are still unable to meet the practical requirements. In this paper, we present a deep convolutional bidirectional long short-term memory (C-BiLSTM) model with boundary regression for cough detection, where cough and noncough parts need to be classified and located. We added convolutional layers before the LSTM to enhance the cough features and preserve the temporal information of the audio data. Considering the importance of the cough event integrity for subsequent analysis, the novel model includes an embedded boundary regression on the last feature map for both higher detection accuracy and more accurate boundaries. We delicately designed, collected and labelled a realistic audio dataset containing recordings of patients with respiratory diseases, named the Corp Dataset. 168 h of recordings with 9969 coughs from 42 different patients are included. The dataset is published online on the MARI Lab website (https://mari.tongji.edu.cn/info/1012/1030.htm). The results show that the system achieves a sensitivity of 84.13%, a specificity of 99.82% and an intersection-over-union (IoU) of 0.89, which is significantly superior to other related models. With the proposed method, all the criteria on cough detection significantly increased. The open source Corp Dataset provides useful material and a benchmark for researchers investigating cough detection. We propose the state-of-the-art system with boundary regression, laying the foundation for identifying cough sounds in real-world audio data.", "journal": "BIOMEDICAL SIGNAL PROCESSING AND CONTROL", "category": "Engineering, Biomedical", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000458569300017", "keywords": "intelligent transportation system; feature extraction; social network; sentiment classification", "title": "Fuzzy Ontology and LSTM-Based Text Mining: A Transportation Network Monitoring System for Assisting Travel", "abstract": "Intelligent Transportation Systems (ITSs) utilize a sensor network-based system to gather and interpret traffic information. In addition, mobility users utilize mobile applications to collect transport information for safe traveling. However, these types of information are not sufficient to examine all aspects of the transportation networks. Therefore, both ITSs and mobility users need a smart approach and social media data, which can help ITSs examine transport services, support traffic and control management, and help mobility users travel safely. People utilize social networks to share their thoughts and opinions regarding transportation, which are useful for ITSs and travelers. However, user-generated text on social media is short in length, unstructured, and covers a broad range of dynamic topics. The application of recent Machine Learning (ML) approach is inefficient for extracting relevant features from unstructured data, detecting word polarity of features, and classifying the sentiment of features correctly. In addition, ML classifiers consistently miss the semantic feature of the word meaning. A novel fuzzy ontology-based semantic knowledge with Word2vec model is proposed to improve the task of transportation features extraction and text classification using the Bi-directional Long Short-Term Memory (Bi-LSTM) approach. The proposed fuzzy ontology describes semantic knowledge about entities and features and their relation in the transportation domain. Fuzzy ontology and smart methodology are developed in Web Ontology Language and Java, respectively. By utilizing word embedding with fuzzy ontology as a representation of text, Bi-LSTM shows satisfactory improvement in both the extraction of features and the classification of the unstructured text of social media.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000636345900002", "keywords": "Cyber-physical systems; Uncertainty; Self-healing; Model execution; Reinforcement learning; Empirical evaluation", "title": "Testing self-healing cyber-physical systems under uncertainty with reinforcement learning: an empirical study", "abstract": "Self-healing is becoming an essential feature of Cyber-Physical Systems (CPSs). CPSs with this feature are named Self-Healing CPSs (SH-CPSs). SH-CPSs detect and recover from errors caused by hardware or software faults at runtime and handle uncertainties arising from their interactions with environments. Therefore, it is critical to test if SH-CPSs can still behave as expected under uncertainties. By testing an SH-CPS in various conditions and learning from testing results, reinforcement learning algorithms can gradually optimize their testing policies and apply the policies to detect failures, i.e., cases that the SH-CPS fails to behave as expected. However, there is insufficient evidence to know which reinforcement learning algorithms perform the best in terms of testing SH-CPSs behaviors including their self-healing behaviors under uncertainties. To this end, we conducted an empirical study to evaluate the performance of 14 combinations of reinforcement learning algorithms, with two value function learning based methods for operation invocations and seven policy optimization based algorithms for introducing uncertainties. Experimental results reveal that the 14 combinations of the algorithms achieved similar coverage of system states and transitions, and the combination of Q-learning and Uncertainty Policy Optimization (UPO) detected the most failures among the 14 combinations. On average, the Q-Learning and UPO combination managed to discover two times more failures than the others. Meanwhile, the combination took 52% less time to find a failure. Regarding scalability, the time and space costs of the value function learning based methods grow, as the number of states and transitions of the system under test increases. In contrast, increasing the system's complexity has little impact on policy optimization based algorithms.", "journal": "EMPIRICAL SOFTWARE ENGINEERING", "category": "Computer Science, Software Engineering", "annotated_keywords": ["reinforcement learning", "reinforcement learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000561414100001", "keywords": "Multiclass obstacles; Bayesian network; NPC plus PC; EM training algorithm; obstacle classification", "title": "Multiclass obstacles detection and classification using stereovision and Bayesian network for intelligent vehicles", "abstract": "Intelligent vehicles should be able to detect various obstacles and also identify their types so that the vehicles can take an appropriate level of protection and intervention. This article presents a method of detecting and classifying multiclass obstacles for intelligent vehicles. A stereovision-based method is used to segment obstacles from traffic background and measure three-dimensional geometrical features. A Bayesian network (BN) model has been established to further classify them into five classes, including pedestrian, cyclist, car, van, and truck. The BN model is trained using substantial data samples. The optimized structure of the model is determined from the necessary path condition method with a presupposition constraint (NPC+PC). The conditional probability table of the discrete nodes and the conditional probability distribution of the continuous nodes are determined from expectation maximization (EM) training algorithm with consideration of prior domain knowledge. Experiments were conducted using the object detection data set on the public KITTI benchmark, and the results show that the proposed BN model exhibits an excellent performance for obstacle classification while the full pipeline of the method including detection and classification is in the upper middle level compared with other existing methods.", "journal": "INTERNATIONAL JOURNAL OF ADVANCED ROBOTIC SYSTEMS", "category": "Robotics", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000378961200018", "keywords": "Chaos; Dendritic neuron model; Financial time series; Lyapunov exponent; Phase space reconstruction", "title": "Financial time series prediction using a dendritic neuron model", "abstract": "As a complicated dynamic system, financial time series calls for an appropriate forecasting model. In this study, we propose a neuron model based on dendritic mechanisms and a phase space reconstruction (PSR) to analyze the Shanghai Stock Exchange Composite Index, Deutscher Aktienindex, N225, and DJI Average. The PSR allows us to reconstruct the financial time series, so we can prove that attractors exist for the systems constructed. Thus, the attractors obtained can be observed intuitively, in a three-dimensional search space, thereby allowing us to analyze the characteristics of dynamic systems. In addition, using the reconstructed phase space, we confirmed the chaotic properties and the reciprocal to determine the limit of prediction through the maximum Lyapunov exponent. We also made short-term predictions based on the nonlinear approximating dendritic neuron model, where the experimental results showed that the proposed methodology which hybridizes PSR and the dendritic model performed better than traditional multi-layered perceptron, the Elman neural network, the single multiplicative neuron model and the neuro-fuzzy inference system in terms of prediction accuracy and training time. Hopefully, this hybrid technology is capable to advance the research for financial time series and provide an effective solution to risk management. (C) 2016 Elsevier B.V. All rights reserved.", "journal": "KNOWLEDGE-BASED SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000395506900019", "keywords": "Acacia melanoxylon; heartwood; pulp properties; Multiple Linear Regression; CART; Multi-Layer Perceptron (MLP); Support Vector Machines (SVM)", "title": "Influence of Heartwood on Wood Density and Pulp Properties Explained by Machine Learning Techniques", "abstract": "The aim of this work is to develop a tool to predict some pulp properties e.g., pulp yield, Kappa number, ISO brightness (ISO 2470:2008), fiber length and fiber width, using the sapwood and heartwood proportion in the raw-material. For this purpose, Acacia melanoxylon trees were collected from four sites in Portugal. Percentage of sapwood and heartwood, area and the stem eccentricity (in N-S and E-W directions) were measured on transversal stem sections of A. melanoxylon R. Br. The relative position of the samples with respect to the total tree height was also considered as an input variable. Different configurations were tested until the maximum correlation coefficient was achieved. A classical mathematical technique (multiple linear regression) and machine learning methods (classification and regression trees, multi-layer perceptron and support vector machines) were tested. Classification and regression trees (CART) was the most accurate model for the prediction of pulp ISO brightness (R = 0.85). The other parameters could be predicted with fair results (R = 0.64-0.75) by CART. Hence, the proportion of heartwood and sapwood is a relevant parameter for pulping and pulp properties, and should be taken as a quality trait when assessing a pulpwood resource.", "journal": "FORESTS", "category": "Forestry", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000405347800003", "keywords": "Entity recognition; Recurrent neural network; Clinical notes; Deep learning; Sequence labeling", "title": "Entity recognition from clinical texts via recurrent neural network", "abstract": "Background: Entity recognition is one of the most primary steps for text analysis and has long attracted considerable attention from researchers. In the clinical domain, various types of entities, such as clinical entities and protected health information (PHI), widely exist in clinical texts. Recognizing these entities has become a hot topic in clinical natural language processing (NLP), and a large number of traditional machine learning methods, such as support vector machine and conditional random field, have been deployed to recognize entities from clinical texts in the past few years. In recent years, recurrent neural network (RNN), one of deep learning methods that has shown great potential on many problems including named entity recognition, also has been gradually used for entity recognition from clinical texts. Methods: In this paper, we comprehensively investigate the performance of LSTM (long-short term memory), a representative variant of RNN, on clinical entity recognition and protected health information recognition. The LSTM model consists of three layers: input layer - generates representation of each word of a sentence; LSTM layer outputs another word representation sequence that captures the context information of each word in this sentence; Inference layer - makes tagging decisions according to the output of LSTM layer, that is, outputting a label sequence. Results: Experiments conducted on corpora of the 2010, 2012 and 2014 i2b2 NLP challenges show that LSTM achieves highest micro-average F1-scores of 85.81% on the 2010 i2b2 medical concept extraction, 92.29% on the 2012 i2b2 clinical event detection, and 94.37% on the 2014 i2b2 de-identification, which is considerably competitive with other state-of-the-art systems. Conclusions: LSTM that requires no hand-crafted feature has great potential on entity recognition from clinical texts. It outperforms traditional machine learning methods that suffer from fussy feature engineering. A possible future direction is how to integrate knowledge bases widely existing in the clinical domain into LSTM, which is a case of our future work. Moreover, how to use LSTM to recognize entities in specific formats is also another possible future direction.", "journal": "BMC MEDICAL INFORMATICS AND DECISION MAKING", "category": "Medical Informatics", "annotated_keywords": ["neural net", "neural net", "machine learning", "natural language processing", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000495976600006", "keywords": "SVSMRG-SBS model; LSTM; surface flow; baseflow; semi-distributed hydrological model; montane catchment; runoff simulation; flood simulation", "title": "Nonintrusive reduced order modeling framework for quasigeostrophic turbulence", "abstract": "In this study, we present a nonintrusive reduced order modeling (ROM) framework for large-scale quasistationary systems. The framework proposed herein exploits the time series prediction capability of long short-term memory (LSTM) recurrent neural network architecture such that (1) in the training phase, the LSTM model is trained on the modal coefficients extracted from the high-resolution data snapshots using proper orthogonal decomposition (POD) transform, and (2) in the testing phase, the trained model predicts the modal coefficients for the total time recursively based on the initial time history. Hence, no prior information about the underlying governing equations is required to generate the ROM. To illustrate the predictive performance of the proposed framework, the mean flow fields and time series response of the field values are reconstructed from the predicted modal coefficients by using an inverse POD transform. As a representative benchmark test case, we consider a two-dimensional quasigeostrophic ocean circulation model which, in general, displays an enormous range of fluctuating spatial and temporal scales. We first demonstrate that the conventional Galerkin projection-based reduced order modeling of such systems requires a high number of POD modes to obtain a stable flow physics. In addition, ROM-Galerkin projection (ROM-GP) does not seem to capture the intermittent bursts appearing in the dynamics of the first few most energetic modes. However, the proposed nonintrusive ROM framework based on LSTM (ROM-LSTM) yields a stable solution even for a small number of POD modes. We also observe that the ROM-LSTM model is able to capture quasiperiodic intermittent bursts accurately, and yields a stable and accurate mean flow dynamics using the time history of a few previous time states, denoted as the lookback time window in this paper. We show several features of ROM-LSTM framework such as significantly higher accuracy than ROM-GP, and faster performance using larger time step size. Throughout the paper, we demonstrate our findings in terms of time series evolution of the field values and mean flow patterns, which suggest that the proposed fully nonintrusive ROM framework is robust and capable of predicting chaotic nonlinear fluid flows in an extremely efficient way compared to the conventional projection-based ROM framework.", "journal": "PHYSICAL REVIEW E", "category": "Physics, Fluids & Plasmas; Physics, Mathematical", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000457726700022", "keywords": "Landfill leachate; Input optimization; Artificial neural network-multilayers perceptron (ANN-MLP); Regression support vector machine (R-SVM)", "title": "Leachate generation rate modeling using artificial intelligence algorithms aided by input optimization method for an MSW landfill", "abstract": "Leachate is one of the main surface water pollution sources in Selangor State (SS), Malaysia. The prediction of leachate amounts is elementary in sustainable waste management and leachate treatment processes, before discharging to surrounding environment. In developing countries, the accurate evaluation of leachate generation rates has often considered a challenge due to the lack of reliable data and high measurement costs. Leachate generation is related to several factors, including meteorological data, waste generation rates, and landfill design conditions. The high variations in these factors lead to complicating leachate modeling processes. This study aims at identifying the key elements contributing to leachate production and developing various AI-based models to predict leachate generation rates. These models included Artificial Neural Network (ANN)-Multi-linear perceptron (MLP) with single and double hidden layers, and support vector machine (SVM) regression time series algorithms. Various performance measures were applied to evaluate the developed model's accuracy. In this study, input optimization process showed that three inputs were acceptable for modeling the leachate generation rates, namely dumped waste quantity, rainfall level, and emanated gases. The initial performance analysis showed that ANN-MLP2 modelwhich applies two hidden layersachieved the best performance, then followed by ANN-MLP1 modelwhich applies one hidden layer and three inputswhile SVM model gave the lowest performance. Ranges and frequency of relative error (RE%) also demonstrate that ANN-MLP models outperformed SVM models. Furthermore, low and peak flow criterion (LFC and PFC) assessment of leachate inflow values in ANN-MLP model with two hidden layers made more accurate values than other models. Since minimizing data collection and processing efforts as well as minimizing modeling complexity are critical in the hydrological modeling process, the applied input optimization process and the developed models in this study were able to provide a good performance in the modeling of leachate generation efficiently.", "journal": "ENVIRONMENTAL SCIENCE AND POLLUTION RESEARCH", "category": "Environmental Sciences", "annotated_keywords": ["artificial intelligen"], "label": "1", "title_label": "1"}
{"id": "WOS:000585202000001", "keywords": "reinforcement learning; autonomous vehicle; vehicle dynamics; lane keeping; planning agents; Q-learning; policy gradient; Monte Carlo Tree Search", "title": "Design of a Reinforcement Learning-Based Lane Keeping Planning Agent for Automated Vehicles", "abstract": "Featured Application The presented method can be used as a real-time trajectory following algorithm for autonomous vehicles using prediction based on lookahead information. Reinforcement learning-based approaches are widely studied in the literature for solving different control tasks for Connected and Autonomous Vehicles, from which this paper deals with the problem of lateral control of a dynamic nonlinear vehicle model, performing the task of lane-keeping. In this area, the appropriate formulation of the goals and environment information is crucial, for which the research outlines the importance of lookahead information, enabling to accomplish maneuvers with complex trajectories. Another critical part is the real-time manner of the problem. On the one hand, optimization or search based methods, such as the presented Monte Carlo Tree Search method, can solve the problem with the trade-off of high numerical complexity. On the other hand, single Reinforcement Learning agents struggle to learn these tasks with high performance, though they have the advantage that after the training process, they can operate in a real-time manner. Two planning agent structures are proposed in the paper to resolve this duality, where the machine learning agents aid the tree search algorithm. As a result, the combined solution provides high performance and low computational needs.", "journal": "APPLIED SCIENCES-BASEL", "category": "Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials Science, Multidisciplinary; Physics, Applied", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000433890600003", "keywords": "CANFIS; MLPNN; Rainfall-runoff modeling; Hilly region; Naula watershed", "title": "Rainfall-runoff modeling in hilly watershed using heuristic approaches with gamma test", "abstract": "In this study, daily rainfall-runoff modeling was done using co-active neuro-fuzzy inference system (CANFIS) and multi-layer perceptron neural network (MLPNN) approaches in the hilly Naula watershed of Ramganga River in Uttarakhand, India. The daily observed rainfall and runoff data from June 1, 2000, to October 31, 2004, were used for training and testing of the applied models. Before starting the modeling process, the gamma test (GT) was used to select the best combination of input variables for each model. The simulated values of runoff from CANFIS and MLPNN models were compared with the observed ones with respect to root mean squared error (RMSE), Nash-Sutcliffe efficiency (CE), Pearson correlation coefficient (PCC). This study provides a conclusive evidence that the CANFIS shows better accuracy than the MLPNN models. Therefore, according to the best fitting CANFIS-10 model, the runoff of the present day depends on rainfall and runoff of current and previous 2 days for the studied area.", "journal": "ARABIAN JOURNAL OF GEOSCIENCES", "category": "Geosciences, Multidisciplinary", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000474718400001", "keywords": "Human health; air quality; air pollution; convolutional neural network; extreme learning machine", "title": "Monitoring air pollution by deep features and extreme learning machine", "abstract": "Smartphones have different kinds of applications thathelp to promote health and care of humans. This paper proposes a practical and low-cost method for predicting air pollution which is applicable to the smartphones based on an image taken by their camera. To find the best method, in the first approach, some convenionalconventional feature extraction methods including wavelet transform, scale-invariant feature transform and histogram of oriented gradients are implemented. Then, to reduce the dimension of the extracted feature vectors, principal component analysis is employed. For classification of the obtained reduced feature vectors, multilayer perceptron is employed. In the second approach, the performance of convolutional neural network (CNN) in classifying the sky images in terms of air quality is investigated. In CNN, the fully connected classifier can be replaced by other classifiers such as extreme learning machine (ELM). The results illustrate that if the deep features obtained by CNN are fed to theELM, an accuracy of 66.92% in predicting the level of air quality is achieved, which is higher than the results of other previous and conventional methods.", "journal": "JOURNAL OF EXPERIMENTAL & THEORETICAL ARTIFICIAL INTELLIGENCE", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000445214600002", "keywords": "Landfill leachate; Input optimization; ANFIS modeling system; Sanitary landfill", "title": "Predicting sanitary landfill leachate generation in humid regions using ANFIS modeling", "abstract": "Landfill leachate is one of the sources of surface water pollution in Selangor State (SS), Malaysia. Leachate volume prediction is essential for sustainable waste management and leachate treatment processes. The accurate estimation of leachate generation rates is often considered a challenge, especially in developing countries, due to the lack of reliable data and high measurement costs. Leachate generation is related to several variable factors, including meteorological data, waste generation rates, and landfill design conditions. Large variations in these factors lead to complicated leachate modeling processes. The aims of this study are to determine the key elements contributing to leachate production and then develop an adaptive neural fuzzy inference system (ANFIS) model to predict leachate generation rates. Accuracy of the final model performance was tested and evaluated using the root mean square error (RMSE), the mean absolute error (MAE), and the correlation coefficient (R). The study results defined dumped waste quantity, rainfall level, and emanated gases as the most significant contributing factors in leachate generation. The best model structure consisted of two triangular fuzzy membership functions and a hybrid training algorithm with eight fuzzy rules. The proposed ANFIS model showed a good performance with an overall correlation coefficient of 0.952.", "journal": "ENVIRONMENTAL MONITORING AND ASSESSMENT", "category": "Environmental Sciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000585122200001", "keywords": "symmetry; earned value management (EVM); artificial neural networks (ANNs); multiple regression analysis; road industry", "title": "Improving the Results of the Earned Value Management Technique Using Artificial Neural Networks in Construction Projects", "abstract": "The cost, time and scope of a construction project are key parameters for its success. Thus, predicting these indices is indispensable. Correct and accurate prediction of cost throughout the progress of a project gives project managers the chance to identify projects that need revision in their schedules in order to result in the maximum benefit. The aim of this study is to minimize the shortcomings of the Earned Value Management (EVM) method using an Artificial Neural Network (ANN) and multiple regression analysis in order to predict project cost indices more precisely. A total of 50 road construction projects in Fars Province, Iran, were selected for analysis in this research. An ANN model was used to predict the projects' cost performance indices, thereby creating a more accurate symmetry between the predicted and actual cost by considering factors that influence project success. The input data of the ANN model were analysed in MATLAB software. A multiple regression model was also used as another analytical tool to validate the outcome of the ANN. The results showed that the ANN model resulted in a lower Mean Squared Error (MSE) and a greater correlation coefficient than both the traditional EVM model and the multiple regression model.", "journal": "SYMMETRY-BASEL", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000387952000002", "keywords": "RBF neural network; Battery energy storage system; Energy management; Frequency and voltage control; Microgrid; Optimization", "title": "RBF neural network-based online intelligent management of a battery energy storage system for stand-alone microgrids", "abstract": "Background: An offline optimization approach based on energy storage management response in a microgrid was not fast and not reliable enough to control and adjust the system efficiently after the loss of the utility grid. Thus, it causes system inefficiency and collapse in the presence of violent changes of loads or outage of distributed generations. To solve such a problem, more real-time management is needed. Any changes in loads/generations should be compensated successfully by a battery energy storage system (BESS) in a short period of time. Methods: This paper presents a new method for the intelligent online management of both active and reactive power of a BESS based on a radial basis function neural network (RBFNN) incorporating particle swarm optimization (PSO) to prevent the stand-alone microgrid from instability and system collapse. BESS is centrally controlled by a controller developed by the proposed RBFNN. PSO is used to determine the optimized active and reactive power at every load/generation changing situation to monitor the effect of system frequency, voltage, and reference power regulation. These optimized power data are then employed as target data for the RBFNN generalization and training process. To enable the online updating of the operating parameters, the proposed RBFNN is implemented in the management process. With an appropriate RBFNN training, the optimum active and reactive power can directly be obtained without the necessity of performing the PSO optimization process at any change of load/generation. Results: The results show that the predictive results of the proposed RBFNN model only slightly differed from the target results based on PSO and have a minimum statistical error compared to the predictive results based on the multilayer perceptron neural network (MLPNN) model. Conclusions: The proposed RBFNN is suitable for the online estimation of the active and reactive power of BESS and can be used for real-time energy storage management as an online controller.", "journal": "ENERGY SUSTAINABILITY AND SOCIETY", "category": "Green & Sustainable Science & Technology; Energy & Fuels", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000475958000007", "keywords": "Gaussian process; Online learning; Kernel methods; Random feature; Regression", "title": "GoGP: scalable geometric-based Gaussian process for online regression", "abstract": "One of the most challenging problems in Gaussian process regression is to cope with large-scale datasets and to tackle an online learning setting where data instances arrive irregularly and continuously. In this paper, we introduce a novel online Gaussian process model that scales efficiently with large-scale datasets. Our proposed GoGP is constructed based on the geometric and optimization views of the Gaussian process regression, hence termed geometric-based online GP (GoGP). We developed theory to guarantee that with a good convergence rate our proposed algorithm always offers a sparse solution, which can approximate the true optima up to any level of precision specified a priori. Moreover, to further speed up the GoGP accompanied with a positive semi-definite and shift-invariant kernel such as the well-known Gaussian kernel and also address the curse of kernelization problem, wherein the model size linearly rises with data size accumulated over time in the context of online learning, we proposed to approximate the original kernel using the Fourier random feature kernel. The model of GoGP with Fourier random feature (i.e., GoGP-RF) can be stored directly in a finite-dimensional random feature space, hence being able to avoid the curse of kernelization problem and scalable efficiently and effectively with large-scale datasets. We extensively evaluated our proposed methods against the state-of-the-art baselines on several large-scale datasets for online regression task. The experimental results show that our GoGP(s) delivered comparable, or slightly better, predictive performance while achieving a magnitude of computational speedup compared with its rivals under online setting. More importantly, its convergence behavior is guaranteed through our theoretical analysis, which is rapid and stable while achieving lower errors.", "journal": "KNOWLEDGE AND INFORMATION SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000336371900038", "keywords": "Acute lymphoblastic leukemia; Cell morphology; Quantitative microscopy image analysis; Functional expansion ensemble classifier", "title": "An ensemble classifier system for early diagnosis of acute lymphoblastic leukemia in blood microscopic images", "abstract": "Leukemia is a malignant neoplasm of the blood or bone marrow that affects both children and adults and remains a leading cause of death around the world. Acute lymphoblastic leukemia (ALL) is the most common type of leukemia and is more common among children and young adults. ALL diagnosis through microscopic examination of the peripheral blood and bone marrow tissue samples is performed by hematologists and has been an indispensable technique long since. However, such visual examinations of blood samples are often slow and are also limited by subjective interpretations and less accurate diagnosis. The objective of this work is to improve the ALL diagnostic accuracy by analyzing morphological and textural features from the blood image using image processing. This paper aims at proposing a quantitative microscopic approach toward the discrimination of lymphoblasts (malignant) from lymphocytes (normal) in stained blood smear and bone marrow samples and to assist in the development of a computer-aided screening of ALL. Automated recognition of lymphoblasts is accomplished using image segmentation, feature extraction, and classification over light microscopic images of stained blood films. Accurate and authentic diagnosis of ALL is obtained with the use of improved segmentation methodology, prominent features, and an ensemble classifier, facilitating rapid screening of patients. Experimental results are obtained and compared over the available image data set. It is observed that an ensemble of classifiers leads to 99 % accuracy in comparison with other standard classifiers, i.e., naive Bayesian (NB), K-nearest neighbor (KNN), multilayer perceptron (MLP), radial basis functional network (RBFN), and support vector machines (SVM).", "journal": "NEURAL COMPUTING & APPLICATIONS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000679687000004", "keywords": "Memristor; stochasticity; deep stochastic spiking neural networks", "title": "An All-Memristor Deep Spiking Neural Computing System: A Step Toward Realizing the Low-Power Stochastic Brain", "abstract": "Deep analog artificial neural networks (ANNs) perform complex classification problems with remarkably high accuracy. However, they rely on humongous amount of power to perform the calculations, veiling the accuracy benefits. The biological brain, on the other hand, is significantly more powerful than such networks and consumes orders of magnitude less power, indicating us about some conceptual mismatch. Given that the biological nett rons comm unicate using energy efficient trains of spikes, and the behavior is nondeterministic, incorporating these effects in deep artificial neural networks may drive us few steps toward a more realistic neuron. In this paper, we propose how the inherent stochasticity of nanoscale resistive devices can be harnessed to emulate the functionality of a spiking neuron that can be incorporated in deep stochastic spiking neural networks (SNN). At the algorithmic level, we propose how the training can be modified to convert an ANN to an SNN while supporting the stochastic activation function offered by these devices. We devise circuit architectures to incorporate stochastic memristive neurons along with memristive crossbars, which perform the functionality of the synaptic weights. We tested the proposed all-memristor deep stochastic SNN for image classification and observed only about 1% degradation in accuracy with the ANN baseline after incorporating the circuit and device related nonidealities. We witnessed that the network is robust to certain variations and consumes similar to 6.4 x less energy than its complementary metal oxide semiconductor (CMOS) counterpart.", "journal": "IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000626579600055", "keywords": "Tomography; Image reconstruction; Imaging; Conductivity; Lung; Shape; Computational modeling; Electrical impedance tomography; convolutional neural network; radial basis function; lung reconstruction", "title": "Shape Reconstruction With Multiphase Conductivity for Electrical Impedance Tomography Using Improved Convolutional Neural Network Method", "abstract": "Image reconstruction of Electrical Impedance Tomography (EIT) is a highly nonlinear ill-posed inverse problem, which is sensitive to the measurement noise and model errors. An improved Convolutional Neural Network (CNN) method is proposed for the EIT lung imaging. The proposed method is optimized based on the Visual Geometry Group (VGG) model, adding the batch normalization (BN) layer, ELU activation function, global average pooling (GAP) layer, and radial basis function (RBF) neural network. These optimizations help speed up network convergence, and improve reconstruction accuracy and robustness. Nearly 10 thousand EIT simulation models generated from chest CT images of 60 patients are used for the network training. The chest deformation, lung hyperdilation and atelectasis are randomly simulated during the model generation process. The proposed method after training is tested through a series of simulation data and experimental models. The reconstruction quality is quantitatively compared by calculating the root mean square error (RMSE) and image correlation coefficient (ICC). On average, the proposed method achieves 0.082 RMSE and 0.892 ICC through experimental results. The proposed method achieves high-resolution and robust shape reconstructions with multiphase conductivity for EIT lung imaging, especially in the presence of the measurement noise and interference. The proposed method is promising in providing quantitative images for potential clinical applications, such as human thorax imaging.", "journal": "IEEE SENSORS JOURNAL", "category": "Engineering, Electrical & Electronic; Instruments & Instrumentation; Physics, Applied", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000732808200001", "keywords": "Ear; Image segmentation; Adhesives; Feature extraction; Agriculture; Semantics; Deep learning; Field conditions; fully convolutional neural network (FCNN); regression convolutional neural network (RCNN); wheat ears counting", "title": "SSRNet: In-Field Counting Wheat Ears Using Multi-Stage Convolutional Neural Network", "abstract": "Fast and accurate counting of wheat ears in field conditions is a key element for determining wheat yield. To obtain the number of wheat ears in a field, we propose a new counting algorithm based on computer vision. This algorithm counts wheat ears in remote images through semantic segmentation regression network (SSRNet). SSRNet is a multistage convolutional neural network that we propose to achieve counting problems through regression. In SSRNet, first, the original image is cropped to increase the amount of data. This method effectively solves the small sample dataset. Next, based on the cropping results, we build a fully convolutional neural network (FCNN) to segment wheat ears in field conditions. FCNN increases the accuracy of wheat ears counting by accurately segmenting wheat ears in a complex background. Then, we build a regression convolutional neural network (RCNN) to count wheat ears based on the segmentation results of FCNN. In RCNN, we propose a new activation function positive rectification linear unit (PrLU) to process the last layer of the fully connected layer, so that RCNN can effectively count the number of wheat ears in the image. Finally, a counting strategy is proposed to count the number of wheat ears in the original image. To verify the counting performance of SSRNet, we compare the counting result of SSRNet with the real value of manual statistics. The results show that the average accuracy (Acc), R-2, and root mean squared error (RMSE) of the SSRNet count results on the test set in this article are 0.980, 0.996, and 9.437, respectively. It can be seen from the results that our proposed method can accurately count wheat ears in field conditions. At the same time, the counting time (0.11 s) shows that SSRNet can quickly estimate the number of wheat ears in field conditions. We concluded that this study can provide important technical support for the high-throughput field wheat ears counting task in large-scale phenotyping work.", "journal": "IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING", "category": "Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000725935000002", "keywords": "Complex network; Network embedding; Network reconstruction; Link prediction; Graph autoencoder", "title": "Graph autoencoder for directed weighted network", "abstract": "Network embedding technology transforms network structure into node vectors, which reduces the complexity of representation and can be effectively applied to tasks such as classification, network reconstruction and link prediction. The main concern of network embedding is to keep the local structural features while effectively capturing the global features of the network. The \"shallow\" network representation models cannot capture the deep nonlinear features of the network, and the generated network embedding is usually not the optimal solution. In this paper, a new graph autoencoder-based network representation model combines the first- and second-order proximity to evaluate the performance of network embedding. Aiming at the shortcomings of existing network representation methods in weighted and directed networks, on one hand, the concepts of receiving vector and sending vector are introduced with a simplification of decoding part of the neural network which reduces computation complexity; on the other hand, a measurement index based on node degree is proposed to better emphasize the weighted information in the application of network representation. Experiments including directed weighted networks and undirected unweighted networks show that the proposed method achieves better results than the baseline methods for network reconstruction and link prediction tasks and is of higher computation efficiency than previous graph autoencoder algorithms. Besides, the proposed weighted index is able to improve performances of all baseline methods as an external assistance.", "journal": "SOFT COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000720521100064", "keywords": "Convergence; Neural networks; Asymptotic stability; Adaptive systems; Control systems; Upper bound; Nonlinear systems; Dead zone; global neural network control; output constraint; predefined accuracy; predefined time control", "title": "Global Predefined Time and Accuracy Adaptive Neural Network Control for Uncertain Strict-Feedback Systems With Output Constraint and Dead Zone", "abstract": "This article proposes a global adaptive neural network tracking control method for uncertain strict-feedback nonlinear system with output constraint and dead zone to achieve predefined-time convergence of the tracking error to predefined accuracy. First, an integral-type Barrier Lyapunov function (BLF) is constructed to handle output constraint. Next, radial basis function neural network (RBFNN) control and robust control are used to tackle unknown nonlinear function. The continuous switching function is designed to switch RBFNN control to robust control when the arguments of the unknown function exceed the active region of neural network. Utilizing the property of radial basis function, we derive the upper bound of the term containing the unknown nonlinear function and design adaptive laws to determine the derived upper bound and robust control gain. Then, the predefined time virtual control inputs are obtained and their derivatives are estimated by finite time differentiator. Finally, we use the dead zone inverse technique to obtain the actual control input. Stability analysis shows the presented control scheme achieves global convergence of the errors to predefined accuracy within predefined time. The simulation results verify the effectiveness of the presented control scheme.", "journal": "IEEE TRANSACTIONS ON SYSTEMS MAN CYBERNETICS-SYSTEMS", "category": "Automation & Control Systems; Computer Science, Cybernetics", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000690256300001", "keywords": "bowel sound; automated analysis; intestine; motility; recording", "title": "Automated Bowel Sound Analysis: An Overview", "abstract": "Despite technological progress, we lack a consensus on the method of conducting automated bowel sound (BS) analysis and, consequently, BS tools have not become available to doctors. We aimed to briefly review the literature on BS recording and analysis, with an emphasis on the broad range of analytical approaches. Scientific journals and conference materials were researched with a specific set of terms (Scopus, MEDLINE, IEEE) to find reports on BS. The research articles identified were analyzed in the context of main research directions at a number of centers globally. Automated BS analysis methods were already well developed by the early 2000s. Accuracy of 90% and higher had been achieved with various analytical approaches, including wavelet transformations, multi-layer perceptrons, independent component analysis and autoregressive-moving-average models. Clinical research on BS has exposed their important potential in the non-invasive diagnosis of irritable bowel syndrome, in surgery, and for the investigation of gastrointestinal motility. The most recent advances are linked to the application of artificial intelligence and the development of dedicated BS devices. BS research is technologically mature, but lacks uniform methodology, an international forum for discussion and an open platform for data exchange. A common ground is needed as a starting point. The next key development will be the release of freely available benchmark datasets with labels confirmed by human experts.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": ["artificial intelligen"], "label": "1", "title_label": "1"}
{"id": "WOS:000732086500001", "keywords": "Gallium nitride; Generative adversarial networks; Radar polarimetry; Imaging; Synthetic aperture radar; Task analysis; Deep learning; Moving and stationary targets separation; shuffle generative adversarial network (GAN); synthetic aperture radar (SAR)", "title": "Shuffle GAN With Autoencoder: A Deep Learning Approach to Separate Moving and Stationary Targets in SAR Imagery", "abstract": "Synthetic aperture radar (SAR) has been widely applied in both civilian and military fields because it provides high-resolution images of the ground target regardless of weather conditions, day or night. In SAR imaging, the separation of moving and stationary targets is of great significance as it is capable of removing the ambiguity stemming from inevitable moving targets in stationary scene imaging and suppressing clutter in moving target imaging. The newly emerged generative adversarial networks (GANs) have great performance in many other signal processing areas; however, they have not been introduced to radar imaging tasks. In this work, we propose a novel shuffle GAN with autoencoder separation method to separate the moving and stationary targets in SAR imagery. The proposed algorithm is based on the independence of well-focused stationary targets and blurred moving targets for creating adversarial constraints. Note that the algorithm operates in a totally unsupervised fashion without requiring a sample set that contains mixed and separated SAR images. Experiments are carried out on synthetic and real SAR data to validate the effectiveness of the proposed method.", "journal": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Hardware & Architecture; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000595547500034", "keywords": "Functional magnetic resonance imaging; Brain modeling; Predictive models; Data models; Biological system modeling; Biomarkers; Infant age prediction; autoencoder; multimodal machine learning; magnetic resonance imaging", "title": "Disentangled-Multimodal Adversarial Autoencoder: Application to Infant Age Prediction With Incomplete Multimodal Neuroimages", "abstract": "Effective fusion of structural magnetic resonance imaging (sMRI) and functional magnetic resonance imaging (fMRI) data has the potential to boost the accuracy of infant age prediction thanks to the complementary information provided by different imaging modalities. However, functional connectivity measured by fMRI during infancy is largely immature and noisy compared to the morphological features from sMRI, thus making the sMRI and fMRI fusion for infant brain analysis extremely challenging. With the conventional multimodal fusion strategies, adding fMRI data for age prediction has a high risk of introducing more noises than useful features, which would lead to reduced accuracy than that merely using sMRI data. To address this issue, we develop a novel model termed as disentangled-multimodal adversarial autoencoder (DMM-AAE) for infant age prediction based on multimodal brain MRI. Specifically, we disentangle the latent variables of autoencoder into common and specific codes to represent the shared and complementary information among modalities, respectively. Then, cross-reconstruction requirement and common-specific distance ratio loss are designed as regularizations to ensure the effectiveness and thoroughness of the disentanglement. By arranging relatively independent autoencoders to separate the modalities and employing disentanglement under cross-reconstruction requirement to integrate them, our DMM-AAE method effectively restrains the possible interference cross modalities, while realizing effective information fusion. Taking advantage of the latent variable disentanglement, a new strategy is further proposed and embedded into DMM-AAE to address the issue of incompleteness of the multimodal neuroimages, which can also be used as an independent algorithm for missing modality imputation. By taking six types of cortical morphometric features from sMRI and brain functional connectivity from fMRI as predictors, the superiority of the proposed DMM-AAE is validated on infant age (35 to 848 days after birth) prediction using incomplete multimodal neuroimages. The mean absolute error of the prediction based on DMM-AAE reaches 37.6 days, outperforming state-of-the-art methods. Generally, our proposed DMM-AAE can serve as a promising model for prediction with multimodal data.", "journal": "IEEE TRANSACTIONS ON MEDICAL IMAGING", "category": "Computer Science, Interdisciplinary Applications; Engineering, Biomedical; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000578352100001", "keywords": "Sample; Article; Health; Vulnerability; Census; Public policy; Open data", "title": "Identification of Potential Type II Diabetes in a Large-Scale Chinese Population Using a Systematic Machine Learning Framework", "abstract": "Background. An estimated 425 million people globally have diabetes, accounting for 12% of the world's health expenditures, and the number continues to grow, placing a huge burden on the healthcare system, especially in those remote, underserved areas. Methods. A total of 584,168 adult subjects who have participated in the national physical examination were enrolled in this study. The risk factors for type II diabetes mellitus (T2DM) were identified bypvalues and odds ratio, using logistic regression (LR) based on variables of physical measurement and a questionnaire. Combined with the risk factors selected by LR, we used a decision tree, a random forest, AdaBoost with a decision tree (AdaBoost), and an extreme gradient boosting decision tree (XGBoost) to identify individuals with T2DM, compared the performance of the four machine learning classifiers, and used the best-performing classifier to output the degree of variables' importance scores of T2DM. Results. The results indicated that XGBoost had the best performance (accuracy = 0.906, precision = 0.910, recall = 0.902, F-1 = 0.906, and AUC = 0.968). The degree of variables' importance scores in XGBoost showed that BMI was the most significant feature, followed by age, waist circumference, systolic pressure, ethnicity, smoking amount, fatty liver, hypertension, physical activity, drinking status, dietary ratio (meat to vegetables), drink amount, smoking status, and diet habit (oil loving). Conclusions. We proposed a classifier based on LR-XGBoost which used fourteen variables of patients which are easily obtained and noninvasive as predictor variables to identify potential incidents of T2DM. The classifier can accurately screen the risk of diabetes in the early phrase, and the degree of variables' importance scores gives a clue to prevent diabetes occurrence.", "journal": "JOURNAL OF DIABETES RESEARCH", "category": "Endocrinology & Metabolism; Medicine, Research & Experimental", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000537106200121", "keywords": "CNN (Convolutional neural networks); deep learning; pavement defects; residual connection; attention gate; atrous spatial pyramid pooling", "title": "Improved Pixel-Level Pavement-Defect Segmentation Using a Deep Autoencoder", "abstract": "Convolutional neural networks perform impressively in complicated computer-vision image-segmentation tasks. Vision-based systems surpass humans in speed and accuracy in quality inspection tasks. Moreover, the maintenance of big infrastructures, such as roads, bridges, or buildings, is tedious and time-demanding work. In this research, we addressed pavement-quality evaluation by pixelwise defect segmentation using a U-Net deep autoencoder. Additionally, to the original neural network architecture, we utilized residual connections, atrous spatial pyramid pooling with parallel and \"Waterfall\" connections, and attention gates to perform better defect extraction. The proposed neural network configurations showed a segmentation performance improvement over U-Net with no significant computational overhead. Statistical and visual performance evaluation was taken into consideration for the model comparison. Experiments were conducted on CrackForest, Crack500, GAPs384, and mixed datasets.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000500759800006", "keywords": "patient stratification; deep learning; multivariate time series; multivariate longitudinal data; clustering", "title": "Deep learning for clustering of multivariate clinical patient trajectories with missing values", "abstract": "Background: Precision medicine requires a stratification of patients by disease presentation that is sufficiently informative to allow for selecting treatments on a per-patient basis. For many diseases, such as neurological disorders, this stratification problem translates into a complex problem of clustering multivariate and relatively short time series because (i) these diseases are multifactorial and not well described by single clinical outcome variables and (ii) disease progression needs to be monitored over time. Additionally, clinical data often additionally are hindered by the presence of many missing values, further complicating any clustering attempts. Findings: The problem of clustering multivariate short time series with many missing values is generally not well addressed in the literature. In this work, we propose a deep learning-based method to address this issue, variational deep embedding with recurrence (VaDER). VaDER relies on a Gaussian mixture variational autoencoder framework, which is further extended to (i) model multivariate time series and (ii) directly deal with missing values. We validated VaDER by accurately recovering clusters from simulated and benchmark data with known ground truth clustering, while varying the degree of missingness. We then used VaDER to successfully stratify patients with Alzheimer disease and patients with Parkinson disease into subgroups characterized by clinically divergent disease progression profiles. Additional analyses demonstrated that these clinical differences reflected known underlying aspects of Alzheimer disease and Parkinson disease. Conclusions: We believe our results show that VaDER can be of great value for future efforts in patient stratification, and multivariate time-series clustering in general.", "journal": "GIGASCIENCE", "category": "Biology; Multidisciplinary Sciences", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000455145000025", "keywords": "Stockwell transform; vibration imaging; fault diagnosis; transfer learning; neural network; vibration signals", "title": "Bearing Fault Diagnosis under Variable Rotational Speeds Using Stockwell Transform-Based Vibration Imaging and Transfer Learning", "abstract": "In this paper, discrete orthonormal Stockwell transform (DOST)-based vibration imaging is proposed as a preprocessing step for supporting load and rotational speed invariant scenarios for signals of various health conditions. For any health condition, features can easily be extracted from its generated health pattern. To automate the feature selection process, a convolutional neural network (CNN)-based transfer learning (TL) approach for diagnosis has also been introduced. Transfer learning allows an established model to use feature knowledge obtained under one set of working conditions through hidden layers to diagnose faults that occur under other working conditions. The network learns from the massive source dataset, and that knowledge is applied to the target data to identify faults. Using the bearing dataset of Case Western Reserve University, the proposed approach yields an average 99.8% classification accuracy and, specifically, 99.99% for healthy condition (HC), 99.95% for inner race fault (IRF), 99.96% for ball fault (BF), 99.68% for outer race fault for 12 o'clock sensor position (ORF@12), 99.93% for outer race fault for 3 o'clock sensor position (ORF@3), and 99.89% for outer race fault for 6 o'clock sensor position (ORF@6). In this paper, the proposed approach is compared with conventional artificial neural networks (ANNs), support vector machines (SVMs), hierarchical CNNs, and deep autoencoders. The proposed approach outperforms these conventional methods in the accuracy under all working conditions.", "journal": "APPLIED SCIENCES-BASEL", "category": "Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials Science, Multidisciplinary; Physics, Applied", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000446287400036", "keywords": "Underwater teleoperation; Adaptive impedance control; Neural network; Sliding mode; Uncertainty", "title": "The master adaptive impedance control and slave adaptive neural network control in underwater manipulator uncertainty teleoperation", "abstract": "This paper proposes a novel bilateral adaptive control scheme to achieve position and force coordination performance of underwater manipulator teleoperation system under model uncertainty and external disturbance. A new nonlinear model reference adaptive impedance controller with bound-gain-forgetting (BGF) composite adaptive law is designed for the master manipulator force tracking of the slave manipulator. The reference position in task space is obtained from the linear second-order impedance model whose input is the force error of the master and the slave. The adaptive terminal sliding mode control based on adaptive uncertainty compensation is proposed to achieve the master position tracking of the reference position. The radial basis function neural network (RBFNN) local approximation method is proposed for the slave manipulator's position tracking. The RBFNN based on Ge-Lee (GL) matrix is adopted to directly approximate each element of the slave manipulator dynamic, and the robust term with a proper update law is designed to suppress the error between the estimate model and the real model, and the external disturbance. The asymptotic tracking performance and global stability of the teleoperation system are proved with Lyapunov stability theorem. The simulation and experiment verify the performance of the proposed controller in teleoperation manipulator model. The results show that the teleoperation system has a good ability of position and force coordination.", "journal": "OCEAN ENGINEERING", "category": "Engineering, Marine; Engineering, Civil; Engineering, Ocean; Oceanography", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000412118700021", "keywords": "Adding power integrator; Underactuated mechanical systems; Model uncertainties; RBFNN; Lyapunov method", "title": "Adaptive RBFNN finite-time control of normal forms for underactuated mechanical systems", "abstract": "This paper presents a constructive design of a continuous finite-time controller for a class of mechanical systems known as underactuated systems that satisfy the symmetry properties. An adaptive radial basis function neural network (RBFNN) finite-time control scheme is proposed to stabilize the underactuated system at a given equilibrium, regardless of the various uncertainties and disturbances that the system contains. First, a coordinate transformation is introduced to decouple the control input so that an n-th order underactuated system can be represented into a special cascade form. Next, an adaptive robust finite-time controller is derived from adding a power integrator technique and the RBFNN to approximate the nonlinear unknown dynamics in the new space, whose bounds are supposedly unknown. The stability and finite-time convergence of the closed-loop system are established by using Lyapunov theory. To show the effectiveness of the proposed method, simulations are carried out on the rotary inverted pendulum, a typical example of an underactuated mechanical system.", "journal": "NONLINEAR DYNAMICS", "category": "Engineering, Mechanical; Mechanics", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000410005400011", "keywords": "image reconstruction; image coding; face recognition; image classification; principal component analysis; regression analysis; image representation; sample reconstruction; one sample per person face recognition; OSPP face recognition; multisample subject to single-sample subject intraclass variations; class-specific deep autoencoder; CDA; minimum L2 distance; sparse represented-based classifier; principle component analysis; softmax regression; extended Yale face database B; AR database; CMU database; image classification", "title": "Sample reconstruction with deep autoencoder for one sample per person face recognition", "abstract": "One sample per person (OSPP) face recognition is a challenging problem in face recognition community. Lack of samples is the main reason for the failure of most algorithms in OSPP. In this study, the authors propose a new algorithm to generalise intra-class variations of multi-sample subjects to single-sample subjects by deep autoencoder and reconstruct new samples. In the proposed algorithm, a generalised deep autoencoder is first trained with all images in the gallery, then a class-specific deep autoencoder (CDA) is fine-tuned for each single-sample subject with its single sample. Samples of the multi-sample subject, which is most like the single-sample subject, are input to the corresponding CDA to reconstruct new samples. For classification, minimum L2 distance, principle component analysis, sparse represented-based classifier and softmax regression are used. Experiments on the Extended Yale Face Database B, AR database and CMU PIE database are provided to show the validity of the proposed algorithm.", "journal": "IET COMPUTER VISION", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000379654200001", "keywords": "convolutional neural networks; biologically inspired feature construction; feature encoding; answer recommendation; community question answering; text analysis", "title": "Visual Cortex Inspired CNN Model for Feature Construction in Text Analysis", "abstract": "Recently, biologically inspired models are gradually proposed to solve the problem in text analysis. Convolutional neural networks (CNN) are hierarchical artificial neural networks, which include a various of multilayer perceptrons. According to biological research, CNN can be improved by bringing in the attention modulation and memory processing of primate visual cortex. In this paper, we employ the above properties of primate visual cortex to improve CNN and propose a biological-mechanism-driven-feature-construction based answer recommendation method (BMFC-ARM), which is used to recommend the best answer for the corresponding given questions in community question answering. BMFC-ARM is an improved CNN with four channels respectively representing questions, answers, asker information and answerer information, and mainly contains two stages: biological mechanism driven feature construction (BMFC) and answer ranking. BMFC imitates the attention modulation property by introducing the asker information and answerer information of given questions and the similarity between them, and imitates the memory processing property through bringing in the user reputation information for answerers. Then the feature vector for answer ranking is constructed by fusing the asker-answerer similarities, answerer's reputation and the corresponding vectors of question, answer, asker, and answerer. Finally, the Softmax is used at the stage of answer ranking to get best answers by the feature vector. The experimental results of answer recommendation on the Stackexchange dataset show that BMFC-ARM exhibits better performance.", "journal": "FRONTIERS IN COMPUTATIONAL NEUROSCIENCE", "category": "Mathematical & Computational Biology; Neurosciences", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000376548100028", "keywords": "Wireless positioning; Deep Neural Networks (DNNs); Hidden Markov model (HMM); Deep Learning; Stacked Denoising Autoencoder (SDA)", "title": "Deep Neural Networks for wireless localization in indoor and outdoor environments", "abstract": "In this paper, we propose a wireless positioning method based on Deep Learning. To deal with the variant and unpredictable wireless signals, the positioning is casted in a four-layer Deep Neural Network (DNN) structure pre-trained by Stacked Denoising Autoencoder (SDA) that is capable of learning reliable features from a large set of noisy samples and avoids hand-engineering. Also, to maintain the temporal coherence, a Hidden Markov Model (HMM)-based fine localizer is introduced to smooth the initial positioning estimate obtained by the DNN-based coarse localizer. The data required for the experiments is collected from the real world in different periods to meet the actual environment. Experimental results indicate that the proposed system leads to substantial improvement on localization accuracy in coping with the turbulent wireless signals. (C) 2016 Elsevier B.V. All rights reserved.", "journal": "NEUROCOMPUTING", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000376601700025", "keywords": "Voice conversion; Deep neural network (DNN); Spectral transformation; Fundamental frequency (F0); Duration modeling; Pretraining", "title": "High quality voice conversion using prosodic and high-resolution spectral features", "abstract": "Voice conversion methods have advanced rapidly over the last decade. Studies have shown that speaker characteristics are captured by spectral feature as well as various prosodic features. Most existing conversion methods focus on the spectral feature as it directly represents the timbre characteristics, while some conversion methods have focused only on the prosodic feature represented by the fundamental frequency. In this paper, a comprehensive framework using deep neural networks to convert both timbre and prosodic features is proposed. The timbre feature is represented by a high-resolution spectral feature. The prosodic features include F0, intensity and duration. It is well known that DNN is useful as a tool to model high-dimensional features. In this work, we show that DNN initialized by our proposed autoencoder pretraining yields good quality DNN conversion models. This pretraining is tailor-made for voice conversion and leverages on autoencoder to capture the generic spectral shape of source speech. Additionally, our framework uses segmental DNN models to capture the evolution of the prosodic features over time. To reconstruct the converted speech, the spectral feature produced by the DNN model is combined with the three prosodic features produced by the DNN segmental models. Our experimental results show that the application of both prosodic and high-resolution spectral features leads to quality converted speech as measured by objective evaluation and subjective listening tests.", "journal": "MULTIMEDIA TOOLS AND APPLICATIONS", "category": "Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000339391000001", "keywords": "Neural constructivism; Learning classifier systems; Neural networks; Genetic algorithms; Data streams; Concept drift", "title": "Robust on-line neural learning classifier system for data stream classification tasks", "abstract": "The increasing integration of technology in the different areas of science and industry has resulted in the design of applications that generate large amounts of data on-line. Most often, extracting information from these data is key, in order to gain a better understanding of the processes that the data are describing. Learning from these data poses new challenges to traditional machine learning techniques, which are not typically designed to deal with data in which concepts and noise levels may vary over time. The purpose of this paper is to present supervised neural constructivist system (SNCS), an accuracy-based neural-constructivist learning classifier system that makes use of multilayer perceptrons to learn from data streams with a fast reaction capacity to concept changes. The behavior of SNCS on data stream problems with different characteristics is carefully analyzed and compared with other state-of-the-art techniques in the field. This comparison is also extended to a large collection of real-world problems. The results obtained show that SNCS can function in a variety of problem situations producing accurate classification of data, whether the data are static or in dynamic streams.", "journal": "SOFT COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000209385900013", "keywords": "oil recovery; artificial intelligence; extraction; neural networks; supercritical extraction", "title": "Predicting the yield of pomegranate oil from supercritical extraction using artificial neural networks and an adaptive-network-based fuzzy inference system", "abstract": "Various simulation tools were used to develop an effective intelligent system to predict the effects of temperature and pressure on an oil extraction yield. Pomegranate oil was extracted using a supercritical CO2 (SC-CO2) process. Several simulation systems including a back-propagation neural network (BPNN), a radial basis function neural network (RBFNN) and an adaptive-network-based fuzzy inference system (ANFIS) were tested and their results were compared to determine the best predictive model. The performance of these networks was evaluated using the coefficient of determination (R-2) and the mean square error (MSE). The best correlation between the predicted and the experimental data was achieved using the BPNN method with an R-2 of 0.9948.", "journal": "FRONTIERS OF CHEMICAL SCIENCE AND ENGINEERING", "category": "Engineering, Chemical", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000455436600001", "keywords": "semantic segmentation of synthetic aperture radar images; region map; hierarchical visual semantic; sketch characteristic; Bayesian learning", "title": "Structural feature learning-based unsupervised semantic segmentation of synthetic aperture radar image", "abstract": "Region map is the sparse representation of a high-resolution synthetic aperture radar (SAR) image on the middle-level semantic layer in its semantic space. Based on the semantic information of the region map, the high-resolution SAR image is divided into hybrid, structural, and homogeneous pixel subspaces. The segmentation of SAR images can be divided into these three subspaces segmentation, of which the segmentation of hybrid subspace has more challenge because of complex structures. There are often many extremely inhomogeneous areas in the hybrid pixel subspace. Are these nonconnected areas in the same or different classes? To solve this problem, a Bayesian learning model with the constraint of sketch characteristic and an initialization method is proposed to construct a structural vector that can reflect the essential features of each extremely inhomogeneous area. Then, the unsupervised segmentation of the hybrid pixel subspace can be realized by using the structural vectors of these areas in this paper. Theoretical analysis and experimental results show that the performance of the hybrid pixel subspace segmentation realized by the structural vectors based on the Bayesian learning model proposed in the paper is better than that only used by hand designing features. (C) 2019 Society of Photo-Optical Instrumentation Engineers (SPIE)", "journal": "JOURNAL OF APPLIED REMOTE SENSING", "category": "Environmental Sciences; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000798811300001", "keywords": "Collaborative filtering; Graph convolutional network; Recommender system; Variational autoencoder", "title": "Handling information loss of graph convolutional networks in collaborative filtering", "abstract": "Collaborative filtering (CF) methods based on graph convolutional network (GCN) and autoencoder (AE) achieve outstanding performance. But the GCN-based CF methods suffer from information loss problems, which are caused by information lossy initialization and using low-order Chebyshev Polynomial to fit the graph convolution kernel. And the AE-based CF methods obtain the prediction results by reconstructing the user-item interaction matrix, which does not conduct deep excavation of the behavior patterns, resulting in the limited-expression ability. To solve the above problems, we propose Variational AutoEncoder-Enhanced Graph Convolutional Network (VE-GCN) for CF. Specifically, we use a variational autoencoder (VAE) to compress the interactive behavior patterns as the prior information of GCN to achieve sufficient learning, thus alleviating the information lossy initialization problem. And then the generalized graph Laplacian convolution kernel is proposed in GCN to handle the high-frequency information loss problem caused by Chebyshev Polynomial fitting in the GCN-based CF. To the best of our knowledge, VE-GCN is a feasible method to handle the information loss problems mentioned above in GCN-based CF for the first time. Meanwhile, the structure of GCN is optimized by removing redundant feature transformation and nonlinear activation function, and using DenseGCN to complete multi-level information interaction. Experiments on four real-world datasets show that the VE-GCN achieves state-of-the-art performance. (c) 2022 Elsevier Ltd. All rights reserved.", "journal": "INFORMATION SYSTEMS", "category": "Computer Science, Information Systems", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000775319800001", "keywords": "deep learning; point cloud; semantic segmentation; self-attention mechanism; graph convolution", "title": "IAGC: Interactive Attention Graph Convolution Network for Semantic Segmentation of Point Clouds in Building Indoor Environment", "abstract": "Point-based networks have been widely used in the semantic segmentation of point clouds owing to the powerful 3D convolution neural network (CNN) baseline. Most of the current methods resort to intermediate regular representations for reorganizing the structure of point clouds for 3D CNN networks, but they may neglect the inherent contextual information. In our work, we focus on capturing discriminative features with the interactive attention mechanism and propose a novel method consisting of the regional simplified dual attention network and global graph convolution network. Firstly, we cluster homogeneous points into superpoints and construct a superpoint graph to effectively reduce the computation complexity and greatly maintain spatial topological relations among superpoints. Secondly, we integrate cross-position attention and cross-channel attention into a single head attention module and design a novel interactive attention gating (IAG)-based multilayer perceptron (MLP) network (IAG-MLP), which is utilized for the expansion of the receptive field and augmentation of discriminative features in local embeddings. Afterwards, the combination of stacked IAG-MLP blocks and the global graph convolution network, called IAGC, is proposed to learn high-dimensional local features in superpoints and progressively update these local embeddings with the recurrent neural network (RNN) network. Our proposed framework is evaluated on three indoor open benchmarks, and the 6-fold cross-validation results of the S3DIS dataset show that the local IAG-MLP network brings about 1% and 6.1% improvement in overall accuracy (OA) and mean class intersection-over-union (mIoU), respectively, compared with the PointNet local network. Furthermore, our IAGC network outperforms other CNN-based approaches in the ScanNet V2 dataset by at least 7.9% in mIoU. The experimental results indicate that the proposed method can better capture contextual information and achieve competitive overall performance in the semantic segmentation task.", "journal": "ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION", "category": "Computer Science, Information Systems; Geography, Physical; Remote Sensing", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000800215600092", "keywords": "Image resolution; Training; Feature extraction; Spatial resolution; Proposals; Internet of Things; Deep learning; Deep learning; image representation; multiresolution; vehicle reidentification", "title": "An Efficient Multiresolution Network for Vehicle Reidentification", "abstract": "In general, vehicle images have varying resolutions due to vehicles' movements and different camera settings. However, most existing vehicle reidentification models are single-resolution deep networks trained with preuniformly resizing vehicle images, which underestimate adverse effects of varying resolutions and lead to unsatisfactory performance. A straightforward solution for dealing with varying resolutions is to train multiple vehicle reidentification models. Each model is independently trained with images of a specific resolution. However, this straightforward solution requires significant overhead and ignores intrinsic associations among different resolution images. For that, an efficient multiresolution network (EMRN) is proposed for vehicle reidentification in this article. First, EMRN embeds a newly designed multiresolution feature dimension uniform module (MR-FDUM) behind a traditional backbone network (i.e., ResNet-50). As a result, the whole model can extract fixed dimensional features from different resolution images so that it can be trained with one loss function of fixed dimensional parameters rather than training multiple models. Second, a multiresolution image randomly feeding strategy is designed to train EMRN, making each minibatch data of a random resolution during the training process. Consequently, EMRN can implicitly learn collaborative multiresolution features via only a unitary deep network. The experiments on three large-scale data sets, i.e., VeRi776, VehicleID, and VRIC, demonstrate that EMRN is superior to state-of-the-art vehicle reidentification methods.", "journal": "IEEE INTERNET OF THINGS JOURNAL", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000876295800002", "keywords": "Remaining useful life prediction; The first predicting time identification; Deep transfer learning; Multiple working conditions", "title": "Heterogeneous Graph Convolutional Network-Based Dynamic Rumor Detection on Social Media", "abstract": "The development of social media has provided open and convenient platforms for people to express their opinions, which leads to rumors being circulated. Therefore, detecting rumors from massive information becomes particularly essential. Previous methods for rumor detection focused on mining features from content and propagation patterns but neglected the dynamic features with joint content and propagation pattern. In this paper, we propose a novel heterogeneous GCN-based method for dynamic rumor detection (HDGCN), mainly composed of a joint content and propagation module and an ODE-based dynamic module. The joint content and propagation module constructs a content-propagation heterogeneous graph to obtain rumor representations by mining and discovering the interaction between post content and propagation structures in the rumor propagation process. The ODE-based dynamic module leverages a GCN integrated with an ordinary differential system to explore dynamic features of heterogeneous graphs. To evaluate the performance of our proposed HDGCN model, we have conducted extensive experiments on two real-world datasets from Twitter. The results of our proposed model have outperformed the mainstream model.", "journal": "COMPLEXITY", "category": "Mathematics, Interdisciplinary Applications; Multidisciplinary Sciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000789226600015", "keywords": "Time series; Adversarial; Shapelets", "title": "Investigating strategies towards adversarially robust time series classification", "abstract": "Deep neural networks have been shown to be vulnerable against specifically-crafted perturbations designed to affect their predictive performance. Such perturbations, formally termed 'adversarial attacks' have been designed for various domains in the literature, most prominently in computer vision and more recently, in time series classification. Therefore there is a need to derive robust strategies to defend deep networks from such attacks. In this work we propose to establish axioms of robustness against adversarial attacks in time series classification. We subsequently design a suitable experimental methodology and empirically validate the hypotheses put forth. Results obtained from our investigations confirm the proposed hypotheses, and provide a strong empirical baseline with a view to mitigating the effects of adversarial attacks in deep time series classification.(c) 2022 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ )", "journal": "PATTERN RECOGNITION LETTERS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000717497100005", "keywords": "learning dynamic; quadratic network; ordinary differential equations", "title": "On the learning dynamics of two-layer quadratic neural networks for understanding deep learning", "abstract": "Deep learning performs as a powerful paradigm in many real-world applications; however, its mechanism remains much of a mystery. To gain insights about nonlinear hierarchical deep networks, we theoretically describe the coupled nonlinear learning dynamic of the two-layer neural network with quadratic activations, extending existing results from the linear case. The quadratic activation, although rarely used in practice, shares convexity with the widely used ReLU activation, thus producing similar dynamics. In this work, we focus on the case of a canonical regression problem under the standard normal distribution and use a coupled dynamical system to mimic the gradient descent method in the sense of a continuous-time limit, then use the high order moment tensor of the normal distribution to simplify these ordinary differential equations. The simplified system yields unexpected fixed points. The existence of these non-global-optimal stable points leads to the existence of saddle points in the loss surface of the quadratic networks. Our analysis shows there are conserved quantities during the training of the quadratic networks. Such quantities might result in a failed learning process if the network is initialized improperly. Finally, We illustrate the comparison between the numerical learning curves and the theoretical one, which reveals the two alternately appearing stages of the learning process.", "journal": "FRONTIERS OF COMPUTER SCIENCE", "category": "Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods", "annotated_keywords": ["neural net", "neural net", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000781938400001", "keywords": "Medical image segmentation; Attention mechanism; Fuzzy logic; Deep convolution network", "title": "An FA-SegNet Image Segmentation Model Based on Fuzzy Attention and Its Application in Cardiac MRI Segmentation", "abstract": "Aiming at the medical images segmentation with low-recognition and high background noise, a deep convolution neural network image segmentation model based on fuzzy attention mechanism is proposed, which is called FA-SegNet. It takes SegNet as the basic framework. In the down-sampling module for image feature extraction, a fuzzy channel-attention module is added to strengthen the discrimination of different target regions. In the up-sampling module for image size restoration and multi-scale feature fusion, a fuzzy spatial-attention module is added to reduce the loss of image details and expand the receptive field. In this paper, fuzzy cognition is introduced into the feature fusion of CNNs. Based on the attention mechanism, fuzzy membership is used to re-calibrate the importance of the pixel value in local regions. It can strengthen the distinguishing ability of image features, and the fusion ability of the contextual information, which improves the segmentation accuracy of the target regions. Taking MRI segmentation as an experimental example, multiple targets such as the left ventricles, right ventricles, and left ventricular myocardium are selected as the segmentation targets. The pixels accuracy is 92.47%, the mean intersection to union is 86.18%, and the Dice coefficient is 92.44%, which are improved compared with other methods. It verifies the accuracy and applicability of the proposed method for the medical images segmentation, especially the targets with low-recognition and serious occlusion.", "journal": "INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000836626800024", "keywords": "Recommender system; social network; graph neural network", "title": "Modelling High-Order Social Relations for Item Recommendation", "abstract": "The prevalence of online social network makes it compulsory to study how social relations affect user choice. However, most existing methods leverage only first-order social relations, that is, the direct neighbors that are connected to the target user. The high-order social relations, e.g., the friends of friends, which are very informative to reveal user preference, have been largely ignored. In this work, we focus on modeling the indirect influence from the high-order neighbors in social networks to improve the performance of item recommendation. Distinct from mainstream social recommenders that regularize the model learning with social relations, we instead propose to directly factor social relations in the predictive model, aiming at learning better user embeddings to improve recommendation. To address the challenge that high-order neighbors increase dramatically with the order size, we propose to recursively \"propagate\" embeddings along the social network, effectively injecting the influence of high-order neighbors into user representation. We conduct experiments on two real datasets of Yelp and Douban to verify our High-Order Social Recommender (HOSR) model. Empirical results show that our HOSR significantly outperforms recent graph regularization-based recommenders NSCR and IF-BPR+, and graph convolutional network-based social influence prediction model Deepinf, achieving new state-of-the-arts of the task.", "journal": "IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000833791900032", "keywords": "Meters; Feature extraction; Substations; Object detection; Deep learning; Convolutional neural networks; Real-time systems; Meter recognition; power equipment meter; real-time detection; Yolov5", "title": "Real Time Power Equipment Meter Recognition Based on Deep Learning", "abstract": "Reading power equipment meters often requires loads of manpower, which is a trivial, repetitive, and error-prone task. While conventional automated recognition methods using computer vision (CV) techniques are inflexible under diverse scenarios, in this article, we propose a lightweight meter recognition method that combines deep learning and traditional CV techniques for automated meter reading. For meter detection, an adaptive anchor and global context (GC) module are deployed to improve the feature extraction ability of lightweight backbone without increasing computational cost. Then, an feature pyramid network (FPN) and a path aggregation network (PANet) are developed to realize the information interaction between different feature layers and achieve multiscale prediction. Our method also includes a multitask segmented network to read the detected meters, accelerating the detection speed. Experiments demonstrate that our proposed method can achieve a detection speed of 123 frame per second (FPS) in GeForce GTX 1080 and can obtain an accuracy of 88.2% mean average precision (mAP)50:95. In the case of insufficient training samples, the method can still achieve an accuracy of 80.9% mAP50:95. In addition, we build a power meter images (PMIs) dataset, which contains 1800 images in real scene. The dataset and method we proposed can help with further upgrades of traditional substations. In the future, we also hope to extend the algorithm to edge computing cameras for substations. The newly developed dataset and code are available at https://github.com/zzfan3/electric_meter_detect_recognize.", "journal": "IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT", "category": "Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000828222600001", "keywords": "Object detection; Knowledge engineering; Feature extraction; Remote sensing; Computational modeling; Computational efficiency; Training; Deep neural network; feature transition; label registration; model distillation; remote sensing; target detection", "title": "Target Detection Model Distillation Using Feature Transition and Label Registration for Remote Sensing Imagery", "abstract": "Deep convolution networks have been widely used in remote sensing target detection for various applications in recent years. Target detection models with many parameters provide better results but are not suitable for resource-constrained devices due to their high computational cost and storage requirements. Furthermore, current lightweight target detection models for remote sensing imagery rarely have the advantages of existing models. Knowledge distillation can improve the learning ability of a small student network from a large teacher network due to acceleration and compression. However, current knowledge distillation methods typically use mature backbones as teacher and student networks are unsuitable for target detection in remote sensing imagery. In this article, we propose a target detection model distillation (TDMD) framework using feature transition and label registration for remote sensing imagery. A lightweight attention network is designed by ranking the importance of the convolutional feature layers in the teacher network. Multiscale feature transition based on a feature pyramid is utilized to constrain the feature maps of the student network. A label registration procedure is proposed to improve the TDMD model's learning ability of the output distribution of the teacher network. The proposed method is evaluated on the DOTA and NWPU VHR-10 remote sensing image datasets. The results show that the TDMD achieves a mean Average Precision (mAP) of 75.47% and 93.81% on the DOTA and NWPU VHR-10 datasets, respectively. Moreover, the model size is 43% smaller than that of the predecessor model (11.8 MB and 11.6 MB for the two datasets).", "journal": "IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING", "category": "Engineering, Electrical & Electronic; Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000902230100001", "keywords": "visual encoding models; deep neural networks; receptive field; a large convolution kernel; RepLKNet; fMRI", "title": "A Mixed Visual Encoding Model Based on the Larger-Scale Receptive Field for Human Brain Activity", "abstract": "Research on visual encoding models for functional magnetic resonance imaging derived from deep neural networks, especially CNN (e.g., VGG16), has been developed. However, CNNs typically use smaller kernel sizes (e.g., 3 x 3) for feature extraction in visual encoding models. Although the receptive field size of CNN can be enlarged by increasing the network depth or subsampling, it is limited by the small size of the convolution kernel, leading to an insufficient receptive field size. In biological research, the size of the neuronal population receptive field of high-level visual encoding regions is usually three to four times that of low-level visual encoding regions. Thus, CNNs with a larger receptive field size align with the biological findings. The RepLKNet model directly expands the convolution kernel size to obtain a larger-scale receptive field. Therefore, this paper proposes a mixed model to replace CNN for feature extraction in visual encoding models. The proposed model mixes RepLKNet and VGG so that the mixed model has a receptive field of different sizes to extract more feature information from the image. The experimental results indicate that the mixed model achieves better encoding performance in multiple regions of the visual cortex than the traditional convolutional model. Also, a larger-scale receptive field should be considered in building visual encoding models so that the convolution network can play a more significant role in visual representations.", "journal": "BRAIN SCIENCES", "category": "Neurosciences", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000767904900002", "keywords": "machine learning; pornography use; data exploration; pornography addiction; prediction", "title": "ASO Author Reflections: Histology-Based Supervised Machine Learning Model Can Predict Recurrence Pattern of Pancreatic Cancer", "abstract": "Deep-learning models trained with images of the external part of the eyes, rather than fundus images of the retina, can also be used to detect severe diabetic conditions, such as diabetic retinopathy.", "journal": "ANNALS OF SURGICAL ONCOLOGY", "category": "Oncology; Surgery", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000839528200001", "keywords": "Deep learning; Migraine; Signal processing; Bidirectional long-short term memory; EEG", "title": "Automatic detection of migraine disease from EEG signals using bidirectional long-short term memory deep learning model", "abstract": "Migraine is a neurological disease defined by recurrent attacks of headache accompanied by nausea and vomiting, which causes autonomic nervous system disturbance, episodes of severe pain, are prolonged, and can have a major impact on quality of life. In this study, a deep learning model was proposed to assist expert judgment in the automatic detection of migraine using electroencephalography (EEG) signals. The dataset consists of EEG signals recorded from 21 healthy and 18 migraine volunteers. Feature vectors were created by calculating the power densities values of the frequencies between 1 and 49 Hz of the EEG signals using the Welch method. The performances of bidirectional long-short term memory (BiLSTM) deep learning algorithm and random forest, support vector machine, and linear discriminant analysis machine learning algorithms in EEG-based migraine classification tasks were compared using the created feature vectors. The algorithm with the highest performance is the BiLSTM (95.99%) deep learning algorithm using 128 channels. The study is a rare attempt in which a deep learning model is used in the effective diagnosis of automatic migraine by analyzing multi-channel EEG signals and provides evidence for the superiority of deep learning algorithms. In comparison to state-of-the-art investigations, more accuracy was achieved. With this model, the accuracy was increased by 6.3%.", "journal": "SIGNAL IMAGE AND VIDEO PROCESSING", "category": "Engineering, Electrical & Electronic; Imaging Science & Photographic Technology", "annotated_keywords": ["machine learning", "deep learning", "deep learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000876254100005", "keywords": "Visual SLAM; Semantic feature; Dynamic outdoor scene; Deep learning; Dense map", "title": "A semantic SLAM-based dense mapping approach for large-scale dynamic outdoor environment", "abstract": "The visual SLAM in dynamic environment has been regarded as a fundamental task for robots. Currently, existing works achieve good performance in only indoor scenes due to the loss of depth information and scene complexity. In this paper, we present a semantic SLAM framework based on geometric constraint and deep learning models. Specifically, our method is built top on the ORB-SLAM2 system with stereo observation. First, the semantic feature and depth information are acquired respectively using different deep learning models. In this way, multiple views projection is generated to reduce the impact of moving objects for pose estimation. Under the hierarchical rule, the feature points are further refined for SLAM tracking via depth local contrast. Finally, multiple dense 3D maps are created for high-level robot navigation in an incremental updating manner. Our method on public KITTI dataset demonstrates that evaluation metrics of most of sequences improve and achieve state-of-the-art performance.", "journal": "MEASUREMENT", "category": "Engineering, Multidisciplinary; Instruments & Instrumentation", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000806061400001", "keywords": "liver transplantation; massive blood transfusion; red cell transfusion; machine learning; prediction model", "title": "Advancing Prediction of Risk of Intraoperative Massive Blood Transfusion in Liver Transplantation With Machine Learning Models. A Multicenter Retrospective Study", "abstract": "BackgroundLiver transplantation surgery is often accompanied by massive blood loss and massive transfusion (MT), while MT can cause many serious complications related to high mortality. Therefore, there is an urgent need for a model that can predict the demand for MT to reduce the waste of blood resources and improve the prognosis of patients. ObjectiveTo develop a model for predicting intraoperative massive blood transfusion in liver transplantation surgery based on machine learning algorithms. MethodsA total of 1,239 patients who underwent liver transplantation surgery in three large grade lll-A general hospitals of China from March 2014 to November 2021 were included and analyzed. A total of 1193 cases were randomly divided into the training set (70%) and test set (30%), and 46 cases were prospectively collected as a validation set. The outcome of this study was an intraoperative massive blood transfusion. A total of 27 candidate risk factors were collected, and recursive feature elimination (RFE) was used to select key features based on the Categorical Boosting (CatBoost) model. A total of ten machine learning models were built, among which the three best performing models and the traditional logistic regression (LR) method were prospectively verified in the validation set. The Area Under the Receiver Operating Characteristic Curve (AUROC) was used for model performance evaluation. The Shapley additive explanation value was applied to explain the complex ensemble learning models. ResultsFifteen key variables were screened out, including age, weight, hemoglobin, platelets, white blood cells count, activated partial thromboplastin time, prothrombin time, thrombin time, direct bilirubin, aspartate aminotransferase, total protein, albumin, globulin, creatinine, urea. Among all algorithms, the predictive performance of the CatBoost model (AUROC: 0.810) was the best. In the prospective validation cohort, LR performed far less well than other algorithms. ConclusionA prediction model for massive blood transfusion in liver transplantation surgery was successfully established based on the CatBoost algorithm, and a certain degree of generalization verification is carried out in the validation set. The model may be superior to the traditional LR model and other algorithms, and it can more accurately predict the risk of massive blood transfusions and guide clinical decision-making.", "journal": "FRONTIERS IN NEUROINFORMATICS", "category": "Mathematical & Computational Biology; Neurosciences", "annotated_keywords": ["machine learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000780583000001", "keywords": "wind power forecasting; data-driven; machine learning; ensemble learning", "title": "Efficient Wind Power Prediction Using Machine Learning Methods: A Comparative Study", "abstract": "Wind power represents a promising source of renewable energies. Precise forecasting of wind power generation is crucial to mitigate the challenges of balancing supply and demand in the smart grid. Nevertheless, the major difficulty in wind power is its high fluctuation and intermittent nature, making it challenging to forecast. This study aims to develop efficient data-driven models to accurately forecast wind power generation. Crucially, the main contributions of this work are listed in the following major elements. Firstly, we investigate the performance of enhanced machine learning models to forecast univariate wind power time-series data. Specifically, we employed Bayesian optimization (BO) to optimally tune hyperparameters of the Gaussian process regression (GPR), Support Vector Regression (SVR) with different kernels, and ensemble learning (ES) models (i.e., Boosted trees and Bagged trees) and investigated their forecasting performance. Secondly, dynamic information has been incorporated in their construction to further enhance the forecasting performance of the investigated models. Specifically, we introduce lagged measurements to enable capturing time evolution into the design of the considered models. Furthermore, more input variables (e.g., wind speed and wind direction) are used to further improve wind prediction performance. Actual measurements from three wind turbines in France, Turkey, and Kaggle are used to verify the efficiency of the considered models. The results reveal the benefit of considering lagged data and input variables to better forecast wind power. The results also showed that the optimized GPR and ensemble models outperformed the other machine learning models.", "journal": "ENERGIES", "category": "Energy & Fuels", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000834820800001", "keywords": "Groundwater; Vulnerability assessment; GALDIT index; Machine learning; Hybrid models", "title": "The use of hybrid machine learning models for improving the GALDIT model for coastal aquifer vulnerability mapping", "abstract": "The objective of this study was to improve the predictability of the GALDIT (G: groundwater occurrence, A: aquifer hydraulic conductivity, L: level of groundwater above sea level, D: distance from the shoreline, I: impact of the seawater intrusion, and T: thickness of the aquifer) groundwater vulnerability model using machine leaning methods. This study evaluated eight state-of-the-art machine learning methods, including the naive Bayes tree (NBT) and logistic model tree (LMT) methods, and their combinations with the dagging (DA), bagging (BA), and random subspace (RS) methods. The results of the machine leaning methods were compared against the benchmark GALDIT model. The coastal Gharesoo-Gorgan Rood aquifer, North Iran, was used as a case study for the proposed methodology. Two sets of total dissolved solids (TDS) samples from 53 wells were collected in 2017 and 2018 and used for the GALDIT modeling and validation purposes, respectively. Correlation coefficient (r) values were calculated for model validation and prediction accuracy by comparison with the TDS data. All eight machine learning models performed well in assessing the coastal aquifer vulnerability with respect to the GALDIT model. The best result was obtained by the BA-LMT model (r = 0.931), followed by the DA-LMT model (r = 0.911), the BA-NBT model (r = 0.904), the DA-NBT model (r = 0.896), the RS-NBT model (r = 0.882), the RS-LMT (r = 0.873), the LMT (r = 0.863), the NBT (r = 0.850), and GALDIT model (r = 0.480).", "journal": "ENVIRONMENTAL EARTH SCIENCES", "category": "Environmental Sciences; Geosciences, Multidisciplinary; Water Resources", "annotated_keywords": ["machine learning", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000830497700004", "keywords": "Structural adhesives; Finite element simulation; Materials modelling; Machine learning; Data driven modelling", "title": "A machine learning material model for structural adhesives in finite element analysis", "abstract": "The computational analysis of adhesives within finite element simulations is important for many engineering applications, from automotive structures to civil engineering. The underlying material descriptions are closed, analytically formulated models representing the stress-response of the material for given strains. In this publication, we investigate and present a methodology for adopting artificial neural networks for this purpose. The aim is to show feasibility and performance of such models to be able to uncouple the material description from fixed formulations to reap the benefits of neural networks for materials modelling. A methodology is presented for creating training data for a structural adhesive, whereby the data is obtained from specimen and from generic strain paths of a baseline material model. From this, feed-forward neural networks are trained and evaluated using machine learning tools as well as from an engineering perspective by pre-defined test strain paths. It is shown that, given the quality and the quantity of training data is adequate, the model is able to generalize with respect to the response. Especially, the coverage of complete strain space is shown to be crucial. A framework for using machine learning models in a commercial finite element code is presented, integrating models trained through high-level programming interfaces. This enables validating the performance and accuracy of such models in explicit simulations, which is shown to be a crucial step in evaluation of machine learning material models. The model is confirmed to be able to represent structural adhesives with high precision comparable to an analytical model and to generalize for unknown geometries and loading conditions. An approach to predict element failure with the machine learning model is shown and finally, the neural network model response is made explainable from an engineering point of view using feature importance.", "journal": "INTERNATIONAL JOURNAL OF ADHESION AND ADHESIVES", "category": "Engineering, Chemical; Materials Science, Multidisciplinary", "annotated_keywords": ["neural net", "machine learning", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000855327200008", "keywords": "Augmented reality; Object detection; Navigation; Monocular depth extraction", "title": "Enhanced Memetic Algorithm-Based Extreme Learning Machine Model for Smart Grid Stability Prediction", "abstract": "The smart grid is considered a conventional application domain of cyber-physical system (CPS) tools in the electrical utility industry. The physical system dynamics of SG with the assistance of CPS are generally controlled by connected sensors and controllers via a communication link. These CPSs, which rely heavily on an expansive communication network and intelligent computing algorithms, are susceptible to cyber-physical attacks and are also sensitive to various technical, economical, and social factors compromising their stability. Assessment and prediction of the stability of CPSs are very vital in this context. In this work, a novel optimized (memetic algorithm-based) extreme learning machine model for smart grid-CPS stability prediction has been proposed. Here, the teaching-learning-based optimization and simulated annealing techniques are used to design the memetic algorithm. The experimental result regarding the proposed model is then compared with other contemporary machine learning and deep learning models.", "journal": "INTERNATIONAL TRANSACTIONS ON ELECTRICAL ENERGY SYSTEMS", "category": "Engineering, Electrical & Electronic", "annotated_keywords": ["machine learning", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000806702300001", "keywords": "Insulator location; Defect identification; Transfer learning; Dempster-Shafer evidence theory", "title": "A light defect detection algorithm of power insulators from aerial images for power inspection", "abstract": "With the rapid growth of high-voltage transmission lines, the number of power transmission line equipments is correspondingly increasing. Power insulator is the basic component which plays the key role in the stable operation of power system. As a common defect of power insulators, missing-cap issue will affect the structural strength and durability of different power insulators. Therefore, the condition monitoring of power insulators is a daily but priority power line inspection task. Faced with the weak image features of small insulator defects in the aerial images, the conventional handcrafted features could not extract effectively powerful image features. Meanwhile, the small-scale insulator defects will bring a certain effect to the model training of deep learning. Therefore, the high-efficiency and accurate defect inspection still present a challenging task against complex backgrounds. To address the above issues, aimed at the missing-cap defects of power insulators, a novel defect identification algorithm from aerial images is proposed by taking advantage of state-of-the-art deep learning and transfer learning models. Fused with Spatial Pyramid Pooling (SPP) and MobileNet networks, a light deep convolutional neural network (DCNN) model based on You Only Look Once (YOLO) V3 network is proposed for fast and accurate insulator location to remove complex background interference. On the basis, combined with Dempster-Shafer (DS) evidence theory, the improved transfer learning model based on feature fusion is proposed for high-precision defect identification of power insulators. Experiments show that the proposed method could acquire a better identification performance against complex power inspection environment compared with other related detection models.", "journal": "NEURAL COMPUTING & APPLICATIONS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000757111000001", "keywords": "coronavirus; COVID-19; mental disorders; machine learning; natural language processing", "title": "Monitoring of COVID-19 pandemic-related psychopathology using machine learning", "abstract": "The COVID-19 pandemic is believed to have a major negative impact on global mental health due to the viral disease itself as well as the associated lockdowns, social distancing, isolation, fear, and increased uncertainty. Individuals with preexisting mental illness are likely to be particularly vulnerable to these conditions and may develop outright 'COVID-19-related psychopathology'. Here, we trained a machine learning model on structured and natural text data from electronic health records to identify COVID-19 pandemic-related psychopathology among patients receiving care in the Psychiatric Services of the Central Denmark Region. Subsequently, applying this model, we found that pandemic-related psychopathology covaries with the pandemic pressure over time. These findings may aid psychiatric services in their planning during the ongoing and future pandemics. Furthermore, the results are a testament to the potential of applying machine learning to data from electronic health records.", "journal": "ACTA NEUROPSYCHIATRICA", "category": "Neurosciences; Psychiatry", "annotated_keywords": ["machine learning", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000826437700001", "keywords": "Electroencephalography; Privacy; Data privacy; Computational modeling; Data models; Brain modeling; Feature extraction; Brain-computer interfaces (BCIs); electroencephalogram (EEG); machine learning (ML); privacy", "title": "Privacy-Preserving Brain-Computer Interfaces: A Systematic Review", "abstract": "A brain-computer interface (BCI) establishes a direct communication pathway between the human brain and a computer. It has been widely used in medical diagnosis, rehabilitation, education, entertainment, and so on. Most research so far focuses on making BCIs more accurate and reliable, but much less attention has been paid to their privacy. Developing a commercial BCI system usually requires close collaborations among multiple organizations, e.g., hospitals, universities, and/or companies. Input data in BCIs, e.g., electroencephalogram (EEG), contain rich privacy information, and the developed machine learning model is usually proprietary. Data and model transmission among different parties may incur significant privacy threats, and hence, privacy protection in BCIs must be considered. Unfortunately, there does not exist any contemporary and comprehensive review on privacy-preserving BCIs. This article fills this gap, by describing potential privacy threats and protection strategies in BCIs. It also points out several challenges and future research directions in developing privacy-preserving BCIs.", "journal": "IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS", "category": "Computer Science, Cybernetics; Computer Science, Information Systems", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000866229800004", "keywords": "CBCT images; NHP; Mask-RCNN; Deep learning", "title": "Automated analysis of three-dimensional CBCT images taken in natural head position that combines facial profile processing and multiple deep-learning models", "abstract": "Background and objectives: Analyzing three-dimensional cone beam computed tomography (CBCT) im-ages has become an indispensable procedure for diagnosis and treatment planning of orthodontic pa-tients. Artificial intelligence, especially deep-learning techniques for analyzing image data, shows great potential for medical and dental image analysis and diagnosis. To explore the feasibility of automating measurement of 13 geometric parameters from three-dimensional cone beam computed tomography im-ages taken in natural head position (NHP), this study proposed a smart system that combined a facial profile analysis algorithm with deep-learning models.Materials and methods: Using multiple views extracted from the cone beam computed tomography data of 170 cases as a dataset, our proposed method automatically calculated 13 dental parameters by parti-tioning, detecting regions of interest, and extracting the facial profile. Subsequently, Mask-RCNN, a trained decentralized convolutional neural network was applied to detect 23 landmarks. All the techniques were integrated into a software application with a graphical user interface designed for user convenience. To demonstrate the system's ability to replace human experts, 30 CBCT data were selected for validation. Two orthodontists and one advanced general dentist located required landmarks by using a commercial dental program. The differences between manual and developed methods were calculated and reported as the errors. Results: The intraclass correlation coefficients (ICCs) and 95% confidence interval (95% CI) for intra-observer reliability were 0.98 (0.97-0.99) for observer 1; 0.95 (0.93-0.97) for observer 2; 0.98 (0.97- 0.99) for observer 3 after measuring 13 parameters two times at two weeks interval. The combined ICC for intra-observer reliability was 0.97. The ICCs and 95% CI for inter-observer reliability were 0.94 (0.91- 0.97). The mean absolute value of deviation was around 1 mm for the length parameters, and smaller than 2 degrees for angle parameters. Furthermore, ANOVA test demonstrated the consistency between the mea-surements of the proposed method and those of human experts statistically ( F dis= 2.68, alpha = 0.05).Conclusions: The proposed system demonstrated the high consistency with the manual measurements of human experts and its applicability. This method aimed to help human experts save time and effort s f or analyzing three-dimensional CBCT images of orthodontic patients.(c) 2022 Elsevier B.V. All rights reserved.", "journal": "COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE", "category": "Computer Science, Interdisciplinary Applications; Computer Science, Theory & Methods; Engineering, Biomedical; Medical Informatics", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000751614500001", "keywords": "Incremental learning; One-stage object detection; Imbalanced data; Autonomous driving assistance systems", "title": "Utilizing incremental branches on a one-stage object detection framework to avoid catastrophic forgetting", "abstract": "The tremendous success of deep learning on object detection tasks compels researchers to adopt deep learning models for autonomous driving vehicles. As autonomous driving vehicles grows sophisticated, the top models are expected to detect novel classes beyond its prior objectives. Thus, incremental learning for object detection essentially ensures that a model is able to detect additional classes on the fly. In this work, we demonstrate how to update a model on new data and an existing model to append new classes on the existing model. The proposed method utilizes episodic memory to save finite samples of data and replay them during incremental learning. The results on PASCAL VOC2007 have suggested that the proposed method obtains the least mAP reduction, at 4.3%, compared against the all-classes learning in the 10+10 classes scenario, which is the lowest amongst other prior arts. Our method also has the highest backward and forward transfer among incremental learning strategies, indicating better memorization and adaptability.", "journal": "MACHINE VISION AND APPLICATIONS", "category": "Computer Science, Artificial Intelligence; Computer Science, Cybernetics; Engineering, Electrical & Electronic", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000844934700002", "keywords": "Detection; Traffic density; Ensemble; SSD; Faster R-CNN; Convolutional neural network; Deep learning", "title": "Vehicle detection and traffic density estimation using ensemble of deep learning models", "abstract": "Traffic density estimation can be used for controlling traffic light signals to provide effective traffic management. It can be done in two steps: vehicle recognition and counting. Deep learning (DL) technologies are being explored more and more as CNN grows in popularity. In this study, initially, data was collected from various open-source libraries that is FLIR, KITTI, and MB7500. Vehicles in the images are labelled in six different classes. To deal with an imbalanced dataset, data augmentation techniques were applied. Then, a model based on an ensemble of the faster region-based convolutional neural networks (Faster R-CNN) and Single-shot detector (SSD) were trained on finally processed datasets. The results of the proposed model were compared with base estimators of the FLIR dataset (Thermal and RGB images separately), MB7500, and KITTI dataset. Experimental results depict that the highest mAP obtained was 94% by the proposed Ensemble on FLIR thermal dataset which was 34% better than SSD and 6% from the Faster R-CNN model. Overall, the proposed ensemble achieves better and more promising results as compared to base estimators. Experimental results also show that detection with thermal images was better than visible images. In addition, three algorithms were compared for estimated density and the proposed model shows significant potential for traffic density estimation.", "journal": "MULTIMEDIA TOOLS AND APPLICATIONS", "category": "Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": ["neural net", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000894483400003", "keywords": "Few-shot learning; Object recognition; Class distribution estimation; Similar class classification", "title": "(NCE)-E-2: boosting few-shot learning with novel class center estimation", "abstract": "Accurate class distribution estimation is expected to solve the problem of the poor generalization ability that exists in few-shot learning models due to data shortages. However, the reliability of class distributions estimates based on limited samples and knowledge is questionable, especially for similar classes. We find that the distribution calibration method is inaccurate in estimating similar classes due to limited knowledge being reused through double-validation experiments. To address this issue, we propose a novel class center estimation ((NCE)-E-2) method, which consists of a two-stage center estimation (TCE) algorithm and a class centroid estimation (CCE) algorithm. The class centers estimated by TCE in two stages are closer to the truth, and its superiority is demonstrated by error theory. CCE searches for the centroid of the base class iteratively and is used as the basis for the novel class calibration. Sufficient simulation samples are generated based on the estimated class distribution to augment the training data. The experimental results show that, compared with the distribution calibration method, the proposed method achieves an approximately 1% performance improvement on the miniImageNet and CUB datasets; an approximately 1.45% performance improvement for similar class classification; and an approximately 6.06% performance improvement for non-similar class classification.", "journal": "NEURAL COMPUTING & APPLICATIONS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000839164400001", "keywords": "machine learning; COVID-19; hospitalization; predictive model", "title": "A Machine Learning Model for Predicting Hospitalization in Patients with Respiratory Symptoms during the COVID-19 Pandemic", "abstract": "A machine learning approach is a useful tool for risk-stratifying patients with respiratory symptoms during the COVID-19 pandemic, as it is still evolving. We aimed to verify the predictive capacity of a gradient boosting decision trees (XGboost) algorithm to select the most important predictors including clinical and demographic parameters in patients who sought medical support due to respiratory signs and symptoms (RAPID RISK COVID-19). A total of 7336 patients were enrolled in the study, including 6596 patients that did not require hospitalization and 740 that required hospitalization. We identified that patients with respiratory signs and symptoms, in particular, lower oxyhemoglobin saturation by pulse oximetry (SpO(2)) and higher respiratory rate, fever, higher heart rate, and lower levels of blood pressure, associated with age, male sex, and the underlying conditions of diabetes mellitus and hypertension, required hospitalization more often. The predictive model yielded a ROC curve with an area under the curve (AUC) of 0.9181 (95% CI, 0.9001 to 0.9361). In conclusion, our model had a high discriminatory value which enabled the identification of a clinical and demographic profile predictive, preventive, and personalized of COVID-19 severity symptoms.", "journal": "JOURNAL OF CLINICAL MEDICINE", "category": "Medicine, General & Internal", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000783573900043", "keywords": "Moyamoya disease; Hemorrhage; Risk factor; Transfer learning", "title": "A Magnetic Resonance Angiography-Based Study Comparing Machine Learning and Clinical Evaluation: Screening Intracranial Regions Associated with the Hemorrhagic Stroke of Adult Moyamoya Disease", "abstract": "Objectives: Moyamoya disease patients with hemorrhagic stroke usually have a poor prognosis. This study aimed to determine whether hemorrhagic moyamoya disease could be distinguished from MRA images using transfer deep learning and to screen potential regions that contain rich distinguishing information from MRA images in moyamoya disease. Materials and methods: A total of 116 adult patients with bilateral moyamoya diseases suffering from hemorrhagic or ischemia complications were retrospectively screened. Based on original MRA images at the level of the basal cistern, basal ganglia, and centrum semiovale, we adopted the pretrained ResNetl8 to build three models for differentiating hemorrhagic moyamoya disease. Grad-CAM was applied to visualize the regions of interest. Results: For the test set, the accuracies of model differentiation in the basal cistern, basal ganglia, and centrum semiovale were 93.3%, 91.5%, and 86.4%, respectively. Visualization of the regions of interest demonstrated that the models focused on the deep and periventricular white matter and abnormal collateral vessels in hemorrhagic moyamoya disease. Conclusion: A transfer learning model based on MRA images of the basal cistern and basal ganglia showed a good ability to differentiate between patients with hemorrhagic moyamoya disease and those with ischemic moyamoya disease. The deep and periventricular white matter and collateral vessels at the level of the basal cistern and basal ganglia may contain rich distinguishing information.", "journal": "JOURNAL OF STROKE & CEREBROVASCULAR DISEASES", "category": "Neurosciences; Peripheral Vascular Disease", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000755331900001", "keywords": "drug discovery; class imbalance; machine learning; protein conformation selecton; drug candidates; ADORA2A; OPRK1", "title": "Novel Big Data-Driven Machine Learning Models for Drug Discovery Application", "abstract": "Most contemporary drug discovery projects start with a 'hit discovery' phase where small chemicals are identified that have the capacity to interact, in a chemical sense, with a protein target involved in a given disease. To assist and accelerate this initial drug discovery process, 'virtual docking calculations' are routinely performed, where computational models of proteins and computational models of small chemicals are evaluated for their capacities to bind together. In cutting-edge, contemporary implementations of this process, several conformations of protein targets are independently assayed in parallel 'ensemble docking' calculations. Some of these protein conformations, a minority of them, will be capable of binding many chemicals, while other protein conformations, the majority of them, will not be able to do so. This fact that only some of the conformations accessible to a protein will be 'selected' by chemicals is known as 'conformational selection' process in biology. This work describes a machine learning approach to characterize and identify the properties of protein conformations that will be selected (i.e., bind to) chemicals, and classified as potential binding drug candidates, unlike the remaining non-binding drug candidate protein conformations. This work also addresses the class imbalance problem through advanced machine learning techniques that maximize the prediction rate of potential protein molecular conformations for the test case proteins ADORA2A (Adenosine A2a Receptor) and OPRK1 (Opioid Receptor Kappa 1), and subsequently reduces the failure rates and hastens the drug discovery process.", "journal": "MOLECULES", "category": "Biochemistry & Molecular Biology; Chemistry, Multidisciplinary", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000868786400017", "keywords": "time series; data imputation; health assessment", "title": "An innovative machine learning model for supply chain management", "abstract": "Supply chain management (SCM) integrates all links and business processes involved in the supply chain through the information management system. Applying artificial intelligence algorithms to the SCM system can realize the visualization, automation, and intelligent management of all links in the supply chain. This can effectively help enterprises reduce operating costs and improve their ability to respond to market demands, thereby increasing overall operational efficiency. An effective member selection method is an important basis for smooth dynamic supply chain operation. To address the problem of high numbers of decision attributes and low numbers of data samples for decision analysis, this paper proposes a dynamic supply chain member selection algorithm based on conditional generative adversarial networks (CGANs). To ensure that classification performance will not be reduced, the member classification method on the chain can successfully reduce the data dimension and complexity in the classification process. Furthermore, machine learning is used for analyzing and predicting purchase and inventory links in the supply chain. For the vehicle scheduling module, the path is reasonably planned to improve the operation efficiency. The integrated implementation of the SCM system is finalized using the SSH framework.(C) 2022 The Authors. Published by Elsevier Espana, S.L.U. on behalf of Journal of Innovation & Knowledge.", "journal": "JOURNAL OF INNOVATION & KNOWLEDGE", "category": "Business; Management", "annotated_keywords": ["artificial intelligen", "machine learning", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000813030200001", "keywords": "Security; Machine learning; Deep learning; Data privacy; Industrial wireless sensor network; Quantum readout; Authentication; Authorization; Energy consumption", "title": "Quantum readout and gradient deep learning model for secure and sustainable data access in IWSN", "abstract": "The industrial wireless sensor network (IWSN) is a surface-type of wireless sensor network (WSN) that suffers from high levels of security breaches and energy consumption. In modern complex industrial plants, it is essential to maintain the security, energy efficiency, and green sustainability of the network. In an IWSN, sensors are connected to the Internet in a non-monitored environment. Hence, non-authorized sensors can retrieve information from the IWSN. Therefore, to ensure that data access between sensors remains sustainable and secure, energy-efficient authentication and authorization are required. In this article, a novel Quantum Readout Gradient Secured Deep Learning (QR-GSDL) model is proposed to ensure that only trustworthy sensors can access IWSN data. The major objective of this QR-GSDL model is to create secure, energy-efficient IWSN to attain green sustainability and reduce the industrial impact on the environment. First, using the quantum readout and hash function, a registration method is designed to efficiently perform the registration process. Next, a gradient secured deep learning method is adopted to implement the authentication and authorization process in order to ensure energy-saving and secure data access. Simulations are conducted to evaluate the QR-GSDL model and compare its performance with that of three well-known models: online threshold anomaly detection, machine learning-based anomaly detection, and dynamic CNN. The simulation outcomes show that the proposed model is secure and energy-efficient for use in the IWSN. Moreover, the experimental results prove that the QR-SGDL model outperforms the existing models in terms of energy consumption, authentication rate, authentication time, and false acceptance rate.", "journal": "PEERJ COMPUTER SCIENCE", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods", "annotated_keywords": ["machine learning", "deep learning", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000766267100011", "keywords": "Feature extraction; Adaptation models; Training; Remote sensing; Data models; Graph neural networks; Aggregates; Coteaching; domain adaptation (DA); graph neural network (GNN); multimodal learning; multitarget adaptation", "title": "Multitarget Domain Adaptation for Remote Sensing Classification Using Graph Neural Network", "abstract": "Remote sensing deals with huge variations in geography, acquisition season, and a plethora of sensors. Considering the difficulty of collecting labeled data uniformly representing all scenarios, data-hungry deep learning models are often trained with labeled data in a source domain that is limited in the above-mentioned aspects. Domain adaptation (DA) methods can adapt such model for applying on target domains with different distributions from the source domain. However, most remote sensing DA methods are designed for single-target, thus requiring a separate target classifier to be trained for each target domain. To mitigate this, we propose multitarget DA in which a single classifier is learned for multiple unlabeled target domains. To build a multitarget classifier, it may be beneficial to effectively aggregate features from the labeled source and different unlabeled target domains. Toward this, we exploit coteaching based on the graph neural network that is capable of leveraging unlabeled data. We use a sequential adaptation strategy that first adapts on the easier target domains assuming that the network finds it easier to adapt to the closest target domain. We validate the proposed method on two different datasets, representing geographical and seasonal variation. Code is available at https://gitlab.lrz.de/ai4eo/da-multitarget-gnn/.", "journal": "IEEE GEOSCIENCE AND REMOTE SENSING LETTERS", "category": "Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": ["neural net", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000913909200001", "keywords": "Air pollution; Emotions; Social media; China; Fear; Haze pollution", "title": "The long-term impacts of air quality on fine-grained online emotional responses to haze pollution in 160 Chinese cities", "abstract": "Air pollution poses a great threat to public health and social stability by influencing multiple emotions. In particular, the air quality in developing countries is deteriorating along with rapid industrialization and urbanization, and multi-ple emotions may change along with regulation updates and air quality trending. Monitoring changes in public emo-tion is crucial for environmental governance. However, limited evidence exists for long-term effects of air quality on fine-grained emotions. Traditional surveys have the drawbacks of spatial limitations and high costs of time and money. Here, we use deep learning models to identify multiple emotions of over 10 million haze-related tweets and evaluate the effect of air quality on emotional predispositions for 160 cities from 2014 to 2019 in China. We find that sadness and joy are persistently associated with air quality, while anger and disgust are not. Surprisingly, the ef-fects on fear vanished in the last three years. Moreover, air pollution initially had a greater impact on expressed fear in cities with higher income, poorer air quality and a greater percentage of women. Through popularity ranking and dy-namic topic model, we interpretively revealed that people are no longer overly panicked and their attention is shifting toward policies and sources of haze. Our findings highlight the temporal evolution in the public's emotional response and provide significant implications for equitable public policies.", "journal": "SCIENCE OF THE TOTAL ENVIRONMENT", "category": "Environmental Sciences", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000850164400001", "keywords": "Higher heating value; ultimate analysis; refused derive fuel; American Society for Testing and Materials", "title": "Calorific value prediction models of processed refuse derived fuel 3 using ultimate analysis", "abstract": "Various models have been developed to predict the calorific value of Biomass but only a few models exist to predict this measure for the urban waste like Refuse Derived Fuel (RDF). In this paper, new models are introduced to predict the calorific value of RDF, as more advanced studies are required to be conducted with a focus on a distinct group of RDFs for validating the robustness of the models in the existing literature. The calorific value based on ultimate (elemental) analysis considers the contents of C, H, N, S, and O elements in RDF. Using empirical and machine learning methods, the newly established models accurately predicted the calorific value of the samples provided by a local municipality situated in Edmonton, Alberta, Canada. Furthermore, these new models demonstrated a lower bias and average absolute error than the other twelve previously published models pertinent to RDF material. Based on the established workflow the ultimate analysis-based models gave a higher coefficient of determination (R-2) value in the range 0.78 - 0.80, indicating that the developed model improves the prediction of calorific value for RDF. The newly developed machine-learning models showed better results than the empirical models developed in this study implying that complex correlations can be dealt effectively while predicting calorific values for RDF.", "journal": "BIOFUELS-UK", "category": "Energy & Fuels", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000825246000001", "keywords": "Ant colony optimizer; Gaussian mixture model; Histogram equalization; Histogram of oriented gradients; Local ternary pattern; Stack autoencoder; Vehicle type classification", "title": "Vehicle type classification using graph ant colony optimizer based stack autoencoder model", "abstract": "In the intelligent transport system, vehicle type classification technology plays a major role. With the growth of video processing and pattern recognition application, a deep learning model is proposed in this research article to improve vehicle type classification under dynamic background. Initially, the original video sequences are collected from MIOvision Traffic Camera Dataset (MIO-TCD), and CDnet2014 dataset. Additionally, the contrast and visible level of the video frames are improved by implementing histogram equalization method. Next, the moving vehicles are detected and tracked using Gaussian Mixture Model (GMM) and Kalman filter. Then, the feature extraction is accomplished using Dual Tree Complex Wavelet Transform (DTCWT), Histogram of Oriented Gradients (HOG), and Local Ternary Pattern (LTP) to extract the texture feature vectors. Further, a new graph clustering-Ant Colony Optimization (ACO) algorithm is proposed to select the active feature vectors for better vehicle type classification. Lastly, the selected active feature vectors are given as the input to stack autoencoder classifier to classify eleven vehicle types in MIO-TCD and four vehicle types in CDnet2014 dataset. In the experimental section, the graph ACO based stack autoencoder model achieved 99.09%, and 89.89% of classification accuracy on both MIO-TCD, and CDnet2014 dataset, which are better related to the existing models like attention based method, improved spatiotemporal sample consistency algorithm, and generative adversarial nets.", "journal": "MULTIMEDIA TOOLS AND APPLICATIONS", "category": "Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000744508100002", "keywords": "Bayes methods; Uncertainty; Computational modeling; Predictive models; Artificial neural networks; Data models; Training; Bayes method; deep neural network (NN); knowledge distillation; predictive uncertainty", "title": "Diffusion kernel-based predictive modeling of KRAS dependency in KRAS wild type cancer cell lines", "abstract": "Recent progress in clinical development of KRAS inhibitors has raised interest in predicting the tumor dependency on frequently mutated RAS-pathway oncogenes. However, even without such activating mutations, RAS proteins represent core components in signal integration of several membrane-bound kinases. This raises the question of applications of specific inhibitors independent from the mutational status. Here, we examined CRISPR/RNAi data from over 700 cancer cell lines and identified a subset of cell lines without KRAS gain-of-function mutations (KRAS(wt)) which are dependent on KRAS expression. Combining machine learning-based modeling and whole transcriptome data with prior variable selection through protein-protein interaction network analysis by a diffusion kernel successfully predicted KRAS dependency in the KRAS(wt) subgroup and in all investigated cancer cell lines. In contrast, modeling by RAS activating events (RAE) or previously published RAS RNA-signatures did not provide reliable results, highlighting the heterogeneous distribution of RAE in KRAS(wt) cell lines and the importance of methodological references for expression signature modeling. Furthermore, we show that predictors of KRAS(wt) models contain non-substitutable information signals, indicating a KRAS dependency phenotype in the KRAS(wt) subgroup. Our data suggest that KRAS dependent cancers harboring KRAS wild type status could be targeted by directed therapeutic approaches. RNA-based machine learning models could help in identifying responsive and non-responsive tumors.", "journal": "NPJ SYSTEMS BIOLOGY AND APPLICATIONS", "category": "Mathematical & Computational Biology", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000852517200004", "keywords": "Music; machine learning; human emotion; statistical modeling; genre-diverse music library; psychophysiology", "title": "Machine Learning-based Modeling and Prediction of the Intrinsic Relationship between Human Emotion and Music", "abstract": "Human emotion is one of the most complex psychophysiological phenomena and has been reported to be affected significantly by music listening. It is supposed that there is an intrinsic relationship between human emotion and music, which can be modeled and predicted quantitatively in a supervised manner. Here, a heuristic clustering analysis is carried out on large-scale free music archive to derive a genre-diverse music library, to which the emotional response of participants is measured using a standard protocol, consequently resulting in a systematic emotion-to-music profile. Eight machine learning methods are employed to statistically correlate the basic sound features of music audio tracks in the library with the measured emotional response of tested people to the music tracks in a training set and to blindly predict the emotional response from sound features in a test set. This study found that nonlinear methods are more robust and predictable but considerably more time-consuming than linear approaches. The neural networks have strong internal fittability but are associated with a significant overfitting issue. The support vector machine and Gaussian process exhibit both high internal stability and satisfactory external predictability in all used methods; they are considered as promising tools to model, predict, and explain the intrinsic relationship between human emotion and music. The psychological basis and perceptional implication underlying the built machine learning models are also discussed to find out the key music factors that affect human emotion.", "journal": "ACM TRANSACTIONS ON APPLIED PERCEPTION", "category": "Computer Science, Software Engineering", "annotated_keywords": ["neural net", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000854835300002", "keywords": "Artificial neural networks; Aspect ratio; Yield of nanofibrillation; Lignocellulosic micro/nanofibers; Multiple linear regression; Random forest", "title": "Prediction of cellulose micro/nanofiber aspect ratio and yield of nanofibrillation using machine learning techniques", "abstract": "Predictive monitoring of two key properties of nanocellulose, aspect ratio and yield of nanofibrillation, would help manufacturers control and optimize production processes, given the uncertainty that still surrounds their influential factors. For that, 20 different types of cellulosic and lignocellulosic micro/nanofibers produced from spruce and pine softwoods, and by different pre-treatment and fibrillation techniques, were used as training and testing datasets aiming at the development and evaluation of three machine learning models. The models used were Random Forests (RF), Linear Regression (LR) and Artificial Neural Networks (ANN), broadening the scope of our previous work (Santos et al. in Cellulose 29:5609-5622, 2022. https://doi.org/10.1007/s10570-022-04631-5). Performance of these models were evaluated by comparing statistical parameters such as Mean Absolute Percentage Error (MAPE) and R-2. For the aspect ratio and the yield of nanofibrillation, inputs were chosen among these easily controlled or measured variables: Total lignin (wt%), Cellulose (wt%), Hemicellulose (wt%), Extractives (wt%), HPH Energy Consumption (kWh/kg), Cationic Demand (mu eq/g), Transmittance at 600 nm and Consistency index (Ostwald-De Waele's k). In both cases, the ANN models trained here provided satisfactory estimates of aspect ratio (MAPE = 4.54% and R-2 =0.96) and the yield of nanofibrillation (MAPE= 6.74% and R-2 =0.98), being able to capture the effect of the applied energy along the fibrillation process. RF and LR models resulted in correlation coefficients of 0.93 and 0.95, respectively, for aspect ratio, while for yield of nanofibrillation the correlation coefficients were 0.87 and 0.92.", "journal": "CELLULOSE", "category": "Materials Science, Paper & Wood; Materials Science, Textiles; Polymer Science", "annotated_keywords": ["machine learning", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000835303700007", "keywords": "artificial intelligence; automated diagnosis; digital diagnosis; supervised learning; orthodontics; dento-facial orthopedics; deep learning; machine learning; artificial neural networks; convolutional neural networks", "title": "Machine learning-based prediction of massive perioperative allogeneic blood transfusion in cardiac surgery", "abstract": "BACKGROUND Massive perioperative allogeneic blood transfusion, that is, perioperative transfusion of more than 10 units of packed red blood cells (pRBC), is one of the main contributors to perioperative morbidity and mortality in cardiac surgery. Prediction of perioperative blood transfusion might enable preemptive treatment strategies to reduce risk and improve patient outcomes while reducing resource utilisation. We, therefore, investigated the precision of five different machine learning algorithms to predict the occurrence of massive perioperative allogeneic blood transfusion in cardiac surgery at our centre. OBJECTIVE Is it possible to predict massive perioperative allogeneic blood transfusion using machine learning? DESIGN Retrospective, observational study. SETTING Single adult cardiac surgery centre in Austria between 01 January 2010 and 31 December 2019. PATIENTS Patients undergoing cardiac surgery. MAIN OUTCOME MEASURES Primary outcome measures were the number of patients receiving at least 10 units pRBC, the area under the curve for the receiver operating characteristics curve, the F1 score, and the negative-predictive (NPV) and positive-predictive values (PPV) of the five machine learning algorithms used to predict massive perioperative allogeneic blood transfusion. RESULTS A total of 3782 (1124 female:) patients were enrolled and 139 received at least 10 pRBC units. Using all features available at hospital admission, massive perioperative allogeneic blood transfusion could be excluded rather accurately. The best area under the curve was achieved by Random Forests: 0.810 (0.76 to 0.86) with high NPV of 0.99). This was still true using only the eight most important features [area under the curve 0.800 (0.75 to 0.85)]. CONCLUSION Machine learning models may provide clinical decision support as to which patients to focus on for perioperative preventive treatment in order to preemptively reduce massive perioperative allogeneic blood transfusion by predicting, which patients are not at risk.", "journal": "EUROPEAN JOURNAL OF ANAESTHESIOLOGY", "category": "Anesthesiology", "annotated_keywords": ["machine learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000876451100001", "keywords": "CDOM; Hyperspectral imagery; Reflectance band selection; Absorption coefficient; Machine learning; Spatiotemporal distribution", "title": "Application of airborne hyperspectral imagery to retrieve spatiotemporal CDOM distribution using machine learning in a reservoir", "abstract": "Colored dissolved organic matter (CDOM) in inland waters is used as a proxy to estimate dissolved organic carbon (DOC) and may be a key indicator of water quality and nutrient enrichment. CDOM is optically active fraction of DOC so that remote sensing techniques can remotely monitor CDOM with wide spatial coverage. However, to effectively retrieve CDOM using optical algorithms, it may be critical to select the absorption co-efficient at an appropriate wavelength as an output variable and to optimize input reflectance wavelengths. In this study, we constructed a CDOM retrieval model using airborne hyperspectral reflectance data and a machine learning model such as random forest. We evaluated the best combination of input wavelength bands and the CDOM absorption coefficient at various wavelengths. Seven sampling events for airborne hyperspectral imagery and CDOM absorption coefficient data from 350 nm to 440 nm over two years (2016-2017) were used, and the collected data helped train and validate the random forest model in a freshwater reservoir. An absorption co-efficient of 355 nm was selected to best represent the CDOM concentration. The random forest exhibited the best performance for CDOM estimation with an R2 of 0.85, Nash-Sutcliffe efficiency of 0.77, and percent bias of 3.88, by using a combination of three reflectance bands: 475, 497, and 660 nm. The results show that our model can be utilized to construct a CDOM retrieving algorithm and evaluate its spatiotemporal variation across a reservoir.", "journal": "INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION", "category": "Remote Sensing", "annotated_keywords": ["machine learning", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000759533300001", "keywords": "flood runoff; deep neural network; river stage; precipitation; visualization; data-driven modeling", "title": "River Stage Modeling with a Deep Neural Network Using Long-Term Rainfall Time Series as Input Data: Application to the Shimanto-River Watershed", "abstract": "The increasing frequency of devastating floods from heavy rainfall-associated with climate change-has made river stage prediction more important. For steep, forest-covered mountainous watersheds, deep-learning models may improve prediction of river stages from rainfall. Here we use the framework of multilayer perceptron (MLP) neural networks to develop such a river stage model. The MLP is constructed for the Shimanto river, which lies in southwestern Japan under a mild, rain-heavy climate. Our input for stage estimation, as well as prediction, is a long-term rainfall time series. With a one-year time series of rainfall, the model estimates the stage with RMSE less than 67 cm for about 10 m of stage peaks, as well as accurately simulating stage-time fluctuations. Furthermore, the forecast model can predict the stage without rainfall forecasts up to three hours ahead. To estimate the base flow stages as well as flood peaks with high precision, we found that the rainfall time series should be at least one year. This indicates that the use of a long rainfall time series enables one to model the contributions of ground water and evaporation. Given that the delay between the arrival time of rainfall at a rain-gauge to the outlet change is well-simulated, the physical concepts of runoff appear to be soundly embedded in the MLP.", "journal": "WATER", "category": "Environmental Sciences; Water Resources", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000855673700006", "keywords": "scRNA-seq; imputation; gene expression; dropout event; a discriminative stacked autoencoder; DSAE-Impute", "title": "DSAE-Impute: Learning Discriminative Stacked Autoencoders for Imputing Single-cell RNA-seq Data", "abstract": "Background: Due to the limited amount of mRNA in single-cell, there are always many missing values in scRNA-seq data, making it impossible to accurately quantify the expression of single-cell RNA. The dropout phenomenon makes it impossible to detect the truly expressed genes in some cells, which greatly affects the downstream analysis of scRNA-seq data, such as cell cluster analysis and cell development trajectories. Objective: This research proposes an accurate deep learning method to impute the missing values in scRNA-seq data. DSAE-Impute employs stacked autoencoders to capture gene expression characteristics in the original missing data and combines the discriminative correlation matrix between cells to capture global expression features during the training process to accurately predict missing values. Methods: We propose a novel deep learning model based on the discriminative stacked autoencoders to impute the missing values in scRNA-seq data, named DSAE-Impute. DSAE-Impute embeds the discriminative cell similarity to perfect the feature representation of stacked autoencoders and comprehensively learns the scRNA-seq data expression pattern through layer-by-layer training to achieve accurate imputation. Results: We have systematically evaluated the performance of DSAE-Impute in the simulation and real datasets. The experimental results demonstrate that DSAE-Impute significantly improves downstream analysis, and its imputation results are more accurate than other state-of-the-art imputation methods. Conclusion: Extensive experiments show that compared with other state-of-the-art methods, the imputation results of DSAE-Impute on simulated and real datasets are more accurate and helpful for downstream analysis.", "journal": "CURRENT BIOINFORMATICS", "category": "Biochemical Research Methods; Mathematical & Computational Biology", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000929926300001", "keywords": "Post -inflammation pigmentation; Susceptibility; Back propagation neural network; Multi-head self-attention", "title": "Novel neural network model for predicting susceptibility of facial post-inflammatory hyperpigmentation", "abstract": "Background:: To construct a neural network model (ATBP) for predicting susceptibility to Post-inflammatory hyperpigmentation (PIH), which is a rapid, objective, and reliable decision-support method before physical and chemical interventions in dermatology clinics for pigment disorders. Material and methods: : A dataset was established based on the VISIA Skin Analysis System detection results of 1953 patients with pigment disorders including 93,477 labeled data under 8 indicators. A novel Post-inflammatory hyperpigmentation susceptibility prediction model incorporating Multi-head self-attention mechanism and Back-propagation neural network is proposed to capture the patterns of skin detection data to predict PIH susceptibility. Results: : The results of comparison experiments indicate that Attentive BP (Back Propagation Neural Network) has a significant superiority in prediction accuracy (0.8604) compared with other machine learning models. The ablation experiments prove that the Multi-head self-attention mechanism substantially improves the accuracy and the stability of prediction. The results of the 10-fold cross-validation experiment prove that ATBP is robust and avoids turbulence in predicting. Conclusion:: Leveraging Multi-head self-attention mechanism and the architecture advantage of BPNN, the proposed model ATBP obtains the robust and efficient prediction performance in predicting PIH susceptibility via processing large-scale and hi-dimension data, i.e., considering comprehensive skin conditions of individual pa-tient. It can be proved from the experimental results that the proposed model is reliable for decision-support work of PIH susceptibility.", "journal": "MEDICAL ENGINEERING & PHYSICS", "category": "Engineering, Biomedical", "annotated_keywords": ["neural net", "neural net", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000899205600002", "keywords": "Rainfall; Minimum temperature; Maximum temperature; Terrain features; Machine learning; Thin -plate spline regression", "title": "Predicting temperature and rainfall for plantation forestry in Mpumalanga, South Africa, using locally developed climate models", "abstract": "Climate is known to impact the growth of plantation forests in South Africa. To characterize the conditions, this study developed temperature and rainfall models for specific plantations in the Lowveld Escarpment and Highveld forestry regions of South Africa. Global position, altitude, slope, aspect, and topographic position indices were considered when developing the models. Seasonal minimum (Tmin) and maximum (Tmax) temper-ature models were developed using observations obtained from 43 weather stations for the period 2013 until 2019. Out of six models which were initially evaluated, two machine learning models, random forests and linear -based, were selected for further scrutiny. R-square values for models predicting Tmin on the Lowveld Escarpment ranged from 0.64 to 0.89 for seasonal models. For the Highveld, seasonal models to predict Tmin had R-square values ranging from 0.47 to 0.84. Seasonal Lowveld Escarpment Tmax models were also significant with R-square values ranging from 0.76 to 0.89. The equivalent models for the Highveld had R-square values ranging from 0.54 to 0.83. Rainfall models were developed from 69 rainfall stations spanning a twenty-year period (1999 - 2019). Multiple median linear regression was used to model mean seasonal rainfall (Pmedian) with R-square values ranging from 0.87 to 0.90. These findings demonstrate that it is possible to develop accurate local climate models using global positioning and local terrain features. Comparing the models to transformed WorldClim 2.1 pre-dictions, it was found that the majority of the newly developed climate models were more accurate than the global models, with the exception of minimum temperature models which was marginally better. However, the tranformed WorldClim 2.1 predictions were surprisingly accurate. The local climate models use a finer scale, and they improve the understanding of how terrain features impact regional climate and historic tree growth. In addition, the finer scale models give insights into the relationship between the local terrain condition and regional climate. It is also likely that these models can be used for other forestry regions within Southern Africa, and similar approaches may be relevant globally.", "journal": "AGRICULTURAL AND FOREST METEOROLOGY", "category": "Agronomy; Forestry; Meteorology & Atmospheric Sciences", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000787395400001", "keywords": "pavement distress; image classification; deep learning; vision transformer; LeViT; visual interpretation", "title": "A Fast Inference Vision Transformer for Automatic Pavement Image Classification and Its Visual Interpretation Method", "abstract": "Traditional automatic pavement distress detection methods using convolutional neural networks (CNNs) require a great deal of time and resources for computing and are poor in terms of interpretability. Therefore, inspired by the successful application of Transformer architecture in natural language processing (NLP) tasks, a novel Transformer method called LeViT was introduced for automatic asphalt pavement image classification. LeViT consists of convolutional layers, transformer stages where Multi-layer Perception (MLP) and multi-head self-attention blocks alternate using the residual connection, and two classifier heads. To conduct the proposed methods, three different sources of pavement image datasets and pre-trained weights based on ImageNet were attained. The performance of the proposed model was compared with six state-of-the-art (SOTA) deep learning models. All of them were trained based on transfer learning strategy. Compared to the tested SOTA methods, LeViT has less than 1/8 of the parameters of the original Vision Transformer (ViT) and 1/2 of ResNet and InceptionNet. Experimental results show that after training for 100 epochs with a 16 batch-size, the proposed method acquired 91.56% accuracy, 91.72% precision, 91.56% recall, and 91.45% F1-score in the Chinese asphalt pavement dataset and 99.17% accuracy, 99.19% precision, 99.17% recall, and 99.17% F1-score in the German asphalt pavement dataset, which is the best performance among all the tested SOTA models. Moreover, it shows superiority in inference speed (86 ms/step), which is approximately 25% of the original ViT method and 80% of some prevailing CNN-based models, including DenseNet, VGG, and ResNet. Overall, the proposed method can achieve competitive performance with fewer computation costs. In addition, a visualization method combining Grad-CAM and Attention Rollout was proposed to analyze the classification results and explore what has been learned in every MLP and attention block of LeViT, which improved the interpretability of the proposed pavement image classification model.", "journal": "REMOTE SENSING", "category": "Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": ["neural net", "natural language processing", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000853051800002", "keywords": "exponential stabilization; Hopfield neural network; non-Lipschitz continuous activation functions", "title": "Choice of Activation Function in Convolutional Neural Networks for Person Re-Identification in Video Surveillance Systems", "abstract": "In this paper, we improve the accuracy of person re-identification in images obtained from distributed video surveillance systems by choosing activation functions for convolutional neural networks. The most popular activation functions used for object detection, namely, ReLU, Leaky-ReLU, PReLU, RReLU, ELU, SELU, GELU, Swish, and Mish, are analyzed based on the following metrics: Rank1, Rank5, Rank10, mAP, and training time. For feature extraction, ResNet-50, DenseNet-121, and DarkNet-53 architectures are employed. The experimental study is carried out on open datasets Market1501 and PolReID. The accuracy of person re-identification is assessed after thrice-repeated training and testing with different activation functions, neural network architectures, and datasets by averaging the values of the selected metrics.", "journal": "PROGRAMMING AND COMPUTER SOFTWARE", "category": "Computer Science, Software Engineering", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000855530400001", "keywords": "Diaphragm wall deflection; Braced excavation; Finite element analysis; Clays; Meta-heuristic algorithms; Functional linked neural network", "title": "Refractive Index of Hemoglobin Analysis: A Comparison of Alternating Conditional Expectations and Computational Intelligence Models br", "abstract": "Hemoglobin is one of the most important blood elements, and its optical properties will determine all other optical properties of human blood. Since the refractive index (RI) of hemoglobin plays a vital role as a non-invasive indicator of some illnesses, accurate calculation of it would be of great importance. Moreover, measurement of the RI of hemoglobin in the laboratory is time-consuming and expensive; thus, developing a smart approach to estimate this parameter is necessary. In this research, four viable strategies were used to make a quantitative correlation between the RI of hemoglobin and its influencing parameters including the concentration, wavelength, and temperature. First, alternating conditional expectations (ACE), a statistical approach, was employed to generate a correlation to predict the RI of hemoglobin. Then, three different optimized intelligent techniques-optimized neural network (ONN), optimized fuzzy inference system (OFIS), and optimized support vector regression (OSVR)-were used to model the RI. A bat-inspired (BA) algorithm was embedded in the formulation of intelligent models to obtain the optimal values of weights and biases of an artificial neural network, membership functions of the fuzzy inference system, and free parameters of support vector regression. The coefficient of determination, root-mean-square error, average absolute relative error, and symmetric mean absolute percentage error for each of the ACE, ONN, OFIS, and OSVR were found as the measure of each model's accuracy. Results showed that ACE and optimized models (ONN, OFIS, and OSVR) have promising results in the estimation of hemoglobin's RI. Collectively, ACE outperformed ONN, OFIS, and OSVR, while sensitivity analysis indicated that the concentration, wavelength, and, lastly, temperature would have the highest impact on the RI.", "journal": "ACS OMEGA", "category": "Chemistry, Multidisciplinary", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000832820200001", "keywords": "Activation function; Deep learning architectures; Flexible and trainable P plus FELU", "title": "P plus FELU: Flexible and trainable fast exponential linear unit for deep learning architectures", "abstract": "Activation functions have an important role in obtaining the most appropriate output by processing the information coming into the network in deep learning architectures. Deep learning architectures are widely used in areas such as image processing applications, time series, and disease classification, generally in line with the analysis of large and complex data. Choosing the appropriate architecture and activation function is an important factor in achieving successful learning and classification performance. There are many studies to improve the performance of deep learning architectures and to overcome the disappearing gradient and negative region problems in activation functions. A flexible and trainable fast exponential linear unit (P + FELU) activation function is proposed to overcome existing problems. With the proposed P + FELU activation function, a higher success rate and faster calculation time can be achieved by incorporating the advantages of fast exponentially linear unit (FELU), exponential linear unit (ELU), and rectified linear unit (RELU) activation functions. Performance evaluations of the proposed P + FELU activation function were made on MNIST, CIFAR-10, and CIFAR-100 benchmark datasets. Experimental evaluations have shown that the proposed activation function outperforms the ReLU, ELU, SELU, MPELU, TReLU, and FELU activation functions and effectively improves the noise robustness of the network. Experimental results show that this activation function with \"flexible and trainable\" properties can effectively prevent vanishing gradient and make multilayer perceptron neural networks deeper.", "journal": "NEURAL COMPUTING & APPLICATIONS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net", "deep learning", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000820522900001", "keywords": "Polynomial neural networks; tensor decompositions; high-order polynomials; generative models; discriminative models; face verification", "title": "Deep Polynomial Neural Networks", "abstract": "Deep convolutional neural networks (DCNNs) are currently the method of choice both for generative, as well as for discriminative learning in computer vision and machine learning. The success of DCNNs can be attributed to the careful selection of their building blocks (e.g., residual blocks, rectifiers, sophisticated normalization schemes, to mention but a few). In this paper, we propose pi-Nets, a new class of function approximators based on polynomial expansions. pi-Nets are polynomial neural networks, i.e., the output is a high-order polynomial of the input. The unknown parameters, which are naturally represented by high-order tensors, are estimated through a collective tensor factorization with factors sharing. We introduce three tensor decompositions that significantly reduce the number of parameters and show how they can be efficiently implemented by hierarchical neural networks. We empirically demonstrate that pi-Nets are very expressive and they even produce good results without the use of non-linear activation functions in a large battery of tasks and signals, i.e., images, graphs, and audio. When used in conjunction with activation functions, pi-Nets produce state-of-the-art results in three challenging tasks, i.e., image generation, face verification and 3D mesh representation learning. The source code is available at https://github.com/grigorisg9gr/polynomial_nets.", "journal": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": ["neural net", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000874583300001", "keywords": "Natural gradient; Helmholtz machine; Wake-Sleep; Information geometry", "title": "Natural Reweighted Wake-Sleep", "abstract": "Helmholtz Machines (HMs) are a class of generative models composed of two Sigmoid Belief Networks (SBNs), acting respectively as an encoder and a decoder. These models are commonly trained using a two-step optimization algorithm called Wake-Sleep (WS) and more recently by improved versions, such as Reweighted Wake-Sleep (RWS) and Bidirectional Helmholtz Machines (BiHM). The locality of the connections in an SBN induces sparsity in the Fisher Information Matrices associated to the probabilistic models, in the form of a finely-grained block-diagonal structure. In this paper we exploit this property to efficiently train SBNs and HMs using the natural gradient. We present a novel algorithm, called Natural Reweighted Wake-Sleep (NRWS), that corresponds to the geometric adaptation of its standard version. In a similar manner, we also introduce Natural Bidirectional Helmholtz Machine (NBiHM). Differently from previous work, we will show how for HMs the natural gradient can be efficiently computed without the need of introducing any approximation in the structure of the Fisher information matrix. The experiments performed on standard datasets from the literature show a consistent improvement of NRWS and NBiHM not only with respect to their non-geometric baselines but also with respect to state-of-the-art training algorithms for HMs. The improvement is quantified both in terms of speed of convergence as well as value of the log-likelihood reached after training.(c) 2022 Elsevier Ltd. All rights reserved.", "journal": "NEURAL NETWORKS", "category": "Computer Science, Artificial Intelligence; Neurosciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000829914700001", "keywords": "early prediction; pressure injury; feature extraction; LSTM; neural networks", "title": "Early Prediction of Pressure Injury with Long Short-term Memory Networks", "abstract": "Early diagnosis of pressure injury has always been a challenging problem. Pressure injury can spontaneously heal or develop into decubitus ulcers. Few methods are available to predict the growth trend at the early stage of pressure injury, although this stage is a critical time for preventing and treating pressure injury. To address this issue, artificial intelligence algorithms were used in this work with image processing technology to predict the growth trend of early-stage pressure injury. A long short-term memory (LSTM) network, which is a specialized recurrent neural network, was adopted to predict future events based on images collected from hairless rats that made up the pressure injury models. The images were processed with ImageJ software to extract key features, then used to train the LSTM networks. Two types of LSTM network were used to predict the development trend: single-variate and multivariate. The analysis results demonstrated that multivariate LSTM is more effective than single-variate LSTM and has high potential to be applied in the prediction of early-stage pressure injury.", "journal": "SENSORS AND MATERIALS", "category": "Instruments & Instrumentation; Materials Science, Multidisciplinary", "annotated_keywords": ["artificial intelligen", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000862586700009", "keywords": "Long Short-Term Memory (LSTM); Formal Concept Analysis (FCA); COVID-19; Hospital admissions", "title": "Defining factors in hospital admissions during COVID-19 using LSTM-FCA explainable model", "abstract": "Outbreaks of the COVID-19 pandemic caused by the SARS-CoV-2 infection that started in Wuhan, China, have quickly spread worldwide. The current situation has contributed to a dynamic rate of hospital admissions. Global efforts by Artificial Intelligence (AI) and Machine Learning (ML) communities to develop solutions to assist COVID-19-related research have escalated ever since. However, despite overwhelming efforts from the AI and ML community, many machine learning-based AI systems have been designed as black boxes. This paper proposes a model that utilizes Formal Concept Analysis (FCA) to explain a machine learning technique called Long-short Term Memory (LSTM) on a dataset of hospital admissions due to COVID-19 in the United Kingdom. This paper intends to increase the transparency of decision-making in the era of ML by using the proposed LSTM-FCA explainable model. Both LSTM and FCA are able to evaluate the data and explain the model to make the results more understandable and interpretable. The results and discussions are helpful and may lead to new research to optimize the use of ML in various real-world applications and to contain the disease.", "journal": "ARTIFICIAL INTELLIGENCE IN MEDICINE", "category": "Computer Science, Artificial Intelligence; Engineering, Biomedical; Medical Informatics", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000763126600001", "keywords": "smart healthcare system; wearable devices; person's movement identification; federated Learning; edge servers; deep reinforcement learning; bidirectional long short-term memory", "title": "FL-PMI: Federated Learning-Based Person Movement Identification through Wearable Devices in Smart Healthcare Systems", "abstract": "Recent technological developments, such as the Internet of Things (IoT), artificial intelligence, edge, and cloud computing, have paved the way in transforming traditional healthcare systems into smart healthcare (SHC) systems. SHC escalates healthcare management with increased efficiency, convenience, and personalization, via use of wearable devices and connectivity, to access information with rapid responses. Wearable devices are equipped with multiple sensors to identify a person's movements. The unlabeled data acquired from these sensors are directly trained in the cloud servers, which require vast memory and high computational costs. To overcome this limitation in SHC, we propose a federated learning-based person movement identification (FL-PMI). The deep reinforcement learning (DRL) framework is leveraged in FL-PMI for auto-labeling the unlabeled data. The data are then trained using federated learning (FL), in which the edge servers allow the parameters alone to pass on the cloud, rather than passing vast amounts of sensor data. Finally, the bidirectional long short-term memory (BiLSTM) in FL-PMI classifies the data for various processes associated with the SHC. The simulation results proved the efficiency of FL-PMI, with 99.67% accuracy scores, minimized memory usage and computational costs, and reduced transmission data by 36.73%.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": ["artificial intelligen", "reinforcement learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000827719100001", "keywords": "Josephson junction; quantum neuron; quantum-classical neural networks; superconducting quantum interferometer", "title": "A superconducting adiabatic neuron in a quantum regime", "abstract": "We explore the dynamics of an adiabatic neural cell of a perceptron artificial neural network in a quantum regime. This mode of cell operation is assumed for a hybrid system of a classical neural network whose configuration is dynamically adjusted by a quantum co-processor. Analytical and numerical studies take into account non-adiabatic processes as well as dissipation, which leads to smoothing of quantum coherent oscillations. The obtained results indicate the conditions under which the neuron possesses the required sigmoid activation function.", "journal": "BEILSTEIN JOURNAL OF NANOTECHNOLOGY", "category": "Nanoscience & Nanotechnology; Materials Science, Multidisciplinary; Physics, Applied", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000809363100001", "keywords": "Heuristic algorithms; Q-learning; Nonlinear dynamical systems; Approximation algorithms; Iterative algorithms; Convergence; Artificial neural networks; Adaptive dynamic programming (ADP); neural network (NN); off-policy technique; optimal tracking control (OTC); Q-learning", "title": "Model-Free Optimal Tracking Control of Nonlinear Input-Affine Discrete-Time Systems via an Iterative Deterministic Q-Learning Algorithm", "abstract": "In this article, a novel model-free dynamic inversion-based Q-learning (DIQL) algorithm is proposed to solve the optimal tracking control (OTC) problem of unknown nonlinear input-affine discrete-time (DT) systems. Compared with the existing DIQL algorithm and the discount factor-based Q-learning (DFQL) algorithm, the proposed algorithm can eliminate the tracking error while ensuring that it is model-free and off-policy. First, a new deterministic Q-learning iterative scheme is presented, and based on this scheme, a model-based off-policy DIQL algorithm is designed. The advantage of this new scheme is that it can avoid the training of unusual data and improve data utilization, thereby saving computing resources. Simultaneously, the convergence and stability of the designed algorithm are analyzed, and the proof that adding probing noise into the behavior policy does not affect the convergence is presented. Then, by introducing neural networks (NNs), the model-free version of the designed algorithm is further proposed so that the OTC problem can be solved without any knowledge about the system dynamics. Finally, three simulation examples are given to demonstrate the effectiveness of the proposed algorithm.", "journal": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Hardware & Architecture; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000816142700001", "keywords": "image analysis; computer vision; machine learning; deep learning", "title": "Using Deep Neural Networks for Human Fall Detection Based on Pose Estimation", "abstract": "Requests for caring for and monitoring the health and safety of older adults are increasing nowadays and form a topic of great social interest. One of the issues that lead to serious concerns is human falls, especially among aged people. Computer vision techniques can be used to identify fall events, and Deep Learning methods can detect them with optimum accuracy. Such imaging-based solutions are a good alternative to body-worn solutions. This article proposes a novel human fall detection solution based on the Fast Pose Estimation method. The solution uses Time-Distributed Convolutional Long Short-Term Memory (TD-CNN-LSTM) and 1Dimentional Convolutional Neural Network (1D-CNN) models, to classify the data extracted from image frames, and achieved high accuracies: 98 and 97% for the 1D-CNN and TD-CNN-LSTM models, respectively. Therefore, by applying the Fast Pose Estimation method, which has not been used before for this purpose, the proposed solution is an effective contribution to accurate human fall detection, which can be deployed in edge devices due to its low computational and memory demands.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000785892500001", "keywords": "GNSS; spoofing detection; jamming detection; multipath detection; LSTM; 1D-CNN", "title": "Research on GNSS interference recognition based on ROI of correlation peaks", "abstract": "The general Global Navigation Satellite System (GNSS) receiver faces several challenges because of jamming signals, spoofing signals, and multipath signals, which severely influence its safety. In this paper, a receiver scheme with an interference recognition function is designed. In the latter, the correlation peak with different shapes is produced according to different interferences. The machine learning method is then applied to recognize and classify these feature maps. This transforms the interference recognition problem into a machine learning-based classification problem. In order to reduce the complexity of the machine learning network, only the finite-length correlation peak region of interest (ROI) is extracted as network input, endowing the shallow neural network with the interference recognition function. Afterward, five data acquisition environments are designed: authentic, spoofing, jamming, non-line-of-sight (NLOS) multipath, and line-of-sight (LOS) multipath. Moreover, several experimental data are acquired, followed by the production of the correlation peak maps dataset, that are then learned and tested using two machine learning networks: one-dimensional convolutional neural network (1D-CNN) and bidirectional long short-term memory neural network (BiLSTM-NN). The results demonstrate that a recognition accuracy rate of over 98% can be reached using the shallow machine learning network.", "journal": "INTERNATIONAL JOURNAL OF SATELLITE COMMUNICATIONS AND NETWORKING", "category": "Engineering, Aerospace; Telecommunications", "annotated_keywords": ["neural net", "machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000806767400005", "keywords": "Time-varying channel estimation; Orthogonal frequency division multiplexing; Multipath effect; UAV; Air ground channel modelling; Deep learning", "title": "Time-Varying Channel Estimation Based on Air-Ground Channel Modelling and Modulated Learning Networks", "abstract": "To improve the time-varying channel estimation accuracy of orthogonal frequency division multiplexing air-ground datalink in complex environment, this paper proposes a time-varying air-ground channel estimation algorithm based on the modulated learning networks, termed as MB-ChanEst-TV. The algorithm integrates the modulated convolutional neural networks (MCNN) with the bidirectional long short term memory (Bi-LSTM), where the MCNN subnetworks accomplish channel interpolation in frequency domain and compress the network model while the Bi-LSTM subnetworks achieve channel prediction in time domain. Considering the unique characteristics of airframe shadowing for unmanned aircraft systems, we propose to combine the classical 2-ray channel model with the tapped delay line model and present a more realistic channel impulse response samples generation approach, whose code and dataset have been made publicly available. We demonstrate the effectiveness of our proposed approach on the generated dataset, where experimental results indicate that the MB-ChanEst-TV model outperforms existing state-of-the-art methods with a lower estimation error and better bit error ratio performance under different signal to noise ratio conditions. We also analyze the effect of roll angle of the aircraft and the duration percentage of the airframe shadow on the channel estimation.", "journal": "CHINESE JOURNAL OF ELECTRONICS", "category": "Engineering, Electrical & Electronic", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000805509400003", "keywords": "Stock prediction; Graph data; Graph neural network; Multi-source data", "title": "A graph neural network-based stock forecasting method utilizing multi-source heterogeneous data fusion", "abstract": "The study of the prediction of stock market volatility is of great significance to rationally control financial market risks and increase excessive investment returns and has received extensive attention from academic and commercial circles. However, as a dynamic and complex system, the stock market is affected by multiple factors and has a comprehensive capability to include complex financial data. Given that the explanatory variables of influencing factors are diverse, heterogeneous and complex, the existing intelligent algorithms have great limitations for the analysis and processing of multi-source heterogeneous data in the stock market. Therefore, this study adopts the edge weight and information transmission mechanism suitable for subgraph data to complete node screening, the gate recurrent unit (GRU) and long short-term memory (LSTM) to aggregate subgraph nodes. The compiled data contain the metapaths of three types of index data, and the introduction of the association relationship attention dimension effectively mines the implicit meanings of multi-source heterogeneous data. The metapath attention mechanism is combined with a graph neural network to complete the classification of multi-source heterogeneous graph data, by which the prediction of stock market volatility is realized. The results show that the above method is feasible for the fusion of heterogeneous stock market data and the mining of implicit semantic information of association relations. The accuracy of the proposed method for the prediction of stock market volatility in this study is 16.64% higher than that of the dimensional reduction index and 14.48% higher than that of other methods for the fusion and prediction of heterogeneous data using the same model.", "journal": "MULTIMEDIA TOOLS AND APPLICATIONS", "category": "Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000744531400015", "keywords": "Double deep Q-network; energy harvesting system; forward progress; nonvolatile processor (NVP); reinforcement learning", "title": "Deep Reinforcement-Learning-Guided Backup for Energy Harvesting Powered Systems", "abstract": "Energy harvesting technology has been widely developed as a promising alternative of battery to power embedded systems. However, energy harvesting powered embedded systems may have potential frequent power interruptions due to unstable energy supply. Nonvolatile processors (NVPs) are proposed to survive power failures by saving volatile data to nonvolatile memory (NVM) upon power failures and resuming them after power comes back. Traditionally, backup is triggered immediately when an energy warning occurs. However, it is also possible to more aggressively utilize the residual energy for program execution to improve forward progress. In this work, we propose a deep reinforcement-learning-guided backup strategy to improve forward progress in energy harvesting powered intermittent embedded systems. The experimental results show an average of 8.3%, 51.6%, and 325.3% improved forward progress compared with Q-learning, the related work ALD, and traditional instant backup, respectively.", "journal": "IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS", "category": "Computer Science, Hardware & Architecture; Computer Science, Interdisciplinary Applications; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000782821300001", "keywords": "Quaternions; Mathematical models; Adaptation models; Convergence; Sun; Learning systems; Computer science; Complex representation; dynamic Sylvester quaternion matrix equation (DSQME); finite-time convergence; zeroing neural network (ZNN)", "title": "ZNNs With a Varying-Parameter Design Formula for Dynamic Sylvester Quaternion Matrix Equation", "abstract": "This article aims to studying how to solve dynamic Sylvester quaternion matrix equation (DSQME) using the neural dynamic method. In order to solve the DSQME, the complex representation method is first adopted to derive the equivalent dynamic Sylvester complex matrix equation (DSCME) from the DSQME. It is proven that the solution to the DSCME is the same as that of the DSQME in essence. Then, a state-of-the-art neural dynamic method is presented to generate a general dynamic-varying parameter zeroing neural network (DVPZNN) model with its global stability being guaranteed by the Lyapunov theory. Specifically, when the linear activation function is utilized in the DVPZNN model, the corresponding model [termed linear DVPZNN (LDVPZNN)] achieves finite-time convergence, and a time range is theoretically calculated. When the nonlinear power-sigmoid activation function is utilized in the DVPZNN model, the corresponding model [termed power-sigmoid DVPZNN (PSDVPZNN)] achieves the better convergence compared with the LDVPZNN model, which is proven in detail. Finally, three examples are presented to compare the solution performance of different neural models for the DSQME and the equivalent DSCME, and the results verify the correctness of the theories and the superiority of the proposed two DVPZNN models.", "journal": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Hardware & Architecture; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000860717900002", "keywords": "Recovery boiler; Ash deposits; Biomass; CO(2 )neutral; Reinforcement learning", "title": "Reinforcement learning-based optimal operation of ash deposit removal system to improve recycling efficiency of biomass for CO2 reduction", "abstract": "Black liquor from pulp mills is valuable biomass that can be recycled as a CO2-neutral, renewable fuel. However, biomass combustion produces significant ash deposits reducing the overall process efficiency. A recovery boiler generally uses an ash deposit removal system (ADRS), but ADRS operation is inefficient, and the recycling efficiency of the biomass is decreased, leading to an increase in CO(2 )emission. This work proposed an optimal operation of ADRS to improve the recycling efficiency of biomass for CO2 emission reduction based on rein-forcement learning. The optimal operation of the ADRS was derived by the following steps. 1) Real-time process operating data (i.e., temperatures of the flue gas, water, and steam) were gathered and a computational fluid dynamics model was developed to predict the flue gas temperature in the superheater section. 2) The decrease in the heat transfer rate was calculated using the gathered data to define a reward update matrix. 3) A modified Q -learning algorithm was developed based on the defined reward update matrix, and the algorithm was used to derive the Q-matrix, a function that predicted the expected dynamic reward (i.e., priority for ash deposit removal) of performing a given action (i.e., sootblowing) at a given state (i.e., each sootblowing location). 4) Using the obtained Q-matrix, the optimal operating sequence was derived. As a result, 22.58 ton/d of black li-quor was saved and the CO2 emission decreased by 755-1390 ton/y with an increase in the net profit by $1,010,000.", "journal": "JOURNAL OF CLEANER PRODUCTION", "category": "Green & Sustainable Science & Technology; Engineering, Environmental; Environmental Sciences", "annotated_keywords": ["learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000774295700001", "keywords": "vibroacoustic signal; osteoarthritis; femoral-tibial joint; kinetic chain; artificial neural network; multilevel perceptron; RBF", "title": "Diagnostics of Articular Cartilage Damage Based on Generated Acoustic Signals Using ANN-Part I: Femoral-Tibial Joint", "abstract": "Osteoarthritis (OA) is a chronic, progressive disease which has over 300 million cases each year. Some of the main symptoms of OA are pain, restriction of joint motion and stiffness of the joint. Early diagnosis and treatment can prolong painless joint function. Vibroarthrography (VAG) is a cheap, reproducible, non-invasive and easy-to-use tool which can be implemented in the diagnostic route. The aim of this study was to establish diagnostic accuracy and to identify the most accurate signal processing method for the detection of OA in knee joints. In this study, we have enrolled a total of 67 patients, 34 in a study group and 33 in a control group. All patients in the study group were referred for surgical treatment due to intraarticular lesions, and the control group consisted of healthy individuals without knee symptoms. Cartilage status was assessed during surgery according to the International Cartilage Repair Society (ICRS) and vibroarthrography was performed one day prior to surgery in the study group. Vibroarthrography was performed in an open and closed kinematic chain for the involved knees in the study and control group. Signals were acquired by two sensors placed on the medial and lateral joint line. Using the neighbourhood component analysis (NCA) algorithm, the selection of optimal signal measures was performed. Classification using artificial neural networks was performed for three variants: I-open kinetic chain, II-closed kinetic chain, and III-open and closed kinetic chain. Vibroarthrography showed high diagnostic accuracy in determining healthy cartilage from cartilage lesions, and the number of repetitions during examination can be reduced only to closed kinematic chain.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000892838500001", "keywords": "machine learning; traditional Chinese medicine; I-III stage colorectal cancer; recurrence and metastasis; prognostic model", "title": "Machine learning based prognostic model of Chinese medicine affecting the recurrence and metastasis of I-III stage colorectal cancer: A retrospective study in China", "abstract": "BackgroundTo construct prognostic model of colorectal cancer (CRC) recurrence and metastasis (R&M) with traditional Chinese medicine (TCM) factors based on different machine learning (ML) methods. Aiming to offset the defects in the existing model lacking TCM factors. MethodsPatients with stage I-III CRC after radical resection were included as the model data set. The training set and the internal verification set were randomly divided at a ratio of 7: 3 by the \"set aside method\". The average performance index and 95% confidence interval of the model were calculated by repeating 100 tests. Eight factors were used as predictors of Western medicine. Two types of models were constructed by taking \"whether to accept TCM intervention\" and \"different TCM syndrome types\" as TCM predictors. The model was constructed by four ML methods: logistic regression, random forest, Extreme Gradient Boosting (XGBoost) and support vector machine (SVM). The predicted target was whether R&M would occur within 3 years and 5 years after radical surgery. The area under curve (AUC) value and decision curve analysis (DCA) curve were used to evaluate accuracy and utility of the model. ResultsThe model data set consisted of 558 patients, of which 317 received TCM intervention after radical resection. The model based on the four ML methods with the TCM factor of \"whether to accept TCM intervention\" showed good ability in predicting R&M within 3 years and 5 years (AUC value > 0.75), and XGBoost was the best method. The DCA indicated that when the R&M probability in patients was at a certain threshold, the models provided additional clinical benefits. When predicting the R&M probability within 3 years and 5 years in the model with TCM factors of \"different TCM syndrome types\", the four methods all showed certain predictive ability (AUC value > 0.70). With the exception of the model constructed by SVM, the other methods provided additional clinical benefits within a certain probability threshold. ConclusionThe prognostic model based on ML methods shows good accuracy and clinical utility. It can quantify the influence degree of TCM factors on R&M, and provide certain values for clinical decision-making.", "journal": "FRONTIERS IN ONCOLOGY", "category": "Oncology", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000871398900001", "keywords": "CatBoost; electric maloperation accidents; human and organization factors; Human Factors Analysis and Classification System; SHAP", "title": "Quantifying and comparing the effects of human and organizational factors in electric maloperation accidents with HFACS-CatBoost and SHAP", "abstract": "The proportion of electric maloperation accidents (EMAs) in substations caused by human and organizational factors (HOFs) has gradually increased. Although there has been some research into the factors affecting EMAs in substations, the available results are insufficient to support the interpretation of HOFs in EMAs. This article explores the relationships between the HOFs and EMAs using Human Factors Analysis and Classification System-gradient boosting with categorical features support (HFACS-CatBoost) and Shapley Additive exPlanation (SHAP) methods. First, the HFACS framework was introduced to identify 135 EMAs in the Southern Power Grid risk causation. CatBoost was used to construct an accident classification model to analyze the important relationship between accidents and HOFs and to compare and analyze with the extreme gradient boosting (XGBoost) and the binary logistic regression (BLR) to verify the superiority of CatBoost. Finally, to solve the problem of inadequate interpretation of the CatBoost black-box model, the SHAP value plot was applied to express the contribution degree relationship between accidents and HOFs. The results show that the above method can explore and explain the importance and contribution of HOFs in EMAs. And from this, it is concluded that poor psychological state, poor communication and coordination, inadequate supervision, and inadequate training and education are highly correlated with the occurrence of EMAs. The findings will help substation operations and maintenance staff to develop safety measures to address the confusion of HOFs in substations and prevent the occurrence of EMAs.", "journal": "HUMAN FACTORS AND ERGONOMICS IN MANUFACTURING & SERVICE INDUSTRIES", "category": "Engineering, Manufacturing; Ergonomics", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000873764400001", "keywords": "lung cancer; bone metastasis; machine learning; mortality; clinical decision-making; model explainability", "title": "Machine learning approaches for prediction of early death among lung cancer patients with bone metastases using routine clinical characteristics: An analysis of 19,887 patients", "abstract": "PurposeBone is one of the most common sites for the spread of malignant tumors. Patients with bone metastases whose prognosis was shorter than 3 months (early death) were considered as surgical contraindications. However, the information currently available in the literature limits our capacity to assess the risk likelihood of 3 month mortality. As a result, the study's objective is to create an accurate prediction model utilizing machine-learning techniques to predict 3 month mortality specifically among lung cancer patients with bone metastases according to easily available clinical data. MethodsThis study enrolled 19,887 lung cancer patients with bone metastases between 2010 and 2018 from a large oncologic database in the United States. According to a ratio of 8:2, the entire patient cohort was randomly assigned to a training (n = 15881, 80%) and validation (n = 4,006, 20%) group. In the training group, prediction models were trained and optimized using six approaches, including logistic regression, XGBoosting machine, random forest, neural network, gradient boosting machine, and decision tree. There were 13 metrics, including the Brier score, calibration slope, intercept-in-large, area under the curve (AUC), and sensitivity, used to assess the model's prediction performance in the validation group. In each metric, the best prediction effectiveness was assigned six points, while the worst was given one point. The model with the highest sum score of the 13 measures was optimal. The model's explainability was performed using the local interpretable model-agnostic explanation (LIME) according to the optimal model. Predictor importance was assessed using H2O automatic machine learning. Risk stratification was also evaluated based on the optimal threshold. ResultsAmong all recruited patients, the 3 month mortality was 48.5%. Twelve variables, including age, primary site, histology, race, sex, tumor (T) stage, node (N) stage, brain metastasis, liver metastasis, cancer-directed surgery, radiation, and chemotherapy, were significantly associated with 3 month mortality based on multivariate analysis, and these variables were included for developing prediction models. With the highest sum score of all the measurements, the gradient boosting machine approach outperformed all the other models (62 points), followed by the XGBooting machine approach (59 points) and logistic regression (53). The area under the curve (AUC) was 0.820 (95% confident interval [CI]: 0.807-0.833), 0.820 (95% CI: 0.807-0.833), and 0.815 (95% CI: 0.801-0.828), respectively, calibration slope was 0.97, 0.95, and 0.96, respectively, and accuracy was all 0.772. Explainability of models was conducted to rank the predictors and visualize their contributions to an individual's mortality outcome. The top four important predictors in the population according to H2O automatic machine learning were chemotherapy, followed by liver metastasis, radiation, and brain metastasis. Compared to patients in the low-risk group, patients in the high-risk group were more than three times the odds of dying within 3 months (P < 0.001). ConclusionsUsing machine learning techniques, this study offers a number of models, and the optimal model is found after thoroughly assessing and contrasting the prediction performance of each model. The optimal model can be a pragmatic risk prediction tool and is capable of identifying lung cancer patients with bone metastases who are at high risk for 3 month mortality, informing risk counseling, and aiding clinical treatment decision-making. It is better advised for patients in the high-risk group to have radiotherapy alone, the best supportive care, or minimally invasive procedures like cementoplasty.", "journal": "FRONTIERS IN PUBLIC HEALTH", "category": "Public, Environmental & Occupational Health", "annotated_keywords": ["neural net", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000838878300001", "keywords": "SFRC; building material; compressive strength; steel fiber; concrete", "title": "Compressive Strength Estimation of Steel-Fiber-Reinforced Concrete and Raw Material Interactions Using Advanced Algorithms", "abstract": "Steel-fiber-reinforced concrete (SFRC) has been introduced as an effective alternative to conventional concrete in the construction sector. The incorporation of steel fibers into concrete provides a bridging mechanism to arrest cracks, improve the post-cracking behavior of concrete, and transfer stresses in concrete. Artificial intelligence (AI) approaches are in use nowadays to predict concrete properties to conserve time and money in the construction industry. Accordingly, this study aims to apply advanced and sophisticated machine-learning (ML) algorithms to predict SFRC compressive strength. In the current work, the applied ML approaches were gradient boosting, random forest, and XGBoost. The considered input variables were cement, fine aggregates (sand), coarse aggregates, water, silica fume, super-plasticizer, fly ash, steel fiber, fiber diameter, and fiber length. Previous studies have not addressed the effects of raw materials on compressive strength in considerable detail, leaving a research gap. The integration of a SHAP analysis with ML algorithms was also performed in this paper, addressing a current research need. A SHAP analysis is intended to provide an in-depth understanding of the SFRC mix design in terms of its strength factors via complicated, nonlinear behavior and the description of input factor contributions by assigning a weighing factor to each input component. The performances of all the algorithms were evaluated by applying statistical checks such as the determination coefficient (R-2), the root mean square error (RMSE), and the mean absolute error (MAE). The random forest ML approach had a higher, i.e., 0.96, R-2 value with fewer errors, producing higher precision than other models with lesser R-2 values. The SFRC compressive strength could be anticipated by applying the random forest ML approach. Further, it was revealed from the SHapley Additive exPlanations (SHAP) analysis that cement content had the highest positive influence on the compressive strength of SFRC. In this way, the current study is beneficial for researchers to effectively and quickly evaluate SFRC compressive strength.", "journal": "POLYMERS", "category": "Polymer Science", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000828419100003", "keywords": "Parkinson&rsquo; s disease; SHAP value; Feature selection; Classification; Machine learning", "title": "Diagnosis of Parkinson's disease based on SHAP value feature selection", "abstract": "To address the problem of high feature dimensionality of Parkinson's disease medical data, this paper introduces SHapley Additive exPlanations (SHAP) value for feature selection of Parkinson's disease medical dataset. This paper combines SHAP value with four classifiers, namely deep forest (gcForest), extreme gradient boosting (XGBoost), light gradient boosting machine (LightGBM) and random forest (RF), respectively. Then this paper applies them to Parkinson's disease diagnosis. First, the classifier is used to calculate the magnitude of con-tribution of SHAP value to the features, then the features with significant contribution in the classification task are selected, and then the data after feature selection is used as input to classify the Parkinson's disease dataset for diagnosis using the classifier. The experimental results show that compared to Fscore, analysis of variance (Anova-F) and mutual informa-tion (MI) feature selection methods, the four models based on SHAP-value feature selection achieved good classification results. The SHAP-gcForest model combined with gcForest achieves classification accuracy of 91.78% and F1-score of 0.945 when 150 features are selected. The SHAP-LightGBM model combined with LightGBM achieves classification accu-racy and F1-score of 91.62% and 0.945 when 50 features are selected, respectively. The clas-sification effectiveness is second only to the SHAP-gcForest model, but the SHAP-LightGBM model is more computationally efficient than the SHAP-gcForest model. Finally, the effec-tiveness of the proposed method is verified by comparing it with the results of existing lit-erature. The findings demonstrate that machine learning with SHAP value feature selection method has good classification performance in the diagnosis of Parkinson's disease, and provides a reference for physicians in the diagnosis and prevention of Parkinson's disease.(c) 2022 Nalecz Institute of Biocybernetics and Biomedical Engineering of the Polish Academy of Sciences. Published by Elsevier B.V. All rights reserved.", "journal": "BIOCYBERNETICS AND BIOMEDICAL ENGINEERING", "category": "Engineering, Biomedical", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000777833100001", "keywords": "machine learning; unsupervised learning; anomaly detection", "title": "Source-agnostic gravitational-wave detection with recurrent autoencoders", "abstract": "We present an application of anomaly detection techniques based on deep recurrent autoencoders (AEs) to the problem of detecting gravitational wave (GW) signals in laser interferometers. Trained on noise data, this class of algorithms could detect signals using an unsupervised strategy, i.e. without targeting a specific kind of source. We develop a custom architecture to analyze the data from two interferometers. We compare the obtained performance to that obtained with other AE architectures and with a convolutional classifier. The unsupervised nature of the proposed strategy comes with a cost in terms of accuracy, when compared to more traditional supervised techniques. On the other hand, there is a qualitative gain in generalizing the experimental sensitivity beyond the ensemble of pre-computed signal templates. The recurrent AE outperforms other AEs based on different architectures. The class of recurrent AEs presented in this paper could complement the search strategy employed for GW detection and extend the discovery reach of the ongoing detection campaigns.", "journal": "MACHINE LEARNING-SCIENCE AND TECHNOLOGY", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Multidisciplinary Sciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000809147200001", "keywords": "abnormal gait behavior; OpenPose; machine learning; XGBoost; random forest", "title": "A Lightweight Pose Sensing Scheme for Contactless Abnormal Gait Behavior Measurement", "abstract": "The recognition of abnormal gait behavior is important in the field of motion assessment and disease diagnosis. Currently, abnormal gait behavior is primarily recognized by pressure and inertial data obtained from wearable sensors. However, the data drift and wearing difficulties for patients have impeded the application of these wearable sensors. Here, we propose a contactless abnormal gait behavior recognition method that captures human pose data using a monocular camera. A lightweight OpenPose (OP) model is generated with Depthwise Separable Convolution to recognize joint points and extract their coordinates during walking in real time. For the walking data errors extracted in the 2D plane, a 3D reconstruction is performed on the walking data, and a total of 11 types of abnormal gait features are extracted by the OP model. Finally, the XGBoost algorithm is used for feature screening. The final experimental results show that the Random Forest (RF) algorithm in combination with 3D features delivers the highest precision (92.13%) for abnormal gait behavior recognition. The proposed scheme overcomes the data drift of inertial sensors and sensor wearing challenges in the elderly while reducing the hardware requirements for model deployment. With excellent real-time and contactless capabilities, the scheme is expected to enjoy a wide range of applications in the field of abnormal gait measurement.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000778057700002", "keywords": "Deep learning; Epilepsy; EEG; Fog computing; Internet of things; Healthcare", "title": "Smart neurocare approach for detection of epileptic seizures using deep learning based temporal analysis of EEG patterns", "abstract": "Epilepsy is a psychosocial neurological disorder, which emerges as a major threat to public health. In this age of the internet of things, the smart diagnosis of epilepsy has gained huge research attention with machine learning-based seizure detection in cloud-fog assisted environments. The present paper also proposes a cloud-fog integrated smart neurocare approach, which performs a temporal analysis of raw electroencephalogram (EEG) signals using deep learning to detect the occurrence of epileptic seizures. This patient-independent approach makes use of single-channel EEG signals to achieve real-time and computationally efficient seizure detection at fog layer devices. It employs a maximum variance-based channel selection procedure to select only one channel of raw scalp EEG signals, followed by their filtering and segmentation into various short-duration temporal segments. To analyse EEG patterns, these segments are further fed to the proposed models of convolutional neural network, recurrent neural network and stacked autoencoder deep learning classifiers. The performance analysis through simulation results evidently reveals that the proposed convolutional neural network-based temporal analysis approach performs better than other approaches. It realises an optimum accuracy of 96.43%, sensitivity of 100% and specificity of 93.33% for 30s duration EEG segments of CHB-MIT dataset and achieves 100% accuracy, sensitivity and specificity values for EEG segments of Bonn dataset for 23.6s EEG segments. Thus, the proposed convolutional neural network-based approach emerges as an appropriate method for rapid and accurate detection of epileptic seizures in fog-cloud integrated environment.", "journal": "MULTIMEDIA TOOLS AND APPLICATIONS", "category": "Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": ["neural net", "machine learning", "deep learning", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000772824200002", "keywords": "Industrial intelligence; Defect recognition; Feature extraction; Deep learning; Review", "title": "A Review on Recent Advances in Vision-based Defect Recognition towards Industrial Intelligence", "abstract": "In modern manufacturing, vision-based defect recognition is an essential technology to guarantee product quality, and it plays an important role in industrial intelligence. With the developments of industrial big data, defect images can be captured by ubiquitous sensors. And, how to realize accuracy recognition has become a research hotspot. In the past several years, many vision-based defect recognition methods have been proposed, and some newly-emerged techniques, such as deep learning, have become increasingly popular and have addressed many challenging problems effectively. Hence, a comprehensive review is urgently needed, and it can promote the development and bring some insights in this area. This paper surveys the recent advances in vision based defect recognition and presents a systematical review from a feature perspective. This review divides the recent methods into designed-feature based methods and learned-feature based methods, and summarizes the advantages, disadvantages and application scenarios. Furthermore, this paper also summarizes the performance metrics for vision-based defect recognition methods. And some challenges and development trends are also discussed.", "journal": "JOURNAL OF MANUFACTURING SYSTEMS", "category": "Engineering, Industrial; Engineering, Manufacturing; Operations Research & Management Science", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000871033400006", "keywords": "Measurement by laser beam; Laser radar; Laser beams; Polarization; Optical filters; Target recognition; Surface waves; Laser radar; machine learning; materials identification; polarization detection; pulsed laser", "title": "Materials Identification of Polarized Pulse Laser Detection Based on Sparse Autoencoder and Softmax Classifier Framework", "abstract": "When detecting complex ground objects of different materials, it is difficult to distinguish the target with traditional laser radar. In this article, materials identification of polarized pulsed laser detection system is conducted by sparse autoencoder (SAE) and softmax classifier framework. The polarization parameters of five typical target materials were measured. According to the calculation formula of Stokes parameters, the relevant polarization parameters of the five materials are obtained and analyzed. The uncertainties of the degree of polarization (DOP) with different materials are within 0.063. Afterward, the framework of SAE and softmax classifier is proposed to categorize different materials, and the test accuracy of the trained algorithm is 97.8%. The experimental results show that the proposed algorithm outperforms the support vector machine (SVM) and back propagation (BP) neural network. Moreover, the proposed algorithm is better than the traditional intensity recognition method as well. Thus, the proposed method by using polarization information can effectively recognize different targets by identifying materials.", "journal": "IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT", "category": "Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000576775900003", "keywords": "Similar case matching; Multi-task learning; Legal document; BERT; Text matching", "title": "Similar case matching with explicit knowledge-enhanced text representation", "abstract": "Similar case matching is one of the important practical applications of text matching, which is a crucial issue in natural language processing. For a well-structured natural language document, the sentence-based representation and explicit knowledge elements should be effectively integrated so as to fit the original structure of the text, when constructing the document representation. In this paper, we propose a multi-task learning framework with \"de- and re-construction\", which leverages the extraction of sub-tasks based on sentence-level knowledge elements to enhance the representation at the document level, to improve the performance of the model in the main task of similar case matching. Extensive experiments have proved that our proposed model outperforms methods like latent dirichlet allocation (LDA), LSTM-RNN, BERT, etc, which only focus on constructing document representations. (C) 2020 Elsevier B.V. All rights reserved.", "journal": "APPLIED SOFT COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": ["natural language processing"], "label": "1", "title_label": "1"}
{"id": "WOS:000315013000007", "keywords": "Multi-label learning; Label embedding; Max-margin learning; Cost-sensitive multi-label hinge loss; One versus all (OVA)", "title": "Max-margin embedding for multi-label learning", "abstract": "Multi-label learning refers to methods for learning a classification function that predicts a set of relevant labels for an instance. Label embedding seeks a transformation which maps labels into a latent space where regression is performed to predict a set of relevant labels. The latent space is often a low-dimensional space, so computational and space complexities are reduced. However, the choice of an appropriate transformation to a latent space is not clear. In this paper we present a max-margin embedding method where both instances and labels are mapped into a low-dimensional latent space. In contrast to existing label embedding methods, the pair of instance and label embeddings is determined by minimizing a cost-sensitive multi-label hinge loss, in which label-dependent cost is applied to more penalize the misclassification of positive examples. For implementation, we employ the limited memory Broyden-Fletcher-Goldfarb-Shanno (BEGS) method to determine the instance and label embeddings by a joint optimization. Numerical experiments on a few datasets demonstrate the high performance of our method compared to existing embedding methods in the case where the dimensionality of the latent space is much smaller than that of the original label space. (C) 2012 Elsevier B.V. All rights reserved.", "journal": "PATTERN RECOGNITION LETTERS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000540222200180", "keywords": "missing data generation; industrial big data; air pressure system; generative adversarial network; Gaussian process regression", "title": "An Imbalanced Data Handling Framework for Industrial Big Data Using a Gaussian Process Regression-Based Generative Adversarial Network", "abstract": "The developments in the fields of industrial Internet of Things (IIoT) and big data technologies have made it possible to collect a lot of meaningful industrial process and quality-based data. The gathered data are analyzed using contemporary statistical methods and machine learning techniques. Then, the extracted knowledge can be used for predictive maintenance or prognostic health management. However, it is difficult to gather complete data due to several issues in IIoT, such as devices breaking down, running out of battery, or undergoing scheduled maintenance. Data with missing values are often ignored, as they may contain insufficient information from which to draw conclusions. In order to overcome these issues, we propose a novel, effective missing data handling mechanism for the concepts of symmetry principles. While other existing methods only attempt to estimate missing parts, the proposed method generates a whole set of data set using Gaussian process regression and a generative adversarial network. In order to prove the effectiveness of the proposed framework, we examine a real-world, industrial case involving an air pressure system (APS), where we use the proposed method to make quality predictions and compare the results with existing state-of-the-art estimation methods.", "journal": "SYMMETRY-BASEL", "category": "Multidisciplinary Sciences", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000476877500001", "keywords": "Generative adversarial network; anonymization; deep learning; data sharing; medical image analysis", "title": "Synthetic Gastritis Image Generation via Loss Function-Based Conditional PGGAN", "abstract": "In this paper, a novel synthetic gastritis image generation method based on a generative adversarial network (GAN) model is presented. Sharing medical image data is a crucial issue for realizing diagnostic supporting systems. However, it is still difficult for researchers to obtain medical image data since the data include individual information. Recently proposed GAN models can learn the distribution of training images without seeing real image data, and individual information can be completely anonymized by generated images. If generated images can be used as training images in medical image classification, promoting medical image analysis will become feasible. In this paper, we targeted gastritis, which is a risk factor for gastric cancer and can be diagnosed by gastric X-ray images. Instead of collecting a large amount of gastric X-ray image data, an image generation approach was adopted in our method. We newly propose loss function-based conditional progressive growing generative adversarial network (LC-PGGAN), a gastritis image generation method that can be used for a gastritis classification problem. The LC-PGGAN gradually learns the characteristics of gastritis in gastric X-ray images by adding new layers during the training step. Moreover, the LC-PGGAN employs loss function-based conditional adversarial learning so that generated images can be used as the gastritis classification task. We show that images generated by the LC-PGGAN are effective for gastritis classification using gastric X-ray images and have clinical characteristics of the target symptom.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000494102400001", "keywords": "Magnetic resonance imaging; Feature extraction; Diseases; Positron emission tomography; Medical diagnosis; Brain modeling; Deep learning; Multi-modal neuroimage; incomplete data; generative adversarial network; fisher vector; brain disease diagnosis; MRI; PET", "title": "Design of Semantic-Based Colorization of Graphical User Interface Through Conditional Generative Adversarial Nets", "abstract": "There has recently been a significant movement toward aiding graphic design tasks based on artificial intelligence or machine learning. In addition, colorization plays an important role within the topic of GUI design. Previous studies regarding automatic colorization have focused on a consideration of the realistic aspects of an image without consideration of the design semantics or usability, which are critical aspects for a practical GUI design. We, therefore, propose an end-to-end network for a generative combination of color sets for a GUI design based on the design semantics, while utilizing thousands of actual GUI design datasets acquired from LG Electronics to train the network. By utilizing the GUI design dataset, our network effectively generates color sets for a GUI design by considering various design aspects, such as the usability factors. In detail, we concatenate the textual design concept, characteristics of the application, and usage frequency for the elements of the design semantics. We then construct a conditional generative adversarial net processing of the design semantics as a condition to generate suitable color sets and construct the GUI design based on these sets. The experiments indicate that our proposed method effectively generates color sets for a GUI design based on the design semantics. In addition, our proposed method shows a better score than other methods on a user test conducted to verify the practicality, perception, recognition, diversity, and esthetic features. Moreover, experimental results prove that users can effectively grasp the intended design concept of our generated GUI design with higher top-1, top-2, and top-3 levels of accuracy.", "journal": "INTERNATIONAL JOURNAL OF HUMAN-COMPUTER INTERACTION", "category": "Computer Science, Cybernetics; Ergonomics", "annotated_keywords": ["artificial intelligen", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000513867700001", "keywords": "assembly graph; clustering genera; co-assembly; de novo variation detection; graph comparison; graph kernel", "title": "Graph Traversal Edit Distance and Extensions", "abstract": "Many problems in applied machine learning deal with graphs (also called networks), including social networks, security, web data mining, protein function prediction, and genome informatics. The kernel paradigm beautifully decouples the learning algorithm from the underlying geometric space, which renders graph kernels important for the aforementioned applications. In this article, we give a new graph kernel, which we call graph traversal edit distance (GTED). We introduce the GTED problem and give the first polynomial time algorithm for it. Informally, the GTED is the minimum edit distance between two strings formed by the edge labels of respective Eulerian traversals of the two graphs. Also, GTED is motivated by and provides the first mathematical formalism for sequence co-assembly and de novo variation detection in bioinformatics. We demonstrate that GTED admits a polynomial time algorithm using a linear program in the graph product space that is guaranteed to yield an integer solution. To the best of our knowledge, this is the first approach to this problem. We also give a linear programming relaxation algorithm for a lower bound on GTED. We use GTED as a graph kernel and evaluate it by computing the accuracy of a support vector machine (SVM) classifier on a few data sets in the literature. Our results suggest that our kernel outperforms many of the common graph kernels in the tested data sets. As a second set of experiments, we successfully cluster viral genomes using GTED on their assembly graphs obtained from de novo assembly of next-generation sequencing reads.", "journal": "JOURNAL OF COMPUTATIONAL BIOLOGY", "category": "Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability", "annotated_keywords": ["machine learning", "learning algorithm"], "label": "1", "title_label": "0"}
{"id": "WOS:000509481400003", "keywords": "Welded joints; detection; identification; GAN; deep neural network", "title": "An Automatic Detection and Identification Method of Welded Joints Based on Deep Neural Network", "abstract": "Welding quality is an important factor to affect the performance, quality and strength of different products, and it will affect the safe production. Therefore, welding quality detection is a key process of industrial production. And the detection and identification of welded joints are the premise of welding quality detection, which could reduce the quality detection range and improve the detection precision. Welded joint identification is also important for providing information for automatic control of welding process. Faced with the complex characteristics of industrial environment, such as weak texture, weak contrast and corrosion, we propose a detection and identification method of welded joints based on deep neural network. Firstly, aimed at the problem of insufficient training samples, combined with image processing and Generative Adversarial Network (GAN), the high-quality training samples are generated. Secondly, the updating mechanism of training samples is established to guarantee that the deep neural network model could cover all samples. Finally, the detection and identification of welded joints are realized by the deep neural network which could avoid the handcrafted features of conventional machine learning methods. Experiments show that the proposed method could quickly and efficiently finish the detection and identification task of welded joints. Meanwhile, the proposed method could well solve the detection and identification problems of complex industrial environment.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": ["neural net", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000360413400004", "keywords": "Support vector machines; one-class classification; distributed learning; outlier detection", "title": "Distributed One-Class Support Vector Machine", "abstract": "This paper presents a novel distributed one-class classification approach based on an extension of the nu-SVM method, thus permitting its application to Big Data data sets. In our method we will consider several one-class classifiers, each one determined using a given local data partition on a processor, and the goal is to find a global model. The cornerstone of this method is the novel mathematical formulation that makes the optimization problem separable whilst avoiding some data points considered as outliers in the final solution. This is particularly interesting and important because the decision region generated by the method will be unaffected by the position of the outliers and the form of the data will fit more precisely. Another interesting property is that, although built in parallel, the classifiers exchange data during learning in order to improve their individual specialization. Experimental results using different datasets demonstrate the good performance in accuracy of the decision regions of the proposed method in comparison with other well-known classifiers while saving training time due to its distributed nature.", "journal": "INTERNATIONAL JOURNAL OF NEURAL SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000516826200066", "keywords": "machine learning; meteorological parameters; pan evaporation; advanced statistical analysis; hydrological cycle; big data; hydroinformatics; random forest (RF); support vector regression (SVR)", "title": "Modeling Pan Evaporation Using Gaussian Process Regression K-Nearest Neighbors Random Forest and Support Vector Machines; Comparative Analysis", "abstract": "Evaporation is a very important process; it is one of the most critical factors in agricultural, hydrological, and meteorological studies. Due to the interactions of multiple climatic factors, evaporation is considered as a complex and nonlinear phenomenon to model. Thus, machine learning methods have gained popularity in this realm. In the present study, four machine learning methods of Gaussian Process Regression (GPR), K-Nearest Neighbors (KNN), Random Forest (RF) and Support Vector Regression (SVR) were used to predict the pan evaporation (PE). Meteorological data including PE, temperature (T), relative humidity (RH), wind speed (W), and sunny hours (S) collected from 2011 through 2017. The accuracy of the studied methods was determined using the statistical indices of Root Mean Squared Error (RMSE), correlation coefficient (R) and Mean Absolute Error (MAE). Furthermore, the Taylor charts utilized for evaluating the accuracy of the mentioned models. The results of this study showed that at Gonbad-e Kavus, Gorgan and Bandar Torkman stations, GPR with RMSE of 1.521 mm/day, 1.244 mm/day, and 1.254 mm/day, KNN with RMSE of 1.991 mm/day, 1.775 mm/day, and 1.577 mm/day, RF with RMSE of 1.614 mm/day, 1.337 mm/day, and 1.316 mm/day, and SVR with RMSE of 1.55 mm/day, 1.262 mm/day, and 1.275 mm/day had more appropriate performances in estimating PE values. It was found that GPR for Gonbad-e Kavus Station with input parameters of T, W and S and GPR for Gorgan and Bandar Torkmen stations with input parameters of T, RH, W and S had the most accurate predictions and were proposed for precise estimation of PE. The findings of the current study indicated that the PE values may be accurately estimated with few easily measured meteorological parameters.", "journal": "ATMOSPHERE", "category": "Environmental Sciences; Meteorology & Atmospheric Sciences", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000387362200014", "keywords": "Pattern recognition; Binary classification; Twin support vector machine; Successive overrelaxation technique (SOR)", "title": "A Novel Twin Support Vector Machine for Binary Classification Problems", "abstract": "Based on the recently proposed twin support vector machine and twin bounded support vector machine, in this paper, we propose a novel twin support vector machine (NTSVM) for binary classification problems. The significance of our proposed NTSVM is that the objective function is changed in the spirit of regression, such that hyperplanes separate as much as possible. In addition, the successive overrelaxation technique is used to solve quadratic programming problems to speed up the training process. Experimental results obtained on several artificial and UCI benchmark datasets show the feasibility and effectiveness of the proposed method.", "journal": "NEURAL PROCESSING LETTERS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000359165000009", "keywords": "Multi-agent; Kalman filter; Neural network; Inverse optimal controller", "title": "Decentralized control for stabilization of nonlinear multi-agent systems using neural inverse optimal control", "abstract": "This paper proposes a decentralized control for stabilization of nonlinear multi-agent systems using neural inverse optimal control. This approach consists in synthesizing a suitable controller for each agent; accordingly, each local subsystem is approximated by an identifier using a discrete-time recurrent high order neural network (RHONN), trained with an extended Kalman filter (EKF) algorithm. The neural identifier scheme is used to model each uncertain nonlinear subsystem, and based on this neural model and the knowledge of a control Lyapunov function, then an inverse optimal controller is synthesized to avoid solving the Hamilton Jacobi Bellman (HJB) equation. (C) 2015 Elsevier B.V. All rights reserved.", "journal": "NEUROCOMPUTING", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000368732400009", "keywords": "Unsupervised classification; Absolute value inequalities; Support vector machines", "title": "Unsupervised and Semisupervised Classification Via Absolute Value Inequalities", "abstract": "We consider the problem of classifying completely or partially unlabeled data by using inequalities that contain absolute values of the data. This allows each data point to belong to either one of two classes by entering the inequality with a plus or minus value. By using such absolute value inequalities in linear and nonlinear support vector machines, unlabeled or partially labeled data can be successfully partitioned into two classes that capture most of the correct labels dropped from the unlabeled data.", "journal": "JOURNAL OF OPTIMIZATION THEORY AND APPLICATIONS", "category": "Operations Research & Management Science; Mathematics, Applied", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000375212600056", "keywords": "Music similarity; Relative similarity ratings; Metric learning; Support vector machines; Metric learning to rank; Neural networks", "title": "Identification and Severity Determination of Wheat Stripe Rust and Wheat Leaf Rust Based on Hyperspectral Data Acquired Using a Black-Paper-Based Measuring Method", "abstract": "It is important to implement detection and assessment of plant diseases based on remotely sensed data for disease monitoring and control. Hyperspectral data of healthy leaves, leaves in incubation period and leaves in diseased period of wheat stripe rust and wheat leaf rust were collected under in-field conditions using a black-paper-based measuring method developed in this study. After data preprocessing, the models to identify the diseases were built using distinguished partial least squares (DPLS) and support vector machine (SVM), and the disease severity inversion models of stripe rust and the disease severity inversion models of leaf rust were built using quantitative partial least squares (QPLS) and support vector regression (SVR). All the models were validated by using leave-one-out cross validation and external validation. The diseases could be discriminated using both distinguished partial least squares and support vector machine with the accuracies of more than 99%. For each wheat rust, disease severity levels were accurately retrieved using both the optimal QPLS models and the optimal SVR models with the coefficients of determination (R-2) of more than 0.90 and the root mean square errors (RMSE) of less than 0.15. The results demonstrated that identification and severity evaluation of stripe rust and leaf rust at the leaf level could be implemented based on the hyperspectral data acquired using the developed method. A scientific basis was provided for implementing disease monitoring by using aerial and space remote sensing technologies.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000538004100011", "keywords": "Support vector machines; Pediatrics; Predictive models; Training; Radio frequency; Vegetation; Big data; Feature selection; machine learning; prediction model; small for gestational age", "title": "Comparison of Different Machine Learning Approaches to Predict Small for Gestational Age Infants", "abstract": "Diagnosing infants who are small for gestational age (SGA) at early stages could help physicians to introduce interventions for SGA infants earlier. Machine learning (ML) is envisioned as a tool to identify SGA infants. However, ML has not been widely studied in this field. To develop effective SGA prediction models, we conducted four groups of experiments that considered basic ML methods, imbalanced data, feature selection and the time characteristics of variables, respectively. Infants with SGA data collected from 2010 to 2013 with gestational weeks between 24 and 42 were detected. Support vector machine (SVM), random forest (RF), logistic regression (LR) and Sparse LR models were trained on 10-fold cross validation. Precision and the area under the curve (AUC) of the receiver operator characteristic curve were evaluated. For each group, the performance of SVM and Sparse LR was similarly well. LR without any sparsity penalties performed worst, possibly caused by the overfitting problem. With the combination of handling imbalanced data and feature selection, the RF ensemble classifier performed best, which even obtained the highest AUC value (0.8547) with the help of expert knowledge. In other cases, RF performed worse than Sparse LR and SVM, possibly because of fully grown trees.", "journal": "IEEE TRANSACTIONS ON BIG DATA", "category": "Computer Science, Information Systems; Computer Science, Theory & Methods", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000332484700023", "keywords": "Hidden Markov random field (HMRF); hyperspectral image analysis; image segmentation; support vector machine (SVM) classifier", "title": "Spectral-Spatial Classification of Hyperspectral Images Based on Hidden Markov Random Fields", "abstract": "Hyperspectral remote sensing technology allows one to acquire a sequence of possibly hundreds of contiguous spectral images from ultraviolet to infrared. Conventional spectral classifiers treat hyperspectral images as a list of spectral measurements and do not consider spatial dependences, which leads to a dramatic decrease in classification accuracies. In this paper, a new automatic framework for the classification of hyperspectral images is proposed. The new method is based on combining hidden Markov random field segmentation with support vector machine (SVM) classifier. In order to preserve edges in the final classification map, a gradient step is taken into account. Experiments confirm that the new spectral and spatial classification approach is able to improve results significantly in terms of classification accuracies compared to the standard SVM method and also outperforms other studied methods.", "journal": "IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING", "category": "Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000343541600001", "keywords": "ensemble methods; adaptive classifiers; recurrent concepts; concept drift; stock price direction prediction", "title": "A Novel Support Vector Machine with Globality-Locality Preserving", "abstract": "Support vector machine (SVM) is regarded as a powerful method for pattern classification. However, the solution of the primal optimal model of SVM is susceptible for class distribution and may result in a nonrobust solution. In order to overcome this shortcoming, an improved model, support vector machine with globality-locality preserving (GLPSVM), is proposed. It introduces globality-locality preserving into the standard SVM, which can preserve the manifold structure of the data space. We complete rich experiments on the UCI machine learning data sets. The results validate the effectiveness of the proposed model, especially on the Wine and Iris databases; the recognition rate is above 97% and outperforms all the algorithms that were developed from SVM.", "journal": "SCIENTIFIC WORLD JOURNAL", "category": "Multidisciplinary Sciences", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000332787200006", "keywords": "Biped robot; Energy cost; Fuzzy logic system; Support vector machine", "title": "Interval type-2 fuzzy weighted support vector machine learning for energy efficient biped walking", "abstract": "An interval type-2 fuzzy weighted support vector machine (IT2FW-SVM) is proposed to address the problem of high energy consumption for biped walking robots. Different from the traditional machine learning method of 'copy learning', the proposed IT2FW-SVM obtains lower energy cost and larger zero moment point (ZMP) stability margin using a novel strategy of 'selective learning', which is similar to human selections based on experience. To handle the uncertainty of the experience, the learning weights in the IT2FW-SVM are deduced using an interval type-2 fuzzy logic system (IT2FLS), which is an extension of the previous weighted SVM. Simulation studies show that the existing biped walking which generates the original walking samples is improved remarkably in terms of both energy efficiency and biped dynamic balance using the proposed IT2FW-SVM.", "journal": "APPLIED INTELLIGENCE", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["machine learning", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000427597400001", "keywords": "bacteriophage virion proteins; feature selection; hybrid features; machine learning; support vector machine", "title": "PVP-SVM: Sequence-Based Prediction of Phage Virion Proteins Using a Support Vector Machine", "abstract": "Accurately identifying bacteriophage virion proteins from uncharacterized sequences is important to understand interactions between the phage and its host bacteria in order to develop new antibacterial drugs. However, identification of such proteins using experimental techniques is expensive and often time consuming: hence, development of an efficient computational algorithm for the prediction of phage virion proteins (PVPs) prior to in vitro experimentation is needed. Here, we describe a support vector machine (SVM)-based PVP predictor, called PVP-SVM, which was trained with 136 optimal features. A feature selection protocol was employed to identify the optimal features from a large set that included amino acid composition, dipeptide composition, atomic composition, physicochemical properties, and chain-transition-distribution. PVP-SVM achieved an accuracy of 0.870 during leave-one-out cross-validation, which was 6% higher than control SVM predictors trained with all features, indicating the efficiency of the feature selection method. Furthermore, PVP-SVM displayed superior performance compared to the currently available method, PVPred, and two other machine-learning methods developed in this study when objectively evaluated with an independent dataset. For the convenience of the scientific community, a user-friendly and publicly accessible web server has been established at www.thegleelab.org/PVP-SVM/PVP-SVM.html.", "journal": "FRONTIERS IN MICROBIOLOGY", "category": "Microbiology", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000440458100058", "keywords": "Endocrine disrupting chemicals; Machine learning; Early-warning technical system; Hormone activities", "title": "Ternary classification models for predicting hormonal activities of chemicals via nuclear receptors", "abstract": "Endocrine disrupting chemicals (EDCs) can exhibit adverse effects by increasing or blocking hormonal activities as agonists or antagonists through nuclear receptors. Computational toxicology research provides a fast and automated screening tool for determining the potential effects of EDCs. Here, we collected a large dataset of known hormonal activities to develop ternary classification models of androgen receptor (AR) and thyroid hormone receptor (TR), in combination linear discriminant analysis (LDA), classification and regression trees (CART), and support vector machines (SVM). The optimum model for classifying AR and TR activities was SVM. These newly developed models constitute a rapidly systematic early-warning technical system for identifying different hormone activities. (C) 2018 Elsevier B.V. All rights reserved.", "journal": "CHEMICAL PHYSICS LETTERS", "category": "Chemistry, Physical; Physics, Atomic, Molecular & Chemical", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000445767900015", "keywords": "support vector machines; cellular automata; probability maps; urban development", "title": "Integration of Local and Global Support Vector Machines to Improve Urban Growth Modelling", "abstract": "The use of local information for the classification and modelling of spatial variables has increased with the application of statistical and machine learning algorithms, such as support vector machines (SVMs). This study presents a new local SVM (LSVM) model that was developed to model the probability of urban development and simulate urban growth in a subregion in the southwestern suburb of the Tehran metropolitan area, Iran, for the periods of 1992-1996 and 1996-2002. Based on the focal training sample, the model was calibrated using the cross-validation method, and the optimal bandwidth was determined. The results were compared with those of a nonlinear global SVM (GSVM) model that was calibrated based on the ten-fold cross-validation method. This study then evaluated an integrated SVM model (LGSVM) obtained based on a weighted combination of the local and global urban development probabilities. A comparison of the probability maps showed a higher accuracy for the LGSVM than for either the LSVM or GSVM model. To assess the performance of the LSVM, GSVM and LGSVM models in the simulation of urban growth, probability maps were employed as the transition rules for urban cellular automata. The results show that a trade-off between local and global SVM models can enhance the performance of urban growth modelling.", "journal": "ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION", "category": "Computer Science, Information Systems; Geography, Physical; Remote Sensing", "annotated_keywords": ["machine learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000342250300016", "keywords": "Support Vector Machines; Mahalanobis distance; Classification; Data preprocessing", "title": "Two ellipsoid Support Vector Machines", "abstract": "In classification problems classes usually have different geometrical structure and therefore it seems natural for each class to have its own margin type. Existing methods using this principle lead to the construction of the different (from SVM) optimization problems. Although they outperform the standard model, they also prevent the utilization of existing SVM libraries. We propose an approach, named 2esvm, which allows use of such method within the classical SVM framework. This enables to perform a detailed comparison with the standard SVM. It occurs that classes in the resulting feature space are geometrically easier to separate and the trained model has better generalization properties. Moreover, based on evaluation on standard datasets, 2 eSVM brings considerable profit for the linear classification process in terms of training time and quality. We also construct the 2eSVM kernelization and perform the evaluation on the 5-HT2A ligand activity prediction problem (real, fingerprint based data from the cheminformatic domain) which shows increased classification quality, reduced training time as well as resulting model's complexity. (C) 2014 Elsevier Ltd. All rights reserved.", "journal": "EXPERT SYSTEMS WITH APPLICATIONS", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000572923700001", "keywords": "Rotors; Mathematical model; Predictive models; Bifurcation; Radio frequency; Training; Support vector machines; Machine learning; maximum Lyapunov exponents (MLEs); prediction model; spherical porous air bearing (SPAB)", "title": "A High-Precision Random Forest-Based Maximum Lyapunov Exponent Prediction Model for Spherical Porous Gas Bearing Systems", "abstract": "Spherical porous air bearing (SPAB) systems have been extensively used in various mechanical engineering applications. SPABs are promising materials in high-rotational speed, high-precision, and high-stiffness instruments. In SPAB systems, a rotor is supported by gas bearings, which provides higher rotational speed and lower heat generation environment than oil bearings do. Furthermore, SPAB does not cause deformation. Although, the supporting force of gas bearings is less, their stability is better than that of oil films. However, because the pressure distribution in the gas films is nonlinear, they are prone to failure at specific critical speeds, rotor imbalances, or inappropriate operations, which results in nonperiodic or chaotic motion and causes structural fatigue to the system. To understand and control the operating conditions of the SPAB systems during the nonperiodic motion, first, the governing equations of the SPAB system were solved to obtain the dynamic behavior of the rotor center. Then, the performance of the SPAB system were examined under different operating conditions by generating the maximum Lyapunov exponents (MLEs). However, the calculation process of MLE is extremely time consuming and complex. To solve this problem efficiently, a high-precision machine learning (ML)-based MLE prediction model was proposed in this study. The results show that the training process can be finished within few minutes, and the prediction process is able to be completed within few seconds. Meanwhile, the results demonstrate the merit of using the machine learning method for solving the MLE prediction problem and shorten the calculation time significantly. The proposed prediction model achieves excellent prediction outcome and it is more efficient and precise than traditional iteration scheme for the calculation of MLE. The feasibility of the proposed model is validated and the results also are the major contribution of this study.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000532638400007", "keywords": "Imbalanced data; Information granule; Support vector machine (SVM); K-nearest-neighbor (KNN); Under-sampling", "title": "A design of information granule-based under-sampling method in imbalanced data classification", "abstract": "In numerous real-world problems, we are faced with difficulties in learning from imbalanced data. The classification performance of a \"standard\" classifier (learning algorithm) is evidently hindered by the imbalanced distribution of data. The over-sampling and under-sampling methods have been researched extensively with the aim to increase the predication accuracy over the minority class. However, traditional under-sampling methods tend to ignore important characteristics pertinent to the majority class. In this paper, a novel under-sampling method based on information granules is proposed. The method exploits the concepts and algorithms of granular computing. First, information granules are built around the selected patterns coming from the majority class to capture the essence of the data belonging to this class. In the sequel, the resultant information granules are evaluated in terms of their quality and those with the highest specificity values are selected. Next, the selected numeric data are augmented by some weights implied by the size of information granules. Finally, a support vector machine and a K-nearest-neighbor classifier, both being regarded here as representative classifiers, are built based on the weighted data. Experimental studies are carried out using synthetic data as well as a suite of imbalanced data sets coming from the public machine learning repositories. The experimental results quantify the performance of support vector machine and K-nearest-neighbor with under-sampling method based on information granules. The results demonstrate the superiority of the performance obtained for these classifiers endowed with conventional under-sampling method. In general, the improvement of performance expressed in terms of G-means is over 10% when applying information granule under-sampling compared with random under-sampling.", "journal": "SOFT COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": ["machine learning", "learning algorithm"], "label": "1", "title_label": "0"}
{"id": "WOS:000440122300058", "keywords": "Myo armband; Service robot vehicle; Hand gesture HCIs; Disabled people with mobility problems; Identity recognition; Support vector machine", "title": "Service robot system with integration of wearable Myo armband for specialized hand gesture human-computer interfaces for people with disabilities with mobility problems", "abstract": "Hand gestures will become a mainstream method of manipulating human computer interfaces (HCIs). For disabled people with mobility problems, hand gesture-based HCIs should be specifically designed. To achieve effective hand gesture HCIs, this study integrated a mobile service robot platform, three-dimensional (3D) imaging sensors, and wearable Myo armband device. Four kernel techniques are presented: (1) Myo armband software development kit hand gesture recognition using a two-layer hierarchy scheme to significantly increase hand gesture command numbers, (2) identity recognition of users using clustering-based support vector machine classifiers with a designed root mean square surface electromyography (RMS-sEMG) feature, (3) robot vehicle navigation with effective obstacle avoidance using a conceptually simple and computationally fast approach, and (4) efficient vehicle positioning based on the face-detection information of the user provided from the 3D imaging sensor to receive the hand gestures commands of the user with disabilities. (C) 2018 Elsevier Ltd. All rights reserved.", "journal": "COMPUTERS & ELECTRICAL ENGINEERING", "category": "Computer Science, Hardware & Architecture; Computer Science, Interdisciplinary Applications; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000316644600001", "keywords": "ECG beat classification; GA; PVC; SVM", "title": "HEART ARRHYTHMIA DETECTION USING SUPPORT VECTOR MACHINES", "abstract": "This paper deals with the discrimination of premature ventricular contraction (PVC) arrhythmia using support vector machine (SVM) and genetic algorithm (GA). Feature extraction module extracts ten electrocardiogram (ECG) morphological features and two timing interval features. Then a number of SVM classifiers with different values of C and the GRBF kernel parameter, sigma, are designed and compared their ability for classification of three different classes of ECG signals. However, the parameters were not optimum choices. So, GAs are used to find the optimum values of C and sigma. An overall classification accuracy of detection of 99.8112% were achieved using proposed method over nine files from the MIT/BIH arrhythmia database.", "journal": "INTELLIGENT AUTOMATION AND SOFT COMPUTING", "category": "Automation & Control Systems; Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000467990600001", "keywords": "machine learning; bacteria; transcription unit; R package; transcriptome", "title": "rSeqTU-A Machine-Learning Based R Package for Prediction of Bacteria Transcription Units", "abstract": "A transcription unit (TU) is composed of one or multiple adjacent genes on the same strand that are co-transcribed in mostly prokaryotes. Accurate identification of TUs is a crucial first step to delineate the transcriptional regulatory networks and elucidate the dynamic regulatory mechanisms encoded in various prokaryotic genomes. Many genomic features, for example, gene intergenic distance, and transcriptomic features including continuous and stable RNA-seq reads count signals, have been collected from a large amount of experimental data and integrated into classification techniques to computationally predict genome-wide TUs. Although some tools and web servers are able to predict TUs based on bacterial RNA-seq data and genome sequences, there is a need to have an improved machine learning prediction approach and a better comprehensive pipeline handling QC, TU prediction, and TU visualization. To enable users to efficiently perform TU identification on their local computers or high-performance clusters and provide a more accurate prediction, we develop an R package, named rSeqTU. rSeqTU uses a random forest algorithm to select essential features describing TUs and then uses support vector machine (SVM) to build TU prediction models. rSeqTU (available at https://s18692001.githubio/rSeqTU/) has six computational functionalities including read quality control, read mapping, training set generation, random forest-based feature selection, TU prediction, and TU visualization.", "journal": "FRONTIERS IN GENETICS", "category": "Genetics & Heredity", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000405581900001", "keywords": "Fault classification; Empirical Mode Decomposition (EMD); Support Vector Machines (SVMs)", "title": "Fault classification in power systems using EMD and SVM", "abstract": "In recent years, power quality has become the main concern in power system engineering. Classification of power system faults is the first stage for improving power quality and ensuring the system protection. For this purpose a robust classifier is necessary. In this paper, classification of power system faults using Empirical Mode Decomposition (EMD) and Support Vector Machines (SVMs) is proposed. EMD is used for decomposing voltages of transmission line into Intrinsic Mode Functions (IMFs). Hilbert Huang Transform (HHT) is used for extracting characteristic features from IMFs. A multiple SVM model is introduced for classifying the fault condition among ten power system faults. Algorithm is validated using MATLAB/SIMULINK environment. Results demonstrate that the combination of EMD and SVM can be an efficient classifier with acceptable levels of accuracy. (C) 2015 Ain Shams University. Production and hosting by Elsevier B.V. This is an open access article under the CC BY-NC-ND license.", "journal": "AIN SHAMS ENGINEERING JOURNAL", "category": "Engineering, Multidisciplinary", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000456903800016", "keywords": "Complete set; Density functional theory; NMR chemical shifts; Quantitative structure-property relationship; Coumarin derivatives; Support vector machine", "title": "Correlation between C-13 NMR chemical shifts and complete sets of descriptors of natural coumarin derivatives", "abstract": "Choosing the best set of descriptors for quantitative structure-property relationship (QSPR) is subjective and different descriptor set may be obtained for the same case. A complete set of descriptors means that the set is perfect since there is no any element redundant or need to be added. Here we report the first application of complete sets of descriptors calculated with PBE1PBE/6-311G(2d,2p) and B3LYP/6-311G(d) approaches to develop QSPR models for C-13 NMR chemical shifts (delta(c) parameters) of carbon atoms in coumarin derivatives. Four QSPR models for delta(c) parameters were developed with support vector machine (SVM) algorithm, by applying the particle swarm optimization (PSO) technique to optimize SVM parameters C and gamma. The four SVM models based on complete sets of descriptors have root mean square (rms) errors of 1.962 ppm, 2.145 ppm, 1.975 ppm and 2.363 ppm for the total data set (315 delta(c) parameters), which are less than the sins errors from multiple linear regression (MLR) models. To check the method based on complete sets, a large diverse dataset was predicted. Moreover, the total data set from 35 coumarin derivatives was predicted by using ChemDraw to make a head-tohead comparison. Results of the study suggest that applying complete sets of descriptors for QSPR models is successful.", "journal": "CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS", "category": "Automation & Control Systems; Chemistry, Analytical; Computer Science, Artificial Intelligence; Instruments & Instrumentation; Mathematics, Interdisciplinary Applications; Statistics & Probability", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000368189000019", "keywords": "Artificial intelligence; Machine learning; Support vector machines; Transfer learning; Parameter setting", "title": "Setting Parameters for Support Vector Machines using Transfer Learning", "abstract": "Machine Learning algorithms have a broad applicability, although generally a huge effort is necessary to find a good configuration for a given task. The tuning of free parameters, for example, is a task that directly affects the algorithm's performance but is often carried out as an ad hoc process. An alternative approach is to define the problem as a search in the parameters space, which can be computationally expensive and slow. Furthermore, to apply an algorithm to a different problem, all the work must be done from scratch. Transfer learning can be used to avoid this rework. In this paper we propose an approach to tune the parameter by means of transfer learning. The idea is to use data complexity characterization measures to evaluate the similarity among datasets and evaluate whether they share similar configurations of good parameters. To compare our approach, four performance measures were used: area under ROC Curve (AUC), accuracy, F1 and area under Precision-Recall Curve. Results show that the proposed approach may reduce the search space for the parameter tuning by exploiting parameter recommendation of similar datasets and provide competitive performance compared to other widely used techniques.", "journal": "JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS", "category": "Computer Science, Artificial Intelligence; Robotics", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000467280100017", "keywords": "Multivariate output sensitivity analysis; Multi-output support vector regression; Orthogonal polynomial kernel function; Polynomial chaos expansion", "title": "Multivariate output global sensitivity analysis using multi-output support vector regression", "abstract": "Models with multivariate outputs are widely used for risk assessment and decision-making in practical applications. In this paper, multi-output support vector regression (M-SVR) is employed for global sensitivity analysis (GSA) with multivariate output models. The orthogonal polynomial kernel is used to build the M-SVR meta-model, and the covariance-based sensitivity indices of multivariate output are obtained analytically by post-processing the coefficients of M-SVR model. In order to improve the performance of the orthogonal polynomial kernel M-SVR model, a kernel function iteration algorithm is introduced further. The proposed method take advantage of the information of all outputs to get robust meta-model. To validate the performance of the proposed method, two high-dimensional analytical functions and a hydrological model (HYMOD) with multiple outputs are examined, and a detailed comparison is made with the sparse polynomial chaos expansion meta-model developed in UQLab Toolbox. Results show that the proposed methods are efficient and accurate for GSA of the complex multivariate output models.", "journal": "STRUCTURAL AND MULTIDISCIPLINARY OPTIMIZATION", "category": "Computer Science, Interdisciplinary Applications; Engineering, Multidisciplinary; Mechanics", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000516827000173", "keywords": "anomaly detection; hybrid approach; C5; 0 Decision tree; Cyber analytics; data mining; machine learning; Zero-day malware; Intrusion; Intrusion Detection System", "title": "Hybrid Intrusion Detection System Based on the Stacking Ensemble of C5 Decision Tree Classifier and One Class Support Vector Machine", "abstract": "Cyberttacks are becoming increasingly sophisticated, necessitating the efficient intrusion detection mechanisms to monitor computer resources and generate reports on anomalous or suspicious activities. Many Intrusion Detection Systems (IDSs) use a single classifier for identifying intrusions. Single classifier IDSs are unable to achieve high accuracy and low false alarm rates due to polymorphic, metamorphic, and zero-day behaviors of malware. In this paper, a Hybrid IDS (HIDS) is proposed by combining the C5 decision tree classifier and One Class Support Vector Machine (OC-SVM). HIDS combines the strengths of SIDS) and Anomaly-based Intrusion Detection System (AIDS). The SIDS was developed based on the C5.0 Decision tree classifier and AIDS was developed based on the one-class Support Vector Machine (SVM). This framework aims to identify both the well-known intrusions and zero-day attacks with high detection accuracy and low false-alarm rates. The proposed HIDS is evaluated using the benchmark datasets, namely, Network Security Laboratory-Knowledge Discovery in Databases (NSL-KDD) and Australian Defence Force Academy (ADFA) datasets. Studies show that the performance of HIDS is enhanced, compared to SIDS and AIDS in terms of detection rate and low false-alarm rates.", "journal": "ELECTRONICS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Physics, Applied", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000378613400016", "keywords": "Relevance vector machine; uncertainty; image reconstruction; electrical capacitance tomography; standard deviation", "title": "Entropy Based Trees to Support Decision Making for Customer Churn Management", "abstract": "In this work we analyze empirically customer churn problem from a physical point of view to provide objective, data driven and significant answers to support decision making process in business application. In particular, we explore different entropy measures applied to decision trees and assess their performance from the business perspective using set of model quality measures often used in business practice. Additionally, the decision trees are compared with logistic regression and two machine learning methods - neural networks and support vector machines.", "journal": "ACTA PHYSICA POLONICA A", "category": "Physics, Multidisciplinary", "annotated_keywords": ["neural net", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000395790400002", "keywords": "lithology classification; imbalanced data sets; SVM; CCSD-MH; Borderline-SMOTE", "title": "Support vector machine as an alternative method for lithology classification of crystalline rocks", "abstract": "With the expansion of machine learning algorithms, automatic lithology classification that uses well logging data is becoming significant in formation evaluation and reservoir characterization. In fact, the complicated composition and structural variations of metamorphic rocks result in more nonlinear features in well logging data and elevate requirements to algorithms. Herein, the application of the support vector machine (SVM) in classifying crystalline rocks from Chinese Continental Scientific Drilling Main Hole (CCSD-MH) data was reported. We found that the SVM performs poorly on the lithology classification of crystalline rocks when training samples are imbalanced. The fact is that training samples are generally limited and imbalanced as cores cannot be obtained balanced and at 100 percent. In this paper, we introduced the synthetic minority over-sampling technique (SMOTE) and Borderline-SMOTE to deal with imbalanced data. After experiments generating different quantities of training samples by SMOTE and Borderline-SMOTE, the most suitable classifier was selected to overcome the disadvantage of the SVM. Then, the popular supervised classifier back-propagation neural networks (BPNN), which has been proved competent for lithology classification of crystalline rocks in previous studies, was compared to evaluate the performance of the SVM. Results show that Borderline-SMOTE can improve the SVM with substantially increased accuracy even for minority classes in a reasonable manner, while the SVM outperforms BPNN in aspects of lithology prediction and CCSD-MH data generalization. We demonstrate the potential of the SVM as an alternative to current methods for lithology identification of crystalline rocks.", "journal": "JOURNAL OF GEOPHYSICS AND ENGINEERING", "category": "Geochemistry & Geophysics", "annotated_keywords": ["neural net", "machine learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000408104000014", "keywords": "Cross distance minimization algorithm; Kernel function; Convex hull; Nearest point; Support vector machine", "title": "Cross kernel distance minimization for designing support vector machines", "abstract": "Cross distance minimization algorithm (CDMA) is an iterative method for designing a hard margin linear SVM based on the nearest point pair between the convex hulls of two linearly separable data sets. In this paper, we propose a new version of CDMA with clear explanation of its linear time complexity. Using kernel function and quadratic cost, we extend the new CDMA to its kernel version, namely, the cross kernel distance minimization algorithm (CKDMA), which has the requirement of linear memory storage and the advantages over the CDMA including: (1) it is applicable in the non-linear case; (2) it allows violations to classify non-separable data sets. In terms of testing accuracy, training time, and number of support vectors, experimental results show that the CKDMA is very competitive with some well-known and powerful SVM methods such as nearest point algorithm (NPA), kernel Schlesinger-Kozinec (KSK) algorithm and sequential minimal optimization (SMO) algorithm implemented in LIBSVM2.9.", "journal": "INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000577410000007", "keywords": "acne; skincare; care product recommendation system; support vector machine", "title": "System for Recommending Facial Skincare Products", "abstract": "Acne is a skin problem that plagues many young people and adults. Even if it is cured, it leaves acne spots or acne scars and many individuals must use skincare products or undertake medical treatment. However, the use of inappropriate skincare products can exacerbate skin conditions. We propose the use of multifeature processing and classification of skin quality and acne status by machine learning to make recommendations for facial skincare products. The results for 15 subjects show that the proposed system achieves a consumer satisfaction index of 80%.", "journal": "SENSORS AND MATERIALS", "category": "Instruments & Instrumentation; Materials Science, Multidisciplinary", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000345558100009", "keywords": "Common vector approach; support vector machine; artificial neural network; wavelet packet transform; fault classification; short circuit; transmission line", "title": "A Novel Algorithm to Enhance P300 in Single Trials: Application to Lie Detection Using F-Score and SVM", "abstract": "The investigation of lie detection methods based on P300 potentials has drawn much interest in recent years. We presented a novel algorithm to enhance signal-to-noise ratio (SNR) of P300 and applied it in lie detection to increase the classification accuracy. Thirty-four subjects were divided randomly into guilty and innocent groups, and the EEG signals on 14 electrodes were recorded. A novel spatial denoising algorithm (SDA) was proposed to reconstruct the P300 with a high SNR based on independent component analysis. The differences between the proposed method and our/other early published methods mainly lie in the extraction and feature selection method of P300. Three groups of features were extracted from the denoised waves; then, the optimal features were selected by the F-score method. Selected feature samples were finally fed into three classical classifiers to make a performance comparison. The optimal parameter values in the SDA and the classifiers were tuned using a grid-searching training procedure with cross-validation. The support vector machine (SVM) approach was adopted to combine with an F-score because this approach had the best performance. The presented model F-score_SVM reaches a significantly higher classification accuracy for P300 (specificity of 96.05%) and non-P300 (sensitivity of 96.11%) compared with the results obtained without using SDA and compared with the results obtained by other classification models. Moreover, a higher individual diagnosis rate can be obtained compared with previous methods, and the presented method requires only a small number of stimuli in the real testing application.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000331146400004", "keywords": "cross-validation; pattern classification; support vector machine; surrogate modeling; uncertainty", "title": "Characterizing Uncertainty Attributable to Surrogate Models", "abstract": "This paper investigates the characterization of the uncertainty in the prediction of surrogate models. In the practice of engineering, where predictive models are pervasively used, the knowledge of the level of modeling error in any region of the design space is uniquely helpful for design exploration and model improvement. The lack of methods that can explore the spatial variation of surrogate error levels in a wide variety of surrogates (i.e., model-independent methods) leaves an important gap in our ability to perform design domain exploration. We develop a novel framework, called domain segmentation based on uncertainty in the surrogate (DSUS) to segregate the design domain based on the level of local errors. The errors in the surrogate estimation are classified into physically meaningful classes based on the user's understanding of the system and/or the accuracy requirements for the concerned system analysis. The leave-one-out cross-validation technique is used to quantity the local errors. Support vector machine (SVM) is implemented to determine the boundaries between error classes, and to classify any new design point into the pertinent error class. We also investigate the effectiveness of the leave-one-out cross-validation technique in providing a local error measure, through comparison with actual local errors. The utility of the DSUS framework is illustrated using two different surrogate modeling methods: (i) the Kriging method and (ii) the adaptive hybrid functions (AHF). The DSUS framework is applied to a series of standard test problems and engineering problems. In these case studies, the DSUS framework is observed to provide reasonable accuracy in classifying the design-space based on error levels. More than 90% of the test points are accurately classified into the appropriate error classes.", "journal": "JOURNAL OF MECHANICAL DESIGN", "category": "Engineering, Mechanical", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000497707700022", "keywords": "grey systems; diseases; medical image processing; image texture; image segmentation; support vector machines; image colour analysis; endoscopes; image classification; biomedical optical imaging; feature extraction; ulcer disease detection; wireless capsule endoscopy; great technology; entire digestive tract; automatic computer-aided design method; normal WCE images; multiscale analysis-based grey-level co-occurrence matrix; GLCM; sub-band Laplacian pyramid decomposition; common Haralick features; feature descriptor; ulcer detection; encouraging detection rate performance", "title": "Multi-scale analysis of ulcer disease detection from WCE images", "abstract": "Wireless capsule endoscopy (WCE) proves its robustness as a great technology to examine the entire digestive tract or the small intestine. An automatic computer-aided design method is proposed in this study, in a manner to differentiate between ulcer disease and normal WCE images. A multi-scale analysis-based grey-level co-occurrence matrix (GLCM) is conducted here. The main step, the co-occurrence matrix (GLCM), is computed from each sub-band Laplacian pyramid decomposition, so as to extract the common Haralick features. Moreover, the p-value and area under the curve are used to select the relevant characteristics from the feature descriptor. This proposed approach was separately applied to the components of CIELab colour space. Ulcer detection was performed using the support vector machine. The findings demonstrate an encouraging detection rate performance of 95.38% for accuracy and 97.42% for sensitivity based on the first dataset and an average accuracy of 99.25 and 98.51% of sensitivities for the second dataset.", "journal": "IET IMAGE PROCESSING", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000520043200049", "keywords": "Visible/near-infrared reflectance spectroscopy; Multivariate methods; Rapid detection; Adulterated minced beef", "title": "Rapid detection of adulteration of minced beef using Vis/NIR reflectance spectroscopy with multivariate methods", "abstract": "High economic returns induce the continuous occurrence of meat adulteration. In this study, visible/near-infrared (Vis/NIR) reflectance spectroscopy with multivariate methods was used for the rapid detection of adulteration in minced beef. First, the reflectance spectra of different adulterated minced beef samples were measured at 350-2500 nm. Standardization and Savitzky-Golay (SG) smoothing were applied to reduce spectral interference and noise. Then, support vector machine (SVM), random forest (RF), partial least squares regression (PLSR), and deep convolutional neural network (DCNN) were adopted for adulteration type identification and level prediction. Moreover, principal component analysis (PCA), locally linear embedding (LLE), subwindow permutation analysis (SPA), and competitive adaptive reweighted sampling (CARS) were performed to eliminate redundant information. SG smoothing performed better on interference reduction. DCNN and PCA identified adulteration type with the accuracy above 99%. In adulteration level prediction, the RF with spectra of important wavelengths selected by CARS provided optimal performance for beef adulterated with pork, and coefficient of determination of prediction (R-P(2)) and the root mean square error of prediction (RMSEP) were 0.973 and 2.145. The best prediction for beef adulterated with beef heart was obtained using PLSR and CARS with R-P(2) of 0.960 and RMSEP of 2.758. Accordingly, Vis/NIR reflectance spectroscopy coupled with multivariate methods can provide the rapid and accurate detection of adulterated minced beef. (C) 2019 Published by Elsevier B.V.", "journal": "SPECTROCHIMICA ACTA PART A-MOLECULAR AND BIOMOLECULAR SPECTROSCOPY", "category": "Spectroscopy", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000436552700083", "keywords": "three-core medium-voltage cable joint; temperature monitoring; support vector regression; transient thermal analysis; temperature-rise test", "title": "Real-Time Temperature Estimation of Three-Core Medium-Voltage Cable Joint Based on Support Vector Regression", "abstract": "The joint is the weakest link in three-core medium-voltage power cable systems and the temperature is an essential indicator to its insulation condition. Therefore, a model to estimate the temperature inside the three-core cable joint was built based on support vector regression (SVR) with two fixed cable surface temperatures as inputs. The samples for model training were obtained from 3-D transient thermal analyses through finite element method (FEM) under different single-step currents. A temperature-rise test of 15 kV three-core cable joint was carried out and the estimated temperature based on SVR agrees well with the measured result with a maximum error of about 4 degrees C. Besides, the proposed model could accurately estimate the joint temperature even though the thermal conductivity of armor wrap used in thermal analysis for model training differs from its real value. The accuracies and calculation speed of the proposed model were compared with those of FEM, showing a better generality of our model. A temperature-rise test under unbalanced three-phase currents was performed and the temperature estimation errors are within 6 degrees C, indicating the applicability of the method. The effect of contact resistance was briefly discussed in the end. This approach helps improve cable operation and maintenance.", "journal": "ENERGIES", "category": "Energy & Fuels", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000388105300008", "keywords": "Reference models; Least squares support vector regression; Differential evolution; Optimization; Fault detection and diagnosis; Centrifugal chiller", "title": "An enhanced chiller FDD strategy based on the combination of the LSSVR-DE model and EWMA control charts", "abstract": "Development of the fault detection and diagnosis (FDD) for chiller systems is very important for improving the equipment reliability and saving energy consumption. The results of FDD performance are strongly dependent on the accuracy of chiller models. Since the accuracy of the chiller models depends on the indefinite model parameters which are normally chosen by experiments or experiences, an accurate chiller model is difficult to build. Therefore, optimization of model parameters is very useful to increase the accuracy of chiller models. This paper presents a new FDD strategy for centrifugal chillers of building air-conditioning systems, which is the combination between the nonlinear least squares support vector regression (LSSVR) based on the differential evolution (DE) algorithm and the exponentially weighted moving average (EWMA) control charts. In this strategy, the nonlinear LSSVR, which is a reformulation of SVR model with better generalization performances, is adopted to develop the reference feature parameter models in a typical non-linear chiller system. The DE algorithm which is a real-coding optimal algorithm with powerful global searching capacity is employed to enhance the accuracy of LSSVR models. The exponentially weighted moving average (EWMA) control charts are introduced to improve the fault detection capability as well as to reduce the Type II errors in a t-statistics-based way. Six typical faults of the chiller from the real-time experimental data of ASHRAE RP-1043 project are chosen to validate proposed FDD methods. Comprehensive comparisons between the proposed method and two similarly previous studies are performed. The comparison results show that the proposed method has achieved significant improvement in accuracy and reliability, especially at low severity levels. The proposed DE-LSSVR-EWMA strategy is robust for fault detection and diagnosis in centrifugal chiller systems. (C) 2016 Elsevier Ltd and IIR. All rights reserved.", "journal": "INTERNATIONAL JOURNAL OF REFRIGERATION-REVUE INTERNATIONALE DU FROID", "category": "Thermodynamics; Engineering, Mechanical", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000489367500001", "keywords": "Mushroom slices; color; extreme learning machine; back-propagation neural network; bayesian methods", "title": "Color prediction of mushroom slices during drying using Bayesian extreme learning machine", "abstract": "Color is an important appearance attribute of fruits and vegetables during drying processing, as it influences consumer?s preference and acceptability. Establishing color change kinetics model is an effective way for better understanding the quality changes and optimization of drying process. However, it is difficult to quickly and accurately predict color change kinetics during drying as it is highly nonlinear, complex, dynamic, and multivariable. To alleviate this problem, a new model based on extreme learning machine integrated Bayesian methods (BELM) has been developed for the prediction of color changes of mushroom slices during drying process. The effects of drying temperature (55, 60, 65, 70, and 75??C) and air velocity (3, 6, 9, and 12?m/s) on color change kinetics of mushroom slices during hot air impingement drying were firstly explored and the experimental results indicated that both drying temperature and air velocity significantly affected the color attributes. Then, to validate the robustness and effectiveness of BELM, the basic extreme learning machine (ELM) and traditional back-propagation neural network (BPNN) models have also been employed to predict the color quality. In terms of prediction accuracy and execution time, BELM could achieve least similar or even better performance than ELM and BPNN. It overcame the overfitting problems of ELM. The test results of optimal BELM model by two new cases revealed that the lowest R-2 and highest RMSE of BELM model were 0.9725 and 0.0563, respectively. The absolute values of relative errors between the actual and predicted values were lower than 8.5%.", "journal": "DRYING TECHNOLOGY", "category": "Engineering, Chemical; Engineering, Mechanical", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000457939400060", "keywords": "tree species; classification; ZiYuan-3; stereo image; machine learning", "title": "Classification of Land Cover, Forest, and Tree Species Classes with ZiYuan-3 Multispectral and Stereo Data", "abstract": "The global availability of high spatial resolution images makes mapping tree species distribution possible for better management of forest resources. Previous research mainly focused on mapping single tree species, but information about the spatial distribution of all kinds of trees, especially plantations, is often required. This research aims to identify suitable variables and algorithms for classifying land cover, forest, and tree species. Bi-temporal ZiYuan-3 multispectral and stereo images were used. Spectral responses and textures from multispectral imagery, canopy height features from bi-temporal stereo imagery, and slope and elevation from the stereo-derived digital surface model data were examined through comparative analysis of six classification algorithms including maximum likelihood classifier (MLC), k-nearest neighbor (kNN), decision tree (DT), random forest (RF), artificial neural network (ANN), and support vector machine (SVM). The results showed that use of multiple source data-spectral bands, vegetation indices, textures, and topographic factors-considerably improved land-cover and forest classification accuracies compared to spectral bands alone, which the highest overall accuracy of 84.5% for land cover classes was from the SVM, and, of 89.2% for forest classes, was from the MLC. The combination of leaf-on and leaf-off seasonal images further improved classification accuracies by 7.8% to 15.0% for land cover classes and by 6.0% to 11.8% for forest classes compared to single season spectral image. The combination of multiple source data also improved land cover classification by 3.7% to 15.5% and forest classification by 1.0% to 12.7% compared to the spectral image alone. MLC provided better land-cover and forest classification accuracies than machine learning algorithms when spectral data alone were used. However, some machine learning approaches such as RF and SVM provided better performance than MLC when multiple data sources were used. Further addition of canopy height features into multiple source data had no or limited effects in improving land-cover or forest classification, but improved classification accuracies of some tree species such as birch and Mongolia scotch pine. Considering tree species classification, Chinese pine, Mongolia scotch pine, red pine, aspen and elm, and other broadleaf trees as having classification accuracies of over 92%, and larch and birch have relatively low accuracies of 87.3% and 84.5%. However, these high classification accuracies are from different data sources and classification algorithms, and no one classification algorithm provided the best accuracy for all tree species classes. This research implies the same data source and the classification algorithm cannot provide the best classification results for different land cover classes. It is necessary to develop a comprehensive classification procedure using an expert-based approach or hierarchical-based classification approach that can employ specific data variables and algorithm for each tree species class.", "journal": "REMOTE SENSING", "category": "Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": ["neural net", "machine learning", "learning algorithm"], "label": "1", "title_label": "0"}
{"id": "WOS:000364857000003", "keywords": "Biomechanics; machine learning; regression model; walking", "title": "Classification of Parkinson's Disease Gait Using Spatial-Temporal Gait Features", "abstract": "Quantitative gait assessment is important in diagnosis and management of Parkinson's disease (PD); however, gait characteristics of a cohort are dispersed by patient physical properties including age, height, body mass, and gender, as well as walking speed, which may limit capacity to discern some pathological features. The aim of this study was twofold. First, to use a multiple regression normalization strategy that accounts for subject age, height, body mass, gender, and self-selected walking speed to identify differences in spatial-temporal gait features between PD patients and controls; and second, to evaluate the effectiveness of machine learning strategies in classifying PDgait after gait normalization. Spatial-temporal gait data during self-selected walking were obtained from 23 PD patients and 26 aged-matched controls. Data were normalized using standard dimensionless equations and multiple regression normalization. Machine learning strategies were then employed to classify PD gait using the raw gait data, data normalized using dimensionless equations, and data normalized using the multiple regression approach. After normalizing data using the dimensionless equations, only stride length, step length, and double support time were significantly different between PD patients and controls (p < 0.05); however, normalizing data using the multiple regression method revealed significant differences in stride length, cadence, stance time, and double support time. Random Forest resulted in a PD classification accuracy of 92.6% after normalizing gait data using the multiple regression approach, compared to 80.4% (support vector machine) and 86.2% (kernel Fisher discriminant) using raw data and data normalized using dimensionless equations, respectively. Our multiple regression normalization approach will assist in diagnosis and treatment of PD using spatial-temporal gait data.", "journal": "IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS", "category": "Computer Science, Information Systems; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Medical Informatics", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000414683400002", "keywords": "schizophrenia; Parkinson's disease; normal aging; support vector machine; resting-state fMRI; functional connectivity; brain networks; machine learning", "title": "On the Integrity of Functional Brain Networks in Schizophrenia, Parkinson's Disease, and Advanced Age: Evidence from Connectivity-Based Single-Subject Classification", "abstract": "Previous whole-brain functional connectivity studies achieved successful classifications of patients and healthy controls but only offered limited specificity as to affected brain systems. Here, we examined whether the connectivity patterns of functional systems affected in schizophrenia (SCZ), Parkinson's disease (PD), or normal aging equally translate into high classification accuracies for these conditions. We compared classification performance between pre-defined networks for each group and, for any given network, between groups. Separate support vector machine classifications of 86 SCZ patients, 80 PD patients, and 95 older adults relative to their matched healthy/young controls, respectively, were performed on functional connectivity in 12 task-based, meta-analytically defined networks using 25 replications of a nested 10-fold cross-validation scheme. Classification performance of the various networks clearly differed between conditions, as those networks that best classified one disease were usually non-informative for the other. For SCZ, but not PD, emotion-processing, empathy, and cognitive action control networks distinguished patients most accurately from controls. For PD, but not SCZ, networks subserving autobiographical or semantic memory, motor execution, and theory-of-mind cognition yielded the best classifications. In contrast, young-old classification was excellent based on all networks and outperformed both clinical classifications. Our pattern-classification approach captured associations between clinical and developmental conditions and functional network integrity with a higher level of specificity than did previous whole-brain analyses. Taken together, our results support resting-state connectivity as a marker of functional dysregulation in specific networks known to be affected by SCZ and PD, while suggesting that aging affects network integrity in a more global way. (C) 2017 Wiley Periodicals, Inc.", "journal": "HUMAN BRAIN MAPPING", "category": "Neurosciences; Neuroimaging; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000346406500038", "keywords": "Hyperspectral imaging; Support Vector Machine; Artificial fish swarm algorithm; Hollow heart; Potato", "title": "Diagnostic Accuracy of Parkinson Disease by Support Vector Machine (SVM) Analysis of I-123-FP-CIT Brain SPECT Data", "abstract": "Brain single-photon-emission-computerized tomography (SPECT) with I-123-ioflupane (I-123-FP-CIT) is useful to diagnose Parkinson disease (PD). To investigate the diagnostic performance of I-123-FP-CIT brain SPECT with semiquantitative analysis by Basal Ganglia V2 software (BasGan), we evaluated semiquantitative data of patients with suspect of PD by a support vector machine classifier (SVM), a powerful supervised classification algorithm. I-123-FP-CIT SPECT with BasGan analysis was performed in 90 patients with suspect of PD showing mild symptoms (bradykinesia-rigidity and mild tremor). PD was confirmed in 56 patients, 34 resulted non-PD (essential tremor and drug-induced Parkinsonism). A clinical follow-up of at least 6 months confirmed diagnosis. To investigate BasGan diagnostic performance we trained SVM classification models featuring different descriptors using both a \"leave-one-out'' and a \"five-fold'' method. In the first study we used as class descriptors the semiquantitative radiopharmaceutical uptake values in the left (L) and right (R) putamen (P) and in the L and R caudate nucleus (C) for a total of 4 descriptors (CL, CR, PL, PR). In the second study each patient was described only by CL and CR, while in the third by PL and PR descriptors. Age was added as a further descriptor to evaluate its influence in the classification performance. I-123-FP-CIT SPECT with BasGan analysis reached a classification performance higher than 73.9% in all the models. Considering the \"Leave-one-out'' method, PL and PR were better predictors (accuracy of 91% for all patients) than CL and CR descriptors; using PL, PR, CL, and CR diagnostic accuracy was similar to that of PL and PR descriptors in the different groups. Adding age as a further descriptor accuracy improved in all the models. The best results were obtained by using all the 5 descriptors both in PD and non-PD subjects (CR and CL+PR and PL+age = 96.4% and 94.1%, respectively). Similar results were observed for the \"five-fold'' method. I-123-FP-CIT SPECT with BasGan analysis using SVM classifier was able to diagnose PD. Putamen was the most discriminative descriptor for PD and the patient age influenced the classification accuracy.", "journal": "MEDICINE", "category": "Medicine, General & Internal", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000358217600008", "keywords": "Children's drawing and handwriting; Feature extraction; Support vector machine (SVM)", "title": "Analyses of pupils' polygonal shape drawing strategy with respect to handwriting performance", "abstract": "Polygonal shape drawing tasks are commonly used in psychological, clinical and standard handwriting tests to evaluate children's development. Early detection of physical/mental disorders within subjects therefore requires objective analysis of the drawing tasks. This analysis would help to identify specific rehabilitation needs and accurate detection of disorders. Herein, the aim is to determine the correlation between the performance of polygonal shape drawing and levels in handwriting performance. In the reported experimentation two groups of participants aged between 6 and 7 were studied. The first group was identified by educational experts as being below-average writers within their age group whilst the second group was age-matched controls of average and above. Subjects were required to draw an isosceles triangle within a novel computer-based framework founded on a pen-based graphic tablet capture device. Subsequently, a sequential feature vector containing performance values relating to the order in which they drew the triangle was extracted from tablet data and compared against one another when presented in constructional strategy models. Statistical analyses and automated classification were applied to sequences to infer handwriting level based on the triangle drawing strategy. From our experiments drawing strategies showed significant differences in drawing end-point position, number of strokes used, and the frequency of particular drawing strategies amongst average and below-average handwriting groups. Additionally, a support vector machine classifier was used to detect group membership based on the triangle drawing strategy. From this exemplar polygonal shape drawing study it is revealed that there are details in children's drawing strategy which considerably differs in grouping based on handwriting performance.", "journal": "PATTERN ANALYSIS AND APPLICATIONS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000407048700001", "keywords": "Context; Contextual information; Cyber-security; Netflows; Intrusion detection; Semantics", "title": "Contextual information fusion for intrusion detection: a survey and taxonomy", "abstract": "Research in cyber-security has demonstrated that dealing with cyber-attacks is by no means an easy task. One particular limitation of existing research originates from the uncertainty of information that is gathered to discover attacks. This uncertainty is partly due to the lack of attack prediction models that utilize contextual information to analyze activities that target computer networks. The focus of this paper is a comprehensive review of data analytics paradigms for intrusion detection along with an overview of techniques that apply contextual information for intrusion detection. A new research taxonomy is introduced consisting of several dimensions of data mining techniques, which create attack prediction models. The survey reveals the need to use multiple categories of contextual information in a layered manner with consistent, coherent, and feasible evidence toward the correct prediction of cyber-attacks.", "journal": "KNOWLEDGE AND INFORMATION SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000516611900009", "keywords": "electroencephalography; e-learning; eye gaze", "title": "Investigation of Relationship Between Eye Gaze and Brain Waves towards Smart Sensing for E-learning", "abstract": "In recent years, we have witnessed a new trend of learning called e-learning where learners can take courses using electronic devices with Internet connection. Although e-learning is convenient because it removes temporal and spatial limitations, it is difficult to know whether the learner is really paying attention to the learning materials. To address this problem, we tried to use electroencephalography (EEG) to investigate a learner's concentration in our previous study. However, our previous study relied on subjective evaluation, and there was no objective observation to relate the EEG signals to the learner's concentration. Given this background, we compared eye gaze and EEG results to find the appropriate position and frequency band of EEG in e-learning in this study. We compared the EEG result obtained during a period when the subjects were watching a video lecture and that obtained during a period when the subjects were not watching, and determined that the viewing state could be predicted from EEG logistic regression and a support vector machine (SVM). The results suggested that measuring beta and gamma waves and examining the parietal and occipital regions are both effective.", "journal": "SENSORS AND MATERIALS", "category": "Instruments & Instrumentation; Materials Science, Multidisciplinary", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000346661500001", "keywords": "karst rocky desertification; object-based; image segmentation; optimal object scale; different karst landscapes; China", "title": "Comparison and combination of different models for optimal landslide susceptibility zonation", "abstract": "It is important to compare different methods and apply combined models for landslide susceptibility zonation on a regional scale for land-use planning and hazard mitigation. The purpose of this study is attempt to obtain an optimal landslide susceptibility zonation in a severely landslide affected region where the available data are very limited. Six single models (analytical hierarchy process (AHP), logistic regression (LR), fuzzy logic (FL), weight of evidence integrated logistic regression (WL), artificial neural network (ANN) and support vector machine (SVM)), were applied to obtain the single landslide susceptibility zonations along the middle reaches of the Bailong River from Zhouqu to Wudu, southern Gansu, China, then these single models were compared, after which the three single models that performed better (LR, ANN and SVM) were selected to prepare the combined zonations. Six conditional independent environmental factors were selected as the explanatory variables that contribute to landslide occurrence (elevation, slope, aspect, distance from fault, lithology and settlement density). The mapped landslides in this region were randomly partitioned into two sets: 80% of the landslides were used for the model training and the remaining 20% were used for validation of the models. Receiver operating characteristic and cost curves were plotted as means of evaluating the quality of the susceptibility zonations for the single and combined models. Results show that the single LR, ANN and SVM are models with superior prediction performance and are more suitable for constructing the combined models in this study. Compared with single models, the combined models provided an improved prediction capability and reduced uncertainties.", "journal": "QUARTERLY JOURNAL OF ENGINEERING GEOLOGY AND HYDROGEOLOGY", "category": "Engineering, Geological; Geosciences, Multidisciplinary", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000456333500002", "keywords": "neural modeling; peripheral nerve; sieve electrode; TIME electrode; nerve stimulation; nerve recording; electroneurogram", "title": "Micro-channel sieve electrode for concurrent bidirectional peripheral nerve interface. Part B: stimulation", "abstract": "Objective. Successful use of a prosthetic limb by an amputee is facilitated by haptic feedback- both a sense of touch and proprioception. Stimulating afferent fibers within peripheral nerves has been shown to provide somatosensation enabling amputees to modulate the control of prosthetic limbs. Peripheral nerve interfaces (PNIs) have also been used to decode patients' motor intentions. It seems ideal to use PNIs to record efferent fibers for motor control while stimulating afferent fibers to create concurrent sensory feedback. However, while many PNIs claim to be bi-directional, few can both stimulate and record at the same time due to stimulation artifacts which are orders of magnitude larger than the recorded motor signals. This study uses computational modelling to compare the stimulation artifact at threshold levels of stimulation for thin-film transverse intrafascicular multichannel electrodes (tfTIMEs) with micro-channel sieve electrodes. Approach. Finite element models of micro-channel sieves and tfTIMESs were used to solve for electric fields generated during peripheral nerve stimulation. Electrophysiological responses were simulated using axon models. Stimulation artifacts were calculated for stimuli eliciting axonal action potentials. Simulations were carried out for multiple micro-channel geometries and electrode configurations. Main results. Stimulation artifacts generated for threshold stimulation currents are lower for micro-channel devices compared to tfTIMEs. Consequently, stimulus artifacts at threshold currents were substantially higher for the tfTIME. Micro-channel width has a moderate impact on recruitment thresholds and stimulus artifacts. Using the micro-channel sieve in bipolar and tripolar stimulation configurations greatly decreases stimulation artifacts particularly for optimized contact placements (CPs). Electroneurogram (ENG) signals from the companion paper were incorporated showing a great improvement in signal-to-artifact ratio for the micro-channel electrode compared to tfTIMEs. Significance. Stimulating regenerated nerve tissue using micro-channel sieve electrodes can decrease stimulation artifacts and elicit neuronal responses at low stimulation amplitudes. Further analysis provides clues to optimal implementations of micro-channel devices. Finally, stimulation artifacts for simulated tfTIME devices were 2-3 orders of magnitude larger than ENG levels. In contrast, for some micro-channel configurations stimulation artifacts were 3-4 orders of magnitude smaller than ENG levels.", "journal": "JOURNAL OF NEURAL ENGINEERING", "category": "Engineering, Biomedical; Neurosciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000392679800008", "keywords": "PM2.5; Aerosol optical depth; ANFIS; SVM; BPANN; Australia", "title": "A satellite-based model for estimating PM2.5 concentration in a sparsely populated environment using soft computing techniques", "abstract": "We applied three soft computing methods including adaptive neuro-fuzzy inference system (ANFIS), support vector machine (SVM) and back-propagation artificial neural network (BPANN) algorithms for estimating the ground-level PM2.5 concentration. These models were trained by comprehensive satellite based, meteorological, and geographical data. A 10-fold cross-validation (CV) technique was used to identify the optimal predictive model. Results showed that ANFIS was the best-performing model for predicting the variations in PM2.5 concentration. Our findings demonstrated that the CV-R-2 of the ANFIS (0.81) is greater than that of the SVM (0.67) and BPANN (0.54) model. The results suggested that soft computing methods like ANFIS, in combination with spatiotemporal data from satellites, meteorological data and geographical information improve the estimate of PM2.5 concentration in sparsely populated areas. (C) 2016 Elsevier Ltd. All rights reserved.", "journal": "ENVIRONMENTAL MODELLING & SOFTWARE", "category": "Computer Science, Interdisciplinary Applications; Engineering, Environmental; Environmental Sciences; Water Resources", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000462955600006", "keywords": "Machine learning; Prediction model; Computational intelligence; Desulfurization; Sulfur contents", "title": "Supervised machine learning techniques in the desulfurization of oil products for environmental protection: A review", "abstract": "Desulfurization, known as the removal of sulfur from oil, is extremely important in the petroleum processing industry and in the environmental protection. Several oil-upgrading processes such as desulfurization and catalysts such as alumina loaded with molybdenum have been proposed to deal with the problem of removing sulfur-containing compounds from light oil. Thus, several parameters are required to be experimentally optimized which demands a lot of work including reagents. Advanced mathematical tools can be used to optimize the desulfurization process and to study the related factors. The modeling and simulation of the desulfurization process have been proposed in several studies in order to facilitate a better understanding of the process operations. Machine Learning (ML) is regarded as a promising methodological area to perform such optimization and analysis. This review describes the relevant methods for dealing with the applications of ML for desulfurization in oil. Although a good number of research papers have appeared in recent years, the application of ML for desulfurization is still a promising area of research. The review presents an overview of the ML methods and their categories in desulfurization. It discusses and compares the methods that employ ML to optimize the desulfurization process. The review also highlights the findings and possible research directions. (C) 2018 Institution of Chemical Engineers. Published by Elsevier B.V. All rights reserved.", "journal": "PROCESS SAFETY AND ENVIRONMENTAL PROTECTION", "category": "Engineering, Environmental; Engineering, Chemical", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000501621200006", "keywords": "Mapping; Multispectral; Phenology; Tasselled cap; Vegetation classification", "title": "Efficacy of multi-season Sentinel-2 imagery for compositional vegetation classification", "abstract": "Vegetation maps are essential tools for the conservation and management of landscapes as they contain essential information for informing conservation decisions. Traditionally, maps have been created using field-based approaches which, due to limitations in costs and time, restrict the size of the area for which they can be created and frequency at which they can be updated. With the increasing availability of satellite sensors providing multi-spectral imagery with high temporal frequency, new methods for efficient and accurate vegetation mapping have been developed. The objective of this study was to investigate to what extent multi-seasonal Sentinel-2 imagery can assist in mapping complex compositional classifications at fine spatial scales. We deliberately chose a challenging case study, namely a visually and structurally homogenous scrub vegetation (known as kwongan) of Western Australia. The classification scheme consists of 24 target classes and a random 60/40 split was used for model building and validation. We compared several multi-temporal (seasonal) feature sets, consisting of numerous combinations of spectral bands, vegetation indices as well as principal component and tasselled cap transformations, as input to four machine learning classifiers (Support Vector Machines; SVM, Nearest Neighbour; NN, Random Forests; RF, and Classification Trees; CT) to separate target classes. The results show that a multi-temporal feature set combining autumn and spring images sufficiently captured the phenological differences between the classes and produced the best results, with SVM (74%) and NN (72%) classifiers returning statistically superior results compared to RF (65%) and CT (50%). The SWIR spectral bands captured during spring, the greenness indices captured during spring and the tasselled cap transformations derived from the autumn image emerged as most informative, which suggests that ecological factors (e.g. shared species, patch dynamics) occurring at a sub-pixel level likely had the biggest impact on class confusion. However, despite these challenges, the results are auspicious and suggest that seasonal Sentinel-2 imagery has the potential to predict compositional vegetation classes with high accuracy. Further work is needed to determine whether these results are replicable in other vegetation types and regions.", "journal": "INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION", "category": "Remote Sensing", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000494280000005", "keywords": "Unmanned surface vehicles; ship target; image recognition; deep learning; SSD", "title": "A novel image recognition algorithm of target identification for unmanned surface vehicles based on deep learning", "abstract": "Rich image information is one of the important means through which unmanned surface vehicles effectively and reliably identify targets during autonomous navigation. However, the adaptability of traditional artificial design feature methods in target representation and differentiation remains limited due to the diversity of ship target types, different scales, and complex and dynamic outdoor scenes. This study proposed a ship target recognition method based on single shot multibox detector (SSD) deep learning. First, training and test sample sets were constructed by acquiring and creating a ship's target image and background image under different types and scenes in an actual river environment. Subsequently, the sample set was used to train and optimize the SSD depth model to achieve adaptive extraction and recognition of target features. Lastly, ship identification experiments with different background environments and foreground targets were performed to test the effectiveness of the proposed method. The support vector machine method based on artificial feature extraction was used for the comparative experiments. Experiment results showed that the SSD-based deep learning method achieved better results than the artificial design feature method in terms of recall and precision rates.", "journal": "JOURNAL OF INTELLIGENT & FUZZY SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["deep learning", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000503381500178", "keywords": "irregular surfaces; defect detection; K-Means; feature fusion", "title": "Detection of Micro-Defects on Irregular Reflective Surfaces Based on Improved Faster R-CNN", "abstract": "The detection of defects on irregular surfaces with specular reflection characteristics is an important part of the production process of sanitary equipment. Currently, defect detection algorithms for most irregular surfaces rely on the handcrafted extraction of shallow features, and the ability to recognize these defects is limited. To improve the detection accuracy of micro-defects on irregular surfaces in an industrial environment, we propose an improved Faster R-CNN model. Considering the variety of defect shapes and sizes, we selected the K-Means algorithm to generate the aspect ratio of the anchor box according to the size of the ground truth, and the feature matrices are fused with different receptive fields to improve the detection performance of the model. The experimental results show that the recognition accuracy of the improved model is 94.6% on a collected ceramic dataset. Compared with SVM (Support Vector Machine) and other deep learning-based models, the proposed model has better detection performance and robustness to illumination, which proves the practicability and effectiveness of the proposed method.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000432907600008", "keywords": "Hybrid beamforming; data-driven solution; mm-wave; beam selection; SVM", "title": "Data-Driven-Based Analog Beam Selection for Hybrid Beamforming Under mm-Wave Channels", "abstract": "Hybrid beamforming is a promising low-cost solution for large multiple-input multiple-output systems, where the base station is equipped with fewer radio frequency chains. In these systems, the selection of codewords for analog beamforming is essential to optimize the uplink sum rate. In this paper, based on machine learning, we propose a data-driven method of analog beam selection to achieve a near-optimal sum rate with low complexity, which is highly dependent on training data. Specifically, we take the beam selection problem as a multiclass-classification problem, where the training dataset consists of a large number of samples of the millimeter-wave channel. Using this training data, we exploit the support vector machine algorithm to obtain a statistical classification model, which maximizes the sum rate. For real-time transmissions, with the derived classification model, we can select, with low complexity, the optimal analog beam of each user. We also propose a novel method to determine the optimal parameter of Gaussian kernel function via McLaughlin expansion. Analysis and simulation results reveal that, as long as the training data are sufficient, the proposed data-driven method achieves a near-optimal sum-rate performance, while the complexity reduces by several orders of magnitude, compared to the conventional method.", "journal": "IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING", "category": "Engineering, Electrical & Electronic", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000505989100001", "keywords": "Parking AGV; vehicle pose estimation; 3-D point clouds; ground segmentation; registration algorithm", "title": "Vehicle pose estimation algorithm for parking automated guided vehicle", "abstract": "Parking automated guided vehicle is more and more widely used for efficient automatic parking and one of the tough challenges for parking automated guided vehicle is the problem of vehicle pose estimation. The traditional algorithms rely on the profile information of vehicle body and sensors are required to be mounted at the top of the vehicle. However, the sensors are always mounted at a lower place because the height of a parking automated guided vehicle is always beyond 0.2mm, where we can only get the vehicle wheel information and limited vehicle body information. In this article, a novel method is given based on the symmetry of wheel point clouds collected by 3-D lidar. Firstly, we combine cell-based method with support vector machine classifier to segment ground point clouds. Secondly, wheel point clouds are segmented from obstacle point clouds and their symmetry are corrected by iterative closest point algorithm. Then, we estimate the vehicle pose by the symmetry plane of wheel point clouds. Finally, we compare our method with registration method that combines sample consensus initial alignment algorithm and iterative closest point algorithm. The experiments have been carried out.", "journal": "INTERNATIONAL JOURNAL OF ADVANCED ROBOTIC SYSTEMS", "category": "Robotics", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000377319800016", "keywords": "Soil temperature; Extreme Learning Machine (ELM); Self-Adaptive Evolutionary Extreme; Learning Machine (SaE-ELM); Estimation; Agent", "title": "Using self-adaptive evolutionary algorithm to improve the performance of an extreme learning machine for estimating soil temperature", "abstract": "In this study, the self-adaptive evolutionary (SaE) agent is employed to structure the contributing elements to process the management of extreme learning machine (ELM) architecture based on a logical procedure. In fact, the SaE algorithm is utilized for possibility of enhancing the performance of the ELM to estimate daily soil temperature (ST) at 6 different depths of 5, 10, 20, 30, 50 and 100 cm. In the developed SaE-ELM model, the network hidden node parameters of the ELM are optimized using SaE algorithm. The precision of the SaE-ELM is then compared with the ELM model. Daily weather data sets including minimum, maximum and average air temperatures (T-min, T-max and T-avg), atmospheric pressure (P) and global solar radiation (R-s) collected for two Iranian stations of Bandar Abbas and Kerman with different climate conditions have been utilized. After primary evaluation, T-min, T-max and T-avg are considered as final inputs for the ELM and SaE-ELM models due to their high correlations with ST at all depths. The achieved results for both stations reveal that both ELM and SaE-ELM models offer desirable performance to estimate daily ST at all depths; nevertheless, a slightly more precision can be obtained by the SaE-ELM model. The performance of the ELM and SaE-ELM models are verified against genetic programming (GP) and artificial neural network (ANN) models developed in this study. For Bandar Abbass station, the obtained mean absolute bias error (MABE) and correlation coefficient (R) for the ELM model at different depths are in the range of 0.9116-1.5988 degrees C and 0.9023-0.9840, respectively while for the SaE-ELM model they are in the range of 0.8660-1.5338 degrees C and 0.9084-0.9893, respectively. In addition, for Kerman Station the attained MABE and RMSE for the ELM model vary from 1.6567 to 2.4233 degrees C and 0.8661 to 0.9789, respectively while for the SaE-ELM model they vary from 1.5818 to 2.3422 degrees C and 0.8736 to 0.9831, respectively. (c) 2016 Elsevier B.V. All rights reserved.", "journal": "COMPUTERS AND ELECTRONICS IN AGRICULTURE", "category": "Agriculture, Multidisciplinary; Computer Science, Interdisciplinary Applications", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000362851000088", "keywords": "Humanoid robot locomotion; instrumented system; computational intelligence techniques; floor classification", "title": "One-class classification based authentication of peanut oils by fatty acid profiles", "abstract": "Developing a method of identifying oil authenticity is becoming critical for protecting customers' rights as adulteration of edible oils is a particular concern in food quality. Since adulterants in edible oils are usually unknown, the authenticity identification is a one-class classification problem in chemometrics. In this study, a one-class classification model was built to identify the authenticity of peanut oils by fatty acid profiles. Based on previous studies, 28 fatty acids were identified and quantified for peanut oils. The authenticity identification model was built by one-class partial least squares (OCPLS) classifier for peanut oils. Subsequently, the established model was validated by independent test sets. The results indicated that the OCPLS classifier could effectively detect adulterated oils and was therefore employed for authenticity assessment. Moreover, counterfeit oils adulterated with different levels of other edible oils were simulated by the Monte Carlo method and employed to test the lowest adulteration level of this one-class classifier. As a result, the model could identify peanut oils and sensitively detect adulteration of edible oils with other vegetable oils at adulteration level of more than 4%.", "journal": "RSC ADVANCES", "category": "Chemistry, Multidisciplinary", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000366715900002", "keywords": "dynamic markings; loudness-level representation; machine learning; loudness prediction; marking classification", "title": "In-Vivo Imaging of Cell Migration Using Contrast Enhanced MRI and SVM Based Post-Processing", "abstract": "The migration of cells within a living organism can be observed with magnetic resonance imaging (MRI) in combination with iron oxide nanoparticles as an intracellular contrast agent. This method, however, suffers from low sensitivity and specificty. Here, we developed a quantitative non-invasive in-vivo cell localization method using contrast enhanced multiparametric MRI and support vector machines (SVM) based post-processing. Imaging phantoms consisting of agarose with compartments containing different concentrations of cancer cells labeled with iron oxide nanoparticles were used to train and evaluate the SVM for cell localization. From the magnitude and phase data acquired with a series of T-2(+)-weighted gradient-echo scans at different echo-times, we extracted features that are characteristic for the presence of superparamagnetic nanoparticles, in particular hyper-and hypointensities, relaxation rates, short-range phase perturbations, and perturbation dynamics. High detection quality was achieved by SVM analysis of the multiparametric featurespace. The in-vivo applicability was validated in animal studies. The SVM detected the presence of iron oxide nanoparticles in the imaging phantoms with high specificity and sensitivity with a detection limit of 30 labeled cells per mm(3), corresponding to 19 mu M of iron oxide. As proof-of-concept, we applied the method to follow the migration of labeled cancer cells injected in rats. The combination of iron oxide labeled cells, multiparametric MRI and a SVM based post processing provides high spatial resolution, specificity, and sensitivity, and is therefore suitable for non-invasive in-vivo cell detection and cell migration studies over prolonged time periods.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000444920000035", "keywords": "Smart meter analytics; Socio-demographic classification; Customer segmentation; Demand side management", "title": "Rethinking the privacy of the smart grid: What your smart meter data can reveal about your household in Ireland", "abstract": "The global market for smart electricity meters has grown rapidly in recent years and is anticipated to sustain its solid increase in the near future. By analyzing half-hourly meter data from over 4000 Irish households, this study seeks to examine the relationship between households' attributes and their electricity demand through the following questions: (1) knowing a given set of household attributes, can we accurately classify households according to their demand volume and daily demand pattern and (2) can we infer some of the key households' characteristics from their meter data. The attributes considered include the size, presence of kids, social class, employment status and the annual income of the households. A range of machine learning methods including tree-based algorithms, support vector machines and neural networks are deployed to answer these questions. The results suggest the potential for reasonably accurate segmentation of consumers according to their demand volume while the classification based on daily demand patterns were shown to be more challenging. For pre dicing household attributes, higher accuracy values are reported when predicting the household size, social class, and employment status. On the contrary, inferring the household income category and the presence of children in the household were shown to be more difficult", "journal": "ENERGY RESEARCH & SOCIAL SCIENCE", "category": "Environmental Studies", "annotated_keywords": ["neural net", "machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000434008600118", "keywords": "Expansion Potential; Nighttime Light Data; Urban Expansion Pattern; Fully Convolution Neural Network", "title": "Urban Expansion Pattern Analysis and Planning Implementation Evaluation Based on using Fully Convolution Neural Network to Extract Land Range", "abstract": "In recent years, due to the rapid development of China's urban, it is significant for effective implementation of urban science development and planning that grasp the process of urban development, analyze the potential of subsequent development, and evaluate the matching degree of the development status and the planning. Thereinto, an effective way we exercise today is to evaluate urban expansion pattern analysis and planning implementation. According to research results of the urban land range extraction method based on the support vector machine (SVM) and fully convolution neural network (FCN) of the depth learning method for the night light image data, this paper describes an integration of remote sensing (RS) and geographic information system (GIS) and analyzes the urban expansion pattern of Beijing based on the computed results of landscape pattern indices. The results unveil that from 1990s to 2010s, Beijing took on a circle expansion mode on the ground the spatial agglomeration degree gradually increases and the expansion potential has spatial distinctions, which basically meets the requirements of the overall planning.", "journal": "NEUROQUANTOLOGY", "category": "Neurosciences", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000404696700029", "keywords": "Multisensory; Hoteling transform; Support vector machine; Zernike moments; Pattern recognition; Imaging sensors", "title": "Multisensor of thermal and visual images to detect concealed weapon using harmony search image fusion approach", "abstract": "Perils of terror threats are a problem that can be managed by precise and competent surveillance systems. It is an imperative demand to swiftly conduct real time verification of person suspected to borne threat. Real time verification exercise involves identification of concealed weapon threat, interrogation of suspect and seizure of weapons or explosives in a covert controlled fashion. There is an urgent need for an intelligent and safe surveillance system with inbuilt sensors that can detect concealed weapons and pinpoint their location on the body without any physical contact from a standoff distance. Fused imagery of visual and infrared images is processed to identify weapons specifically. When the weapon template is matched with the suspected object using Zernike moments, the identity of the object becomes obvious, and any effort to conceal it becomes futile. In this paper, a novel approach to detect concealed weapons based on discrete wavelet transform in conjunction with dimension reduced meta-heuristic algorithm, the harmony search, and shape matching based K means SVM classification is presented. Experimental results are provided to demonstrate that the proposed hybrid approach provides superior performance. (C) 2016 Elsevier B.V. All rights reserved.", "journal": "PATTERN RECOGNITION LETTERS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000351091200027", "keywords": "Computer-aided detection (CADe); computed tomography (CT) imaging; false positive (FP) reduction; lung nodules; vector quantization (VQ)", "title": "Fast and Adaptive Detection of Pulmonary Nodules in Thoracic CT Images Using a Hierarchical Vector Quantization Scheme", "abstract": "Computer-aided detection (CADe) of pulmonary nodules is critical to assisting radiologists in early identification of lung cancer from computed tomography (CT) scans. This paper proposes a novel CADe system based on a hierarchical vector quantization (VQ) scheme. Compared with the commonly-used simple thresholding approach, the high-level VQ yields a more accurate segmentation of the lungs from the chest volume. In identifying initial nodule candidates (INCs) within the lungs, the low-level VQ proves to be effective for INCs detection and segmentation, as well as computationally efficient compared to existing approaches. False-positive (FP) reduction is conducted via rule-based filtering operations in combination with a feature-based support vector machine classifier. The proposed system was validated on 205 patient cases from the publically available online Lung Image Database Consortium database, with each case having at least one juxta-pleural nodule annotation. Experimental results demonstrated that our CADe system obtained an overall sensitivity of 82.7% at a specificity of 4 FPs/scan. Especially for the performance on juxta-pleural nodules, we observed 89.2% sensitivity at 4.14 FPs/scan. With respect to comparable CADe systems, the proposed system shows outperformance and demonstrates its potential for fast and adaptive detection of pulmonary nodules via CT imaging.", "journal": "IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS", "category": "Computer Science, Information Systems; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Medical Informatics", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000464483900026", "keywords": "Risk analysis; Finance; Structural model; Random forests; Credit default swaps", "title": "Credit spread approximation and improvement using random forest regression", "abstract": "Credit Default Swap (CDS) levels provide a market appreciation of companies' default risk. These derivatives are not always available, creating a need for CDS approximations. This paper offers a simple, global and transparent CDS structural approximation, which contrasts with more complex and proprietary approximations currently in use. This Equity-to-Credit formula (E2C), inspired by CreditGrades, obtains better CDS approximations, according to empirical analyses based on a large sample spanning 2016-2018. A random forest regression run with this E2C formula and selected additional financial data results in an 87.3% out-of-sample accuracy in CDS approximations. The transparency property of this algorithm confirms the predominance of the E2C estimate, and the impact of companies' debt rating and size, in predicting their CDS. (C) 2019 Elsevier B.V. All rights reserved.", "journal": "EUROPEAN JOURNAL OF OPERATIONAL RESEARCH", "category": "Management; Operations Research & Management Science", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000419855700006", "keywords": "Emotion assessment; EEG; Peripheral signal; Feature extraction; Classification", "title": "An innovative emotion assessment using physiological signals based on the combination mechanism", "abstract": "The main purpose of this paper is the assessment of emotions using Electroencephalogram (EEG) and peripheral physiological signals and improvement of recognition accuracy of emotional states using combination mechanism. In the first step, according to the type of signals, effective features were extracted in the time and frequency domains; then, by using the Fisher's Linear Discriminant (FLD) method, the most effective features were selected. Based on these features, six classifiers were used: Support Vector Machine (SVM), Nearest Mean (NM), K-Nearest Neighborhood (K-NN), 1-Nearest Neighborhood (1-NN), FLD, and Linear Discriminant Analysis (LDA). They classified emotions in two classes (low and high) through arousal, valence, and liking dimensions. The Leave-One-Out Cross-Validation (LOOCV) method has been implemented to evaluate the performance of classifiers. To enhance the accuracy of classification, combination at feature and classifier levels was performed. Via the concatenation method, combination at feature level was done. Then, by Majority voting, Fixed and Stacking algorithms, combination at classifier level was implemented. Results showed that these classifiers were selected properly and, thanks to them, good improvements were achieved compared with previous studies. Finally, by using combination methods, obtained recognition accuracy was much more reliable and combination at classifier level resulted in significant improvement. (c) 2017 Sharif University of Technology. All rights reserved.", "journal": "SCIENTIA IRANICA", "category": "Engineering, Multidisciplinary", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000473773600026", "keywords": "ANN; bio-contaminations detection; EPANET-SMX; pattern recognition; SVM", "title": "Artificial intelligence-based monitoring system of water quality parameters for early detection of non-specific bio-contamination in water distribution systems", "abstract": "This research aims to simulate bio-contamination risk propagation under real-life conditions in the water distribution system (WDS) of Lille University's Scientific City Campus (France), solving the source identification and the response modeling. Neglecting dynamic reactions and not considering the possible chemical decay of most of the contaminants leads to an overestimation of the exposed population. Therefore, unlike the available event detection models, this study considers the interrelated change of several water-quality parameters such as free chlorine concentration, pH, alkalinity, and total organic carbon (TOC) resulting from the pollutants blending. In fact, starting from regular WDS monitoring, the baseline thresholds for each of the mentioned parameters are established; then, significant deviations from the baseline are used as indication for contaminations. For this reason, the purpose of the research was to develop and demonstrate the feasibility of an artificial intelligence (AI)-based smart monitoring system that will effectively enable water operators to ensure a quasi real-time quality control for early chemical and/or bio-contamination detection and preemptive risk management. Advanced pattern recognizers, such as Support Vector Machines (SVMs), and innovative sensing technology solutions, such as Artificial Neural Network (ANN), have been used for this purpose, identifying the anomalies and the severity-level assessment.", "journal": "WATER SUPPLY", "category": "Engineering, Environmental; Environmental Sciences; Water Resources", "annotated_keywords": ["artificial intelligen"], "label": "1", "title_label": "1"}
{"id": "WOS:000426675800011", "keywords": "EEG; Human cognition; Manual assembly; Mental fatigue", "title": "Detection and estimation of mental fatigue in manual assembly process of complex products", "abstract": "Purpose This paper aims to investigate an approach for mental fatigue detection and estimation of assembly operators in the manual assembly process of complex products, with the purpose of founding the basis for adaptive transfer and demonstration of assembly process information (API), and eventually making the manual assembly process smarter and more human-friendly. Design/methodology/approach The proposed approach detects and estimates the mental state of assembly operators by electroencephalography (EEG) signal recording and analysis in an engine assembly experiment. When the subjects perform assembly tasks, their EEG signal is recorded by a portable EEG recording system called Emotiv EPOC+ headset. The feature set of the EEG signal is then extracted by calculating its power spectrum density (PSD), followed by data dimension reduction based on principal component analysis (PCA). The dimension-reduced data are classified by using support vector machines (SVMs), and hence, the mental state of assembly operators can be estimated during the assembly process. Findings The experimental result shows that the proposed approach is able to estimate the mental state of assembly operators within an acceptable accuracy range, and the PCA-based dimension reduction method performs very well by representing the high-dimensional EEG feature set with just a few principal components. Originality/value This paper provides theoretical and experimental basis for the API transfer and demonstration based on human cognition. It provides a new idea to seek balance between the improvement of production efficiency and the sustainable utilization of human resources.", "journal": "ASSEMBLY AUTOMATION", "category": "Automation & Control Systems; Engineering, Manufacturing", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000551865400002", "keywords": "Time series analysis; Neural networks, fuzzy logic; Seismic monitoring and test-ban treaty verification; Early warning; Probability distributions", "title": "Modeling the dielectric constants of crystals using machine learning", "abstract": "The relative permittivity of a crystal is a fundamental property that links microscopic chemical bonding to macroscopic electromagnetic response. Multiple models, including analytical, numerical, and statistical descriptions, have been made to understand and predict dielectric behavior. Analytical models are often limited to a particular type of compound, whereas machine learning (ML) models often lack interpretability. Here, we combine supervised ML, density functional perturbation theory, and analysis based on game theory to predict and explain the physical trends in optical dielectric constants of crystals. Two ML models, support vector regression and deep neural networks, were trained on a dataset of 1364 dielectric constants. Analysis of Shapley additive explanations of the ML models reveals that they recover correlations described by textbook Clausius-Mossotti and Penn models, which gives confidence in their ability to describe physical behavior, while providing superior predictive power.", "journal": "JOURNAL OF CHEMICAL PHYSICS", "category": "Chemistry, Physical; Physics, Atomic, Molecular & Chemical", "annotated_keywords": ["neural net", "machine learning", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000393081300003", "keywords": "Flat-topped Gaussian distribution; maximum likelihood estimation; moment fitting strategy; system identification; quantization error", "title": "Estimation of Flat-topped Gaussian distribution with application in system identification", "abstract": "Uniformly distributed uncertainty exists in industrial process; additive error introduced by quantization is an example. To be able to handle additive uniform and Gaussian measurement uncertainty simultaneously in system identification, the Flat-topped Gaussian distribution is considered in this paper as an alternative to the Gaussian distribution. To incorporate this type of uncertainty in the maximum likelihood estimation framework, the explicit form of its density function is of necessity. This work proposes an approach for obtaining both the functional structure and corresponding parameter estimation of Flat-topped Gaussian distribution by a moment fitting strategy. The performance of the proposed approximation function is verified by comparison to the Flat-topped Gaussian distributed random variable with different Gaussian and uniform components. Results of numerical simulations and industrial applications in system identification are presented to verify the effectiveness of the Flat-topped Gaussian distribution for noise distribution in handling additional uniform uncertainty.", "journal": "JOURNAL OF CHEMOMETRICS", "category": "Automation & Control Systems; Chemistry, Analytical; Computer Science, Artificial Intelligence; Instruments & Instrumentation; Mathematics, Interdisciplinary Applications; Statistics & Probability", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000331263600019", "keywords": "Sepsis; Clinical Decision Support; Mortality Prediction; Lactate Level Prediction; Machine Learning; Electronic Health Records", "title": "From vital signs to clinical outcomes for patients with sepsis: a machine learning basis for a clinical decision support system", "abstract": "Objective To develop a decision support system to identify patients at high risk for hyperlactatemia based upon routinely measured vital signs and laboratory studies. Materials and methods Electronic health records of 741 adult patients at the University of California Davis Health System who met at least two systemic inflammatory response syndrome criteria were used to associate patients' vital signs, white blood cell count (WBC), with sepsis occurrence and mortality. Generative and discriminative classification (naive Bayes, support vector machines, Gaussian mixture models, hidden Markov models) were used to integrate heterogeneous patient data and form a predictive tool for the inference of lactate level and mortality risk. Results An accuracy of 0.99 and discriminability of 1.00 area under the receiver operating characteristic curve (AUC) for lactate level prediction was obtained when the vital signs and WBC measurements were analysed in a 24h time bin. An accuracy of 0.73 and discriminability of 0.73 AUC for mortality prediction in patients with sepsis was achieved with only three features: median of lactate levels, mean arterial pressure, and median absolute deviation of the respiratory rate. Discussion This study introduces a new scheme for the prediction of lactate levels and mortality risk from patient vital signs and WBC. Accurate prediction of both these variables can drive the appropriate response by clinical staff and thus may have important implications for patient health and treatment outcome. Conclusions Effective predictions of lactate levels and mortality risk can be provided with a few clinical variables when the temporal aspect and variability of patient data are considered.", "journal": "JOURNAL OF THE AMERICAN MEDICAL INFORMATICS ASSOCIATION", "category": "Computer Science, Information Systems; Computer Science, Interdisciplinary Applications; Health Care Sciences & Services; Information Science & Library Science; Medical Informatics", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000582288000003", "keywords": "Myocardial infarction; Computer aided diagnostic system; Electrocardiogram; Pan Thompkins algorithm; Classifiers", "title": "Accurate detection of myocardial infarction using non linear features with ECG signals", "abstract": "Interrupted blood flow to regions of the heart causes damage to heart muscles, resulting in myocardial infarction (MI). MI is a major source of death worldwide. Accurate and timely detection of MI facilitates initiation of emergency revascularization in acute MI and early secondary prevention therapy in established MI. In both acute and ambulatory settings, the electrocardiogram (ECG) is a standard data type for diagnosis. ECG abnormalities associated with MI can be subtle, and may escape detection upon clinical reading. Experience and training are required to visually extract salient information present in the ECG signals. This process of characterization is manually intensive, and prone to intra-and inter-observer-variability. The clinical problem can be posed as one of diagnostic classification of MI versus no MI on the ECG, which is amenable to computational solutions. Computer Aided Diagnosis (CAD) systems are designed to be automated, rapid, efficient, and ultimately cost-effective systems that can be employed to detect ECG abnormalities associated with MI. In this work, ECGs from 200 subjects were analyzed (52 normal and 148 MI). The proposed methodology involves pre-processing of signals and subsequent detection of R peaks using the Pan-Tompkins algorithm. Nonlinear features were extracted. The extracted features were ranked based on Student's t-test and input to k-Nearest Neighbor (KNN), Support Vector Machine (SVM), Probabilistic Neural Network (PNN), and Decision Tree (DT) classifiers for distinguishing normal versus MI classes. This method yielded the highest accuracy 97.96%, sensitivity 98.89%, and specificity 93.80% using the SVM classifier.", "journal": "JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems; Telecommunications", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000456222700023", "keywords": "Adaptive arbitrary structuring elements; Mathematical morphology (MM); Particle swarm optimization (PSO); Roads; Support vector machines (SVM)", "title": "Automatic generation of adaptive structuring elements for road identification in VHR images", "abstract": "Road extraction from very high resolution remotely sensed images is crucial in many urban applications. Acquiring automatically up-to-date and accurate information about roads is significant for various intelligent applications such as smart vehicle navigation, planning urban areas, roads monitoring and traffic management for intelligent transportation systems, and leading proper military operations. All possible knowledge about roads properties must be incorporated in designing intelligent systems that interpret and decide with high precision the existence of roads in remote sensing images. Various extraction techniques rely on mathematical morphology (MM) that detects desired road structures through a sliding standard and empirically chosen structuring element (SE) over the input image. In this paper, we design an intelligent process that not only combines spectral and spatial properties of roads but also impacts significantly the flexibility in retrieving spatial information. Indeed, we propose an adaptive algorithm that supplies tailored and most adequate arbitrary structuring elements for every image at hand. It has the significant impact of providing flexibility since every arbitrary generated SE is exclusively dedicated to the processed image. The processing consists of two major steps: a) we use the particle swarm optimization algorithm to look for the adaptive SEs; b) we introduce a priori knowledge based on human visual interpretation of roads characteristics and define some spatial indices to refine the results. We evaluated our method over many remotely sensed images; accuracy results show that the proposed method outperforms standard approaches which are limited to utilize only empirically chosen and standard SEs. (C) 2018 Elsevier Ltd. All rights reserved.", "journal": "EXPERT SYSTEMS WITH APPLICATIONS", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000439401800057", "keywords": "Cross-validation; ETo; Neuro-fuzzy; Random forest; Wavelet decomposition", "title": "Improving the performance of the mass transfer-based reference evapotranspiration estimation approaches through a coupled wavelet random forest methodology", "abstract": "Among different reference evapotranspiration (ETo) modeling approaches, mass transfer-based methods have been less studied. These approaches utilize temperature and wind speed records. On the other hand, the empirical equations proposed in this context generally produce weak simulations, except when a local calibration is used for improving their performance. This might be a crucial drawback for those equations in case of local data scarcity for calibration procedure. So, application of heuristic methods can be considered as a substitute for improving the performance accuracy of the mass transfer-based approaches. However, given that the wind speed records have usually higher variation magnitudes than the other meteorological parameters, application of a wavelet transform for coupling with heuristic models would be necessary. In the present paper, a coupled wavelet -random forest (WRF) methodology was proposed for the first time to improve the performance accuracy of the mass transfer-based ETo estimation approaches using cross-validation data management scenarios in both local and cross-station scales. The obtained results revealed that the new coupled WRF model (with the minimum scatter index values of 0.150 and 0.192 for local and external applications, respectively) improved the performance accuracy of the single RF models as well as the empirical equations to great extent.", "journal": "JOURNAL OF HYDROLOGY", "category": "Engineering, Civil; Geosciences, Multidisciplinary; Water Resources", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000464605500002", "keywords": "artificial intelligence (AI); cervical myelopathy (CM); diffusion tensor imaging (DTI); prognosis", "title": "Revisiting the effect of spatial resolution on information content based on classification results", "abstract": "Polarimetric Synthetic Aperture Radar (PolSAR) images are an important source of information. Speckle noise gives SAR images a granular appearance that makes interpretation and analysis hard tasks. A major issue is the assessment of information content in these kinds of images, and how it is affected by usual processing techniques. Previous works have resulted in various approaches for quantifying image information content. In this paper, we study this problem from the classification accuracy viewpoint, focusing on the filtering and the classification stages. Thus, through classified images, we verify how changing the properties of the input data affects their quality. The input is an actual PolSAR image, the control parameters are (i) the filter (Local Mean, LM, or Model-Based PolSAR, MBPolSAR) and the size of their support, and (ii) the classification method (Maximum Likelihood, ML, or Support Vector Machine, SVM), and the output is the precision of the classification algorithm applied to the filtered data. To expand the conclusions, this study deals not only with Classification Accuracy but also with Kappa and Overall Accuracy as measures of map precision. Experiments were conducted on two airborne PolSAR images. Differently from what was observed in previous works, almost all quality measures are good and increase with degradation, i.e. the filtering algorithms that we used always improve the classification results at least up to supports of size 7 x 7.", "journal": "INTERNATIONAL JOURNAL OF REMOTE SENSING", "category": "Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000344466600002", "keywords": "Shot boundary detection; Walsh Hadamard transform kernel; basis vectors; feature vector; probabilistic classifier with feature weights; procedure based identification", "title": "Walsh-Hadamard Transform Kernel-Based Feature Vector for Shot Boundary Detection", "abstract": "Video shot boundary detection (SBD) is the first step of video analysis, summarization, indexing, and retrieval. In SBD process, videos are segmented into basic units called shots. In this paper, a new SBD method is proposed using color, edge, texture, and motion strength as vector of features (feature vector). Features are extracted by projecting the frames on selected basis vectors of Walsh-Hadamard transform (WHT) kernel and WHT matrix. After extracting the features, based on the significance of the features, weights are calculated. The weighted features are combined to form a single continuity signal, used as input for Procedure Based shot transition Identification process (PBI). Using the procedure, shot transitions are classified into abrupt and gradual transitions. Experimental results are examined using large-scale test sets provided by the TRECVID 2007, which has evaluated hard cut and gradual transition detection. To evaluate the robustness of the proposed method, the system evaluation is performed. The proposed method yields F1-Score of 97.4% for cut, 78% for gradual, and 96.1% for overall transitions. We have also evaluated the proposed feature vector with support vector machine classifier. The results show that WHT-based features can perform well than the other existing methods. In addition to this, few more video sequences are taken from the Openvideo project and the performance of the proposed method is compared with the recent existing SBD method.", "journal": "IEEE TRANSACTIONS ON IMAGE PROCESSING", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000361728200010", "keywords": "Computational intelligence; Ensemble techniques; Homogenous ensemble; Heterogeneous ensemble; Software maintenance; Prediction; Empirical studies", "title": "Three empirical studies on predicting software maintainability using ensemble methods", "abstract": "More accurate prediction of software maintenance effort contributes to better management and control of software maintenance. Several research studies have recently investigated the use of computational intelligence models for software maintainability prediction. The performance of these models, however, may vary from dataset to dataset. Consequently, ensemble methods have become increasingly popular as they take advantage of the capabilities of their constituent computational intelligence models toward a dataset to come up with more accurate or at least competitive prediction accuracy compared to individual models. This paper investigates and empirically evaluates different homogenous and heterogeneous ensemble methods in predicting software maintenance effort and change proneness. Three major empirical studies were designed and conducted taken into consideration different design such as the types of the investigated ensembles methods, types of prediction problems, used datasets, and other experimental setup. Overall empirical evidence obtained from the three studies confirms that some ensemble methods provide more accurate or at least competitive prediction accuracy compared to individual models across datasets, and thus they are more reliable.", "journal": "SOFT COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000446411600010", "keywords": "Biomarker; Functional outcomes; Imaging; Machine learning; Resting-state fMRI; Schizophrenia", "title": "Resting-State Connectivity Biomarkers of Cognitive Performance and Social Function in Individuals With Schizophrenia Spectrum Disorder and Healthy Control Subjects", "abstract": "BACKGROUND: Deficits in neurocognition and social cognition are drivers of reduced functioning in schizophrenia spectrum disorders, with potentially shared neurobiological underpinnings. Many studies have sought to identify brain-based biomarkers of these clinical variables using a priori dichotomies (e.g., good vs. poor cognition, deficit vs. nondeficit syndrome). METHODS: We evaluated a fully data-driven approach to do the same by building and validating a brain connectivity-based biomarker of social cognitive and neurocognitive performance in a sample using resting-state and task-based functional magnetic resonance imaging (n = 74 healthy control participants, n = 114 persons with schizophrenia spectrum disorder, 188 total). We used canonical correlation analysis followed by clustering to identify a functional connectivity signature of normal and poor social cognitive and neurocognitive performance. RESULTS: Persons with poor social cognitive and neurocognitive performance were differentiated from those with normal performance by greater resting-state connectivity in the mirror neuron and mentalizing systems. We validated our findings by showing that poor performers also scored lower on functional outcome measures not included in the original analysis and by demonstrating neuroanatomical differences between the normal and poorly performing groups. We used a support vector machine classifier to demonstrate that functional connectivity alone is enough to distinguish normal and poorly performing participants, and we replicated our findings in an independent sample (n = 75). CONCLUSIONS: A brief functional magnetic resonance imaging scan may ultimately be useful in future studies aimed at characterizing long-term illness trajectories and treatments that target specific brain circuitry in those with impaired cognition and function", "journal": "BIOLOGICAL PSYCHIATRY", "category": "Neurosciences; Psychiatry", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000338334200029", "keywords": "Defoliation; N deposition; Critical loads; Random forests analysis", "title": "Random Forests Analysis: a Useful Tool for Defining the Relative Importance of Environmental Conditions on Crown Defoliation", "abstract": "Defoliation is one of the most important parameters monitored in the International Cooperative Programme on Assessment and Monitoring of Air Pollution Effects on Forests (ICP Forests). Defoliation is an indicator for forest health and vitality. Conventional statistical analysis shows weak or not significant correlations between tree crown defoliation and climatic conditions or air pollution parameters, because of its high variability. The study aims to evaluate the most important factors among climatic, pollutants (N-ox and NHy) and stand parameters affecting crown defoliation of the main European tree species (Fagus sylvatica, Picea abies, Quercus ilex, Pinus sylvestris and Quercus petraea) through application of a new and powerful statistical classifier, the random forests analysis (RFA). RFA highlighted that tree crown defoliation was mainly related to age in P. abies, to geographic location in F. sylvatica and to air pollution predictors in Q. ilex, while it was similarly linked to meteorological and air pollution predictors in P. sylvestris and Q. petraea. In this study, RFA has proven to be, for the first time, a useful tool to discern the most important predictors affecting tree crown defoliation, and consequently, it can be used for an appropriate forest management.", "journal": "WATER AIR AND SOIL POLLUTION", "category": "Environmental Sciences; Meteorology & Atmospheric Sciences; Water Resources", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000401679400001", "keywords": "Parkinson's disease; Resting-state functional magnetic resonance imaging; Amplitude of low-frequency fluctuations; Fractional amplitude of low-frequency fluctuations", "title": "Identifying the presence of Parkinson's disease using low-frequency fluctuations in BOLD signals", "abstract": "Parkinson's disease (PD) is a chronic, progressive, and degenerative neurological disorder that is characterized by the degeneration of dopamine neurons in the substantia nigra and the formation of intracellular Lewy inclusion bodies. Resting-state functional magnetic resonance imaging (RS-fMRI) has demonstrated evidence of changes in metabolic patterns in individuals with PD. The purpose of this study was to determine whether the presence of PD could be \"predicted\" based on resting fluctuations in the blood oxygenation level dependent signal. We utilized RS-fMRI to measure the amplitude of low-frequency fluctuation (ALFF) and the fractional ALFF (fALFF) in 51 patients with PD and 50 age- and sex-matched healthy controls. Compared with the healthy controls, the individuals with PD exhibited altered ALFFs in the bilateral lingual gyrus and left putamen and an altered fALFF in the right cerebellum posterior lobe. Support vector machines (SVMs), which comprise a supervised pattern recognition method that enables predictions at the individual level, were trained to separate individuals with PD from healthy controls based on the ALFF and fALFF. Using the leave-one-out cross-validation method to analyze our sample, we reliably distinguished the participants with PD from the controls with 92% sensitivity and 87% specificity. Overall, these findings suggest that the SVM-neuroimaging approach may be of particular clinical value because it enables the accurate identification of PD at the individual level. RS-fMRI should be considered for development as a biomarker and an analytical tool for the evaluation of PD. (C) 2017 Elsevier B.V. All rights reserved.", "journal": "NEUROSCIENCE LETTERS", "category": "Neurosciences", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000427373300005", "keywords": "Huntington&#8217; s disease; Machine learning; Transcriptional regulation; Enrichment analysis", "title": "A regression modelling approach for optimizing segmentation scale parameters to extract buildings of different sizes", "abstract": "Multiresolution segmentation (MRS) is one of the most commonly used image segmentation algorithms in the remote-sensing community. This algorithm has three user-defined parameters: scale, shape, and compactness. The scale parameter (SP) is the most crucial one in determining the average size of the image segments generated. Since setting this parameter typically requires a trial-and-error process, automatically estimating it can expedite the segmentation process. However, most of automatic approaches are still iterative and can lead to a time-consuming process. In this article, we propose a new, non-iterative framework for estimating the SP with an emphasis on extracting individual urban buildings. The basis of the proposed method is to investigate the feasibility of associating the size of urban buildings with a corresponding 'optimal' (or at least reasonable) SP using an explicit mathematical equation. Using the proposed method, these two variables are related to each other by constructing a mathematical (regression) model. In this framework, the independent variables were chosen to be the typical size of buildings in a given urban area and the spatial resolution of the image under consideration; and the dependent variable was chosen to be the corresponding optimal SP. To assess the potential of the proposed approach, two regression models that yielded explicit equations (i.e. degree-2 polynomial (DP), and regression tree (RT)) were employed. In addition, as a sophisticated and versatile nonlinear model, support vector regression (SVR) was utilized to further measure the performances of DP and RT models compared with it. According to the comparisons, the DP model was selected as a representative of the proposed approach. In the end, to evaluate the proposed methodology, we also compared the results derived from the DP model with those derived from the Estimation of Scale Parameter (ESP) tool. Based on our experiments, not only did the DP model produce acceptable results, but also it outperformed ESP tool in this study for extracting individual urban buildings.", "journal": "INTERNATIONAL JOURNAL OF REMOTE SENSING", "category": "Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000325746400001", "keywords": "autism; effective connectivity; fMRI; classification; machine learning; theory-of-mind", "title": "Identification of neural connectivity signatures of autism using machine learning", "abstract": "Alterations in interregional neural connectivity have been suggested as a signature of the pathobiology of autism. There have been many reports of functional and anatomical connectivity being altered while individuals with autism are engaged in complex cognitive and social tasks. Although disrupted instantaneous correlation between cortical regions observed from functional MRI is considered to be an explanatory model for autism, the causal influence of a brain area on another (effective connectivity) is a vital link missing in these studies. The current study focuses on addressing this in an fMRI study of Theory-of-Mind (ToM) in 15 high-functioning adolescents and adults with autism and 15 typically developing control participants. Participants viewed a series of comic strip vignettes in the MRI scanner and were asked to choose the most logical end to the story from three alternatives, separately for trials involving physical and intentional causality. The mean time series, extracted from 18 activated regions of interest, were processed using a multivariate autoregressive model (MVAR) to obtain the causality matrices for each of the 30 participants. These causal connectivity weights, along with assessment scores, functional connectivity values, and fractional anisotropy obtained from DTI data for each participant, were submitted to a recursive cluster elimination based support vector machine classifier to determine the accuracy with which the classifier can predict a novel participant's group membership (autism or control). We found a maximum classification accuracy of 95.9% with 19 features which had the highest discriminative ability between the groups. All of the 19 features were effective connectivity paths, indicating that causal information may be critical in discriminating between autism and control groups. These effective connectivity paths were also found to be significantly greater in controls as compared to ASD participants and consisted predominantly of outputs from the fusiform face area and middle temporal gyrus indicating impaired connectivity in ASD participants, particularly in the social brain areas. These findings collectively point toward the fact that alterations in causal connectivity in the brain in ASD could serve as a potential non-invasive neuroimaging signature for autism.", "journal": "FRONTIERS IN HUMAN NEUROSCIENCE", "category": "Neurosciences; Psychology", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000466533800001", "keywords": "thyroid nodule; ultrasound; machine learning; random forest; diagnosis", "title": "Machine Learning-Assisted System for Thyroid Nodule Diagnosis", "abstract": "Background: Ultrasound (US) examination is helpful in the differential diagnosis of thyroid nodules (malignant vs. benign), but its accuracy relies heavily on examiner experience. Therefore, the aim of this study was to develop a less subjective diagnostic model aided by machine learning. Methods: A total of 2064 thyroid nodules (2032 patients, 695 male; M-age = 45.25 +/- 13.49 years) met all of the following inclusion criteria: (i) hemi- or total thyroidectomy, (ii) maximum nodule diameter 2.5 cm, (iii) examination by conventional US and real-time elastography within one month before surgery, and (iv) no previous thyroid surgery or percutaneous thermotherapy. Models were developed using 60% of randomly selected samples based on nine commonly used algorithms, and validated using the remaining 40% of cases. All models function with a validation data set that has a pretest probability of malignancy of 10%. The models were refined with machine learning that consisted of 1000 repetitions of derivatization and validation, and compared to diagnosis by an experienced radiologist. Sensitivity, specificity, accuracy, and area under the curve (AUC) were calculated. Results: A random forest algorithm led to the best diagnostic model, which performed better than radiologist diagnosis based on conventional US only (AUC = 0.924 [confidence interval (CI) 0.895-0.953] vs. 0.834 [CI 0.815-0.853]) and based on both conventional US and real-time elastography (AUC = 0.938 [CI 0.914-0.961] vs. 0.843 [CI 0.829-0.857]). Conclusions: Machine-learning algorithms based on US examinations, particularly the random forest classifier, may diagnose malignant thyroid nodules better than radiologists.", "journal": "THYROID", "category": "Endocrinology & Metabolism", "annotated_keywords": ["machine learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000393053200013", "keywords": "Anomaly detection; Arbitrary shape clustering; Gaussian Mixture Model; Distribution distance", "title": "A fast and noise resilient cluster-based anomaly detection", "abstract": "Clustering, while systematically applied in anomaly detection, has a direct impact on the accuracy of the detection methods. Existing cluster-based anomaly detection methods are mainly based on spherical shape clustering. In this paper, we focus on arbitrary shape clustering methods to increase the accuracy of the anomaly detection. However, since the main drawback of arbitrary shape clustering is its high memory complexity, we propose to summarize clusters first. For this, we design an algorithm, called Summarization based on Gaussian Mixture Model (SGMM), to summarize clusters and represent them as Gaussian Mixture Models (GMMs). After GMMs are constructed, incoming new samples are presented to the GMMs, and their membership values are calculated, based on which the new samples are labeled as \"normal\" or \"anomaly.\" Additionally, to address the issue of noise in the data, instead of labeling samples individually, they are clustered first, and then each cluster is labeled collectively. For this, we present a new approach, called Collective Probabilistic Anomaly Detection (CPAD), in which, the distance of the incoming new samples and the existing SGMMs is calculated, and then the new cluster is labeled the same as of the closest cluster. To measure the distance of two GMM-based clusters, we propose a modified version of the Kullback-Libner measure. We run several experiments to evaluate the performances of the proposed SGMM and CPAD methods and compare them against some of the well-known algorithms including ABACUS, local outlier factor (LOF), and one-class support vector machine (SVM). The performance of SGMM is compared with ABACUS using Dunn and DB metrics, and the results indicate that the SGMM performs superior in terms of summarizing clusters. Moreover, the proposed CPAD method is compared with the LOF and one-class SVM considering the performance criteria of (a) false alarm rate, (b) detection rate, and (c) memory efficiency. The experimental results show that the CPAD method is noise resilient, memory efficient, and its accuracy is higher than the other methods.", "journal": "PATTERN ANALYSIS AND APPLICATIONS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000443313300005", "keywords": "Random forest; Kriging; Predictive modeling; R statistical computing; Sampling; Spatiotemporal data; Spatial data; Geostatistics; Pedometrics", "title": "Random forest as a generic framework for predictive modeling of spatial and spatio-temporal variables", "abstract": "Random forest and similar Machine Learning techniques are already used to generate spatial predictions, but spatial location of points (geography) is often ignored in the modeling process. Spatial auto-correlation, especially if still existent in the cross-validation residuals, indicates that the predictions are maybe biased, and this is suboptimal. This paper presents a random forest for spatial predictions framework (RFsp) where buffer distances from observation points are used as explanatory variables, thus incorporating geographical proximity effects into the prediction process. The RFsp framework is illustrated with examples that use textbook datasets and apply spatial and spatio-temporal prediction to numeric, binary, categorical, multivariate and spatiotemporal variables. Performance of the RFsp framework is compared with the state-of-the-art kriging techniques using fivefold cross-validation with refitting. The results show that RFsp can obtain equally accurate and unbiased predictions as different versions of kriging. Advantages of using RFsp over kriging are that it needs no rigid statistical assumptions about the distribution and stationarity of the target variable, it is more flexible towards incorporating, combining and extending covariates of different types, and it possibly yields more informative maps characterizing the prediction error. RFsp appears to be especially attractive for building multivariate spatial prediction models that can be used as \"knowledge engines'' in various geoscience fields. Some disadvantages of RFsp are the exponentially growing computational intensity with increase of calibration data and covariates and the high sensitivity of predictions to input data quality. The key to the success of the RFsp framework might be the training data quality-especially quality of spatial sampling (to minimize extrapolation problems and any type of bias in data), and quality of model validation (to ensure that accuracy is not effected by overfitting). For many data sets, especially those with lower number of points and covariates and close-to-linear relationships, model-based geostatistics can still lead to more accurate predictions than RFsp.", "journal": "PEERJ", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000465015500009", "keywords": "iPSC; IOPD; deep machine learning; recurrent neural networks (RNN); cognitive maps (CM); support vector machines (SVM)", "title": "Artificial intelligence and the analysis of multiplatform metabolomics data for the detection of intrauterine growth restriction", "abstract": "Objective To interrogate the pathogenesis of intrauterine growth restriction (IUGR) and apply Artificial Intelligence (AI) techniques to multi-platform i.e. nuclear magnetic resonance (NMR) spectroscopy and mass spectrometry (MS) based metabolomic analysis for the prediction of IUGR. Materials and methods MS and NMR based metabolomic analysis were performed on cord blood serum from 40 IUGR (birth weight < 10th percentile) cases and 40 controls. Three variable selection algorithms namely: Correlation-based feature selection (CFS), Partial least squares regression (PLS) and Learning Vector Quantization (LVQ) were tested for their diagnostic performance. For each selected set of metabolites and the panel consists of metabolites common in three selection algorithms so-called overlapping set (OL), support vector machine (SVM) models were developed for which parameter selection was performed busing 10-fold cross validations. Area under the receiver operating characteristics curve (AUC), sensitivity and specificity values were calculated for IUGR diagnosis. Metabolite set enrichment analysis (MSEA) was performed to identify which metabolic pathways were perturbed as a direct result of IUGR in cord blood serum. Results All selected metabolites and their overlapping set achieved statistically significant accuracies in the range of 0.78-0.82 for their optimized SVM models. The model utilizing all metabolites in the dataset had an AUC = 0.91 with a sensitivity of 0.83 and specificity equal to 0.80. CFS and OL (Creatinine, C2, C4, lysoPC. a. C16.1, lysoPC. a. C20.3, lysoPC. a. C28.1, PC.aa.C24.0) showed the highest performance with sensitivity (0.87) and specificity (0.87), respectively. MSEA revealed significantly altered metabolic pathways in IUGR cases. Dysregulated pathways include: beta oxidation of very long fatty acids, oxidation of branched chain fatty acids, phospholipid biosynthesis, lysine degradation, urea cycle and fatty acid metabolism. Conclusion A systematically selected panel of metabolites was shown to accurately detect IUGR in newborn cord blood serum. Significant disturbance of hepatic function and energy generating pathways were found in IUGR cases.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000498443000001", "keywords": "LULC; Urbanization; Big data; Nile Delta; Random forests; Sustainable development", "title": "Spatiotemporal dynamics of urbanization and cropland in the Nile Delta of Egypt using machine learning and satellite big data: implications for sustainable development", "abstract": "The Nile Delta of Egypt is increasingly facing sustainability threats, due to a combination of nature- and human-induced changes in land cover and land use. In this paper, an analysis of big time series data from remotely sensed satellite images and the random forests classifier was undertaken to assess the spatial and temporal dynamics of urbanization and cropland in the Nile Delta between 2007 and 2017. Out of thirteen variables, five spectral indices were chosen to build 500 decision trees, with a resulting overall accuracy average of 91.9 +/- 1.5%. The results revealed that the urban extent in the Nile Delta has increased, between 2007 and 2017, by 592.4 km(2) (1.92%). Particularly, the results indicated that the years 2011 and 2012, which coincided the 2011 political uprising in Egypt, so-called the Arab Spring, were associated with significant land-use changes in the Nile Delta, both in rate and scale. As a result, the cropland area in the region decreased between 2010 and 2011 by 1.63% (502.21 km(2)). Moreover, the results showed that during the period 2012-2017, the mean annual urbanization rate in the region stood at 60 km(2)/year. In contrast, croplands decreased during the same period at an average annual rate of 2 km(2)/year. At the governorates' level, the results suggested that top agricultural producing governorates in the Nile Delta, such as Elmonoufia, Elkalubia, Elbouhyra, and Elghrbia, witnessed the highest rates of decrease in cropland areas during the period 2012-2017. Over the same period, urban areas increased the most in Elkalubia, Domiate, and Elmonoufia by 1.98%, 1.72%, and 1.34%, respectively. The f indings from this analysis are discussed along with their implications for sustainable land-use and urban planning policies.", "journal": "ENVIRONMENTAL MONITORING AND ASSESSMENT", "category": "Environmental Sciences", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000483420700046", "keywords": "Hyperspectral remote sensing; Heavy metal in soil; Transfer learning; Indoor and outdoor spectral sampling", "title": "Soil Heavy Metal Qualitative Classification Model Based on Hyperspectral Measurements and Transfer Learning", "abstract": "The current qualitative classification models of soil heavy metal content based hyperspectral remote sensing technology mostly use indoor measured spectral data from the same area for model training and testing. However, the indoor spectrum measurement requires a complicated processing process with high cost and low efficiency, and thus cannot obtain the spatially continuous spectral information in the target area quickly. Moreover, whether this kind of model can be transferred to the outdoor measured spectral data in different test areas is still unclear. In order to answer this question, two lead-zinc mining areas in Chenzhou City and Hengyang City of Hunan Province were selected as research areas. Support Vector Machine was used as classifier. Then 83 sample data from indoor sampling in Zhangzhou experimental area and 46 sample data from indoor sampling in Hengyang experimental area were used for classifier training, and 46 sample data from field sampling in Hengyang area were used for classification testing. The difference of spectral distribution between the indoor and outdoor measured spectral data was reduced by the transfer learning method based on joint distribution adaptation (JDA), and then the domain adaption model for two research areas was constructed. The experimental results show that: (1) The spectral data measured by outdoor samples may be affected by factors such as solar radiation and differences in extracted soil components, leading to the significantly spectral difference for indoor and outdoor samples. As a result, it is difficult to directly transfer the qualitative classification model of soil heavy metal pollution trained by indoor samples to the outdoor samples from the same area. However, after the reduction of indoor and outdoor distribution differences by JDA transformation, the transfer ability of the model has been significantly improved, and the classification accuracy of three heavy metals As, Pb and Zn has reached over 84%. The accuracy of classification of Zn elements exceeding the standard even reached 89%. (2) Due to seasonal influences, regional component interference, and spectral noise, there are even more significant differences in the distribution of spectral data in different areas. This further increases the difficulty of soil heavy metal pollution monitoring in different areas, and it is difficult to directly transfer the qualitative classification model of soil heavy metals based on indoor sampling spectral data to field sampling data in other areas (with an average classification accuracy of about 50%). After the indoor and outdoor spectral transformation processing by JDA, the transfer ability of the model has been greatly improved. Therefore, the outdoor spectral sampled can be directly used to investigate the pollution situation of heavy metals (As, Pb and Zn) in different test areas.", "journal": "SPECTROSCOPY AND SPECTRAL ANALYSIS", "category": "Spectroscopy", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000384865700028", "keywords": "Psoriasis; Coexpression; WGCNA; System biology; Random forest; Classifier; Biomarker", "title": "Weighted gene co-expression based biomarker discovery for psoriasis detection", "abstract": "Psoriasis is a chronic inflammatory disease of the skin with an unknown aetiology. The disease manifests itself as red and silvery scaly plaques distributed over the scalp, lower back and extensor aspects of the limbs. After receiving scant consideration for quite a few years, psoriasis has now become a prominent focus for new drug development. A group of closely connected and differentially co-expressed genes may act in a network and may serve as molecular signatures for an underlying phenotype. A weighted gene coexpression network analysis (WGCNA), a system biology approach has been utilized for identification of new molecular targets for psoriasis. Gene coexpression relationships were investigated in 58 psoriatic lesional samples resulting in five gene modules, clustered based on the gene coexpression patterns. The coexpression pattern was validated using three psoriatic datasets. 10 highly connected and informative genes from each module was selected and termed as psoriasis specific hub signatures. A random forest based binary classifier built using the expression profiles of signature genes robustly distinguished psoriatic samples from the normal samples in the validation set with an accuracy of 0.95 to 1. These signature genes may serve as potential candidates for biomarker discovery leading to new therapeutic targets. WGCNA, the network based approach has provided an alternative path to mine out key controllers and drivers of psoriasis. The study principle from the current work can be extended to other pathological conditions. (C) 2016 Elsevier B.V. All rights reserved.", "journal": "GENE", "category": "Genetics & Heredity", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000366791000004", "keywords": "Risk stratification; Probabilistic topic model; Latent Dirichlet Allocation; Electronic health record; Patient sub-profile; Joint patient sub-profile-risk modeling", "title": "A probabilistic topic model for clinical risk stratification from electronic health records", "abstract": "Background and objective: Risk stratification aims to provide physicians with the accurate assessment of a patient's clinical risk such that an individualized prevention or management strategy can be developed and delivered. Existing risk stratification techniques mainly focus on predicting the overall risk of an individual patient in a supervised manner, and, at the cohort level, often offer little insight beyond a flat score-based segmentation from the labeled clinical dataset. To this end, in this paper, we propose a new approach for risk stratification by exploring a large volume of electronic health records (EHRs) in an unsupervised fashion. Methods: Along this line, this paper proposes a novel probabilistic topic modeling framework called probabilistic risk stratification model (PRSM) based on Latent Dirichlet Allocation (LDA). The proposed PRSM recognizes a patient clinical state as a probabilistic combination of latent sub-profiles, and generates sub-profile-specific risk tiers of patients from their EHRs in a fully unsupervised fashion. The achieved stratification results can be easily recognized as high-, medium- and low-risk, respectively. In addition, we present an extension of PRSM, called weakly supervised PRSM (WS-PRSM) by incorporating minimum prior information into the model, in order to improve the risk stratification accuracy, and to make our models highly portable to risk stratification tasks of various diseases. Results: We verify the effectiveness of the proposed approach on a clinical dataset containing 3463 coronary heart disease (CHD) patient instances. Both PRSM and WS-PRSM were compared with two established supervised risk stratification algorithms, i.e., logistic regression and support vector machine, and showed the effectiveness of our models in risk stratification of CHD in terms of the Area Under the receiver operating characteristic Curve (AUC) analysis. As well, in comparison with PRSM, WS-PRSM has over 2% performance gain, on the experimental dataset, demonstrating that incorporating risk scoring knowledge as prior information can improve the performance in risk stratification. Conclusions: Experimental results reveal that our models achieve competitive performance in risk stratification in comparison with existing supervised approaches. In addition, the unsupervised nature of our models makes them highly portable to the risk stratification tasks of various diseases. Moreover, patient sub-profiles and sub-profile-specific risk tiers generated by our models are coherent and informative, and provide significant potential to be explored for the further tasks, such as patient cohort analysis. We hypothesize that the proposed framework can readily meet the demand for risk stratification from a large volume of EHRs in an open-ended fashion. (C) 2015 Elsevier Inc. All rights reserved.", "journal": "JOURNAL OF BIOMEDICAL INFORMATICS", "category": "Computer Science, Interdisciplinary Applications; Medical Informatics", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000369814600006", "keywords": "Performance monitoring; Condition monitoring; Machine learning; Data mining; Wind energy", "title": "Machine learning and data mining in complex genomic data-a review on the lessons learned in Genetic Analysis Workshop 19", "abstract": "In the analysis of current genomic data, application of machine learning and data mining techniques has become more attractive given the rising complexity of the projects. As part of the Genetic Analysis Workshop 19, approaches from this domain were explored, mostly motivated from two starting points. First, assuming an underlying structure in the genomic data, data mining might identify this and thus improve downstream association analyses. Second, computational methods for machine learning need to be developed further to efficiently deal with the current wealth of data. In the course of discussing results and experiences from the machine learning and data mining approaches, six common messages were extracted. These depict the current state of these approaches in the application to complex genomic data. Although some challenges remain for future studies, important forward steps were taken in the integration of different data types and the evaluation of the evidence. Mining the data for underlying genetic or phenotypic structure and using this information in subsequent analyses proved to be extremely helpful and is likely to become of even greater use with more complex data sets.", "journal": "BMC GENETICS", "category": "Genetics & Heredity", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000481688500142", "keywords": "Diabetes mellitus prediction; machine learning; adaptive synthetic sampling", "title": "DMP_MI: An Effective Diabetes Mellitus Classification Algorithm on Imbalanced Data With Missing Values", "abstract": "As a widely known chronic disease, diabetes mellitus is called a silent killer. It makes the body produce less insulin and causes increased blood sugar, which leads to many complications and affects the normal functioning of various organs, such as eyes, kidneys, and nerves. Although diabetes has attracted high attention in research, due to the existence of missing values and class imbalance in the data, the overall performance of diabetes classification using machine learning is relatively low. In this paper, we propose an effective Prediction algorithm for Diabetes Mellitus classification on Imbalanced data with Missing values (DMP_MI). First, the missing values are compensated by the Naive Bayes (NB) method for data normalization. Then, an adaptive synthetic sampling method (ADASYN) is adopted to reduce the influence of class imbalance on the prediction performance. Finally, a random forest (RF) classifier is used to generate predictions and evaluated using comprehensive set of evaluation indicators. Experiments performed on Pima Indians diabetes dataset from the University of California at Irvine, Irvine (UCI) Repository, have demonstrated the effectiveness and superiority of our proposed DMP_MI.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000509628700001", "keywords": "Radiomics; Magnetic resonance imaging; Extrahepatic cholangiocarcinoma; Differentiation degree; Lymph node metastases", "title": "Radiomics model of magnetic resonance imaging for predicting pathological grading and lymph node metastases of extrahepatic cholangiocarcinoma", "abstract": "The aim of this study was to evaluate diagnostic performance of radiomics models of MRI in the detection of differentiation degree (DD) and lymph node metastases (LNM) of extrahepatic cholangiocarcinoma (ECC). We retrospectively enrolled 100 patients with ECC confirmed by pathology from January 2011 to December 2018. Three hundred radiomics features were extracted from each region of interest using MaZda software. Next, the radiomics model was developed by incorporating the optimal radiomics signatures and ADC values of tumors to predict DD (model A) and LNM (model B) of ECC, respectively, through the random forest algorithm. After which, the performance of the radiomics models were further evaluated. The model A showed better performance in both training and testing cohorts to discriminate high and medium-low differentiation groups of ECC, with an average AUC of 0.78 and 0.80, respectively. The model B also yielded the good average AUC of 0.80 and 0.90 to predict the LNM of ECC in training and testing cohorts. The radiomics models based on MRI performed well in predicting DD and LNM of ECC and have significant potential in clinical noninvasive diagnosis and in the prediction of ECC.", "journal": "CANCER LETTERS", "category": "Oncology", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000502573100025", "keywords": "Microarray data analysis; feature selection; cancer; cross-laboratory experiment", "title": "Machine Learning Enables Accurate Prediction of Asparagine Deamidation Probability and Rate", "abstract": "The spontaneous conversion of asparagine residues to aspartic acid or iso-aspartic acid, via deamidation, is a major pathway of protein degradation and is often seriously disruptive to biological systems. Deamidation has been shown to negatively affect both in vitro stability and in vivo biological function of diverse classes of proteins. During protein therapeutics development, deamidation liabilities that are overlooked necessitate expensive and time-consuming remediation strategies, sometimes leading to termination of the project. In this paper, we apply machine learning to a large (n = 776) liquid chromatographytandem mass spectrometry (LC-MS/MS) dataset of monoclonal antibody peptides to create computational models for the post-translational modification asparagine deamidation, using the random decision forest method. We show that our categorical model predicts antibody deamidation with nearly 5% increased accuracy and 0.2 MCC over the best currently available models. Surprisingly, our model also paces or outperforms advanced and conventional models on an independent non-antibody dataset. In addition to deamidation probability, we are able to accurately predict deamidation rate (R-2 = 0.963 and Q(2) = 0.822), a capability with no peer in current models. This method should enable significant improvement in protein candidate selection, especially in biopharmaceutical development, and can be applied with similar accuracy to enzymes, monoclonal antibodies, next-generation formats, vaccine component antigens, and gene therapy vectors such as adeno-associated virus.", "journal": "MOLECULAR THERAPY-METHODS & CLINICAL DEVELOPMENT", "category": "Medicine, Research & Experimental", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000413880000005", "keywords": "Classification; Terrestrial laser scanning; Radius search; Radiometric feature; Geometric feature", "title": "Foliar and woody materials discriminated using terrestrial LiDAR in a mixed natural forest", "abstract": "Separation of foliar and woody materials using remotely sensed data is crucial for the accurate estimation of leaf area index (LAI) and woody biomass across forest stands. In this paper, we present a new method to accurately separate foliar and woody materials using terrestrial LiDAR point clouds obtained from ten test sites in a mixed forest in Bavarian Forest National Park, Germany. Firstly, we applied and compared an adaptive radius near neighbor search algorithm with a fixed radius near-neighbor search method in order to obtain both radiometric and geometric features derived from terrestrial LiDAR point clouds. Secondly, we used a random forest machine learning algorithm to classify foliar and woody materials and examined the impact of understory and slope on the classification accuracy. An average overall accuracy of 84.4% (Kappa = 0.75) was achieved across all experimental plots. The adaptive radius near-neighbor search method outperformed the fixed radius near-neighbor search method. The classification accuracy was significantly higher when the combination of both radiometric and geometric features was utilized. The analysis showed that increasing slope and understory coverage had a significant negative effect on the overall classification accuracy. Our results suggest that the utilization of the adaptive radius near-neighbor search method coupling both radiometric and geometric features has the potential to accurately discriminate foliar and woody materials from terrestrial LiDAR data in a mixed natural forest.", "journal": "INTERNATIONAL JOURNAL OF APPLIED EARTH OBSERVATION AND GEOINFORMATION", "category": "Remote Sensing", "annotated_keywords": ["machine learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000435410200002", "keywords": "Female genital mutilation; FGM; Female genital cutting; Posttraumatic stress disorder (PTSD); Dissociation; Depression; Hair cortisol", "title": "Psychopathological sequelae of female genital mutilation and their neuroendocrinological associations", "abstract": "Background: Anecdotal evidence suggests the frequently traumatic nature of female genital mutilation (FGM). At present, systematic research on the psychological sequelae of this tradition has remained limited. The study provides preliminary, high-quality psychodiagnostic data on potential psychopathological consequences of FGM, with a focus on posttraumatic stress disorder (PTSD), shutdown dissociation and other stress-related variables. Methods: We investigated a convenience sample of N = 167 women, supported by the women's affairs headquarters in Jijiga (capital of the Ethiopian Somali Region) and a local Ethiopian non-governmental organization. Our main outcome measures were PTSD (PSS-I) and shutdown dissociation (ShuD). We also assessed depression and anxiety (HSCL-25), major depression, substance abuse and dependence, suicidality and psychotic disorders (M.I.N.I.; sub-scales A., B., K., and L.). In addition, we collected hair samples to assess hair cortisol concentrations (HCC) as a neuroendocrinological measure. Results: The majority of women endured FGM (FGM I: 36%, FGM II/III: 52%) and, regardless of the level of the physical invasiveness, almost all women reported having felt intense fear and/or helplessness. FGM II/III, the more invasive form, was associated with a greater vulnerability to PTSD symptoms (p < .001) and shutdown dissociation (p < .001). Symptoms of depression (p < .05) and anxiety (p < .01) were also elevated. Random forest regression with conditional inference trees revealed evidence of an alteration of the cortisol levels in relation to the age when FGM was experienced (< 1 year) and the invasiveness of the procedure. Conclusion: More extensive forms of FGM are associated with more severe psychopathological symptoms-particularly with an increased vulnerability to PTSD. Higher hair cortisol levels in women who experienced FGM before their first year of age or had more severe forms of FGM indicate long-term neuroendocrinological consequences of FGM and trauma in general on the stress system.", "journal": "BMC PSYCHIATRY", "category": "Psychiatry", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000458770300001", "keywords": "powdery mildew; cucumber leaf; convolutional neural network; image segmentation; deep-learning", "title": "Deep Learning-Based Segmentation and Quantification of Cucumber Powdery Mildew Using Convolutional Neural Network", "abstract": "Powdery mildew is a common disease in plants, and it is also one of the main diseases in the middle and final stages of cucumber (Cucumis sativus). Powdery mildew on plant leaves affects the photosynthesis, which may reduce the plant yield. Therefore, it is of great significance to automatically identify powdery mildew. Currently, most image-based models commonly regard the powdery mildew identification problem as a dichotomy case, yielding a true or false classification assertion. However, quantitative assessment of disease resistance traits plays an important role in the screening of breeders for plant varieties. Therefore, there is an urgent need to exploit the extent to which leaves are infected which can be obtained by the area of diseases regions. In order to tackle these challenges, we propose a semantic segmentation model based on convolutional neural networks (CNN) to segment the powdery mildew on cucumber leaf images at pixel level, achieving an average pixel accuracy of 96.08%, intersection over union of 72.11% and Dice accuracy of 83.45% on twenty test samples. This outperforms the existing segmentation methods, K-means, Random forest, and GBDT methods. In conclusion, the proposed model is capable of segmenting the powdery mildew on cucumber leaves at pixel level, which makes a valuable tool for cucumber breeders to assess the severity of powdery mildew.", "journal": "FRONTIERS IN PLANT SCIENCE", "category": "Plant Sciences", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000381594000023", "keywords": "Natural gas network planning; Layout optimization; Nonlinear fitting; Risk cost function; Minimum spanning tree", "title": "Layout optimization of natural gas network planning: Synchronizing minimum risk loss with total cost", "abstract": "The synchronization of the minimum risk loss and total cost of natural gas pipeline networks at the planning stage is discussed in this article. Herein, new procedures for optimizing layout are proposed to minimize the investment cost, operation expense, and the risk loss of the pipeline network. The procedures include two crucial steps: the first step is fitting two risk cost functions (i.e., leakage risk cost function and corrosion risk cost function), and the second one is achieving the optimal layout by using the risk cost functions as the edge weight of the minimum spanning tree algorithm. The suggested method is applied in three different real cases, leading to three distinct optimal layouts, which are more suitable than that calculated using intelligent algorithms for practical engineering. Then, two optimal strategies for pipeline fretwork layouts are presented. Different applications that respectively focus on the leakage risk cost for urban areas and the corrosion risk cost or leakage risk cost for suburban areas are shown in the above two strategies. These strategies realize a 6.9-21% greater economic benefit than that of the shortest layout. (C) 2016 Elsevier B.V. All rights reserved.", "journal": "JOURNAL OF NATURAL GAS SCIENCE AND ENGINEERING", "category": "Energy & Fuels; Engineering, Chemical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000486010800033", "keywords": "Subarachnoid hemorrhage; Delayed cerebral ischemia; Matricellular protein; Machine learning; Prediction", "title": "Machine Learning Analysis of Matricellular Proteins and Clinical Variables for Early Prediction of Delayed Cerebral Ischemia After Aneurysmal Subarachnoid Hemorrhage", "abstract": "Although delayed cerebral ischemia (DCI) is a well-known complication after subarachnoid hemorrhage (SAH), there are no reliable biomarkers to predict DCI development. Matricellular proteins (MCPs) have been reported relevant to DCI and expected to become biomarkers. As machine learning (ML) enables the classification of various input data and the result prediction, the aim of this study was to construct early prediction models of DCI development with clinical variables and MCPs using ML analyses. Early-stage clinical data of 95 SAH patients in a prospective cohort were analyzed and applied to a ML algorithm, random forest, to construct three prediction models: (1) a model with only clinical variables on admission, (2) a model with only plasma levels of MCP (periostin, osteopontin, and galectin-3) at post-onset days 1-3, and (3) a model with both clinical variables on admission and MCP values at days 1-3. The prediction accuracy of the development of DCI, angiographic vasospasm, or cerebral infarction and the importance of each feature were computed. The prediction accuracy of DCI development was 93.9% in model 1, 87.2% in model 2, and 95.1% in model 3, but that of angiographic vasospasm or cerebral infarction was lower. The three most important features in model 3 for DCI were periostin, osteopontin, and galectin-3, followed by aneurysm location. All of the early-stage prediction models of DCI development constructed by ML worked with high accuracy and sensitivity. One-time early-stage measurement of plasma MCPs served for reliable prediction of DCI development, suggesting their potential utility as biomarkers.", "journal": "MOLECULAR NEUROBIOLOGY", "category": "Neurosciences", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000454251200015", "keywords": "Distributed power generation; Evolutionary computation; Genetic programming; Intelligent systems; Islanding; Machine learning; Power distribution; Power system protection", "title": "Islanding detection of distributed generation by using multi-gene genetic programming based classifier", "abstract": "This paper proposed a new method for detecting islanding of distributed generation (DG), using Multigene Genetic Programming (MGP). Islanding has been a serious concern among power distribution utilities and distributed generation owners, because it poses risks to the safety of utilities' workers and consumers, and can cause damage to power distribution systems' equipment. Therefore, a DG must be disconnected as soon as an islanding is detected. In addition, an islanding detection method must have high degree of dependability to correctly discriminate islanding from other events, such as load switching, in order to avoid unnecessary disconnection of the distributed generator. In this context, the novelty of the proposed method is that the MGP is capable of obtaining a set of mathematical and logic functions employed to detect and classify islanding correctly. This is a new approach among the computational intelligent methods proposed for DG islanding detection. The main idea was to use local voltage measurements as input of the method, eliminating the need of complex and expensive communication infrastructure. The method has been trained with several islanding and non-islanding cases, by using a power distribution system comprising five concentrated loads, a synchronous distributed generator and a wind power plant. The results showed that the proposed method was successful in differentiating the islanding events from other disturbances, revealing its great potential to be applied in anti-islanding protection schemes for distributed generation. (C) 2018 Elsevier B.V. All rights reserved.", "journal": "APPLIED SOFT COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000460468400010", "keywords": "functional reassembly; metagenomics; microbial resilience; plant invasion; response strategy; soil contamination; taxonomic levels", "title": "Resilience and Assemblage of Soil Microbiome in Response to Chemical Contamination Combined with Plant Growth", "abstract": "A lack of knowledge of the microbial responses to environmental change at the species and functional levels hinders our ability to understand the intrinsic mechanisms underlying the maintenance of microbial ecosystems. Here, we present results from temporal microcosms that introduced inorganic and organic contaminants into agro-soils for 90days, with three common legume plants. Temporal dynamics and assemblage of soil microbial communities and functions in response to contamination under the influence of growth of different plants were explored via sequencing of the 16S rRNA amplicon and by shotgun metagenomics. Soil microbial alpha diversity and structure at the taxonomic and functional levels exhibited resilience patterns. Functional profiles showed greater resilience than did taxonomic ones. Different legume plants imposed stronger selection on taxonomic profiles than on functional ones. Network and random forest analyses revealed that the functional potential of soil microbial communities was fostered by various taxonomic groups. Betaproteobacteria were important predictors of key functional traits such as amino acid metabolism, nucleic acid metabolism, and hydrocarbon degradation. Our study reveals the strong resilience of the soil microbiome to chemical contamination and sensitive responses of taxonomic rather than functional profiles to selection processes induced by different legume plants. This is pivotal to develop approaches and policies for the protection of soil microbial diversity and functions in agro-ecosystems with different response strategies from global environmental drivers, such as soil contamination and plant invasion. IMPORTANCE Exploring the microbial responses to environmental disturbances is a central issue in microbial ecology. Understanding the dynamic responses of soil microbial communities to chemical contamination and the microbe-soil-plant interactions is essential for forecasting the long-term changes in soil ecosystems. Nevertheless, few studies have applied multi-omits approaches to assess the microbial responses to soil contamination and the microbe-soil-plant interactions at the taxonomic and functional levels simultaneously. Our study reveals clear succession and resilience patterns of soil microbial diversity and structure in response to chemical contamination. Different legume plants exerted stronger selection processes on taxonomic than on functional profiles in contaminated soils, which could benefit plant growth and fitness as well as foster the potential abilities of hydrocarbon degradation and metal tolerance. These results provide new insight into the resilience and assemblage of soil microbiome in response to environmental disturbances in agro-ecosystems at the species and functional levels.", "journal": "APPLIED AND ENVIRONMENTAL MICROBIOLOGY", "category": "Biotechnology & Applied Microbiology; Microbiology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000382935000012", "keywords": "Bed load; Extreme learning machines (ELM); Limit of deposition; Sediment transport; Storm water", "title": "Extreme learning machine assessment for estimating sediment transport in open channels", "abstract": "The minimum velocity required to prevent sediment deposition in open channels is examined in this study. The parameters affecting transport are first determined and then categorized into different dimensionless groups, including \"movement,\" \"transport,\" \"sediment,\" \"transport mode,\" and \"flow resistance.\" Six different models are presented to identify the effect of each of these parameters. The feed-forward neural network (FFNN) is used to predict the densimetric Froude number (Fr) and the extreme learning machine (ELM) algorithm is utilized to train it. The results of this algorithm are compared with back propagation (BP), genetic programming (GP) and existing sediment transport equations. The results indicate that FFNN-ELM produced better results than FNN-BP, GP and existing sediment transport methods in both training (RMSE = 0.26 and MARE = 0.052) and testing (RMSE = 0.121 and MARE = 0.023). Moreover, the performance of FFNN-ELM is examined for different pipe diameters.", "journal": "ENGINEERING WITH COMPUTERS", "category": "Computer Science, Interdisciplinary Applications; Engineering, Mechanical", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000404497300010", "keywords": "Feature selection; Intrinsic dimension; Morisita index; Measure of relevance; Data mining", "title": "Feature selection for regression problems based on the Morisita estimator of intrinsic dimension", "abstract": "Data acquisition, storage and management have been improved, while the key factors of many phenomena are not well known. Consequently, irrelevant and redundant features artificially increase the size of datasets, which complicates learning tasks, such as regression. To address this problem, feature selection methods have been proposed. This paper introduces a new supervised filter based on the Morisita estimator of intrinsic dimension. It can identify relevant features and distinguish between redundant and irrelevant information. Besides, it offers a clear graphical representation of the results, and it can be easily implemented in different programming languages. Comprehensive numerical experiments are conducted using simulated datasets characterized by different levels of complexity, sample size and noise. The suggested algorithm is also successfully tested on a selection of real world applications and compared with RReliefF using extreme learning machine. In addition, a new measure of feature relevance is presented and discussed. (C) 2017 Elsevier Ltd. All rights reserved.", "journal": "PATTERN RECOGNITION", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000441387800009", "keywords": "Location-based social networks; Recommendation systems; Distributed ELM; Large-scale recommendation", "title": "A novel recommendation system in location-based social networks using distributed ELM", "abstract": "Location-based social networks (LBSNs) have become a popular platform for people to communicate with each other. The recommendation problem has attracted considerable attention in both academia and industry as increasingly more users share their experiences and feelings using LBSNs. Machine learning has been widely used in many recommendation systems for recommending new friends or places of interest (POIs) to users in LBSNs. However, the majority of the existing recommendation systems were single function and only used small-scale datasets to provide recommendation services. In the era of big data, recommendation systems should have the ability to fully utilize limited computing resources for mining potential relationships from large-scale LBSN data. In this paper, a novel generic recommendation system is proposed by utilizing a distributed extreme learning machine called GR-DELM, which considers both friend recommendation and POI recommendation in large-scale datasets. For POI recommendation, three features are extracted: (1) geography-influenced feature, (2) popularity-influenced feature, and (3) social-influenced feature. For friend recommendation, two features are extracted: (1) neighborhood-based feature and (2) path-based feature. These features further improve the efficiency and accuracy of large-scale recommendation. Finally, a series of experiments demonstrate that the GR-DELM system outperforms the existing recommendation systems.", "journal": "MEMETIC COMPUTING", "category": "Computer Science, Artificial Intelligence; Operations Research & Management Science", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000357312800040", "keywords": "Extreme learning machine; interval; uncertainty reduction; big data", "title": "Interval extreme learning machine for big data based on uncertainty reduction", "abstract": "Choosing representative samples and removing data redundancy are two key issues in large-scale data classification. This paper proposes a new model, named interval extreme learning machine (ELM), for big data classification with continuous-valued attributes. The interval ELM model is built up based on two techniques, i.e., discretization of conditional attributes and fuzzification of class labels. First, inspired by the traditional decision tree (DT) induction algorithm, each conditional attribute is discretized into a number of intervals based on uncertainty reduction scheme. Then, the center and range of each interval are calculated as the mean and standard deviation of the values in it. Afterwards, the samples in the same intervals with regard to all the conditional attributes are merged as one record, and a fuzzification process is performed on the class labels. As a result, the original data set is transferred into a smaller one with fuzzy classes, and the interval ELM model is developed. Experimental comparisons demonstrate the feasibility and effectiveness of the proposed approach.", "journal": "JOURNAL OF INTELLIGENT & FUZZY SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000540222200133", "keywords": "hidden dangers; coal mine; neural networks; gray model (GM); particle swarm optimization (PSO); extreme learning machine (ELM); prediction", "title": "Predicting Hidden Danger Quantity in Coal Mines Based on Gray Neural Network", "abstract": "The hidden danger is the direct cause of coal mine accidents, and the number of hidden dangers in a certain area not only reflects the current safety situation, but also determines the development trend of safety production in this area to a large extent. By analyzing the formation and development law of the hidden dangers and hidden danger accident-induced mechanism in coal mines, it is concluded that there are some objective laws in the process of occurrence, development, weakening, and even stabilization of hidden dangers in a certain area. The development of the number of hidden dangers for a coal mine generally presents the law of similar normal distribution curve, with a certain degree of partial symmetry. Many years of hidden danger elimination in coal mines will accumulate large-scale hidden danger data. In this paper, by using the average value of hidden danger quantity in consecutive months to weaken the oscillation of hidden danger quantity sequence, and combining with gray model (1,1) and the neural network of extreme learning machine, and employing big data of hidden dangers available, a hidden danger quantity prediction model based on the gray neural network was established, and the experimental analysis and verification carried out. The results show that the model can achieve good prediction effect on the number of hidden dangers in a coal mine, which not only reflects the complex gray system behavior of hidden dangers of a coal mine, but also can predict dynamically. The safety management efficiency and emergency capacity of the coal mine enterprise will be greatly improved.", "journal": "SYMMETRY-BASEL", "category": "Multidisciplinary Sciences", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000319930000003", "keywords": "Fuzzy rules interpolation; genetic algorithms; interval type-2 Gaussian fuzzy sets; sparse fuzzy rule-based systems", "title": "Fuzzy Rules Interpolation for Sparse Fuzzy Rule-Based Systems Based on Interval Type-2 Gaussian Fuzzy Sets and Genetic Algorithms", "abstract": "In this paper, we present a new method for fuzzy rules interpolation for sparse fuzzy rule-based systems based on interval type-2 Gaussian fuzzy sets and genetic algorithms. First, we present a method to deal with the interpolation of fuzzy rules based on interval type-2 Gaussian fuzzy sets. We also prove that the proposed method guarantees to produce normal interval type-2 Gaussian fuzzy sets. Then, we present a method to learn optimal interval type-2 Gaussian fuzzy sets for sparse fuzzy rule-based systems based on genetic algorithms. We also apply the proposed fuzzy rules interpolation method and the proposed learning method to deal with multivariate regression problems and time series prediction problems. The experimental results show that the proposed fuzzy rules interpolation method using the optimally learned interval type-2 Gaussian fuzzy sets gets higher average accuracy rates than the existing methods.", "journal": "IEEE TRANSACTIONS ON FUZZY SYSTEMS", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000367463400005", "keywords": "Metric learning; Similarity learning; Generalization bound; Rademacher complexity", "title": "Generalization bounds for metric and similarity learning", "abstract": "Recently, metric learning and similarity learning have attracted a large amount of interest. Many models and optimization algorithms have been proposed. However, there is relatively little work on the generalization analysis of such methods. In this paper, we derive novel generalization bounds of metric and similarity learning. In particular, we first show that the generalization analysis reduces to the estimation of the Rademacher average over \"sums-of-i.i.d.\" sample-blocks related to the specific matrix norm. Then, we derive generalization bounds for metric/similarity learning with different matrix-norm regularizers by estimating their specific Rademacher complexities. Our analysis indicates that sparse metric/similarity learning with -norm regularization could lead to significantly better bounds than those with Frobenius-norm regularization. Our novel generalization analysis develops and refines the techniques of U-statistics and Rademacher complexity analysis.", "journal": "MACHINE LEARNING", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000425619100004", "keywords": "Face alignment; Personalized modeling; Incremental learning; Ensemble learning; Sparse coding", "title": "Toward Personalized Modeling: Incremental and Ensemble Alignment for Sequential Faces in the Wild", "abstract": "Fitting facial landmarks on unconstrained videos is a challenging task with broad applications. Both generic and joint alignment methods have been proposed with varying degrees of success. However, many generic methods are heavily sensitive to initializations and usually rely on offline-trained static models, which limit their performance on sequential images with extensive variations. On the other hand, joint methods are restricted to offline applications, since they require all frames to conduct batch alignment. To address these limitations, we propose to exploit incremental learning for personalized ensemble alignment. We sample multiple initial shapes to achieve image congealing within one frame, which enables us to incrementally conduct ensemble alignment by group-sparse regularized rank minimization. At the same time, incremental subspace adaptation is performed to achieve personalized modeling in a unified framework. To alleviate the drifting issue, we leverage a very efficient fitting evaluation network to pick out well-aligned faces for robust incremental learning. Extensive experiments on both controlled and unconstrained datasets have validated our approach in different aspects and demonstrated its superior performance compared with state of the arts in terms of fitting accuracy and efficiency.", "journal": "INTERNATIONAL JOURNAL OF COMPUTER VISION", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000353221000011", "keywords": "system identification; principal component analysis (PCA); fuzzy logic; neural network; adaptive neuro-fuzzy inference system (ANFIS); earthquake; magnetorheological damper; smart structures", "title": "PCA-based neuro-fuzzy model for system identification of smart structures", "abstract": "This paper proposes an efficient system identification method for modeling nonlinear behavior of civil structures. This method is developed by integrating three different methodologies: principal component analysis (PCA), artificial neural networks, and fuzzy logic theory, hence named PANFIS (PCA-based adaptive neuro-fuzzy inference system). To evaluate this model, a 3-story building equipped with a magnetorheological (MR) damper subjected to a variety of earthquakes is investigated. To train the input-output function of the PANFIS model, an artificial earthquake is generated that contains a variety of characteristics of recorded earthquakes. The trained model is also validated using the 1940 El-Centro, Kobe, Northridge, and Hachinohe earthquakes. The adaptive neuro-fuzzy inference system (ANFIS) is used as a baseline. It is demonstrated from the training and validation processes that the proposed PANFIS model is effective in modeling complex behavior of the smart building. It is also shown that the proposed PANFIS produces similar performance with the benchmark ANFIS model with significant reduction of computational loads.", "journal": "SMART STRUCTURES AND SYSTEMS", "category": "Engineering, Civil; Engineering, Mechanical; Instruments & Instrumentation", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000366058000003", "keywords": "Manifold learning; dimensionality reduction", "title": "Piecewise-linear manifold learning: A heuristic approach to non-linear dimensionality reduction", "abstract": "The need to reduce the dimensionality of a dataset whilst retaining its inherent manifold structure is key to many pattern recognition, machine learning, and computer vision problems. This process is often referred to as manifold learning since the structure is preserved during dimensionality reduction by learning the intrinsic low-dimensional manifold that the data lies upon. In this paper a heuristic approach is presented to tackle this problem by approximating the manifold as a set of piecewise linear models. By merging these linear models in an order defined by their global topology a globally stable and locally accurate model of the manifold can be obtained. A detailed analysis of the proposed approach is presented along with comparison with existing manifold learning techniques. Results obtained on both artificial and image based data show that in many cases this heuristic approach to manifold learning is able to out-perform traditional techniques.", "journal": "INTELLIGENT DATA ANALYSIS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000437077400014", "keywords": "Neuro-fuzzy; Carbon dioxide emission intensity; Energy; Economic growth", "title": "Economic growth based in carbon dioxide emission intensity", "abstract": "Energy consumption could has great impact of economic development and on carbon dioxide emission (CO2) intensity. However there are many possibilities to avoid these undesirable problems. One of the solution is to use more alternative and renewable energy sources but CO2 emission intensity from the other sources need to be investigated more. Therefore in this article was analyzed the CO2 emission intensity based on the alternative, fossil and renewable energy. It was analyzed also the economic growth based on the CO2 emission intensity. Neuro-fuzzy method was applied to select the most impactful energy factor for the CO2 emission intensity and economic growth. Presented results confirmed that the alternative energy has the highest influence on the CO2 emission intensity. CO2 emissions intensity from solid fuel has the highest influence on the economic growth. (C) 2018 Elsevier B.V. All rights reserved.", "journal": "PHYSICA A-STATISTICAL MECHANICS AND ITS APPLICATIONS", "category": "Physics, Multidisciplinary", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000458908900002", "keywords": "electroencephalogram; epilepsy; genetic programming; hybrid crossover; hybrid mutation", "title": "Classification of electroencephalogram signal for the detection of epilepsy using Innovative Genetic Programming", "abstract": "Epilepsy, sometimes called seizure disorder, is a neurological condition that justifies itself as a susceptibility to seizures. A seizure is a sudden burst of rhythmic discharges of electrical activity in the brain that causes an alteration in behaviour, sensation, or consciousness. It is essential to have a method for automatic detection of seizures, as these seizures are arbitrary and unpredictable. A profound study of the electroencephalogram (EEG) recordings is required for the accurate detection of these epileptic seizures. In this study, an Innovative Genetic Programming framework is proposed for classification of EEG signals into seizure and nonseizure. An empirical mode decomposition technique is used for the feature extraction followed by genetic programming for the classification. Moreover, a method for intron deletion, hybrid crossover, and mutation operation is proposed, which are responsible for the increase in classification accuracy and a decrease in time complexity. This suggests that the Innovative Genetic Programming classifier has a potential for accurately predicting the seizures in an EEG signal and hints on the possibility of building a real-time seizure detection system.", "journal": "EXPERT SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Theory & Methods", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000469153200011", "keywords": "Online transfer learning; Multi-class classification; Knowledge transfer", "title": "OTLAMC: An Online Transfer Learning Algorithm for Multi-class Classification", "abstract": "Transfer learning handles a learning task on target domain by transferring the knowledge which was already learned from source domain, often under the situation where the labeled data in the target domain are insufficient or difficult to acquire. Transfer learning can be generally classified into two types, offline transfer learning and online transfer learning. As the former requires the data of target domain be given in advance, which may not be hold in real-life situations, the latter attracted more and more research in recent years. Online transfer learning deals with the situation where the data of target domain arrive in an online manner. Existing research on online transfer learning only deals with binary classification tasks, which appears to be important insufficiency. In this paper, the problem of online transfer learning for multi-class classification is studied, and an algorithm called Online Transfer Learning Algorithm for Multi-class Classification (OTLAMC) is proposed. OTLAMC learns a multi-class classifier in an online manner based on the knowledge from two sources, the obtained feedback of each datum in the target domain upon its arrival and the knowledge transferred from the source domain. A new loss function and a new updating method are proposed and adopted in OTLAMC, which improved the performance of OTLAMC on multi-class classification task. The mistake bound of OTLAMC is derived. Experiments on two widely used datasets illustrate that OTLAMC has good performance. (C) 2019 Published by Elsevier B.V.", "journal": "KNOWLEDGE-BASED SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000333451300006", "keywords": "Latent variable grammar; Syntax parsing; Graph-based label propagation", "title": "Lexicon expansion for latent variable grammars", "abstract": "This study investigates the use of unlabeled data, i.e., raw texts, to strengthen latent variable probabilistic context-free grammars, in particular lexical models. A graph-based lexicon expansion approach is proposed to achieve this goal. It aims to discover additional lexical knowledge from a large amount of unlabeled data to help the syntax parsing. The proposed approach is based on a transductive graph-based label propagation technique. The approach builds k-nearest-neighbor (k-NN) similarity graphs over the words of labeled and unlabeled data, for propagating lexical emission probabilities. The intuition is that different word under similar syntactic environment should have approximate lexical emission distributions. The derived words, together with lexical emission probabilities, are incorporated into the parsing. This approach is very effective in parsing out-of-vocabulary (OOV) words. Empirical results for English, Chinese, and Portuguese revealed its effectiveness. (C) 2014 Elsevier B.V. All rights reserved.", "journal": "PATTERN RECOGNITION LETTERS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000491625700008", "keywords": "Nanocomposite; MWCNT/TiO2@Ag/Fe; Experimental design; ANFIS; GRNN", "title": "A rapid and efficient sono-chemistry process for removal of pollutant: Statistical modeling study", "abstract": "In this research paper, Ag and Fe doped into TiO2 loaded on the Multi wall carbon nanotube (MWCNT/TiO2@Ag/Fe-NC) was prepared and characterized. The adsorption efficiency was modeled by ANFIS (Adaptive Neuro-Fuzzy Inference System), GRNN (Generalized Regression Neural Network) and RSM (response surface methodology). The effect of process factors i.e. sonication time, the concentration of Methylene Blue (MB), MWCNT/TiO2@Ag/Fe-NC mass and pH on the decolorization of MB was investigated by the RSM, GRRN, and ANFIS. The ability of all three models was examined by four statistical visualization such as R-2, RMSE (root mean square error), MAE (mean absolute error) and %AAD (absolute average deviation). The statistical visualization result for the validation dataset shows that the proposed approaches (i.e. ANFIS, GRNN and RSM) will be able to predicate and model the removal MB. Nevertheless, from obtained result it clear that the ANFIS approach has more precise in respect to the other models. Though, it was known that the Generalized Regression Neural Network is easier and take a little time for modeling than the Adaptive Neuro-Fuzzy Inference System approach. Therefore, the GRNN algorithm can be built a new prospect in predication and/or modeling and is also feasible that could be applied in actual systems. (C) 2019 Elsevier Ltd. All rights reserved.", "journal": "POLYHEDRON", "category": "Chemistry, Inorganic & Nuclear; Crystallography", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000510106700002", "keywords": "Big data; Hadoop; Crop yields; Agricultural; Satellite images; Artificial neural network (ANN); Population-based incremental learning (PBIL)", "title": "Efficient agricultural yield prediction using metaheuristic optimized artificial neural network using Hadoop framework", "abstract": "The low-resolution imagery of satellite is used extensively for monitoring crops and forecasting of yield which has a major role to play in the operational systems. A combination of high levels of temporal frequency along with an extended coverage was connected with lower costs per each area unit making the images a choice that is convenient at the national level and the regional level scales. There are various quantitative and qualitative approaches for low-resolution satellite imagery to be used for the primary predictor of the final yield of crops. But, very little work is done on the yield prediction that is based on environmental and satellite data. To handle such satellite images may be very challenging owing to large data amounts. Big data analysis is efficient in handling a large amount of data generated for predicting agricultural yield. In this work, a neural network is used for prediction and to enhance its performance; a population-based incremental learning technique is proposed for optimizing the weights. The results of the experiment proved that the method proposed has better results compared to that of the other methods.", "journal": "SOFT COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000446228500020", "keywords": "Mini-tablets; Quality by design (QbD); Particle swarm optimization ANNs; Genetic programming; Flow properties; DoE optimization", "title": "Comparison of multi-linear regression, particle swarm optimization artificial neural networks and genetic programming in the development of mini-tablets", "abstract": "In the present study, the preparation of pharmaceutical mini-tablets was attempted in the framework of Quality by Design (QbD) context, by comparing traditionally used multi-linear regression (MLR), with artificially-intelligence based regression techniques (such as standard artificial neural networks (ANNs), particle swarm optimization (PSO) ANNs and genetic programming (GP)) during Design of Experiment (DoE) implementation. Specifically, the effect of diluent type and particle size fraction for three commonly used direct compression diluents (lactose, pregelatinized starch and dibasic calcium phosphate dihydrate, DCPD) blended with either hydrophilic or hydrophobic flowing aids was evaluated in terms of: a) powder blend properties (such as bulk (Y1) and tapped (Y-2) density, Carr's compressibility index (Y-3, CCI), Kawakita's compaction fitting parameters a (Y-4) and 1/b (Y-5)), and b) mini-tablet's properties (such as relative density (Y-6), average weight (Y-7) and weight variation (Y-8)). Results showed better flowing properties for pregelatinized starch and improved packing properties for lactose and DPCD. MLR analysis showed high goodness of fit for the Y-1, Y-2, Y-4, Y-6 and Y-8 with RMSE values of Y-1 = 0.028, Y-2 = 0.032, Y-4 = 0.019, Y-6 = 0.015 and Y-8 = 0.130; while for rest responses, high correlation was observed from both standard ANNs and GP. PSO-ANNs fitting was the only regression technique that was able to adequately fit all responses simultaneously (RMSE values of Y-1 = 0.026, Y-2 = 0.022, Y-3 = 0.025, Y-4 = 0.010, Y-5 = 0.063, Y-6 = 0.013, Y-7 = 0.064 and Y-8 = 0.104).", "journal": "INTERNATIONAL JOURNAL OF PHARMACEUTICS", "category": "Pharmacology & Pharmacy", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000445125200006", "keywords": "Artificial neural network; GP; Modeling; Nanofiltration; Membrane; Purification", "title": "Modeling of CaCl2 removal by positively charged polysulfone-based nanofiltration membrane using artificial neural network and genetic programming", "abstract": "Artificial neural network (ANN) and genetic programming (GP) models were used to predict rejection (R) and permeability coefficient of water flux (L-p) with respect to CaCl2 in nanofiltration (NF) membrane process. The model inputs were concentration of the poly(ethylene imine) (PEI), p-xylene dichloride (XDC) and methyl iodide (MI), coating and crosslinking time of PEI, and pH of the solution. With this respect, ANN with 3:17:1 and 3:23:1 neurons, the lowest mean squad error (MSE) of 0.0023 and 0.000028 and the highest coefficient of determination (R-2) values of 0.9830 and 0.9990 for R and L-p, respectively, was found. In addition, the sensitivity analysis suggested that PEI coating time and pH had the significant effect on R and L-p, respectively. GP was used to make a mathematical function for prediction of R and L-p in terms of the input parameters. The GP model successfully described the R and L-p as function of input parameters. The GP results with R-2 values of more than 0.99 had an excellent preciseness.", "journal": "DESALINATION AND WATER TREATMENT", "category": "Engineering, Chemical; Water Resources", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000473644200004", "keywords": "Bearing; fault classification; fault detection; fault diagnosis; fuzzy entropy; fault prognosis; fuzzy clustering; fuzzy rules", "title": "A Systematic Review of Fuzzy Formalisms for Bearing Fault Diagnosis", "abstract": "Bearings are fundamental mechanical components in rotary machines (engines, gearboxes, generators, radars, turbines, etc.) that have been identified as one of the primary causes of failure in these machines. This makes bearing fault diagnosis (detection, classification, and prognosis) an economic very relevant topic, as well as a technically challenging one as evaluated by the extensive research literature on the subject. This paper employs a systematic methodology to identify, summarize, analyze, and interpret the primary literature on fuzzy formalisms for bearing fault diagnosis from 2000 to 2017 (March). The main contribution is an updated, unbiased, and (to a higher extend) repeatable search, review, and analysis (summary, classification, and critique) of the available approaches resorting to fuzzy formalisms in this trendy topic. A discussion on a new promising future research direction is provided. A comprehensive list of references is also included.", "journal": "IEEE TRANSACTIONS ON FUZZY SYSTEMS", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000354965300092", "keywords": "Lower extremity exoskeleton; humanoid walking; fuzzy-PID; PID control; fuzzy theory; co-simulation", "title": "Solubility prediction of supercritical carbon dioxide in 10 polymers using radial basis function artificial neural network based on chaotic self-adaptive particle swarm optimization and K-harmonic means", "abstract": "A novel model combined with chaos theory, self-adaptive particle swarm optimization (PSO) algorithm, K-harmonic means (KHM) clustering and radial basis function artificial neural network (RBF ANN) is proposed, hereafter called CSPSO-KHM RBF ANN. Traditional PSO algorithm is modified by chaos theory and self-adaptive inertia weight factor in order to reduce premature convergence problem. The modified PSO algorithm is employed to trim the RBF ANN connection weights and biases, whereas KHM is used to tune the hidden centers and spreads. The CSPSO-KHM RBF ANN model was employed to investigate the solubility of supercritical carbon dioxide in 10 polymers. Compared with other methods, such as RBF ANN, adaptive neuro-fuzzy inference system and PSO ANN, the proposed model displays optimal prediction performance. Results discover that the CSPSO-KHM RBF ANN model is an effective method for solubility prediction with high accuracy, and is a practicable method for chemical process analyzing and designing.", "journal": "RSC ADVANCES", "category": "Chemistry, Multidisciplinary", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000460710300033", "keywords": "Nonlinear process monitoring; Kernel; ELKPCA; Local structure analysis; Fault detection", "title": "Improved nonlinear process monitoring based on ensemble KPCA with local structure analysis", "abstract": "A new nonlinear process monitoring algorithm called ensemble local kernel principal component analysis (ELKPCA) is proposed. Conventionally, the performance of kernel-based model depends on the width parameter selected empirically in Gaussian kernel function, which means a single parameter corresponds to a single model. Once a poor width parameter is decided, a single kernel-based model may be only effective for part faults. As a typical kernel-based method, the local kernel principal component analysis (LKPCA), considering both global and local structure information of the original data, faces the problem of width parameter selection as well. Since a single model is one-sided, an available way is to combine different single models and take advantage of them. The ensemble kernel principal component analysis (EKPCA) uses single KPCA models as its sub-models and ensemble learning approach is used to combine them. Due to inherit drawbacks from KPCA models, EKPCA only preserves global structure information of data, but ignores important local structure information. In this paper, to solve the above issues, both LKPCA and EKPCA is unified in the proposed framework. First, single LKPCA models are chosen instead and combined by using ensemble learning strategy. Then two monitoring statistics are turned into fault probabilities through Bayesian inference approach and weighted combination strategy, which makes the monitoring behavior easier and more clear. The result shows that ELKPCA model can not only take advantage of sub-LKPCA models effectively, allowing it selecting width parameter more easily and stably, but also retain both global and local structure information from input data by introducing the local structure analysis in the EKPCA model. Case studies on synthetic example and Tennessee Eastman process demonstrate the proposed method outperforms LKPCA and EKPCA and enhances monitoring performance significantly. (C) 2019 Institution of Chemical Engineers. Published by Elsevier B.V. All rights reserved.", "journal": "CHEMICAL ENGINEERING RESEARCH & DESIGN", "category": "Engineering, Chemical", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000395952200006", "keywords": "Document embedding; Smooth affine map; Generative probabilistic model; Multi-agent random walk; Regularized auto-encoders", "title": "Discriminative locally document embedding: Learning a smooth affine map by approximation of the probabilistic generative structure of subspace", "abstract": "Document embedding is a technology that captures informative representations from high-dimensional observations by some structure-preserving maps over corpus and has been intensively explored in machine learning. Recently, some manifold-inspired embedding methods become a hot topic, mainly due to their ability in capturing discriminative embedding. However, the existing methods capture the embeddings based on the geometrical information of nearest neighbors without considering the intrinsic documents-generating structure on a subspace, thus leads to a limitation to uncover intrinsic semantic information. In this paper, we propose a semi-supervised local-invariant method, called Discriminative Locally Document Embedding (Disc-LDE), aiming to build a smooth affine map for document embedding by preserving documents-generating structure on a subspace. Disc-LDE models the documents-generating structure as a pseudo-document by a generative probabilistic model of subspace, where the subspace is acquired by a transductive learning of multi-agent random walk on neighborhood graph, and regularizes the training of Auto-Encoders (AEs) to jointly recover the input document and its pseudo-document. Under a general regularized function learning framework, the regularized training can impact the parameterized encoder network become smooth to variations along the documents-generating structure of the local field on manifold. The experimental results on three widely-used corpora demonstrate Disc-LDE could efficient capture the intrinsic semantic structure to improve the clustering and classification performance to the state-of-the-arts methods. (C) 2017 Elsevier B.V. All rights reserved.", "journal": "KNOWLEDGE-BASED SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000424988400005", "keywords": "Collaborative-competitive representation; feature extraction; hyperspectral imagery; manifold learning; signal processing on graph", "title": "Rational design of patchy colloids via landscape engineering", "abstract": "We present a new data-driven inverse design platform for self-assembling materials that we term \"land-scape engineering\". The essence of the approach is to sculpt the self-assembly free energy landscape to favor the formation of target aggregates by rational manipulation of building block properties. The approach integrates nonlinear manifold learning with hybrid Monte Carlo techniques to efficiently recover self-assembly landscapes, which we subsequently optimize using the covariance matrix adaptation evolutionary strategy (CMA-ES). We demonstrate the effectiveness of this technique in the design of anisotropic patchy colloids to form hollow polyhedral capsids. In the case of icosahedral capsids, our approach discovers a building block possessing a 76% improvement in the assembly rate over an initial expert-designed building block. In the case of octahedral clusters, our platform produces a building block with a 60% yield despite being challenged with a poor initial building block design incapable of forming stable octahedra.", "journal": "MOLECULAR SYSTEMS DESIGN & ENGINEERING", "category": "Chemistry, Physical; Engineering, Chemical; Nanoscience & Nanotechnology; Materials Science, Multidisciplinary", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000484397700028", "keywords": "feature extraction; locality sensitive discriminant projection; outliers; manifold learning", "title": "Locality sensitive discriminant projection for feature extraction and face recognition", "abstract": "As an effective feature extraction method, locality sensitive discriminant analysis (LSDA) utilizes the neighbor relationship of data to characterize the manifold structure of data and uses label information of data to adapt to classification tasks. However, the performance of LSDA is affected by outliers and the destruction of local structure. Aiming at solving the limitations of LSDA, a locality sensitive discriminant projection (LSDP) algorithm is proposed. LSDP minimizes the distance of intraclass neighbor samples to maintain local structure and minimizes the intraclass non-neighbor samples to increase the compactness of intraclass samples after projection. The problem of outliers is alleviated by increasing the compactness of intraclass samples in subspace. At the same time, we redefine the weights of interclass neighbor samples to maintain the neighbor relationship of different labels samples. Holding the local structure of interclass samples maintains the manifold structure of data. Experiments on face datasets demonstrate the effectiveness of the LSDP algorithm. (C) 2019 SPIE and IS&T", "journal": "JOURNAL OF ELECTRONIC IMAGING", "category": "Engineering, Electrical & Electronic; Optics; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000404078500071", "keywords": "Nano additives; Diesel-biodiesel blends; Ultrasonic; Genetic programming", "title": "Performance and emission characteristics of a CI engine using nano particles additives in biodiesel-diesel blends and modeling with GP approach", "abstract": "The performance and the exhaust emissions of a diesel engine operating on nano-diesel-biodiesel blended fuels has been investigated. Multi wall carbon nano tubes (CNT) (40, 80 and 120 ppm) and nano silver particles (40, 80 and 120 ppm) were produced and added as additive to the biodiesel-diesel blended fuel. Six cylinders, four-stroke diesel engine was fuelled with these new blended fuels and operated at different engine speeds. Experimental test results indicated the fact that adding nano particles to diesel and biodiesel fuels, increased diesel engine performance variables including engine power and torque output up to 2% and brake specific fuel consumption (bsfc) was decreased 7.08% compared to the net diesel fuel. CO2 emission increased maximum 17.03% and CO emission in a biodiesel-diesel fuel with nano-particles was lower significantly (25.17%) compared to pure diesel fuel. UHC emission with silver nano-diesel-biodiesel blended fuel decreased (28.56%) while with fuels that contains CNT nano particles increased maximum 14.21%. With adding nano particles to the blended fuels, NOx increased 25.32% compared to the net diesel fuel. This study also presents genetic programming (GP) based model to predict the performance and emission parameters of a CI engine in terms of nano-fuels and engine speed. Experimental studies were completed to obtain training and testing data. The optimum models were selected according to statistical criteria of root mean square error (RMSE) and coefficient of determination (R-2). It was observed that the GP model can predict engine performance and emission parameters with correlation coefficient (R-2) in the range of 0.93-1 and RMSE was found to be near zero. The simulation results demonstrated that GP model is a good tool to predict the CI engine performance and emission parameters. (C) 2017 Elsevier Ltd. All rights reserved.", "journal": "FUEL", "category": "Energy & Fuels; Engineering, Chemical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000443834700009", "keywords": "CO2 sequestration; Coal; Compressive strength; Young's modulus; ANN; ANFIS", "title": "Regression and soft computing models to estimate young's modulus of CO2 saturated coals", "abstract": "Young's modulus of coal is a very important deformational property which dictates how the material will behave in presence of sub- and super-critical carbon dioxide during sequestration. But this is also a difficult property to measure due to the extensive instrumental requirements, and wide compositional and structural heterogeneity of the coal. Therefore, an indirect method to measure the saturated Young's modulus of coals has been developed in the present research using artificial neural network (ANN) and adaptive neuro-fuzzy inference system (ANFIS). Low and high rank specimens from three different basins of India and Australia have been used for the analysis. Saturation pressure and the compressive strength (UCS) of the coal specimens have been used as the input parameters to build the models. The performance of the models were evaluated against the multivariate regression model using four different types of statistical parameters such as root mean square error (RMSE), coefficient of determination (R2), mean absolute error percentage (MAPE), and variables accounted for (VAF). Results show that ANFIS model is the best performing one with the least RMSE and MAPE, and highest VAF and R-2. This research demonstrates that it is possible to develop soft computing models that can successfully estimate some of the critical rock mechanics parameters essential for the technical evaluation of the sequestration projects on coal. This generalized model is also excellent in incorporating the possible effect of heterogeneity in specimens and performs well for samples that show similar data distribution. If applied, this can potentially reduce the requirement of extensive, complex and expensive instruments that are required for similar investigations.", "journal": "MEASUREMENT", "category": "Engineering, Multidisciplinary; Instruments & Instrumentation", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000317380800003", "keywords": "Transfer learning; Collaborative filtering; Missing ratings", "title": "Transfer learning in heterogeneous collaborative filtering domains", "abstract": "A major challenge for collaborative filtering (CF) techniques in recommender systems is the data sparsity that is caused by missing and noisy ratings. This problem is even more serious for CF domains where the ratings are expressed numerically, e.g. as 5-star grades. We assume the 5-star ratings are unordered bins instead of ordinal relative preferences. We observe that, while we may lack the information in numerical ratings, we sometimes have additional auxiliary data in the form of binary ratings. This is especially true given that users can easily express themselves with their preferences expressed as likes or dislikes for items. In this paper, we explore how to use these binary auxiliary preference data to help reduce the impact of data sparsity for CF domains expressed in numerical ratings. We solve this problem by transferring the rating knowledge from some auxiliary data source in binary form (that is, likes or dislikes), to a target numerical rating matrix. In particular, our solution is to model both the numerical ratings and ratings expressed as like or dislike in a principled way. We present a novel framework of Transfer by Collective Factorization (TCF), in which we construct a shared latent space collectively and learn the data-dependent effect separately. A major advantage of the TCF approach over the previous bilinear method of collective matrix factorization is that we are able to capture the data-dependent effect when sharing the data-independent knowledge. This allows us to increase the overall quality of knowledge transfer. We present extensive experimental results to demonstrate the effectiveness of TCF at various sparsity levels, and show improvements of our approach as compared to several state-of-the-art methods. (C) 2013 Elsevier B.V. All rights reserved.", "journal": "ARTIFICIAL INTELLIGENCE", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000459599300004", "keywords": "Morphology; priming; Semitic; semantic transparency; stems", "title": "Attaining landmark status: Rumelhart and McClelland's PDP Volumes and the Connectionist Paradigm", "abstract": "In 1986, David Rumelhart and James McClelland published their two-volume work, Parallel distributed processing: Explorations in microcognition, Volume 1: Foundations and Volume 2: Psychological and biological models. These volumes soon become classic texts in both connectionism, specifically, and in the cognitive science field more generally. Drawing on oral histories, book reviews, translations, citation records, and close textual analysis, this paper analyzes how and why they attained landmark status. It argues that McClelland and Rumelhart's volumes became classics largely as a result of a confluence of rhetorical factors. Specifically, the PDP Volumes appeared at a kairotic moment in the history of connectionism, publishing dynamics that facilitated their circulation played an important role, and the volumes were ambiguous about the relationship between model and brain in a manner that enabled them to address an expansive audience. In so doing, this paper offers insight into both the history of cognitive science and rhetoric's role in establishing classic texts.", "journal": "JOURNAL OF THE HISTORY OF THE BEHAVIORAL SCIENCES", "category": "History Of Social Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000366701700012", "keywords": "Neurorobotics; Brain-inspired robotics; Spiking neural networks; STDP; Neuromorphic; Learning", "title": "Neuromorphic implementations of neurobiological learning algorithms for spiking neural networks", "abstract": "The application of biologically inspired methods in design and control has a long tradition in robotics. Unlike previous approaches in this direction, the emerging field of neurorobotics not only mimics biological mechanisms at a relatively high level of abstraction but employs highly realistic simulations of actual biological nervous systems. Even today, carrying out these simulations efficiently at appropriate timescales is challenging. Neuromorphic chip designs specially tailored to this task therefore offer an interesting perspective for neurorobotics. Unlike Von Neumann CPUs, these chips cannot be simply programmed with a standard programming language. Like real brains, their functionality is determined by the structure of neural connectivity and synaptic efficacies. Enabling higher cognitive functions for neurorobotics consequently requires the application of neurobiological learning algorithms to adjust synaptic weights in a biologically plausible way. In this paper, we therefore investigate how to program neuromorphic chips by means of learning. First, we provide an overview over selected neuromorphic chip designs and analyze them in terms of neural computation, communication systems and software infrastructure. On the theoretical side, we review neurobiological learning techniques. Based on this overview, we then examine on-die implementations of these learning algorithms on the considered neuromorphic chips. A final discussion puts the findings of this work into context and highlights how neuromorphic hardware can potentially advance the field of autonomous robot systems. The paper thus gives an in-depth overview of neuromorphic implementations of basic mechanisms of synaptic plasticity which are required to realize advanced cognitive capabilities with spiking neural networks. (C) 2015 Elsevier Ltd. All rights reserved.", "journal": "NEURAL NETWORKS", "category": "Computer Science, Artificial Intelligence; Neurosciences", "annotated_keywords": ["neural net", "neural net", "learning algorithm", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000878107600003", "keywords": "Multi-view; Multi-label; Subspace learning; Manifold learning", "title": "Multi-view multi-label learning with double orders manifold preserving", "abstract": "In multi-view multi-label learning, each instance has multiple heterogeneous views and is marked with a collection of non-exclusive discrete labels. This type of data is usually subject to dimensional catastrophe. Previous multi-view multi-label works look for a low-dimensional shared subspace to tackle this problem. However, these methods ignore the global structural information of the original feature space during dimension reduction. In this paper, we propose Multi-view Multi-label learning with Double Orders Manifold Preserving (MMDOM). MMDOM utilizes manifold preserving constraint to guide the formation of low-dimensional shared subspace. To obtain exact manifold preserving, the first-order and the second-order similarity matrices are both introduced to explore the local and global structural information of the original feature space. Experiments on various benchmark datasets demonstrate the superior effectiveness of MMDOM against state-of-the-art methods.", "journal": "APPLIED INTELLIGENCE", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000670554200007", "keywords": "Kernel; Matrix decomposition; Data integration; Softening; Fitting; Task analysis; Training; Label softening; manifold learning; multi-Kernel learning; remote sensing; semantic-based multimodal fusion", "title": "Multimodal Data Fusion Using Non-Sparse Multi-Kernel Learning With Regularized Label Softening", "abstract": "Due to the need of practical application, multiple sensors are often used for data acquisition, so as to realize the multimodal description of the same object. How to effectively fuse multimodal data has become a challenge problem in different scenarios including remote sensing. Nonsparse multi-Kernel learning has won many successful applications in multimodal data fusion due to the full utilization of multiple Kernels. Most existing models assume that the nonsparse combination of multiple Kernels is infinitely close to a strict binary label matrix during the training process. However, this assumption is very strict so that label fitting has very little freedom. To address this issue, in this article, we develop a novel nonsparse multi-Kernel model for multimodal data fusion. To be specific, we introduce a label softening strategy to soften the binary label matrix which provides more freedom for label fitting. Additionally, we introduce a regularized term based on manifold learning to anti over fitting problems caused by label softening. Experimental results on one synthetic dataset, several UCI multimodal datasets and one multimodal remoting sensor dataset demonstrate the promising performance of the proposed model.", "journal": "IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING", "category": "Engineering, Electrical & Electronic; Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000903348800001", "keywords": "Computational modeling; deep learning; generative adversarial networks; physics-informed deep generative models; uncertainty quantification", "title": "MGDGAN: Multiple Generator and Discriminator Generative Adversarial Networks for Solving Stochastic Partial Differential Equations", "abstract": "We propose novel structures of generator and discriminator in physics-informed generative adversarial networks called multiple-generator-and-discriminator generative adversarial networks (MGDGANs), that are designed to solve stochastic partial differential equations (SPDEs). MGDGANs for SPDEs consist of three steps: a generator that samples a solution to the SPDEs, a physics-informed operator that enforces the governing equation, and a discriminator that distinguishes between samples from the generator and training samples. Inspired by the polynomial chaos, we represent the solution by the inner product of functions in spatial and random variables, and model each function by a separate generator. We show that the proposed multiple generator structure offers huge computational savings in training and prediction. If multiple stochastic processes exist in the system, then a distinct discriminator is used for each of them. We show that the loss function obtained by these distinct discriminators provides an equivalent metric to the Wasserstein distance loss by a single discriminator, and provide numerical examples to demonstrate that these multiple discriminators enhance the training accuracy. Numerical examples are demonstrated to verify that the proposed model is efficient in computation and memory; the model reduces computing time by more than a factor of 10 and relative l2 error by about one-third in the SPDE example.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000742659700001", "keywords": "Sprite generation; body segmentation; pose estimation; generative adversarial network; deep learning", "title": "Image Translation Method for Game Character Sprite Drawing", "abstract": "Two-dimensional (2D) character animation is one of the most important visual elements on which users' interest is focused in the game field. However, 2D character animation works in the game field are mostly performed manually in two dimensions, thus generating high production costs. This study proposes a generative adversarial network based production tool that can easily and quickly generate the sprite images of 2D characters. First, we proposed a methodology to create a synthetic dataset for training using images from the real world in the game resource production field where machine learning datasets are insufficient. In addition, we have enabled effective sprite generation while minimizing user input in the process of using the tool. To this end, we proposed a mixed input method with a small number of segmentations and skeletal bone paintings. The proposed image-to-image translation network effectively generated sprite images from the user input images using the skeletal loss. We conducted an experiment regarding the number of images required and showed that 2D sprite resources can be generated even with a small number of segmentation inputs and one skeletal bone drawing.", "journal": "CMES-COMPUTER MODELING IN ENGINEERING & SCIENCES", "category": "Engineering, Multidisciplinary; Mathematics, Interdisciplinary Applications", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000839795900001", "keywords": "SAR image; optical image; SAR-to-optical transformation; conditional generative adversarial network (CGAN); deep learning", "title": "Sar2color: Learning Imaging Characteristics of SAR Images for SAR-to-Optical Transformation", "abstract": "Optical images are rich in spectral information, but difficult to acquire under all-weather conditions, while SAR images can overcome adverse meteorological conditions, but geometric distortion and speckle noise will reduce the quality of SAR images and thus make image interpretation more challenging. Therefore, transforming SAR images to optical images to assist SAR image interpretation will bring opportunities for SAR image application. With the advancement of deep learning technology, the ability of SAR-to-optical transformation has been greatly improved. However, most of the current mainstream transformation methods do not consider the imaging characteristics of SAR images, and there will be failures such as noisy color spots and regional landform deformation in the generated optical images. Moreover, since the SAR image itself does not contain color information, there also exist many color errors in these results. Aiming at the above problems, Sar2color, an end-to-end general SAR-to-optical transformation model, is proposed based on a conditional generative adversarial network (CGAN). The model uses DCT residual block to reduce the effect of coherent speckle noise on the generated optical images, and constructs the Light atrous spatial pyramid pooling (Light-ASPP) module to mitigate the negative effect of geometric distortion on the generation of optical images. These two designs ensure the precision of texture details when the SAR image is transformed into an optical image, and use the correct color memory block (CCMB) to improve the color accuracy of transformation results. Towards the Sar2color model, we have carried out evaluations on the homologous heterogeneous SAR image and optical image pairing dataset SEN1-2. The experimental results show that, compared with other mainstream transformation models, Sar2color achieves the state-of-the-art effect on all three objective and one subjective evaluation metrics. Furthermore, we have carried out various ablation experiments, and the results show the effectiveness of each designed module of Sar2color.", "journal": "REMOTE SENSING", "category": "Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000726094900046", "keywords": "Cloud detection; deep learning; geoinformatics; optical imagery; spatial-temporal feature fusion", "title": "Thick Clouds Removing From Multitemporal Landsat Images Using Spatiotemporal Neural Networks", "abstract": "Landsat images have played an important role in the field of Earth observation and geoinformatics. However, optical Landsat images are frequently contaminated by cloud cover, especially in tropical and subtropical regions, which limits the utilization of these images. To improve the utilization of Landsat images, in this study, we propose a novel spatiotemporal neural network with four modules: a cloud detection module, a spatial-temporal learning module, a spatial-temporal feature fusion module, and a reconstruction module. The results of the experiments demonstrate that the proposed method is quantitatively effective (root mean square error < 0.0179) and can achieve a better result for reconstructing Landsat images than some of the widely used existing deep learning methods and multitemporal methods. The proposed neural network method provides an effective tool for the removal of contiguous, thick clouds from satellite images, so as to improve the quality of subsequent remote sensing mapping and geoinformation extraction.", "journal": "IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING", "category": "Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": ["neural net", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000752608200002", "keywords": "Artificial neural network; Multiple linear regression; Statistical analysis; Wall temperature prediction; Thermal behaviour", "title": "A comparative study on regression model and artificial neural network for the prediction of wall temperature in a building", "abstract": "This paper demonstrates the validation and prediction of the wall temperature of a building exposed to a composite climate. Two artificial intelligence models, such as multiple linear regression and artificial neural networks, have been used to predict. The wall temperature has been predicted mainly based on the parameters like ambient temperature, wind speed and relative humidity in all four directions of the buildings. Three statistical analyses were used to validate the model's outcome: R-Squared, Root Mean Square Error (RMSE) and Mean Absolute Error (MAE). In terms of train and control data, the two models yield comparable findings. The artificial neural network model has more adaptability since it was able to adjust to unexpected changes in the input data, according to a comparison of the applied mathematics of each model. The regression model was used for this investigation because it gives constant estimate values for the factors, though the neural model isn't numerically characterized. The review infers that the neural model must be utilized as an additional way to predict the wall temperature. This study can plan and adapt buildings for future work, considering the most critical climatic conditions. It also assists architects and engineers in determining the appropriate insulation for the building envelope, which improves its thermal performance.", "journal": "JOURNAL OF ENGINEERING RESEARCH", "category": "Engineering, Multidisciplinary", "annotated_keywords": ["artificial intelligen", "neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000764755100004", "keywords": "Feature extraction; Support vector machines; Training; Adaptation models; Real-time systems; Entropy; Working environment noise; Adaptive strategy; feature extraction; seismic signal; support vector machine (SVM); target detection", "title": "Adaptive Moving Ground-Target Detection Method Based on Seismic Signal", "abstract": "Moving ground-target detection system is widely used to monitor illegal activities of pedestrians and vehicles. However, existing detection methods are restricted by the power consumption in hardware and are usually based on some single feature of the seismic signal, which leads to low detection accuracy and false alarms. To address these issues, we propose a new moving ground-target detection method for detecting the weak seismic signals generated by distant moving ground targets. This method combines an adaptive strategy and support vector machines (SVMs). Both time- and frequency-domain features of seismic signals are considered in the detection method. Additionally, we carry out field experiments to evaluate the performance of the proposed method. The results show that the proposed moving ground-target detection method can detect distant moving ground targets and avoid false alarms as many as possible, which indicates good performance.", "journal": "IEEE GEOSCIENCE AND REMOTE SENSING LETTERS", "category": "Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000764615000001", "keywords": "Training; Naive Bayes methods; Weight measurement; Optimization; Information filters; Classification algorithms; Training data; Attribute weighting; classification; exponential loss; naive bayes; nonlinear optimization", "title": "Exponential Loss Minimization for Learning Weighted Naive Bayes Classifiers", "abstract": "The naive Bayesian classification method has received significant attention in the field of supervised learning. This method has an unrealistic assumption in that it views all attributes as equally important. Attribute weighting is one of the methods used to alleviate this assumption and consequently improve the performance of the naive Bayes classification. This study, with a focus on nonlinear optimization problems, proposes four attribute weighting methods by minimizing four different loss functions. The proposed loss functions belong to a family of exponential functions that makes the optimization problems more straightforward to solve, provides analytical properties of the trained classifier, and allows for the simple modification of the loss function such that the naive Bayes classifier becomes robust to noisy instances. This research begins with a typical exponential loss which is sensitive to noise and provides a series of its modifications to make naive Bayes classifiers more robust to noisy instances. Based on numerical experiments conducted using 28 datasets from the UCI machine learning repository, we confirmed that the proposed scheme successfully determines optimal attribute weights and improves the classification performance.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": ["machine learning", "supervised learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000669939900002", "keywords": "China's export; New competitive advantages; Export competitive advantage; Core competitiveness; Fuzzy least squares support vector machine; Soft measurement", "title": "Combined soft measurement on key indicator parameters of new competitive advantages for China's export", "abstract": "The estimation of the difference between the new competitive advantages of China's export and the world's trading powers have been the key measurement problems in China-related studies. In this work, a comprehensive evaluation index system for new export competitive advantages is developed, a soft-sensing model for China's new export competitive advantages based on the fuzzy entropy weight analytic hierarchy process is established, and the soft-sensing values of key indexes are derived. The obtained evaluation values of the main measurement index are used as the input variable of the fuzzy least squares support vector machine, and a soft-sensing model of the key index parameters of the new export competitive advantages of China based on the combined soft-sensing model of the fuzzy least squares support vector machine is established. The soft-sensing results of the new export competitive advantage index of China show that the soft measurement model developed herein is of high precision compared with other models, and the technical and brand competitiveness indicators of export products have more significant contributions to the new competitive advantages of China's export, while the service competitiveness indicator of export products has the least contribution to new competitive advantages of China's export.", "journal": "FINANCIAL INNOVATION", "category": "Business, Finance; Social Sciences, Mathematical Methods", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000672761900001", "keywords": "Wind power project; Cost prediction; Sparrow search algorithm; BP neural network model", "title": "Model-agnostic online forecasting for PV power output", "abstract": "A reliable forecasting model is required for photovoltaic (PV) power output because solar energy is highly volatile. Another driver for the need of a reliable forecasting model is concept drift, which means that the statistical properties of the data change over time. In this paper, an online forecasting method to handle concept drift is proposed. First, the problem of forecasting in batch learning is transformed into a forecasting in online learning setting. Then, an online learning algorithm is applied, which is good for handling concept drift. Through experiments using the real-world data, it is shown that the method noticeably improves performance compared to the case where a trained model is used. Under various concept drift scenarios, the method improves performance by up to 87.3%. It is also shown that the re-training method (a representative existing method) has several limitations. This method requires several issues to be solved, such as selection of a proper window size, and this is evident through results showing different performance under different settings. In contrast, the method shows a reliable and desirable performance under various concept drift scenarios and thus outperforms the re-training method. The method improves performance by up to 79%.", "journal": "IET RENEWABLE POWER GENERATION", "category": "Green & Sustainable Science & Technology; Energy & Fuels; Engineering, Electrical & Electronic", "annotated_keywords": ["learning algorithm"], "label": "1", "title_label": "0"}
{"id": "WOS:000888006400001", "keywords": "depression; dynamic causal modeling (DCM); biomarkers; event-related potentials (ERPs); machine learning", "title": "Toward biophysical markers of depression vulnerability", "abstract": "A major difficulty with treating psychiatric disorders is their heterogeneity: different neural causes can lead to the same phenotype. To address this, we propose describing the underlying pathophysiology in terms of interpretable, biophysical parameters of a neural model derived from the electroencephalogram. We analyzed data from a small patient cohort of patients with depression and controls. Using DCM, we constructed biophysical models that describe neural dynamics in a cortical network activated during a task that is used to assess depression state. We show that biophysical model parameters are biomarkers, that is, variables that allow subtyping of depression at a biological level. They yield a low dimensional, interpretable feature space that allowed description of differences between individual patients with depressive symptoms. They could capture internal heterogeneity/variance of depression state and achieve significantly better classification than commonly used EEG features. Our work is a proof of concept that a combination of biophysical models and machine learning may outperform earlier approaches based on classical statistics and raw brain data.", "journal": "FRONTIERS IN PSYCHIATRY", "category": "Psychiatry", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000646956600006", "keywords": "Road safety; Driving behavior; Machine learning; Neural networks; Support vector machines", "title": "Machine Learning Techniques to Identify Unsafe Driving Behavior by Means of In-Vehicle Sensor Data", "abstract": "Traffic crashes are one of the biggest causes of accidental death in the way where, every year, more than 1.35 million of people die. In most of them, the main cause is related to the driver?s behavior. The driver performs a set of actions on the vehicle commands, such as steering, braking, accelerating or changing gear, which generate a direct response of the vehicle, or other tasks, such as visual, auditory, or haptic related tasks (e.g. looking for items, listening to radio, and using a smartphone), which can still impact on the driving safety. In this work we propose a methodology based on machine learning techniques aimed at recognizing safe and unsafe driving behaviors by means of in-vehicle sensor data. Starting from these signals we compute a set of descriptive features capable to accurately describe the behavior of the driver. Two different classification tools, namely Support Vector Machines and feed-forward neural networks, have been trained and tested on a publicly available dataset containing more than 26 hours of total driving time. The classification results report an average accuracy above 90% for both classifiers and the McNemar test shows no performance difference between the models at the 0.05 significance level, demonstrating a concrete possibility of identifying unsafe driving using in-vehicle sensor data.", "journal": "EXPERT SYSTEMS WITH APPLICATIONS", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science", "annotated_keywords": ["neural net", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000770765600001", "keywords": "Farmland fertility algorithm; Feature selection; Hybridization; Intrusion detection systems", "title": "A Feature Selection Based on the Farmland Fertility Algorithm for Improved Intrusion Detection Systems", "abstract": "The development and expansion of the Internet and cyberspace have increased computer systems attacks; therefore, Intrusion Detection Systems (IDSs) are needed more than ever. Machine learning algorithms have recently been used as successful IDSs; however, due to the high dimensions in IDSs, Feature Selection (FS) plays an essential role in these systems' performance. In this paper, a binary version of the Farmland Fertility Algorithm (FFA) called BFFA is presented to FS in the classification of IDSs. In the proposed method, the V-shaped function is used to move the FFA processes in the binary space, as a result of which the V-shaped function changes the continuous position of the solutions in the FFA algorithm to binary mode. A hybrid approach to classifiers and the BFFA is presented as a fast and robust IDS. The proposed method is tested on two valid IDSs datasets, namely NSL-KDD and UNSW-NB15, and is compared in Accuracy, Precision, Recall, and F1_Score criteria with K-Nearest Neighbor (KNN), Support Vector Machine (SVM), Decision Tree (DT), Random Forest (RF), Adaboost (ADA_BOOST), and Naive Bayes (NB) classifiers. The simulation results showed that the proposed method performed better than the classifiers in Accuracy, Precision, and Recall criteria; moreover, the proposed method has a better run time in the FS operation.", "journal": "JOURNAL OF NETWORK AND SYSTEMS MANAGEMENT", "category": "Computer Science, Information Systems; Telecommunications", "annotated_keywords": ["learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000845016400001", "keywords": "Underwater environment; Object detection; Object tracking; Bounding box prediction; Deep learning; Support vector regression", "title": "Hybridization of Deep Convolutional Neural Network for Underwater Object Detection and Tracking Model", "abstract": "In this present work, underwater object detection and tracking was studied using the efficient Hybridization of Deep Convolutional Neural Network for Underwater Object Detection and Tracking (HDCNN-UODT) model for three bench mark data sets namely UOT32, brackish, and URPC 2020 datasets. The HDCNN-UODT technique primarily employs data augmentation process for increasing the size of the training dataset to improve the detection and average precision. Besides, a hybridization of two Deep Learning (DL) models namely RetinaNet and EfficientNet models was applied as feature extractors. In addition, the bounding box prediction process takes place via support vector regression (SVR) followed by kernel extreme learning machine (KELM) model. The novelty of the present work was demonstrated by the design of SVR based bounding box regression and fusion based feature extractions. The average precision examination (APE), average success rate(ASR) & average frame per second(AFPS) analysis of the HDCNN-UODT model using UOT32 dataset showed better APE (51.27%), ASR (43.19) & AFPS (310.25) over other reported techniques. Additionally, the detection accuracy of the HDCNN-UODT model for brackish and URPC datasets showed improved accuracies for different objects over YOLO v4, T-YOLO v4, and AFFM-YOLO v4 techniques. Moreover, using brackish dataset, HDCNN-UODT model showed highest accuracy of 94.85% for 'Crab' object and for URPC dataset, a highest accuracy of 88.34% for 'Scallop' object was obtained. Hence based on our outcome, HDCNN-UODT technique might be better suited for the object detection and tracking application.", "journal": "MICROPROCESSORS AND MICROSYSTEMS", "category": "Computer Science, Hardware & Architecture; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000638340400001", "keywords": "fully convolutional networks; Gaussian kernel; conditional random fields; probabilistic model; skin cancer; segmentation", "title": "A Probabilistic-Based Deep Learning Model for Skin Lesion Segmentation", "abstract": "The analysis and detection of skin cancer diseases from skin lesion have always been tedious when done manually. The complex nature of skin lesion images is one of the key reasons for this. The skin lesion images contain noise and artifacts such as hairs, oil and bubbles, blood vessels, and skin lines. They also have variegated colors, low contrast, and irregular borders. Various computational approaches have been designed in the past for aiding in the detection and diagnosis of skin cancer diseases using skin lesion images. The existing techniques have been limited due to the interference of the aforementioned features of skin lesion. Recently, machine learning techniques, in particular the deep learning techniques have been used for the detection of skin cancer. However, they are still limited to the fuzzy and irregular borders of skin lesion images coupled with the low contrast that exists between the diseased lesion and healthy tissues. In this paper, we utilized a probabilistic model for the enhancement of a fully convolutional network-based deep learning system to analyze and segment skin lesion images. The probabilistic model employs an efficient mean-field approximate probabilistic inference approach with a fully connected conditional random field that utilizes a Gaussian kernel. The probabilistic model further performs a refinement of skin lesion borders. The whole framework is tested and evaluated on publicly available skin lesion image datasets of ISBI 2017 and PH2. The system achieved a better performance, having an accuracy of 98%.", "journal": "APPLIED SCIENCES-BASEL", "category": "Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials Science, Multidisciplinary; Physics, Applied", "annotated_keywords": ["machine learning", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000630294300001", "keywords": "ANFIS; lssvm; chlor-alkali; electrolysis; brine; membrane cell", "title": "Development of computational methods for estimation of current efficiency and cell voltage in a Chlor-alkali membrane cell", "abstract": "This work presents proposing two artificial intelligence methods including Least squares support vector machine (LSSVM) and Adaptive neuro fuzzy inference system (ANFIS) for the prediction of caustic current efficiency (CCE) and cell voltage as a function of pH, current density, brine concentration, electrolyte velocity, operating temperature, and run time. The predictions of LSSVM and ANFIS models were evaluated by the experimental values of this process graphically and statistically. The overall R-squared values of LSSVM and ANFIS for prediction of CCE were 0.999 and 0.972, respectively. On the other hand, these values for cell voltage prediction were 1 and 0.998. According to the CCE and cell voltage predictions results, LSSVM algorithm has great performance in prediction of chlor-alkali membrane cell processes. Furthermore, artificial intelligence methods can have wide use in electrolytic processes to enhance power consumption.", "journal": "ENERGY SOURCES PART A-RECOVERY UTILIZATION AND ENVIRONMENTAL EFFECTS", "category": "Energy & Fuels; Engineering, Chemical; Environmental Sciences", "annotated_keywords": ["artificial intelligen"], "label": "1", "title_label": "0"}
{"id": "WOS:000772263400001", "keywords": "IT incident resolution; IT service management; ServiceNow; Cloud-computing; Theory for predicting; Predictive analytics", "title": "Key Factors in Achieving Service Level Agreements (SLA) for Information Technology (IT) Incident Resolution", "abstract": "In this paper, we analyze the impact of various factors on meeting service level agreements (SLAs) for information technology (IT) incident resolution. Using a large IT services incident dataset, we develop and compare multiple models to predict the value of a target Boolean variable indicating whether an incident met its SLA. Logistic regression and neural network models are found to have the best performance in terms of misclassification rates and average squared error. From the best-performing models, we identify a set of key variables that influence the achievement of SLAs. Based on model insights, we provide a thorough discussion of IT process management implications. We suggest several strategies that can be adopted by incident management teams to improve the quality and effectiveness of incident management processes, and recommend avenues for future research.", "journal": "INFORMATION SYSTEMS FRONTIERS", "category": "Computer Science, Information Systems; Computer Science, Theory & Methods", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000641463100002", "keywords": "Hyperspectral; Wavelet analysis; Mathematical morphology; Object-oriented; Support vector machine", "title": "Classifying tree species in the plantations of southern China based on wavelet analysis and mathematical morphology", "abstract": "It is essential to classify tree species accurately for the sustainable management of forest resources and effective monitoring of species diversity. Airborne hyperspectral images have high spatial and spectral resolution, and consequently, the large quantity of information on spectral and spatial structures is effective for tree species classification. In this research, Gaofeng Forest Farm in Nanning, Guangxi Province, China, was used as the study site, and the airborne hyperspectral images were used as the data source. The spectral and textural information extracted by wavelet analysis and edge information extracted by mathematical morphological analysis composed a feature set. The feature set was filtered through a random forest, and object-oriented methods were used to classify tree species through a support vector classifier. The results showed that spectral features extracted by wavelet analysis were highly effective in classifying tree species that had the greatest spectral separability. Horizontal and vertical textures had no positive effect on the classification accuracy, while diagonal textures improved the classification accuracy of tree species. Texture features were not sensitive to stands with small areas and broken distributions, while the edge structure features extracted from mathematical morphology were sensitive to the complex forests. The overall accuracy of tree species classification by combining spectral, textural, and edge structural features was 96.54%, with a Kappa coefficient of 0.96. In the comparative test, the first-derivative and second-derivative of the hyperspectral image and texture features composed a feature set. Using the same classification methods, the OA was 80.91% and Kappa was 0.7711. Therefore, the wavelet analysis and mathematical morphology can significantly improve the tree species classification accuracy of hyperspectral images. Accurate tree species classification can provide basic scientific data for forest resource monitoring and management measures.", "journal": "COMPUTERS & GEOSCIENCES", "category": "Computer Science, Interdisciplinary Applications; Geosciences, Multidisciplinary", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000649674500009", "keywords": "Integrated energy-water model; Sustainable hybrid energy-water systems; Data mining; Machine learning algorithms; Prediction accuracy; Optimization energy-water model", "title": "Data mining with 12 machine learning algorithms for predict costs and carbon dioxide emission in integrated energy-water optimization model in buildings", "abstract": "In recent years, various models, which employ green and sustainable energy-supplying systems, have been presented in different ways to optimize water and energy consumption in several countries, pursuing the purpose of reducing water and energy consumption and costs. The present study considered an integrated and unconcentrated water and energy consumption optimization model in the building and investigated it economically and environmentally using mixed-integer linear programming. In this research, the data of different sections, including the data related to the climatic conditions, environment, costs, technologies, etc., were collected. Next, the researchers sought to predict the model results using 12 highly accurate machine learning algorithms and considering four indices to examine the prediction accuracy of the algorithms. The utilization of the machine learning prediction algorithms helped us economically and environmentally investigate and predict the model conditions in every geographical location with respect to the data that varied according to the climatic conditions. The results obtained from investigating the indices of the algorithms for the data examined for the objective functions, including cost optimization and carbon emission reduction, revealed that the prediction accuracy ranged from 0.8 to 0.96, and 0.79 to 0.91 for the first and second objective functions, respectively, in the 12 examined algorithms. Meanwhile, the Light Gradient Boosting Machine and Extra Tree algorithms enjoyed higher prediction accuracy in this research than other algorithms. Next, to analyze the results, the researchers implemented the Principal Component Analysis method to reduce the input data dimension to the algorithms. The results of the algorithms reflected a decline in the prediction accuracy after the dimension reduction. Finally, the effective variables in the prediction accuracy of the algorithms were presented by using the Stepwise Regression method for both objective functions. Overall, the output results of the algorithms showed high prediction accuracy for investigating the model conditions in various geographical regions.", "journal": "ENERGY CONVERSION AND MANAGEMENT", "category": "Thermodynamics; Energy & Fuels; Mechanics", "annotated_keywords": ["machine learning", "machine learning", "learning algorithm", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000624580100018", "keywords": "Rainfall and runoff; variable precision fuzzy neighborhood rough set; LSTM; multi-span", "title": "Multi-Span and Multiple Relevant Time Series Prediction Based on Neighborhood Rough Set", "abstract": "Rough set theory has been widely researched for time series prediction problems such as rainfall runoff. Accurate forecasting of rainfall runoff is a long standing but still mostly significant problem for water resource planning and management, reservoir and river regulation. Most research is focused on constructing the better model for improving prediction accuracy. In this paper, a rainfall runoff forecast model based on the variable-precision fuzzy neighborhood rough set (VPFNRS) is constructed to predict Watershed runoff value. Fuzzy neighborhood rough set define the fuzzy decision of a sample by using the concept of fuzzy neighborhood. The fuzzy neighborhood rough set model with variable-precision can reduce the redundant attributes, and the essential equivalent data can improve the predictive capabilities of model. Meanwhile VFPFNRS can handle the numerical data, while it also deals well with the noise data. In the discussed approach, VPFNRS is used to reduce superfiuous attributes of the original data, the compact data are employed for predicting the rainfall runoff. The proposed method is examined utilizing data in the Luo River Basin located in Guangdong, China. The prediction accuracy is compared with that of support vector machines and long short ter m memory (LSTM). The experiments show that the method put forward achieves a higher predictive performance.", "journal": "CMC-COMPUTERS MATERIALS & CONTINUA", "category": "Computer Science, Information Systems; Materials Science, Multidisciplinary", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000738972200001", "keywords": "airflow control; aeration system; activated sludge; machine learning; model predictive control", "title": "Modeling Performance of Butterfly Valves Using Machine Learning Methods", "abstract": "Control of airflow of activated sludge systems has significant challenges due to the non-linearity of the control element (butterfly valve). To overcome this challenge, some valve manufacturers developed valves with linear characteristics. However, these valves are 10-100 times more expensive than butterfly valves. By developing models for butterfly valves installed characteristics and utilizing these models for real-time airflow control, the authors of this paper aimed to achieve the same accuracy of control using butterfly valves as achieved using valves with linear characteristics. Several approaches were tested to model the installed valve's characteristics, such as a formal mathematical model utilizing Simscape/Matlab software, a semi-empirical model, and several machine learning methods (MLM), including regression, support vector machine, Gaussian process, decision tree, and deep learning. Several versions of the airflow-valve position models were developed using each machine learning method listed above. The one with the smallest forecast error was selected for field testing at the 55.5x10(3) m(3)/day 12 MGD City of Chico activated sludge system. Field testing of the formal mathematical model, semi-empirical model, and the regularized gradient boosting machine model (the best among MLMs) showed that the regularized gradient boosting machine model (RGBMM) provided the best accuracy. The use of the RGBMMs in airflow control loops since 2019 at the City of Chico wastewater treatment plant showed that these models are robust and accurate (2.9% median error).", "journal": "SUSTAINABILITY", "category": "Green & Sustainable Science & Technology; Environmental Sciences; Environmental Studies", "annotated_keywords": ["machine learning", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000679653600006", "keywords": "CO2 emissions; SVM; Multiobjective algorithms; Environment management", "title": "A hybrid novel SVM model for predicting CO2 emissions using Multiobjective Seagull Optimization", "abstract": "The agricultural sector is one of the most important sources of CO2 emissions. Thus, the current study predicted CO2 emissions based on data from the agricultural sectors of 25 provinces in Iran. The gross domestic product (GDP), the square of the GDP (GDP(2)), energy use, and income inequality (Gini index) were used as the inputs. The study used support vector machine (SVM) models to predict CO2 emissions. Multiobjective algorithms (MOAs), such as the seagull optimization algorithm (MOSOA), salp swarm algorithm (MOSSA), bat algorithm (MOBA), and particle swarm optimization (MOPSO) algorithm, were used to perform three important tasks for improving the SVM models. Additionally, an inclusive multiple model (IMM) used the outputs of the MOSOA, MOSSA, MOBA, and MOPSO algorithms as the inputs for predicting CO2 emissions. It was observed that the best kernel function based on the SVM-MOSOA was the radial function. Additionally, the best input combination used all the gross domestic product (GDP), squared GDP (GDP(2)), energy use, and income inequality (Gini index) inputs. The results indicated that the quality of the obtained Pareto front based on the MOSOA was better than those of the other algorithms. Regarding the obtained results, the IMM model decreased the mean absolute errors of the SVM-MOSOA, SVM-MOSSA, SVM-MOBA, and SVM-PSO models by 24, 31, 69, and 76%, respectively, during the training stage. The current study showed that the IMM model was the best model for predicting CO2 emissions.", "journal": "ENVIRONMENTAL SCIENCE AND POLLUTION RESEARCH", "category": "Environmental Sciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000672607600003", "keywords": "Vessel anchoring pressure; Automatic identification system; Machine learning; Illegal anchoring area; Sustainable marine management", "title": "A comprehensive model of vessel anchoring pressure based on machine learning to support the sustainable management of the marine environments of coastal cities", "abstract": "The increased utilization of marine areas represents a significant challenge to the sustainable eco-environmental management of coastal cities. Machine learning, specifically the support-vector machine classification algorithm, was used to preprocess the massive Automatic identification System (AIS) dataset and extract anchoring vessels. Then, a comprehensive indicator evaluation model for anchoring pressure (CAPI) was constructed to evaluate the potential marine ecological pressure associated with anchoring vessels in the Bohai Sea. Spatial analysis was performed by geographic information system (GIS) to identify improper anchoring areas with high CAPI values. Finally, anchorage management in various coastal cities was assessed. The results showed that: (1) machine learning technology accurately identified anchoring vessels, (2) improper anchoring in the Bohai Sea is common, and (3) the management of anchoring activities is generally poor at boundaries between administrative regions. This study provides a rapid, feasible, and effective visualization method for marine environmental managers both theoretically and practically. The data mining method and CAPI model proposed here facilitate the management of vessel-related social issues in coastal cities, and they will help decision makers to quickly formulate targeted management measures to support the sustainable economic and environmental development of coastal cities.", "journal": "SUSTAINABLE CITIES AND SOCIETY", "category": "Construction & Building Technology; Green & Sustainable Science & Technology; Energy & Fuels", "annotated_keywords": ["machine learning", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000670305400002", "keywords": "Tetrastigma hemsleyanum; Dual-mode microscopic hyperspectral imager; Machine learning classification; Geographical origin; Medicinal variety", "title": "Machine learning classification of origins and varieties of Tetrastigma hemsleyanum using a dual-mode microscopic hyperspectral imager", "abstract": "A dual-mode microscopic hyperspectral imager (DMHI) combined with a machine learning algorithm for the purpose of classifying origins and varieties of Tetrastigma hemsleyanum (T. hemsleyanum) was developed. By switching the illumination source, the DMHI can operate in reflection imaging and fluorescence detection modes. The DMHI system has excellent performance with spatial and spectral resolutions of 27.8 lm and 3 nm, respectively. To verify the capability of the DMHI system, a series of classification experiments of T. hemsleyanum were conducted. Captured hyperspectral datasets were analyzed using principal component analysis (PCA) for dimensional reduction, and a support vector machine (SVM) model was used for classification. In reflection microscopic hyperspectral imaging (RMHI) mode, the classification accuracies of T. hemsleyanum origins and varieties were 96.3% and 97.3%, respectively, while in fluorescence microscopic hyperspectral imaging (FMHI) mode, the classification accuracies were 97.3% and 100%, respectively. Combining datasets in dual mode, excellent predictions of origin and variety were realized by the trained model, both with a 97.5% accuracy on a newly measured test set. The results show that the DMHI system is capable of T. hemsleyanum origin and variety classification, and has the potential for non-invasive detection and rapid quality assessment of various kinds of medicinal herbs. (C) 2021 Elsevier B.V. All rights reserved.", "journal": "SPECTROCHIMICA ACTA PART A-MOLECULAR AND BIOMOLECULAR SPECTROSCOPY", "category": "Spectroscopy", "annotated_keywords": ["machine learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000859422400001", "keywords": "Hydrothermal processes; Subcritical water; Solubility prediction; Machine learning", "title": "Prediction of the solubility of organic compounds in high-temperature water using machine learning", "abstract": "The estimation of the solubility of organic compounds in high-temperature water is important for designing chemical processes. This study aimed at predicting the solubility of organic compounds in high-temperature water in the range of 100-250 C using machine learning. The chemical structure of the organic compound was converted into 196 descriptors (parameters) using an open-source toolkit. The experimental solubility data were regressed using the descriptors, temperature, and water density. The regression methods of ordinary least squares, least absolute shrinkage and selection operator (Lasso), and support vector regression (SVR) were compared. A regression method combining the Lasso and SVR (Lasso + SVR) was developed. The model thus obtained this method was found to accurately predict the solubility of organic compounds in high-temperature water, with a root-mean-square error of 0.5. The findings in this study would be useful for predicting the sol-ubility of any organic compound in high-temperature water.", "journal": "JOURNAL OF SUPERCRITICAL FLUIDS", "category": "Chemistry, Physical; Engineering, Chemical", "annotated_keywords": ["machine learning", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000659651700001", "keywords": "SDN; Distributed Denial of Service attacks; Neighbourhood Component Analysis; machine learning", "title": "Machine Learning Approach Equipped with Neighbourhood Component Analysis for DDoS Attack Detection in Software-Defined Networking", "abstract": "The Software-Defined Network (SDN) is a new network paradigm that promises more dynamic and efficiently manageable network architecture for new-generation networks. With its programmable central controller approach, network operators can easily manage and control the whole network. However, at the same time, due to its centralized structure, it is the target of many attack vectors. Distributed Denial of Service (DDoS) attacks are the most effective attack vector to the SDN. The purpose of this study is to classify the SDN traffic as normal or attack traffic using machine learning algorithms equipped with Neighbourhood Component Analysis (NCA). We handle a public \"DDoS attack SDN Dataset\" including a total of 23 features. The dataset consists of Transmission Control Protocol (TCP), User Datagram Protocol (UDP), and Internet Control Message Protocol (ICMP) normal and attack traffics. The dataset, including more than 100 thousand recordings, has statistical features such as byte_count, duration_sec, packet rate, and packet per flow, except for features that define source and target machines. We use the NCA algorithm to reveal the most relevant features by feature selection and perform an effective classification. After preprocessing and feature selection stages, the obtained dataset was classified by k-Nearest Neighbor (kNN), Decision Tree (DT), Artificial Neural Network (ANN), and Support Vector Machine (SVM) algorithms. The experimental results show that DT has a better accuracy rate than the other algorithms with 100% classification achievement.", "journal": "ELECTRONICS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Physics, Applied", "annotated_keywords": ["machine learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000783091300001", "keywords": "Cloud computing; Data mining; Features selection (FS); Fog computing; Internet of things (IoT); Load forecasting; Smart grids", "title": "Enhancing the performance of smart electrical grids using data mining and fuzzy inference engine", "abstract": "This paper is about enhancing the smart grid by proposing a new hybrid feature-selection method called feature selection-based ranking (FSBR). In general, feature selection is to exclude non-promising features out from the collected data at Fog. This could be achieved using filter methods, wrapper methods, or a hybrid. Our proposed method consists of two phases: filter and wrapper phases. In the filter phase, the whole data go through different ranking techniques (i.e., relative weight ranking, effectiveness ranking, and information gain ranking) The results of these ranks are sent to a fuzzy inference engine to generate the final ranks. In the wrapper phase, data is being selected based on the final ranks and passed on three different classifiers (i.e., Naive Bayes, Support Vector Machine, and neural network) to select the best set of the features based on the performance of the classifiers. This process can enhance the smart grid by reducing the amount of data being sent to the cloud, decreasing computation time, and decreasing data complexity. Thus, the FSBR methodology enables the user load forecasting (ULF) to take a fast decision, the fast reaction in short-term load forecasting, and to provide a high prediction accuracy. The authors explain the suggested approach via numerical examples. Two datasets are used in the applied experiments. The first dataset reported that the proposed method was compared with six other methods, and the proposed method was represented the best accuracy of 91%. The second data set, the generalization data set, reported 90% accuracy of the proposed method compared to fourteen different methods.", "journal": "MULTIMEDIA TOOLS AND APPLICATIONS", "category": "Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000747617900001", "keywords": "depth information; image pyramid; cyclopean map; structural index; visual sensitivity", "title": "Rich Structural Index for Stereoscopic Image Quality Assessment", "abstract": "The human visual system (HVS), affected by viewing distance when perceiving the stereo image information, is of great significance to study of stereoscopic image quality assessment. Many methods of stereoscopic image quality assessment do not have comprehensive consideration for human visual perception characteristics. In accordance with this, we propose a Rich Structural Index (RSI) for Stereoscopic Image objective Quality Assessment (SIQA) method based on multi-scale perception characteristics. To begin with, we put the stereo pair into the image pyramid based on Contrast Sensitivity Function (CSF) to obtain sensitive images of different resolution. Then, we obtain local Luminance and Structural Index (LSI) in a locally adaptive manner on gradient maps which consider the luminance masking and contrast masking. At the same time we use Singular Value Decomposition (SVD) to obtain the Sharpness and Intrinsic Structural Index (SISI) to effectively capture the changes introduced in the image (due to distortion). Meanwhile, considering the disparity edge structures, we use gradient cross-mapping algorithm to obtain Depth Texture Structural Index (DTSI). After that, we apply the standard deviation method for the above results to obtain contrast index of reference and distortion components. Finally, for the loss caused by the randomness of the parameters, we use Support Vector Machine Regression based on Genetic Algorithm (GA-SVR) training to obtain the final quality score. We conducted a comprehensive evaluation with state-of-the-art methods on four open databases. The experimental results show that the proposed method has stable performance and strong competitive advantage.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000877295000070", "keywords": "Integrated circuits; Integrated circuit modeling; Reliability; Measurement; Cryptography; Physical unclonable function; Authentication; Hardware security; modeling attacks; physical unclonable function (PUF)", "title": "CaPUF: Cascaded PUF Structure for Machine Learning Resiliency", "abstract": "With the rise of the Internet of Things (IoT), resource-constrained and power-constrained devices attract more attention. The need for lightweight solutions as alternatives to resource-intensive applications became more urgent. Moreover, as the number of connected devices grew, authenticating them became more challenging. Traditionally, this would be performed by using hash functions and secure memory to store a key, which both come at a high cost. physical unclonable functions (PUFs) emerged as a suitable lightweight alternative to hash functions to authenticate the devices. Using the inherent minute differences between integrated circuits (ICs), they can generate IC-specific responses for input challenges coming from a so-called verifier. Through the years, machine learning (ML) has been used to attack PUFs by modeling them and accurately predicting their response to a given challenge. This stimulated research on ML-resilient PUFs. This resilience came with the significant area and challenge-to-response delay overheads. In this work, we introduce the novel cascaded PUF (CaPUF) and show that it is resilient against state-of-the-art ML-based attacks, i.e., logistic regression (LR) and support vector machines (SVMs). These attacks could not achieve accuracy better than 52% against our CaPUF, which is only as good as flipping a coin. Additionally, our CaPUF requires 89% less area compared to state-of-the-art ML-resilient PUFs.", "journal": "IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS", "category": "Computer Science, Hardware & Architecture; Computer Science, Interdisciplinary Applications; Engineering, Electrical & Electronic", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000875172300004", "keywords": "rock burst; rock mechanics; hierarchical prediction; analysis of variance; large sample data; Spearman correlation coefficient hypothesis test; Bayesian hyperparameter optimization; SVM discriminant analysis model", "title": "Computer-Aided Multiclass Classification of Corn from Corn Images Integrating Deep Feature Extraction", "abstract": "Corn has great importance in terms of production in the field of agriculture and animal feed. Obtaining pure corn seeds in corn production is quite significant for seed quality. For this reason, the distinction of corn seeds that have numerous varieties plays an essential role in marketing. This study was conducted with 14,469 images of BT6470, Calipso, Es_Armandi, and Hiva types of corn licensed by BIOTEK. The classification of images was carried out in three stages. At the first stage, deep feature extraction of the four types of corn images was performed with the pretrained CNN model SqueezeNet 1000 deep features were obtained for each image. In the second stage, in order to reduce these features obtained from deep feature extraction with SqueezeNet, separate feature selection processes were performed with the Bat Optimization (BA), Whale Optimization (WOA), and Gray Wolf Optimization (GWO) algorithms among optimization algorithms. Finally, in the last stage, the features obtained from the first and second stages were classified by using the machine learning methods Decision Tree (DT), Naive Bayes (NB), multi-class Support Vector Machine (mSVM), k-Nearest Neighbor (KNN), and Neural Network (NN). In the classification processes of the features obtained in the first stage, the mSVM model has achieved the highest classification success with 89.40%. In the second stage, as a result of the classifications performed through the active features selected by using three types of feature selection algorithms (BA, WOA, GWO), the classification success obtained with the mSVM model was 88.82%, 88.72%, and 88.95%, respectively. The classification accuracies of the tested methods and the classification accuracies obtained in the first stage are close to each other in terms of classification success. However, with the algorithms used in feature selection, successful classification processes have been carried out with fewer features and in a shorter time. The results of the study, in which classification was carried out in the inexpensive, the objective, and the shorter time of processing for the corn types, present a different perspective in terms of classification performance.", "journal": "COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE", "category": "Mathematical & Computational Biology; Neurosciences", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000777901700005", "keywords": "Text mining; multi-label classification; educational data mining; online education", "title": "Multi-label classification of feedbacks", "abstract": "This work deals with educational text mining, a field of natural language processing applied to education. The objective is to classify the feedback generated by teachers in online courses to the activities sent by students according to the model of Hattie and Timperley (2007), considering that feedback may be at the levels task, process, regulation, praise and other. Four multi-label classification methods of the data transformation approach - binary relevance, classification chains, power labelset and rakel-d - are compared with the base algorithms SVM, Random Forest, Logistic Regression and Naive Bayes. The methodology was applied to a case study in which 11013 feedbacks written in Spanish language from 121 online courses of the Law degree from a public university in Mexico were collected from the Blackboard learning manager system. The results show that the random forests algorithms and vector support machines will have the best performance when using the binary relevance transformation and classifier chains methods.", "journal": "JOURNAL OF INTELLIGENT & FUZZY SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["natural language processing"], "label": "1", "title_label": "1"}
{"id": "WOS:000773229200001", "keywords": "Social networking (online); Sentiment analysis; Feature extraction; Monitoring; Earthquakes; Deep learning; Support vector machines; AI-based disaster monitoring dashboard; artificial intelligence (AI); automated location extraction; disaster intelligence mobile app; named entity recognition (NER); sentiment analysis", "title": "Automated Disaster Monitoring From Social Media Posts Using AI-Based Location Intelligence and Sentiment Analysis", "abstract": "Worldwide disasters like bushfires, earthquakes, floods, cyclones, and heatwaves have affected the lives of social media users in an unprecedented manner. They are constantly posting their level of negativity over the disaster situations at their location of interest. Understanding location-oriented sentiments about disaster situation is of prime importance for political leaders, and strategic decision-makers. To this end, we present a new fully automated algorithm based on artificial intelligence (AI) and natural language processing (NLP), for extraction of location-oriented public sentiments on global disaster situation. We designed the proposed system to obtain exhaustive knowledge and insights on social media feeds related to disaster in 110 languages through AI- and NLP-based sentiment analysis, named entity recognition (NER), anomaly detection, regression, and Getis Ord Gi* algorithms. We deployed and tested this algorithm on live Twitter feeds from 28 September to 6 October 2021. Tweets with 67 515 entities in 39 different languages were processed during this period. Our novel algorithm extracted 9727 location entities with greater than 70% confidence from live Twitter feed and displayed the locations of possible disasters with disaster intelligence. The rates of average precision, recall, and F &#x2081;-Score were measured to be 0.93, 0.88, and 0.90, respectively. Overall, the fully automated disaster monitoring solution demonstrated 97% accuracy. To the best of our knowledge, this study is the first to report location intelligence with NER, sentiment analysis, regression and anomaly detection on social media messages related to disasters and has covered the largest set of languages.", "journal": "IEEE TRANSACTIONS ON COMPUTATIONAL SOCIAL SYSTEMS", "category": "Computer Science, Cybernetics; Computer Science, Information Systems", "annotated_keywords": ["artificial intelligen", "natural language processing"], "label": "1", "title_label": "1"}
{"id": "WOS:000652835600045", "keywords": "Streamflow forecasting; Input variable selection; Binary metaheuristic algorithm; Wrapper method; Machine learning", "title": "Examination and comparison of binary metaheuristic wrapper-based input variable selection for local and global climate information-driven one-step monthly streamflow forecasting", "abstract": "The use of data-driven models to forecast streamflow has received substantial attention from scholars in recent years. However, systematic studies have not been performed to examine binary metaheuristic wrapper-based input variable selection (BMWIVS) in real-world streamflow forecasting. In this study, we explored binary metaheuristic-based shallow machine learning wrappers for one-step monthly streamflow forecasting using local weather information and global climate indices from three catchments with different hydroclimatic conditions. First, the maximal information coefficient (MIC) was employed to investigate the correlations among the forecasting target, streamflow and candidate input variables, which included both local and global climate information. Then, the BMWIVS models obtained by combining eight binary metaheuristic algorithms, five commonly used shallow machine learning algorithms, two combined filter-based input variable selection (FIVS) methods, and two forecasting methods were examined. Finally, the performance of each model was compared with the performance of typical benchmark models, including the univariate seasonal autoregressive integrated moving average model, five machine learning algorithms with no input variable selection, and five machine learning algorithms that use five different FIVS methods. The experimental results emphasized three significant findings. First, an appropriate input variable selection method should be selected in practice because several examined wrappers were inferior to the benchmark models. Second, the BMWIVS model that combined the regularized extreme learning machine method, binary gray wolf optimizer, FIVS results-based initialization method, and forecasted values averaged over multiple runs yielded the best performance in the three cases studied. Third, the correlations in terms of the MIC between the global climate indices and streamflow were lower than those between local weather information and streamflow, and the best wrapper and FIVS would select more local weather information variables than global climate index variables, which suggests that global climate information can be complementary to local weather information for one-step monthly streamflow forecasting. These findings have remarkable practical applications for forecasting monthly streamflow.", "journal": "JOURNAL OF HYDROLOGY", "category": "Engineering, Civil; Geosciences, Multidisciplinary; Water Resources", "annotated_keywords": ["machine learning", "learning algorithm"], "label": "1", "title_label": "0"}
{"id": "WOS:000680792500001", "keywords": "Intelligent algorithms; Processing parameters; Heavy-duty CNC machine tools; Neural network algorithms", "title": "Optimization method of machining parameters based on intelligent algorithm", "abstract": "The processing parameters have a particularly significant impact on the quality and efficiency of processing. Selecting the correct processing parameters can greatly improve the processing performance of the machine tool. To this end, by improving the chromosome structure and genetic operators of the GA algorithm, a new GA-BP neural network algorithm is proposed and combined BP neural network method for adaptive crossover and mutation probability optimization. Then, through comparison experiments. After selecting a certain type of CNC EDM machine, find its standard process parameter table and select 50 groups of data as preparation. 30 groups of data are randomly sampled from the inside to serve as training sample data, and the remaining 20 groups serve as performance test samples. Experimental results show that the prediction accuracy of the new algorithm is higher than that of the conventional algorithm, pulse width or peak current. The new prediction results are often closer to the true value, and the prediction accuracy is higher, which can better meet the processing requirements.", "journal": "DISTRIBUTED AND PARALLEL DATABASES", "category": "Computer Science, Information Systems; Computer Science, Theory & Methods", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000784336800004", "keywords": "Long and short -term memory networks; Random forest; Random decrement technique; Damage detection; Thermoplastic composite pipes", "title": "Machine learning methods for damage detection of thermoplastic composite pipes under noise conditions", "abstract": "Machine learning methods for damage detection of thermoplastic composite pipes (TCPs) under noise conditions are presented, which combine the random decrement technique (RDT) with random forest (RF) and long and short-term memory (LSTM) networks. RDT is applied first to process the measured noisy strain response data of the TCP under random excitation. Then the RF or LSTM method is used to conduct the damage localization and severity estimation of the pipe. The applicability of the proposed methods is verified by means of numerical and experimental studies. The numerical example consists of a TCP subjected to internal pressure and random wave excitation considering several noise levels. The damages are simulated as circular holes on different layers of the pipe and varying severity, characterized by their radii and depths. The damage detection is carried out using RDT-RF and RDT-LSTM methods. The experimental studies consist of laboratory tests of a TCP model using Fiber Bragg Grating sensors. The damage cases, simulated as cracks with different lengths and depths on the rein-forcement layer, are discussed. Both the numerical simulation and experimental tests show that the proposed RDT-RF and RDT-LSTM methods have an excellent performance in damage detection of TCPs.", "journal": "OCEAN ENGINEERING", "category": "Engineering, Marine; Engineering, Civil; Engineering, Ocean; Oceanography", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000645317400001", "keywords": "5G; enhanced mobile broadband (eMBB); neural networks; non-orthogonal multiple access (NOMA); supervised learning; sparse code multiple access (SCMA); singular value decomposition (SVD); uplink channel biointerfaces", "title": "Improving Spectral Efficiency in the SCMA Uplink Channel", "abstract": "The Third Generation Partnership Project (3GPP) and the International Telecommunication Union (ITU) identified the technical requirements that the fifth generation of mobile communications networks (5G) had to meet; within these parameters are the following: an improved data rate and a greater number of users connected simultaneously. 5G uses non-orthogonal multiple access (NOMA) to increase the number of simultaneously connected users, and by encoding data it is possible to increase the spectral efficiency (SE). In this work, eight codewords are used to transmit three bits simultaneously using Sparse Code Multiple Access (SCMA), and through singular value decomposition (SVD) the Euclidean distance between constellation points is optimized. On the other hand, applications of machine intelligence and machine intelligence in 5G and beyond communication systems are still developing; in this sense, in this work we propose to use machine learning for detecting and decoding the SCMA codewords using neural networks. This paper focuses on the Use Case of enhanced mobile broadband (eMBB), where higher data rates are required, with a large number of users connected and low mobility. The simulation results show that it is possible to transmit three bits simultaneously with a low bit error rate (BER) using SVD-SCMA in the uplink channel. Our simulation results were compared against recent methods that use spatial modulation (SM) and antenna arrays in order to increase spectral efficiency. In adverse Signal-to-Noise Ratio (SNR), our proposal performs better than SM, and antenna arrays are not needed for transmission or reception.", "journal": "MATHEMATICS", "category": "Mathematics", "annotated_keywords": ["neural net", "machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000703705300006", "keywords": "Marsh vegetation classification; Backscattering coefficient; Polarimetric decomposition parameters; Multi-scale inheritance segmentation; Variable selection; Random forest algorithm", "title": "Synergy of multi-temporal polarimetric SAR and optical image satellite for mapping of marsh vegetation using object-based random forest algorithm", "abstract": "The accurate classification of marsh vegetation is an important prerequisite for wetland management and pro-tection. In this study, the Honghe National Nature Reserve was used as the research area. The VV and VH polarized backscattering coefficients of Sentinel-1B, the polarimetric decomposition parameters of Sentinel-1B, and Sentinel-2A multi-spectral images from June and September were selected to construct 18 multi-dimensional data sets. A highly correlated variable elimination algorithm, a recursive feature elimination vari-able selection algorithm (RFE-RF), and an optimized random forest algorithm (RF) were used to construct a marsh vegetation identification model. In this study, we searched for an RF model to achieve the accurate classification of marsh vegetation and find the best feature for identifying various types of vegetation. Addi-tionally, the applicability of different optimized RF models to the task of the identification of wetland vegetation and the stability of the identification of marsh vegetation using different classification models were quantita-tively analyzed. The results show the following: (1) RFE-RF variable selection and RF parameter optimization can reduce the data dimensionality, improve the accuracy and stability of the wetland vegetation classification model, and achieve a training accuracy of up to 85.39%. (2) The RF model integrating multi-spectral data, backscattering coefficients, and polarimetric decomposition parameters for June and September can obtain the highest overall accuracy (91.16%), and the model has the strongest applicability. (3) The importance of multi-spectral variables in wetland vegetation classification is higher than that of backscattering coefficients and polarimetric decomposition parameters. The visible bands and vegetation index are the most important vari-ables, while the cross-polarized backscattering coefficient (Mean_VH), polarimetric decomposition eigenvalue (Mean_l1, Mean_l2), and calculated eigenvalues of the matrix (Mean_lambda) are the backscattering coefficient features and polarimetric decomposition parameters with the highest contributions. (4) The modified normalized difference water index in June (MNDWI_ Jun), blue band in September (Mean_B_Sep), location feature pixel coordinates (Y_Max_Pxl), and ratio vegetation index in September (RVI_Sep) have the highest contribution to the identification and classification of deep-water marsh vegetation, shallow-water marsh vegetation, forest, and shrubs, respectively. (5) The identification of forest is the strongest, and the classification accuracy for shrubs and deep-water marsh vegetation is greatly affected by the combination of time phase and data sources.", "journal": "ECOLOGICAL INDICATORS", "category": "Biodiversity Conservation; Environmental Sciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000803028500001", "keywords": "diabetic retinopathy; diabetic macular edema; Th17 cell; bioinformatic analysis; biomarker", "title": "Gene Biomarkers Related to Th17 Cells in Macular Edema of Diabetic Retinopathy: Cutting-Edge Comprehensive Bioinformatics Analysis and In Vivo Validation", "abstract": "BackgroundPrevious studies have shown that T-helper 17 (Th17) cell-related cytokines are significantly increased in the vitreous of proliferative diabetic retinopathy (PDR), suggesting that Th17 cells play an important role in the inflammatory response of diabetic retinopathy (DR), but its cell infiltration and gene correlation in the retina of DR, especially in diabetic macular edema (DME), have not been studied. MethodsThe dataset GSE160306 was downloaded from the Gene Expression Omnibus (GEO) database, which contains 9 NPDR samples and 10 DME samples. ImmuCellAI algorithm was used to estimate the abundance of Th17 cells in 24 kinds of infiltrating immune cells. The differentially expressed Th17 related genes (DETh17RGs) between NPDR and DME were documented by difference analysis and correlation analysis. Through aggregate analyses such as gene ontology (GO) and Kyoto Encyclopedia of Gene and Genome (KEGG) pathway enrichment analysis, a protein-protein interaction (PPI) network was constructed to analyze the potential function of DETh17RGs. CytoHubba plug-in algorithm, Lasso regression analysis and support vector machine recursive feature elimination (SVM-RFE) were implemented to comprehensively identify Hub DETh17RGs. The expression archetypes of Hub DETh17RGs were further verified in several other independent datasets related to DR. The Th17RG score was defined as the genetic characterization of six Hub DETh17RGs using the GSVA sample score method, which was used to distinguish early and advanced diabetic nephropathy (DN) as well as normal and diabetic nephropathy. Finally, real-time quantitative PCR (qPCR) was implemented to verify the transcription levels of Hub DETh17RGs in the STZ-induced DR model mice (C57BL/6J). Results238 DETh17RGs were identified, of which 212 genes were positively correlated while only 26 genes were negatively correlated. Six genes (CD44, CDC42, TIMP1, BMP7, RHOC, FLT1) were identified as Hub DETh17RGs. Because DR and DN have a strong correlation in clinical practice, the verification of multiple independent datasets related to DR and DN proved that Hub DETh17RGs can not only distinguish PDR patients from normal people, but also distinguish DN patients from normal people. It can also identify the initial and advanced stages of the two diseases (NPDR vs DME, Early DN vs Advanced DN). Except for CDC42 and TIMP1, the qPCR transcription levels and trends of other Hub DETh17RGs in STZ-induced DR model mice were consistent with the human transcriptome level in this study. ConclusionThis study will improve our understanding of Th17 cell-related molecular mechanisms in the progression of DME. At the same time, it also provides an updated basis for the molecular mechanism of Th17 cell crosstalk in the eye and kidney in diabetes.", "journal": "FRONTIERS IN IMMUNOLOGY", "category": "Immunology", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000637704500001", "keywords": "Machine intelligence; Image processing", "title": "Hyperspectral imaging for underwater object detection", "abstract": "Purpose This paper aims to demonstrate the principle and practical applications of hyperspectral object detection, carry out the problem we now face and the possible solution. Also some challenges in this field are discussed. Design/methodology/approach First, the paper summarized the current research status of the hyperspectral techniques. Then, the paper demonstrated the development of underwater hyperspectral techniques from three major aspects, which are UHI preprocess, unmixing and applications. Finally, the paper presents a conclusion of applications of hyperspectral imaging and future research directions. Findings Various methods and scenarios for underwater object detection with hyperspectral imaging are compared, which include preprocessing, unmixing and classification. A summary is made to demonstrate the application scope and results of different methods, which may play an important role in the application of underwater hyperspectral object detection in the future. Originality/value This paper introduced several methods of hyperspectral image process, give out the conclusion of the advantages and disadvantages of each method, then demonstrated the challenges we face and the possible way to deal with them.", "journal": "SENSOR REVIEW", "category": "Instruments & Instrumentation", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000861278100002", "keywords": "Housing price; Value estimation; Spatio-temporal modelling; Machine learning; Australia", "title": "Housing price prediction incorporating spatio-temporal dependency into machine learning algorithms", "abstract": "Conventional housing price prediction methods rarely consider the spatiotemporal non-stationary problem in a large data volumes. In this study, four machine learning (ML) models are used to explore the impacts of various features - i.e., property attributes and neighborhood quality -on housing price variations at different geographical scales. Using a 32-year (1984-2016) housing price dataset of Metropolitan Adelaide, Australia, this research relies on 428,000 sale transaction records and 38 explanatory variables. It is shown that non-linear tree -based models, such as Decision Tree, have perform better than linear models. In addition, ensemble machine learning techniques, such as Gradient-Boosting and Random Forest, are better at predicting future housing prices. A spatiotemporal lag (ST-lag) variable was added to improve the prediction accuracy of the models. The study demonstrates that ST-lag (or similar spatio-temporal indicator) can be a useful moderator of spatio-temporal effects in ML applications. This paper will serve as a catalyst for future research into the dynamics of the Australian property market, utilizing the benefits of cutting-edge technologies to develop models for business and property valuation at various geographical levels.", "journal": "CITIES", "category": "Urban Studies", "annotated_keywords": ["machine learning", "machine learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000757420100006", "keywords": "ECG; Culprit lesion; ACS; Machine learning; Features selection; Dimensionality reduction", "title": "Novel ECG features and machine learning to optimize culprit lesion detection in patients with suspected acute coronary syndrome", "abstract": "Background: Novel temporal-spatial features of the 12-lead ECG can conceptually optimize culprit lesions' detection beyond that of classical ST amplitude measurements. We sought to develop a data-driven approach for ECG feature selection to build a clinically relevant algorithm for real-time detection of culprit lesion. Methods: This was a prospective observational cohort study of chest pain patients transported by emergency medical services to three tertiary care hospitals in the US. We obtained raw 10-s, 12-lead ECGs (500 s/s, HeartStart MRx, Philips Healthcare) during prehospital transport and followed patients 30 days after the encounter to adjudicate clinical outcomes. A total of 557 global and lead-specific features of P-QRS-T waveform were harvested from the representative average beats. We used Recursive Feature Elimination and LASSO to identify 35/557, 29/557, and 51/557 most recurrent and important features for LAD, LCX, and RCA culprits, respectively. Using the union of these features, we built a random forest classifier with 10-fold cross-validation to predict the presence or absence of culprit lesions. We compared this model to the performance of a rule-based commercial proprietary software (Philips DXL ECG Algorithm). Results: Our sample included 2400 patients (age 59 +/- 16, 47% female, 41% Black, 10.7% culprit lesions). The area under the ROC curves of our random forest classifier was 0.85 +/- 0.03 with sensitivity, specificity, and negative predictive value of 71.1%, 84.7%, and 96.1%. This outperformed the accuracy of the automated interpretation software of 37.2%, 95.6%, and 92.7%, respectively, and corresponded to a net reclassification improvement index of 23.6%. Metrics of ST80; Tpeak-Tend; spatial angle between QRS and T vectors; PCA ratio of STT waveform; T axis; and QRS waveform characteristics played a significant role in this incremental gain in performance. Conclusions: Novel computational features of the 12-lead ECG can be used to build clinically relevant machine learning-based classifiers to detect culprit lesions, which has important clinical implications. (C) 2021 Elsevier Inc. All rights reserved.", "journal": "JOURNAL OF ELECTROCARDIOLOGY", "category": "Cardiac & Cardiovascular Systems", "annotated_keywords": ["machine learning", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000897574600001", "keywords": "investment export; gross national product; normality of distribution; oil price forecasting; budget revenues; oil and gas impact", "title": "Energy stability and decarbonization in developing countries: Random Forest approach for forecasting of crude oil trade flows and macro indicators", "abstract": "The paper observes the dependence of the main macroeconomic indicators in developing countries from the change in world prices for crude oil. We analyzed a system of simultaneous equations, which makes it possible to verify some of these hypotheses, and developed the model to forecast the impact of oil prices on budget revenues. The practical significance of this work lies in the structuring of existing knowledge on the impact of oil crisis. The results of this work can be considered confirmation of the hypothesis of the sensitivity of U.S. macroeconomic indicators to the dynamics of oil prices. Outcomes assume stable growth even in the period of shock prices for oil, which is confirmed by the statistics that were used in the model. Deep decarbonization modeling is a trend in industrial facilities that are used by developing countries. The major challenge is the issue of availability that is applicable to the countries that want to utilize this facility in their communities. Industrial modeling toward decarbonization is now a developing mechanism to curb the growing issue of atmospheric pollution. This paper proves the relevance of promoting deep decarbonization applied by the developing countries.", "journal": "FRONTIERS IN ENVIRONMENTAL SCIENCE", "category": "Environmental Sciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000640638000003", "keywords": "Trajectory; Task analysis; Sensors; Robots; Tactile sensors; Deformable models; Reinforcement learning; Deformable object manipulation; movement primitive; perceptual coupling; real robot learning; reinforcement learning; tactile sensing", "title": "Discovery of temperature-induced stability reversal in perovskites using high-throughput robotic learning", "abstract": "Stability of perovskite-based photovoltaics remains a topic requiring further attention. Cation engineering influences perovskite stability, with the present-day understanding of the impact of cations based on accelerated ageing tests at higher-than-operating temperatures (e.g. 140 degrees C). By coupling high-throughput experimentation with machine learning, we discover a weak correlation between high/low-temperature stability with a stability-reversal behavior. At high ageing temperatures, increasing organic cation (e.g. methylammonium) or decreasing inorganic cation (e.g. cesium) in multi-cation perovskites has detrimental impact on photo/thermal-stability; but below 100 degrees C, the impact is reversed. The underlying mechanism is revealed by calculating the kinetic activation energy in perovskite decomposition. We further identify that incorporating at least 10 mol.% MA and up to 5 mol.% Cs/Rb to maximize the device stability at device-operating temperature (<100<degrees>C). We close by demonstrating the methylammonium-containing perovskite solar cells showing negligible efficiency loss compared to its initial efficiency after 1800 hours of working under illumination at 30 degrees C. Current view of the impact of A-site cation on the stability of perovskite materials and devices is derived from accelerated ageing tests at high temperature, which is beyond normal operation range. Here, the authors reveal the great impact of ageing condition on assessing the photothermal stability of mixed-cation perovskites using high-throughput robot system coupled with machine learning.", "journal": "NATURE COMMUNICATIONS", "category": "Multidisciplinary Sciences", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000615472800001", "keywords": "heritage; 3D survey; H-BIM; point cloud; classification; semantic annotation; machine learning; Random Forest; laser scanning; photogrammetry", "title": "From the Semantic Point Cloud to Heritage-Building Information Modeling: A Semiautomatic Approach Exploiting Machine Learning", "abstract": "This work presents a semi-automatic approach to the 3D reconstruction of Heritage-Building Information Models from point clouds based on machine learning techniques. The use of digital information systems leveraging on three-dimensional (3D) representations in architectural heritage documentation and analysis is ever increasing. For the creation of such repositories, reality-based surveying techniques, such as photogrammetry and laser scanning, allow the fast collection of reliable digital replicas of the study objects in the form of point clouds. Besides, their output is raw and unstructured, and the transition to intelligible and semantic 3D representations is still a scarcely automated and time-consuming process requiring considerable human intervention. More refined methods for 3D data interpretation of heritage point clouds are therefore sought after. In tackling these issues, the proposed approach relies on (i) the application of machine learning techniques to semantically label 3D heritage data by identification of relevant geometric, radiometric and intensity features, and (ii) the use of the annotated data to streamline the construction of Heritage-Building Information Modeling (H-BIM) systems, where purely geometric information derived from surveying is associated with semantic descriptors on heritage documentation and management. The \"Grand-Ducal Cloister\" dataset, related to the emblematic case study of the Pisa Charterhouse, is discussed.", "journal": "REMOTE SENSING", "category": "Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000870110000001", "keywords": "Pediatric pneumonia; Chest X-rays; Computer-aided diagnosis; Contrast limited adaptive histogram equalization; Deep learning; Transfer learning; Stacking classifier; Stratified K-fold", "title": "Pediatric pneumonia diagnosis using stacked ensemble learning on multi-model deep CNN architectures", "abstract": "Pediatric pneumonia has drawn immense awareness due to the high mortality rates over recent years. The acute respiratory infection caused by bacteria, viruses, or fungi infects the lung region and hinders oxygen transport, making breathing difficult due to inflamed or pus and fluid-filled alveoli. Being non-invasive and painless, chest X-rays are the most common modality for pediatric pneumonia diagnosis. However, the low radiation levels for diagnosis in children make accurate detection challenging. This challenge initiates the need for an unerring computer-aided diagnosis model. Our work proposes Contrast Limited Adaptive Histogram Equalization for image enhancement and a stacking classifier based on the fusion of deep learning-based features for pediatric pneumonia diagnosis. The extracted features from the global average pooling layers of the fine-tuned MobileNet, DenseNet121, DenseNet169, and DenseNet201 are concatenated for the final classification using a stacked ensemble classifier. The stacking classifier uses Support Vector Classifier, Nu-SVC, Logistic Regression, K-Nearest Neighbor, Random Forest Classifier, Gaussian Naive Bayes, AdaBoost classifier, Bagging Classifier, and Extratrees Classifier for the first stage, and Nu-SVC as the meta-classifier. The stacking classifier validated using Stratified K-Fold cross-validation achieves an accuracy of 98.62%, precision of 98.99%, recall of 99.53%, F1 score of 99.26%, and an AUC score of 93.17% on the publicly available pediatric pneumonia dataset. We expect this model to greatly help the real-time diagnosis of pediatric pneumonia.", "journal": "MULTIMEDIA TOOLS AND APPLICATIONS", "category": "Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000815937900002", "keywords": "Yield prediction; Multiple indicators; Machine learning; Optimal lead time; Spatial autocorrelation", "title": "Combining multi-indicators with machine-learning algorithms for maize at the-level in China", "abstract": "The accurate and timely prediction of crop yield at a large scale is important for food security and the development of agricultural policy. An adaptable and robust method for estimating maize yield for the entire territory of China, however, is currently not available. The inherent trade-off between early estimates of yield and the accuracy of yield prediction also remains a confounding issue. To explore these challenges, we employ indicators such as GPP, ET, surface temperature (Ts), LAI, soil properties and maize phenological information with random forest regression (RFR) and gradient boosting decision tree (GBDT) machine learning approaches to provide maize yield estimates within China. The aims were to: (1) evaluate the accuracy of maize yield prediction obtained from multimodal data analysis using machine-learning; (2) identify the optimal period for estimating yield; and (3) determine the spatial robustness and adaptability of the proposed method. The results can be summarized as: (1) RFR estimated maize yield more accurately than GBDT; (2) Ts was the best single indicator for estimating yield, while the combination of GPP, Ts, ET and LAI proved best when multi-indicators were used (R-2 = 0.77 and rRMSE = 16.15% for the RFR); (3) the prediction accuracy was lower with earlier lead time but remained relatively high within at least 24 days before maturity (R-2 > 0.77 and rRMSE < 16.92%); and (4) combining machine-learning algorithms with multi-indicators demonstrated a capacity to cope with the spatial heterogeneity. Overall, this study provides a reliable reference for managing agricultural production.", "journal": "AGRICULTURAL AND FOREST METEOROLOGY", "category": "Agronomy; Forestry; Meteorology & Atmospheric Sciences", "annotated_keywords": ["machine learning", "learning algorithm", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000707419500085", "keywords": "Indeterminate thyroid nodule; Machine learning; Bioinformatics; Thyroid cancer; Risk assessment; Fine needle aspiration biopsy", "title": "Estimation of current and post-treatment retinal function in chronic central serous chorioretinopathy using artificial intelligence", "abstract": "Refined understanding of the association of retinal microstructure with current and future (post-treatment) function in chronic central serous chorioretinopathy (cCSC) may help to identify patients that would benefit most from treatment. In this post-hoc analysis of data from the prospective, randomized PLACE trial (NCT01797861), we aimed to determine the accuracy of AI-based inference of retinal function from retinal morphology in cCSC. Longitudinal spectral-domain optical coherence tomography (SD-OCT) data from 57 eyes of 57 patients from baseline, week 6-8 and month 7-8 post-treatment were segmented using deep-learning software. Fundus-controlled perimetry data were aligned to the SD-OCT data to extract layer thickness and reflectivity values for each test point. Point-wise retinal sensitivity could be inferred with a (leave-one-out) cross-validated mean absolute error (MAE) [95% CI] of 2.93 dB [2.40-3.46] (scenario 1) using random forest regression. With addition of patient-specific baseline data (scenario 2), retinal sensitivity at remaining follow-up visits was estimated even more accurately with a MAE of 1.07 dB [1.06-1.08]. In scenario 3, month 7-8 post-treatment retinal sensitivity was predicted from baseline SD-OCT data with a MAE of 3.38 dB [2.82-3.94]. Our study shows that localized retinal sensitivity can be inferred from retinal structure in cCSC using machine-learning. Especially, prediction of month 7-8 post-treatment sensitivity with consideration of the treatment as explanatory variable constitutes an important step toward personalized treatment decisions in cCSC.", "journal": "SCIENTIFIC REPORTS", "category": "Multidisciplinary Sciences", "annotated_keywords": ["artificial intelligen"], "label": "1", "title_label": "1"}
{"id": "WOS:000795691800004", "keywords": "Global land ecosystems; Carbon exchange; Eddy covariance; Long gaps; Robust gap-filling", "title": "Stable gap-filling for longer eddy covariance data gaps: A globally validated machine-learning approach for carbon dioxide, water, and energy fluxes", "abstract": "Continuous time-series of CO2, water, and energy fluxes are useful for evaluating the impacts of climate-change and management on ecosystems. The eddy covariance (EC) technique can provide continuous, direct measurements of ecosystem fluxes, but to achieve this gaps in data must be filled. Research-standard methods of gapfilling fluxes have tended to focus on CO2 fluxes in temperate forests and relatively short gaps of less than two weeks. A gap-filling method applicable to other fluxes and capable of filling longer gaps is needed. To address this challenge, we propose a novel gap-filling approach, Random Forest Robust (RFR). RFR can accommodate a wide range of data gap sizes, multiple flux types (i.e. CO2, water and energy fluxes). We configured RFR using either three (RFR3) or ten (RFR10) driving variables. RFR was tested globally on fluxes of CO2, latent heat (LE), and sensible heat (H) from 94 suitable FLUXNET2015 sites by using artificial gaps (from 1 to 30 days in length) and benchmarked against the standard marginal distribution sampling (MDS) method. In general, RFR improved on MDS's R2 by 15% (RFR3) and by 30% (RFR10) and reduced uncertainty by 70%. RFR's improvements in R2 for H and LE were more than twice the improvement observed for CO2 fluxes. Unlike MDS, RFR performed well for longer gaps; for example, the R2 of RFR methods in filling 30-day gaps dropped less than 4% relative to 1-day gaps, while the R2 of MDS dropped by 21%. Our results indicate that the RFR method can provide improved gap-filling of CO2, H and LE flux timeseries. Such improved continuous flux measurements, with low bias, can enhance our understanding of the impacts of climate-change and management on ecosystems globally.", "journal": "AGRICULTURAL AND FOREST METEOROLOGY", "category": "Agronomy; Forestry; Meteorology & Atmospheric Sciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000765230900001", "keywords": "invasive species; climate change scenarios; partial ROC; ensemble forecast; Kriging; spatial bias", "title": "Modelling the biological invasion of Prosopis juliflora using geostatistical-based bioclimatic variables under climate change in arid zones of southwestern Iran", "abstract": "Invasive species have been the focus of ecologists due to their undesired impacts on the environment. The extent and rapid increase in invasive plant species is recognized as a natural cause of global-biodiversity loss and degrading ecosystem services. Biological invasions can affect ecosystems across a wide spectrum of bioclimatic conditions. Understanding the impact of climate change on species invasion is crucial for sustainable biodiversity conservation. In this study, the possibility of mapping the distribution of invasive Prosopis juliflora (Swartz) DC. was shown using present background data in Khuzestan Province, Iran. After removing the spatial bias of background data by creating weighted sampling bias grids for the occurrence dataset, we applied six modelling algorithms (generalized additive model (GAM), classification tree analysis (CTA), random forest (RF), multivariate adaptive regression splines (MARS), maximum entropy (MaxEnt) and ensemble model) to predict invasion distribution of the species under current and future climate conditions for both optimistic (RCP 2.6) and pessimistic (RCP 8.5) scenarios for the years 2050 and 2070, respectively. Predictor variables including weighted mean of CHELSA (climatologies at high resolution for the Earth's land surface areas)-bioclimatic variables and geostatistical-based bioclimatic variables (1979-2020), physiographic variables extracted from shuttle radar topography mission (SRTM) and some human factors were used in modelling process. To avoid causing a biased selection of predictors or model coefficients, we resolved the spatial autocorrelation of presence points and multi-collinearity of the predictors. As in a conventional receiver operating characteristic (ROC), the area under curve (AUC) is calculated using presence and absence observations to measure the probability and the two error components are weighted equally. All models were evaluated using partial ROC at different thresholds and other statistical indices derived from confusion matrix. Sensitivity analysis showed that mean diurnal range (Bio2) and annual precipitation (Bio12) explained more than 50% of the changes in the invasion distribution and played a pivotal role in mapping habitat suitability of P. juliflora. At all thresholds, the ensemble model showed a significant difference in comparison with single model. However, MaxEnt and RF outperformed the others models. Under climate change scenarios, it is predicted that suitable areas for this invasive species will increase in Khuzestan Province, and increasing climatically suitable areas for the species in future will facilitate its future distribution. These findings can support the conservation planning and management efforts in ecological engineering and be used in formulating preventive measures.", "journal": "JOURNAL OF ARID LAND", "category": "Environmental Sciences", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000722372300001", "keywords": "climate change; crop rotation; geostatistics; multiapproach; machine learning; suitability; soil properties", "title": "Optimized Land Use through Integrated Land Suitability and GIS Approach in West El-Minia Governorate, Upper Egypt", "abstract": "Land evaluation is imperative for its efficient use in agriculture. Therefore, this study aimed at assessing the suitability of a region in West El-Minia for cultivating some of the major crops using the geographical information system (GIS). The results focus on allocating space for cultivating sugar beet and utilizing the free period of sugar beet in other crops. This exploitation helps to maintain the quality of the land and increase its fertility by using crop rotation with integrated agricultural management. A machine learning technique was implemented using the random forest algorithm (RF) to predict soil suitability classes for sugar beet using geomorphology, terrain attribute and remote sensing data. Fifteen major crops were evaluated using a suitability multicriteria approach in GIS environment for crop rotation decisions. Soil parameters were determined (soil depth, pH, texture, CaCO3, drainage, ECe, and slope) to characterize the land units for soil suitability. Soils of the area were found to be Entisols; Typic Torrifluvents, Typic Torripsamments and Typic Torriorthents and Aridsols; Typic Haplocacids, Calcic Haplosalids and Sodic Haplocalcids. Overall, the studied area was classified into four suitability classes: high \"S1 \", moderate \"S2 \", marginal \"S3 \", and not suitable \"N \". The area of each suitability class changed depending on the crop tested. The highest two crops that occupied S1 class were barley with 471.5 ha (representing 6.8% of the total study area) and alfalfa with 157.4 ha (2.3%). In addition, barley, sugar beet, and sorghum occupied the highest areas in S2 class with 6415.3 ha (92.5%), 6111.3 ha (88.11%) and 6111.3 ha (88.1%), respectively. Regarding the S3 class, three different crops (sesame, green pepper, and maize) were the most highly represented by 6151.8 ha (88.7%), 6126.3 ha (88.3%), and 6116.7 ha (88.2%), respectively. In the end, potato and beans occupied the highest areas in N class with 6916.9 ha (99.7%) and 6853.5 ha (98.8%), respectively. The results revealed that the integration of GIS and soil suitability system consists of an appropriate approach for the evaluation of suitable crop rotations for optimized land use planning and to prevent soil degradation. The study recommends using crop rotation, as it contributes to soil sustainability and the control of plant pests and diseases, where the succession of agricultural crops on a scientific basis aims at maintaining the balance of nutrients and fertilizers in the soil.", "journal": "SUSTAINABILITY", "category": "Green & Sustainable Science & Technology; Environmental Sciences; Environmental Studies", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000604616100010", "keywords": "Coronavirus; contrast enhancement; deep learning; features optimization; fusion; classification", "title": "Classification of Positive COVID-19 CT Scans Using Deep Learning", "abstract": "In medical imaging, computer vision researchers are faced with a variety of features for verifying the authenticity of classifiers for an accurate diagnosis. In response to the coronavirus 2019 (COVID-19) pandemic, new testing procedures, medical treatments, and vaccines are being developed rapidly. One potential diagnostic tool is a reverse-transcription polymerase chain reaction (RT-PCR). RT-PCR, typically a time-consuming process, was less sensitive to COVID-19 recognition in the disease's early stages. Here we introduce an optimized deep learning (DL) scheme to distinguish COVID-19-infected patients from normal patients according to computed tomography (CT) scans. In the proposed method, contrast enhancement is used to improve the quality of the original images. A pretrained DenseNet-201 DL model is then trained using transfer learning. Two fully connected layers and an average pool are used for feature extraction. The extracted deep features are then optimized with a Firefly algorithm to select the most optimal learning features. Fusing the selected features is important to improving the accuracy of the approach; however, it directly affects the computational cost of the technique. In the proposed method, a new parallel high index technique is used to fuse two optimal vectors; the outcome is then passed on to an extreme learning machine for final classification. Experiments were conducted on a collected database of patients using a 70:30 training: Testing ratio. Our results indicated an average classification accuracy of 94.76% with the proposed approach. A comparison of the outcomes to several other DL models demonstrated the effectiveness of our DL method for classifying COVID-19 based on CT scans.", "journal": "CMC-COMPUTERS MATERIALS & CONTINUA", "category": "Computer Science, Information Systems; Materials Science, Multidisciplinary", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000724533200002", "keywords": "k-nearest neighbors; Fuzzy set; Recommendation; Decision Support; Pharmaceutical supply chain; Temperature deviation", "title": "Generating decision support for alarm processing in cold supply chains using a hybrid k-NN algorithm", "abstract": "Real-time temperature monitoring is necessary in cold pharmaceutical supply chains (SCs), where exposures to extreme temperatures can lead to product quality deterioration. Temperature alarms (TAs) triggered by the current rule-based systems still require lengthy examinations before a suitable corrective measure (CM) can be chosen. However, provision of additional information relevant to TAs can expedite the examination process. In the related areas of recommender systems and false alarm/anomaly detection, k-nearest neighbors (k-NN) algorithm has proven to be successful because of its interpretability and ease of use. However, in the context of TA processing, it may suffer from some inherent limitations (i.e., varying neighborhood radius, unreliable classifications in sparse and noisy regions, and blindness to natural class boundaries). To overcome these limitations, we propose a hybrid k-NN (Hk-NN) algorithm based on the principles of local similarity and neighborhood homogeneity. It incorporates a two-step voting procedure with an entropy-optimized k-NN radius, decision trees with k-constrained leaves, and nearest neighbor predictions. We investigate 16,525 comments by alarm personnel for TAs in a pharmaceutical SC and encode them in terms of deviation causes and CMs (target features). We use SC data on cargo location, SC phase, sensor role, and temperature characteristics as predictor features for TA similarity estimation. In eight experimental setups, HkNN consistently outperforms k-NN with an optimized k in terms of accuracy, balanced accuracy, macro-average precision, recall, and specificity. At the same time, Hk-NN refrains from predicting observations, for which kNN's accuracy is close to a random guess.", "journal": "EXPERT SYSTEMS WITH APPLICATIONS", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000772519400007", "keywords": "Person re-identification; Deep metric learning; Local feature learning; Generative adversarial learning; Sequence feature learning", "title": "Deep learning-based person re-identification methods: A survey and outlook of recent works", "abstract": "In recent years, with the increasing demand for public safety and the rapid development of intelligent surveil-lance networks, person re-identification (Re-ID) has become one of the hot research topics in the computer vi-sion field. The main research goal of person Re-ID is to retrieve persons with the same identity from different cameras. However, traditional person Re-ID methods require manual marking of person targets, which consumes a lot of labor cost. With the widespread application of deep neural networks, many deep learning-based person Re-ID methods have emerged. Therefore, this paper is to facilitate researchers to understand the latest research results and the future trends in the field. Firstly, we summarize the studies of several recently published person Re-ID surveys and complement the latest research methods to systematically classify deep learning-based person Re-ID methods. Secondly, we propose a multi-dimensional taxonomy that classifies current deep learning-based person Re-ID methods into four categories according to metric and representation learning, including methods for deep metric learning, local feature learning, generative adversarial learning and sequence feature learning. Furthermore, we subdivide the above four categories according to their methodologies and motivations, discussing the advantages and limitations of part subcategories. Finally, we discuss some challenges and possible research directions for person Re-ID.(c) 2022 Elsevier B.V. All rights reserved.", "journal": "IMAGE AND VISION COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic; Optics", "annotated_keywords": ["neural net", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000685890800019", "keywords": "Electroencephalography; Feature extraction; Land vehicles; Testing; Training; Electrodes; Brain-computer interface (BCI); convolutional neural networks (CNN); electroencephalograph (EEG); ensemble learning; shared control; vehicle motion control", "title": "Ensemble Learning Based Brain-Computer Interface System for Ground Vehicle Control", "abstract": "This article establishes a novel electroencephalograph (EEG)-based brain-computer interface (BCI) system for ground vehicle control with potential application of mobility assistance to the disabled. To enable an intuitive motor imagery (MI) paradigm of \"left,\" \"right,\" \"push,\" and \"pull,\" a driving simulator based EEG data recording and automatic labeling platform is built for dataset making. In the preprocessing stage, a wavelet and canonical correlation analysis (CCA) combined method is used for artifact removal and improving signal-to-noise ratio. An ensemble learning based training and testing framework is proposed for MI EEG data classification. The average classification accuracy of proposed framework is about 91.75%. This approach essentially takes advantage of the common spatial pattern (CSP) with ability of extracting the feature of event-related potentials and the convolutional neural networks (CNNs) with powerful capacity of feature learning and classification. To convert the classification results of EEG data segments into motion control signals of ground vehicle, shared control strategy is used to realize the control command of \"left-steering,\" \"right-steering,\" \"acceleration,\" and \"stop\" considering collision avoidance with obstacles detected by a single-line LIDAR. The online experimental results on a model vehicle platform validate the significant performance of the established BCI system and reveal the application potential of BCI on the vehicle control and automation.", "journal": "IEEE TRANSACTIONS ON SYSTEMS MAN CYBERNETICS-SYSTEMS", "category": "Automation & Control Systems; Computer Science, Cybernetics", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000865679100002", "keywords": "Seoul; Meteorological data; PM10; PM2.5; Visibility estimation; Machine learning; Extreme gradient boosting algorithm", "title": "Precipitation forecast in China based on reservoir computing", "abstract": "Precipitation as the meteorological data is closely related to human life. For this reason, we hope to propose new method to forecast it more accurately. In this article, we aim to forecast precipitation by reservoir computing with some additional processes. The concept of reservoir computing emerged from a specific machine learning paradigm, which is characterized by a three-layered architecture (input, reservoir and output layers). What is different from other machine learning algorithms is that only the output layer is trained and optimized for particular tasks. Since the precipitation data is non-smooth, its prediction is very difficult via the classical methods of prediction of the nonlinear time series. For the predicated precipitation data, we take its first-order moving average to make it smoother, then take the logarithm of smoothed nonzero data and the same negative constant for smoothed zero data to obtain a new series. We train the obtained series by reservoir computing and get the predicated result of its future. After taking its exponent function, the predicated data for original precipitation data are obtained. It indicates that reservoir computing combined with other processes can potentially bring about the accurate precipitation forecast.", "journal": "EUROPEAN PHYSICAL JOURNAL-SPECIAL TOPICS", "category": "Physics, Multidisciplinary", "annotated_keywords": ["machine learning", "learning algorithm"], "label": "1", "title_label": "0"}
{"id": "WOS:000706960500009", "keywords": "Face verification; kinship verification; local metric; Mahalanobis distance; metric learning", "title": "Weakly Supervised Compositional Metric Learning for Face Verification", "abstract": "The aim of metric learning is to learn a mapping relationship, which reduces the intraclass distance and increases the interclass distance. As there are a large number of parameters that need to be optimized in traditional metric learning algorithms, they may suffer from high computational complexity and overfitting problems in the case of insufficient training data. To alleviate this, we propose a weakly supervised compositional metric learning (WSCML) method, which utilizes a set of predetermined local discriminant metrics to learn the optimal weight combination of the component metrics. Under the large margin framework, our WSCML can effectively improve the verification accuracy by constraining the Mahalanobis distance of positive sample pairs to be less than a small threshold and that of negative sample pairs to be greater than a large threshold. In addition, we employ three regularization terms to optimize the proposed WSCML, respectively, to control the sparsity of the solution while maintaining its feasibility. Experimental results on KinFaceW-I, fine-grained face verification (FGFV), and Labled Faces in the Wild (LFW) datasets show the effectiveness of the proposed method.", "journal": "IEEE TRANSACTIONS ON INSTRUMENTATION AND MEASUREMENT", "category": "Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": ["learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000624500300001", "keywords": "brain-computer interface; motor imagery; ensemble learning; multi-objective fruit fly optimization algorithm", "title": "Cluster decomposing and multi-objective optimization based-ensemble learning framework for motor imagery-based brain-computer interfaces", "abstract": "Objective. Motor imagery (MI) is a mental representation of motor behavior and a widely used pattern in electroencephalogram (EEG) based brain-computer interface (BCI) systems. EEG is known for its non-stationary, non-linear features and sensitivity to artifacts from various sources. This study aimed to design a powerful classifier with a strong generalization capability for MI based BCIs. Approach. In this study, we proposed a cluster decomposing based ensemble learning framework (CDECL) for EEG classification of MI based BCIs. The EEG data was decomposed into sub-data sets with different distributions by clustering decomposition. Then a set of heterogeneous classifiers was trained on each sub-data set for generating a diversified classifier search space. To obtain the optimal classifier combination, the ensemble learning was formulated as a multi-objective optimization problem and a stochastic fractal based binary multi-objective fruit fly optimization algorithm was proposed for solving the ensemble learning problem. Main results. The proposed method was validated on two public EEG datasets (BCI Competition IV datasets IIb and BCI Competition IV dataset IIa) and compared with several other competing classification methods. Experimental results showed that the proposed CDECL based methods can effectively construct a diversity ensemble classifier and exhibits superior classification performance in comparison with several competing methods. Significance. The proposed method is promising for improving the performance of MI-based BCIs.", "journal": "JOURNAL OF NEURAL ENGINEERING", "category": "Engineering, Biomedical; Neurosciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000727557900001", "keywords": "compressive strength; volcanic ash; mortar; artificial neural network; adaptive neuro fuzzy interface system", "title": "Modeling Compressive Strength of Eco-Friendly Volcanic Ash Mortar Using Artificial Neural Networking", "abstract": "Forecasting the compressive strength of concrete is a complex task owing to the interactions among concrete ingredients. In addition, an important characteristic of the concrete failure surface is its six-fold symmetry. In this study, an artificial neural network (ANN) and adaptive neuro fuzzy interface system (ANFIS) were employed to model the compressive strength of natural volcanic ash mortar (VAM) by using the six-fold symmetry of concrete failure. The modeling was correlated with four parameters. To train and test the projected models, data for more than 150 samples were collected from the literature. Furthermore, mortar samples with varying proportions of volcanic ash were prepared in the laboratory and tested, and the results were used to validate the models. The performance of the developed models was assessed using numerous statistical measures. The results show that both the ANN and ANFIS models accurately predict the compressive strength of VAM with R-square above 0.9 and lower error statistics. The permutation feature analysis confirmed that the age of specimens affects the strength of VAM the most, followed by the water-to-cement ratio, curing temperature, and percentage of volcanic ash.", "journal": "SYMMETRY-BASEL", "category": "Multidisciplinary Sciences", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000762784400001", "keywords": "fair outlier detection; algorithmic fairness; adversarially fair representation learning; local outlier factors; symmetric structure", "title": "Fair Outlier Detection Based on Adversarial Representation Learning", "abstract": "Outlier detection aims to identify rare, minority objects in a dataset that are significantly different from the majority. When a minority group (defined by sensitive attributes, such as gender, race, age, etc.) does not represent the target group for outlier detection, outlier detection methods are likely to propagate statistical biases in the data and generate unfair results. Our work focuses on studying the fairness of outlier detection. We characterize the properties of fair outlier detection and propose an appropriate outlier detection method that combines adversarial representation learning and the LOF algorithm (AFLOF). Unlike the FairLOF method that adds fairness constraints to the LOF algorithm, AFLOF uses adversarial networks to learn the optimal representation of the original data while hiding the sensitive attribute in the data. We introduce a dynamic weighting module that assigns lower weight values to data objects with higher local outlier factors to eliminate the influence of outliers on representation learning. Lastly, we conduct comparative experiments on six publicly available datasets. The results demonstrate that compared to the density-based LOF method and the recently proposed FairLOF method, our proposed AFLOF method has a significant advantage in both the outlier detection performance and fairness.", "journal": "SYMMETRY-BASEL", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000626331500036", "keywords": "Electroencephalography; Feature extraction; Brain modeling; Training; Adaptation models; Decoding; Calibration; Deep neural network (DNN); domain adaptation; adversarial learning; electroencephalogram (EEG); motor imagery (MI); brain-computer interface (BCI)", "title": "Dynamic Joint Domain Adaptation Network for Motor Imagery Classification", "abstract": "Electroencephalogram (EEG) has been widely used in brain computer interface (BCI) due to its convenience and reliability. The EEG-based BCI applications are majorly limited by the time-consuming calibration procedure for discriminative feature representation and classification. Existing EEG classification methods either heavily depend on the handcrafted features or require adequate annotated samples at each session for calibration. To address these issues, we propose a novel dynamic joint domain adaptation network based on adversarial learning strategy to learn domain-invariant feature representation, and thus improve EEG classification performance in the target domain by leveraging useful information from the source session. Specifically, we explore the global discriminator to align the marginal distribution across domains, and the local discriminator to reduce the conditional distribution discrepancy between sub-domains via conditioning on deep representation as well as the predicted labels from the classifier. In addition, we further investigate a dynamic adversarial factor to adaptively estimate the relative importance of alignment between the marginal and conditional distributions. To evaluate the efficacy of our method, extensive experiments are conducted on two public EEG datasets, namely, Datasets IIa and IIb of BCI Competition IV. The experimental results demonstrate that the proposed method achieves superior performance compared with the state-of-the-art methods.", "journal": "IEEE TRANSACTIONS ON NEURAL SYSTEMS AND REHABILITATION ENGINEERING", "category": "Engineering, Biomedical; Rehabilitation", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000692319300027", "keywords": "Earth slopes; Friction angle; Maximum dissimilarity; Multi-gene genetic programming; Pareto-optimality; Residual strength", "title": "A predictive equation for residual strength using a hybrid of subset selection of maximum dissimilarity method with Pareto optimal multi-gene genetic programming", "abstract": "More accurate and reliable estimation of residual strength friction angle (phi(r)) of clay is crucial in many geotechnical engineering applications, including riverbank stability analysis, design, and assessment of earthen dam slope stabilities. However, a general predictive equation for phi(r), with applicability in a wide range of effective parameters, remains an important research gap. The goal of this study is to develop a more accurate equation for phi(r) using the Pareto Optimal Multi-gene Genetic Programming (POMGGP) approach by evaluating a comprehensive dataset of 290 experiments compiled from published literature databases worldwide. A new framework for integrated equation derivation proposed that hybridizes the Subset Selection of Maximum Dissimilarity Method (SSMD) with Multi-gene Genetic Programming (MGP) and Pareto-optimality (PO) to find an accurate equation for phi(r) with wide range applicability. The final predictive equation resulted from POMGGP modeling was assessed in comparison with some previously published machine learning-based equations using statistical error analysis criteria, Taylor diagram, revised discrepancy ratio (RDR), and scatter plots. Base on the results, the POMGGP has the lowest uncertainty with U95 = 2.25, when compared with Artificial Neural Network (ANN) (U95 = 2.3), Bayesian Regularization Neural Network (BRNN) (U95 = 2.94), Levenberg-Marquardt Neural Network (LMNN) (U95 = 3.3), and Differential Evolution Neural Network (DENN) (U95 = 2.37). The more reliable results in estimation of phi(r) derived by POMGGP with reliability 59.3%, and resiliency 60% in comparison with ANN (reliability = 30.23%, resiliency = 28.33%), BRNN (reliability = 10.47%, resiliency = 10.39%), LMNN (reliability = 19.77%, resiliency = 20.29%) and DENN (reliability = 27.91%, resiliency = 24.19%). Besides the simplicity and ease of application of the new POMGGP equation to a broad range of conditions, using the uncertainty, reliability, and resilience analysis confirmed that the derived equation for phi(r) significantly outperformed other existing machine learning methods, including the ANN, BRNN, LMNN, and DENN equations. (C) 2021 China University of Geosciences (Beijing) and Peking University. Production and hosting by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).", "journal": "GEOSCIENCE FRONTIERS", "category": "Geosciences, Multidisciplinary", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000637807500001", "keywords": "dimensionality reduction; hyperspectral images; manifold learning; classification; spectral-locational-spatial", "title": "EEG Signal Classification Using Manifold Learning and Matrix-Variate Gaussian Model", "abstract": "In brain-computer interface (BCI), feature extraction is the key to the accuracy of recognition. There is important local structural information in the EEG signals, which is effective for classification; and this locality of EEG features not only exists in the spatial channel position but also exists in the frequency domain. In order to retain sufficient spatial structure and frequency information, we use one-versus-rest filter bank common spatial patterns (OVR-FBCSP) to preprocess the data and extract preliminary features. On this basis, we conduct research and discussion on feature extraction methods. One-dimensional feature extraction methods like linear discriminant analysis (LDA) may destroy this kind of structural information. Traditional manifold learning methods or two-dimensional feature extraction methods cannot extract both types of information at the same time. We introduced the bilinear structure and matrix-variate Gaussian model into two-dimensional discriminant locality preserving projection (2DDLPP) algorithm and decompose EEG signals into spatial and spectral parts. Afterwards, the most discriminative features were selected through a weight calculation method. We tested the method on BCI competition data sets 2a, data sets IIIa, and data sets collected by our laboratory, and the results were expressed in terms of recognition accuracy. The cross-validation results were 75.69%, 70.46%, and 54.49%, respectively. The average recognition accuracy of new method is improved by 7.14%, 7.38%, 4.86%, and 3.8% compared to those of LDA, two-dimensional linear discriminant analysis (2DLDA), discriminant locality property projections (DLPP), and 2DDLPP, respectively. Therefore, we consider that the proposed method is effective for EEG classification.", "journal": "COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE", "category": "Mathematical & Computational Biology; Neurosciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000793406900004", "keywords": "Large-eddy simulation; Subgrid-scale modeling; Deep Learning; Transfer learning; Convolutional neural networks; Data-driven modeling", "title": "Stable a posteriori LES of 2D turbulence using convolutional neural networks: Backscattering analysis and generalization to higher Re via transfer learning", "abstract": "There is a growing interest in developing data-driven subgrid-scale (SGS) models for large eddy simulation (LES) using machine learning (ML). In a priori (offline) tests, some recent studies have found ML-based data-driven SGS models that are trained on high-fidelity data (e.g., from direct numerical simulation, DNS) to outperform baseline physics-based models and accurately capture the inter-scale transfers, both forward (diffusion) and backscatter. While promising, instabilities in a posteriori (online) tests and inabilities to generalize to a different flow (e.g., with a higher Reynolds number, Re) remain as major obstacles in broadening the applications of such data-driven SGS models. For example, many of the same aforementioned studies have found instabilities that required often ad-hoc remedies to stabilize the LES at the expense of reducing accuracy. Here, using 2D decaying turbulence as the testbed, we show that deep convolutional neural networks (CNNs) can accurately predict the SGS forcing terms and the inter-scale transfers in a priori tests, and if trained with enough samples, lead to stable and accurate a posteriori LES-CNN. Further analysis attributes aforementioned instabilities to the disproportionately lower accuracy of the CNNs in capturing backscattering (anti-diffusion) when the training set is small. We also show that transfer learning, which involves re-training the CNN with a small amount of data (e.g., 1%) from the new flow, enables accurate and stable a posteriori LES-CNN for flows with 16x higher Re (as well as higher grid resolution if needed). These results show the promise of CNNs with transfer learning to provide stable, accurate, and generalizable LES for practical use. (c) 2022 Elsevier Inc. All rights reserved.", "journal": "JOURNAL OF COMPUTATIONAL PHYSICS", "category": "Computer Science, Interdisciplinary Applications; Physics, Mathematical", "annotated_keywords": ["neural net", "neural net", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000623096400006", "keywords": "False data; Information vulnerability; Multi-area microgrid; Resiliency; Renewable resources; Smart grid", "title": "Increasing resiliency against information vulnerability of renewable resources in the operation of smart multi-area microgrid", "abstract": "With the widespread use of information and communication technologies in smart grids, the vulnerability of these networks has increased significantly. In this paper, the operation of a smart electrical energy system is evaluated by considering the information vulnerability of renewable generators and their sensors. Hence, the false data injection process is modelled by the probability distribution function and different deviations to achieve real conditions. Since the attackers may have various information, an observation-action method is utilized to enhance their capability. Accordingly, an auxiliary variable is considered for real-time decisions and any modification which is required in the process. In return, to resilience the system and mitigate the impact of false data injection, a machine learning method, namely adaptive neuro fuzzy inference system, is used based on a threshold index. Implementing the method on a smart multi-area microgrid shows that if all data points are exposed to attack, the operation cost will be affected by about 8.52% and at least 70% of the false data into each sensor will be detectable. Moreover, sensitivity analysis validates that the wrong decision may be taken by attackers in real-time and, the percentage of detection will decrease if the threshold index increases. (c) 2021 Elsevier Ltd. All rights reserved.", "journal": "ENERGY", "category": "Thermodynamics; Energy & Fuels", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000697822000030", "keywords": "Blockchain; Smart cities; Peer-to-peer computing; Security; Intrusion detection; Internet of Things; Intelligent sensors; Fog-Cloud architecture; Intelligent Blockchain; Internet of Things; Intrusion Detection System; Machine Learning; Privacy-Preservation", "title": "PPSF: A Privacy-Preserving and Secure Framework Using Blockchain-Based Machine-Learning for IoT-Driven Smart Cities", "abstract": "With the evolution of the Internet of Things (IoT), smart cities have become the mainstream of urbanization. IoT networks allow distributed smart devices to collect and process data within smart city infrastructure using an open channel, the Internet. Thus, challenges such as centralization, security, privacy (e.g., performing data poisoning and inference attacks), transparency, scalability, and verifiability limits faster adaptations of smart cities. Motivated by the aforementioned discussions, we present a Privacy-Preserving and Secure Framework (PPSF) for IoT-driven smart cities. The proposed PPSF is based on two key mechanisms: a two-level privacy scheme and an intrusion detection scheme. First, in a two-level privacy scheme, a blockchain module is designed to securely transmit the IoT data and Principal Component Analysis (PCA) technique is applied to transform raw IoT information into a new shape. In the intrusion detection scheme, a Gradient Boosting Anomaly Detector (GBAD) is applied for training and evaluating the proposed two-level privacy scheme based on two IoT network datasets, namely ToN-IoT and BoT-IoT. We also suggest a blockchain-InterPlanetary File System (IPFS) integrated Fog-Cloud architecture to deploy the proposed PPSF framework. Experimental results demonstrate the superiority of the PPSF framework over some recent approaches in blockchain and non-blockchain systems.", "journal": "IEEE TRANSACTIONS ON NETWORK SCIENCE AND ENGINEERING", "category": "Engineering, Multidisciplinary; Mathematics, Interdisciplinary Applications", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000659339000001", "keywords": "Face spoof attack detection; Multiple-feature learning; Capsule neural networks; Hypergraph regularization", "title": "Face Spoof Attack Detection with Hypergraph Capsule Convolutional Neural Networks", "abstract": "Face authentication has been widely used in personal identification. However, face authentication systems can be attacked by fake images. Existing methods try to detect such attacks with different features. Among them, using color images become popular since it is flexible and generic. In this paper, a novel feature representation for face spoof attack detection, namely hypergraph capsule convolutional neural networks (HGC-CNNs), is proposed, which takes advantage of multiple features. To achieve it, capsule neural networks are used to integrate different types of features. In addition, hypergraph regularization is applied to learn the correlations among samples. In this way, the descriptive power is improved. The proposed feature representation is compared with existing features for face spoof attack detection and experimental results on two commonly used datasets emphasize the effectiveness of HGC-CNN. (C) 2021 The Authors. Published by Atlantis Press B.V.", "journal": "INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000832375300001", "keywords": "diabetic retinopathy (DR); DR detection and classification; automatic diagnosis; feature extraction; multi-class segmentation and classification; fundus images (FIs); transfer learning", "title": "AI-Based Automatic Detection and Classification of Diabetic Retinopathy Using U-Net and Deep Learning", "abstract": "Artificial intelligence is widely applied to automate Diabetic retinopathy diagnosis. Diabetes-related retinal vascular disease is one of the world's most common leading causes of blindness and vision impairment. Therefore, automated DR detection systems would greatly benefit the early screening and treatment of DR and prevent vision loss caused by it. Researchers have proposed several systems to detect abnormalities in retinal images in the past few years. However, Diabetic Retinopathy automatic detection methods have traditionally been based on hand-crafted feature extraction from the retinal images and using a classifier to obtain the final classification. DNN (Deep neural networks) have made several changes in the previous few years to assist overcome the problem mentioned above. We suggested a two-stage novel approach for automated DR classification in this research. Due to the low fraction of positive instances in the asymmetric Optic Disk (OD) and blood vessels (BV) detection system, preprocessing and data augmentation techniques are used to enhance the image quality and quantity. The first step uses two independent U-Net models for OD (optic disc) and BV (blood vessel) segmentation. In the second stage, the symmetric hybrid CNN-SVD model was created after preprocessing to extract and choose the most discriminant features following OD and BV extraction using Inception-V3 based on transfer learning, and detects DR by recognizing retinal biomarkers such as MA (microaneurysms), HM (hemorrhages), and exudates (EX). On EyePACS-1, Messidor-2, and DIARETDB0, the proposed methodology demonstrated state-of-the-art performance, with an average accuracy of 97.92%, 94.59%, and 93.52%, respectively. Extensive testing and comparisons with baseline approaches indicate the efficacy of the suggested methodology.", "journal": "SYMMETRY-BASEL", "category": "Multidisciplinary Sciences", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000895447300001", "keywords": "Spatiotemporal forecasting; Traffic forecasting; Ensemble learning; Feature selection; Ensemble deep random vector functional; link", "title": "Graph ensemble deep random vector functional link network for traffic forecasting", "abstract": "Traffic forecasting is crucial to achieving a smart city as it facilitates public transportation management, autonomous driving, and the resource relocation of the sharing economy. Traffic forecasting belongs to the challenging spatiotemporal forecasting task, which is highly demanding because of the complicated geospatial correlation between traffic nodes, inconsistent and highly non-linear temporal patterns due to various events, and sporadic traffic accidents. Previous graph neural network (GNN) models built for transportation forecasting feature the sophisticated structure and heavy computation cost as they combine the deep neural network and graph machine learning to capture the spatiotemporal dynamics for the whole transportation network. However, it may be more practical for practitioners to perform node-wise forecasting for specific nodes of interest rather than network-wise forecasting. To mitigate the gaps mentioned above, we propose a novel graph ensemble deep random vector functional link network (GEdRVFL) to forecast the future traffic volume by combining the well-performing ensemble deep random vector functional link (EdRVFL) with the graph convolution layer for a specific node and realize the node-wise traffic forecasting. After a comprehensive comparison with the state-of-the-art models, our model beats the others in four out of five cases measured by mean absolute scaled error.(c) 2022 Elsevier B.V. All rights reserved.", "journal": "APPLIED SOFT COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": ["neural net", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000757032600026", "keywords": "Self-supervised learning; graph embedding; network embedding; siamese network", "title": "EMBER: No-Code Context Enrichment via Similarity-Based Keyless Joins", "abstract": "Structured data, or data that adheres to a pre-defined schema, can suffer from fragmented context: information describing a single entity can be scattered across multiple datasets or tables tailored for specific business needs, with no explicit linking keys. Context enrichment, or rebuilding fragmented context, using keyless joins is an implicit or explicit step in machine learning (ML) pipelines over structured data sources. This process is tedious, domain-specific, and lacks support in now-prevalent no-code ML systems that let users create ML pipelines using just input data and high-level configuration files. In response, we propose EMBER, a system that abstracts and automates keyless joins to generalize context enrichment. Our key insight is that EMBER can enable a general keyless join operator by constructing an index populated with task-specific embeddings. EMBER learns these embeddings by leveraging Transformer-based representation learning techniques. We describe our architectural principles and operators when developing EMBER, and empirically demonstrate that EMBER allows users to develop no-code context enrichment pipelines for five domains, including search, recommendation and question answering, and can exceed alternatives by up to 39% recall, with as little as a single line configuration change.", "journal": "PROCEEDINGS OF THE VLDB ENDOWMENT", "category": "Computer Science, Information Systems; Computer Science, Theory & Methods", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000862429800027", "keywords": "Principal component analysis; Detectors; Feature extraction; Transformers; Sensitivity; Informatics; Covariance matrices; Deep learning; ensemble learning; fault detection; incipient faults; sliding window patch", "title": "Feature Ensemble Net: A Deep Framework for Detecting Incipient Faults in Dynamical Processes", "abstract": "How to detect incipient faults has been an important problem in the field of fault detection. Although many types of machine and deep learning methods have been proposed, their performance is not as good as expected. In this article, a novel feature ensemble net (FENet) was developed, particularly for faults 3, 9, and 15 in the Tennessee Eastman process (TEP), which are notoriously difficult to detect. For the input feature layer, features extracted by the basic detectors are integrated to expand the detection ability of FENet. For the hidden feature transformer layers, with sliding-window patches and principal component analysis (PCA), the previous feature matrix is transformed. The sliding-window patches can be used to generate singular values, whereas the patches in the well-known convolution technique can only be vectorized, primarily for performing PCA in PCA-based networks. This enhances the sensitivity of the FENet to incipient faults. For the output feature layer, all feature matrices in the last hidden layer are completely stacked into a large feature matrix. The sliding technique is performed at the decision layer, and a detection index is designed with normalized singular values. The superiority of FENet can be completely verified by a continuous stirred tank heater and TEP. As compared with deep PCA, PCA-based monitoring network, and typical ensemble strategies, such as averaging, voting, stacking, and Bayesian inference, FENet can effectively detect Faults 3, 9, and 15 in TEP.", "journal": "IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS", "category": "Automation & Control Systems; Computer Science, Interdisciplinary Applications; Engineering, Industrial", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000683568800015", "keywords": "Spatial transformer network; Rotation invariance; Deep learning; Deep features; Content-based remote sensing image retrieval", "title": "Rotation-aware representation learning for remote sensing image retrieval", "abstract": "The rising number and size of remote sensing (RS) image archives makes content-based RS image retrieval (CBRSIR) more important. Convolutional neural networks (CNNs) offer good CBRSIR performance, but the features they extract are not rotation-invariant. This is problematic as objects in RS images appear in arbitrary rotation angles. We develop and investigate two new rotation-aware CNN-based CBRSIR methods: 1) In the Feature Map Transformation Based Rotation-Aware Network (FMT-RAN), the last pooling layer is rotated in four different angles during training. Its outputs are passed through the same fully connected-, coding-, and classification layer, and the resulting losses are added. 2) The Spatial Transformer-based Rotation-Aware Network (ST-RAN) contains a spatial transformer network (STN) and a rotation aware network (RAN). For training, the original and a randomly rotated version of an image are fed into the ST-RAN. The STN generates a transformed version of the original to match the rotated image. The RAN extracts the features of all three images. We apply two-stage training, which first optimizes the STN and then the RAN. Both of our methods are efficient in terms of retrieval accuracy and time, but ST-RAN has the overall best performance. It outperforms the state-of-the-art CBRSIR methods. (c) 2021 Published by Elsevier Inc.", "journal": "INFORMATION SCIENCES", "category": "Computer Science, Information Systems", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000630185800007", "keywords": "Representation learning; recommender systems; attribute disentangling", "title": "Disentangled Item Representation for Recommender Systems", "abstract": "Item representations in recommendation systems are expected to reveal the properties of items. Collaborative recommender methods usually represent an item as one single latent vector. Nowadays the e-commercial platforms provide various kinds of attribute information for items (e.g., category, price, and style of clothing). Utilizing this attribute information for better item representations is popular in recent years. Some studies use the given attribute information as side information, which is concatenated with the item latent vector to augment representations. However, the mixed item representations fail to fully exploit the rich attribute information or provide explanation in recommender systems. To this end, we propose a fine-grained Disentangled Item Representation (DIR) for recommender systems in this article, where the items are represented as several separated attribute vectors instead of a single latent vector. In this way, the items are represented at the attribute level, which can provide fine-grained information of items in recommendation. We introduce a learning strategy, LearnDIR, which can allocate the corresponding attribute vectors to items. We show how DIR can be applied to two typical models, Matrix Factorization (MF) and Recurrent Neural Network (RNN). Experimental results on two real-world datasets show that the models developed under the framework of DIR are effective and efficient. Even using fewer parameters, the proposed model can outperform the state-of-the-art methods, especially in the cold-start situation. In addition, we make visualizations to show that our proposition can provide explanation for users in real-world applications.", "journal": "ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000678309400001", "keywords": "Drugs; Object recognition; Deep learning; Real-time systems; Licenses; Feature extraction; Bar codes; Blister package identification; deep learning; induction; dispensing error; CNN", "title": "Pharmaceutical Blister Package Identification Based on Induced Deep Learning", "abstract": "Prescription dispensing accuracy is of paramount importance for all hospitals. However, human errors are inevitable due to multiple reasons, such as fatigue, stress, heavy workload, lack of effective verification measures, mismanagement. Such human errors pose serious safety and health concerns on the part of patients and may as well lead to a series of medical disputes. Based on induced deep learning, this paper proposes a real-time Blister Package Identification System (BPIS) to assist pharmacists' drug verification and dispensing. Under the guidance of the induction strategy, image preprocessing is introduced to form a standardized image containing the front and back side of the blister package, which is subsequently sent to CNN-based object identification network for feature extraction and identification. This preprocessing method allows the identification system to promote the deep learning system to focus on feature learning to obtain more information about the appearance of the package ruling out confounding factors such as background noise, size, shape or positioning. In addition, this article collects and establishes an image dataset of adult lozenges. Under this dataset, this paper verifies the enhancement of Induced Deep Learning (IDL) on YOLO v2, ResNet, and SENet. By optimizing the deep learning identification network with the help of the embedded technology and a two-side extraction mechanism, a real-time BPIS is built. Long-term tests in hospitals prove the effectiveness of the proposed system.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000902198500001", "keywords": "deep learning; Bayesian hyperparameter optimization; computer vision; semantic segmentation; U-Net; diamond abrasive grits; diamond grinding wheels", "title": "Deep Learning and Bayesian Hyperparameter Optimization: A Data-Driven Approach for Diamond Grit Segmentation toward Grinding Wheel Characterization", "abstract": "Diamond grinding wheels (DGWs) have a central role in cutting-edge industries such as aeronautics or defense and spatial applications. Characterizations of DGWs are essential to optimize the design and machining performance of such cutting tools. Thus, the critical issue of DGW characterization lies in the detection of diamond grits. However, the traditional diamond detection methods rely on manual operations on DGW images. These methods are time-consuming, error-prone and inaccurate. In addition, the manual detection of diamond grits remains challenging even for a subject expert. To overcome these shortcomings, we introduce a deep learning approach for automatic diamond grit segmentation. Due to our small dataset of 153 images, the proposed approach leverages transfer learning techniques with pre-trained ResNet34 as an encoder of U-Net CNN architecture. Moreover, with more than 8600 hyperparameter combinations in our model, manually finding the best configuration is impossible. That is why we use a Bayesian optimization algorithm using Hyperband early stopping mechanisms to automatically explore the search space and find the best hyperparameter values. Moreover, considering our small dataset, we obtain overall satisfactory performance with over 53% IoU and 69% F1-score. Finally, this work provides a first step toward diamond grinding wheel characterization by using a data-driven approach for automatic semantic segmentation of diamond grits.", "journal": "APPLIED SCIENCES-BASEL", "category": "Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials Science, Multidisciplinary; Physics, Applied", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000849300000034", "keywords": "Training; Pose estimation; Adaptation models; Task analysis; Feature extraction; Training data; Data models; Hand pose estimation; unsupervised domain adaptation; adversarial training; mean teacher", "title": "Multibranch Adversarial Regression for Domain Adaptative Hand Pose Estimation", "abstract": "Although hand pose estimation has achieved a great success in recent years, there are still challenges with RGB-based estimation tasks, the most significant of which is the absence of labeled training data. At present, the synthetic dataset has plenty of images with accurate annotation, but the difference from real-world datasets affects generalization. Therefore, a transfer learning strategy, which tries to transfer knowledge from a labeled source domain to an unlabeled target domain, is a frequent solution. Existing methods such as mean-teacher, Cyclegan, and MCD will train models with the help of some easily accessible domains such as synthetic data. However, these methods are not guaranteed to operate well in real-world settings due to the domain shift. In this paper, we design a new unsupervised domain adaptation method named Multi-branch Adversarial Regressors (MarsDA) in hand pose estimation, where it could be better for feature migration. Specifically, we first generate pseudo-labels for unlabeled target domain data. Then, the new adversarial training loss between multiple regression branches we designed for hand pose estimation is introduced to narrow the domain gap. In this way, our model can reduce the noise of pseudo labels caused by the domain gap and improve the accuracy of pseudo labels. We evaluate our method on two publicly available real-world datasets, H3D and STB. Experimental results show that our method outperforms existing methods by a large margin.", "journal": "IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000830107400006", "keywords": "Chronic disease management; Artificial intelligence; Health care application; Big data; Machine learning", "title": "Improving chronic disease management for children with knowledge graphs and artificial intelligence", "abstract": "Chronic diseases for children pose serious challenges from a health management perspective. When not implemented in a well-designed manner, an inefficient management platform can have a significant negative impact on patients and the utilization of health care resources. Innovations of recent years in information technology, artificial intelligence and machine learning provide possibilities to design and implement knowledge-based systems and platforms that follow-up, monitor and advise child patients with a chronic disease in an automated manner. In this article we propose the Artificial Intelligence Chronic Management System that combines artificial intelligence, knowledge graph, big data and internet of things in a platform to offer an optimized solution from the perspective of treatment and utilization of resources. The system includes patient and hospital clients, data storage and analytic tools for decision support relying on AI-based services. We illustrate the functionality of the system through different situations frequently occurring in pediatric wards. To assess the feasibility of the AI component, we utilize real life health care data from a hospital in China to develop a classification model for patients with asthma. To provide a more qualitative assessment at the same time, we discuss how the Artificial Intelligence Chronic Management System conforms to the requirements set forth by the standard Chronic Care Model.", "journal": "EXPERT SYSTEMS WITH APPLICATIONS", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science", "annotated_keywords": ["artificial intelligen", "artificial intelligen", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000763268500005", "keywords": "Design science research; Design principles; Hate speech detection; Explainable artificial intelligence; Local explanations", "title": "Design Principles for User Interfaces in AI-Based Decision Support Systems: The Case of Explainable Hate Speech Detection", "abstract": "Hate speech in social media is an increasing problem that can negatively affect individuals and society as a whole. Moderators on social media platforms need to be technologically supported to detect problematic content and react accordingly. In this article, we develop and discuss the design principles that are best suited for creating efficient user interfaces for decision support systems that use artificial intelligence (AI) to assist human moderators. We qualitatively and quantitatively evaluated various design options over three design cycles with a total of 641 participants. Besides measuring perceived ease of use, perceived usefulness, and intention to use, we also conducted an experiment to prove the significant influence of AI explainability on end users' perceived cognitive efforts, perceived informativeness, mental model, and trustworthiness in AI. Finally, we tested the acquired design knowledge with software developers, who rated the reusability of the proposed design principles as high.", "journal": "INFORMATION SYSTEMS FRONTIERS", "category": "Computer Science, Information Systems; Computer Science, Theory & Methods", "annotated_keywords": ["artificial intelligen"], "label": "1", "title_label": "1"}
{"id": "WOS:000672766500173", "keywords": "Artificial Intelligence; violence risk assessment; bioethics", "title": "Artificial intelligence in embryo selection of IVF", "abstract": "Artificial intelligence can be a game changer to address the global challenge of humanity-threatening climate change by fostering sustainable development. Since chemical research and development lay the foundation for innovative products and solutions, this study presents a novel chemical research and development process backed with artificial intelligence and guiding ethical principles to account for both process- and outcome-related sustainability. Particularly in ethically salient contexts, ethical principles have to accompany research and development powered by artificial intelligence to promote social and environmental good and sustainability (beneficence) while preventing any harm (non-maleficence) for all stakeholders (i.e., companies, individuals, society at large) affected.", "journal": "HUMAN REPRODUCTION", "category": "Obstetrics & Gynecology; Reproductive Biology", "annotated_keywords": ["artificial intelligen"], "label": "1", "title_label": "1"}
{"id": "WOS:000540216900005", "keywords": "Artificial Intelligence; violence risk assessment; bioethics", "title": "Artificial Intelligence for Education of Vascular Surgeons", "abstract": "Artificial intelligence can be a game changer to address the global challenge of humanity-threatening climate change by fostering sustainable development. Since chemical research and development lay the foundation for innovative products and solutions, this study presents a novel chemical research and development process backed with artificial intelligence and guiding ethical principles to account for both process- and outcome-related sustainability. Particularly in ethically salient contexts, ethical principles have to accompany research and development powered by artificial intelligence to promote social and environmental good and sustainability (beneficence) while preventing any harm (non-maleficence) for all stakeholders (i.e., companies, individuals, society at large) affected.", "journal": "EUROPEAN JOURNAL OF VASCULAR AND ENDOVASCULAR SURGERY", "category": "Surgery; Peripheral Vascular Disease", "annotated_keywords": ["artificial intelligen"], "label": "1", "title_label": "1"}
{"id": "WOS:000640545600045", "keywords": "Financial sharing services; artificial intelligence (AI); big data audit; data processing; process framework; fuzzy analytic hierarchy", "title": "Research on big data audit based on financial sharing service model using fuzzy AHP", "abstract": "With the rapid development of artificial intelligence and big data technology, the traditional audit method has been constantly impacted by big data. In the era of big data, enterprises actively explore and build a financial sharing service model, and through this model, build audit methods based on big data. In this paper, based on the financial sharing service model, we elaborate the preprocessing process of big data collection, clarity and storage, and build the simulation process framework of big data audit under the service model. Evaluation model is developed based on fuzzy analytic hierarchy process (AHP) and methodology for order estimation by similarity of solution. Finally, on the basis of the implementation process framework, the specific content of each link of big data audit is briefly given. Under the financial sharing service mode, it provides theoretical guidance and practical significance for the implementation of big data audit", "journal": "JOURNAL OF INTELLIGENT & FUZZY SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["artificial intelligen"], "label": "1", "title_label": "0"}
{"id": "WOS:000500924400001", "keywords": "artificial intelligence; international trade; machine translation; machine learning; digital platforms", "title": "Does Machine Translation Affect International Trade? Evidence from a Large Digital Platform", "abstract": "Artificial intelligence (AI) is surpassing human performance in a growing number of domains. However, there is limited evidence of its economic effects. Using data from a digital platform, we study a key application of AI: machine translation. We find that the introduction of a new machine translation system has significantly increased international trade on this platform, increasing exports by 10.9%. Furthermore, heterogeneous treatment effects are consistent with a substantial reduction in translation costs. Our results provide causal evidence that language barriers significantly hinder trade and that AI has already begun to improve economic efficiency in at least one domain.", "journal": "MANAGEMENT SCIENCE", "category": "Management; Operations Research & Management Science", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000567066600005", "keywords": "artificial intelligence; AI-enabled decision making; uncertainty", "title": "The evolution of citation graphs in artificial intelligence research", "abstract": "As artificial intelligence (AI) applications see wider deployment, it becomes increasingly important to study the social and societal implications of AI adoption. Therefore, we ask: are AI research and the fields that study social and societal trends keeping pace with each other? Here, we use the Microsoft Academic Graph to study the bibliometric evolution of AI research and its related fields from 1950 to today. Although early AI researchers exhibited strong referencing behaviour towards philosophy, geography and art, modern AI research references mathematics and computer science most strongly. Conversely, other fields, including the social sciences, do not reference AI research in proportion to its growing paper production. Our evidence suggests that the growing preference of AI researchers to publish in topic-specific conferences over academic journals and the increasing presence of industry research pose a challenge to external researchers, as such research is particularly absent from references made by social scientists.", "journal": "NATURE MACHINE INTELLIGENCE", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": ["artificial intelligen", "artificial intelligen"], "label": "1", "title_label": "1"}
{"id": "WOS:000445991900011", "keywords": "Technological change; Robots; Artificial intelligence; Growth; Income distribution; Inequality", "title": "Should we fear the robot revolution? (The correct answer is yes)", "abstract": "Advances in artificial intelligence and robotics may be leading to a new industrial revolution. This paper presents a model with the minimum necessary features to analyze the implications for inequality and output. Two assumptions are key: \"robot\" capital is distinct from traditional capital in its degree of substitutability with human labor; and only capitalists and skilled workers save. We analyze a range of variants that reflect widely different views of how automation may transform the labor market. Our main results are surprisingly robust: automation is good for growth and bad for equality; in the benchmark model real wages fall in the short run and eventually rise, but \"eventually\" can easily take generations. (C) 2018 Published by Elsevier B.V.", "journal": "JOURNAL OF MONETARY ECONOMICS", "category": "Business, Finance; Economics", "annotated_keywords": ["artificial intelligen"], "label": "1", "title_label": "1"}
{"id": "WOS:000881666900001", "keywords": "artificial intelligence; linear multivariate regression; lycopene content; partial least squares regression; post-harvest quality; principal component regression; tomato fruit", "title": "Artificial intelligence-based prediction of lycopene content in raw tomatoes using physicochemical attributes", "abstract": "Introduction Lycopene consumption reduces risk and incidence of cancer and cardiovascular diseases. Tomatoes are a rich source of phytochemical compounds including lycopene as a major constituent. Lycopene estimation using high-performance liquid chromatography is time-consuming and expensive. Objective To develop artificial intelligence models for prediction of lycopene in raw tomatoes using 14 different physicochemical parameters including salinity, total dissolved solids (TDS), electrical conductivity (EC), firmness, pH, total soluble solids (TSS), titratable acidity (TA), colour values on Hunter scale (L, a, b), total phenolic content (TPC), total flavonoid content (TFC) and antioxidant activity (AOA). Material and methods The post-harvest data acquisition was collected through investigation for more than 100 raw tomatoes stored for 15 days. Linear multivariate regression (LMVR), principal component regression (PCR) and partial least squares regression (PLSR) models were developed by splitting data set into train and test datasets. The training of models was performed using 10-fold cross validation (CV). Results Principal component analysis showed strong positive association between lycopene, colour value 'a', TPC, TFC and AOA. The R-2 (CV), root mean square error (RMSE) (CV) and RMSE (Test) for best LMVR model was observed to be at 0.70, 8.48 and 9.69 respectively. The PCR model revealed R-2 (CV) at 0.59, RMSE (CV) at 8.91 and RMSE (Test) at 10.17 while PLSR model revealed R-2 (CV) at 0.60, RMSE (CV) at 9.10 and RMSE (Test) at 10.11. Conclusion Results of the present study show that epidemiological studies suggest fully ripened tomatoes are most beneficial for consumption to ensure recommended daily intake of lycopene content.", "journal": "PHYTOCHEMICAL ANALYSIS", "category": "Biochemical Research Methods; Plant Sciences; Chemistry, Analytical", "annotated_keywords": ["artificial intelligen"], "label": "1", "title_label": "1"}
{"id": "WOS:000857009300001", "keywords": "adversarial machine learning; adversarial training; AI security", "title": "Towards Robustifying Image Classifiers against the Perils of Adversarial Attacks on Artificial Intelligence Systems", "abstract": "Adversarial machine learning (AML) is a class of data manipulation techniques that cause alterations in the behavior of artificial intelligence (AI) systems while going unnoticed by humans. These alterations can cause serious vulnerabilities to mission-critical AI-enabled applications. This work introduces an AI architecture augmented with adversarial examples and defense algorithms to safeguard, secure, and make more reliable AI systems. This can be conducted by robustifying deep neural network (DNN) classifiers and explicitly focusing on the specific case of convolutional neural networks (CNNs) used in non-trivial manufacturing environments prone to noise, vibrations, and errors when capturing and transferring data. The proposed architecture enables the imitation of the interplay between the attacker and a defender based on the deployment and cross-evaluation of adversarial and defense strategies. The AI architecture enables (i) the creation and usage of adversarial examples in the training process, which robustify the accuracy of CNNs, (ii) the evaluation of defense algorithms to recover the classifiers' accuracy, and (iii) the provision of a multiclass discriminator to distinguish and report on non-attacked and attacked data. The experimental results show promising results in a hybrid solution combining the defense algorithms and the multiclass discriminator in an effort to revitalize the attacked base models and robustify the DNN classifiers. The proposed architecture is ratified in the context of a real manufacturing environment utilizing datasets stemming from the actual production lines.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": ["artificial intelligen", "neural net", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000628219100004", "keywords": "Autonomous systems; Planning and scheduling", "title": "On-board autonomy operations for OPS-SAT experiment", "abstract": "Upcoming space missions are requiring a higher degree of on-board autonomy operations to increase quality science return, to minimize closed-loop space-ground decision making, and to enable new scenarios. Artificial Intelligence technologies like Machine Learning and Automated Planning are becoming more and more popular as they can support data analytics conducted directly on-board as input for the on-board decision making system that generates plans or updates them while being executed. This paper describes the planning and execution architecture under development at the European Space Agency to target this need of autonomy for the ops-sat mission.", "journal": "APPLIED INTELLIGENCE", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000891826100008", "keywords": "COVID-19 diagnosis; AI in COVID-19; CT images; CXR images; COVID-19 screening", "title": "Key concepts, common pitfalls, and best practices in artificial intelligence and machine learning: focus on radiomics", "abstract": "Artificial intelligence (AI) and machine learning (ML) are increasingly used in radiology research to deal with large and complex imaging data sets. Nowadays, ML tools have become easily accessible to anyone. Such a low threshold to accessibility might lead to inappropriate usage and misinterpretation, without a clear intention. Therefore, ensuring methodological rigor is of paramount importance. Getting closer to the real-world clinical implementation of AI, a basic understanding of the main concepts should be a must for every radiology professional. In this respect, simplified explanations of the key concepts along with pitfalls and recommendations would be helpful for general radiology community to develop and improve their AI mindset. In this work, 22 key issues are reviewed within 3 categories: pre-modeling, modeling, and post-modeling. Firstly, the concept is shortly defined for each issue. Then, related common pitfalls and best practices are provided. Specifically, the issues included in this article are validity of the scientific question, unrepresentative samples, sample size, missing data, quality of reference standard, batch effect, reliability of features, feature scaling, multi-collinearity, class imbalance, data and target leakage, high-dimensional data, optimization, overfitting, generalization, performance metrics, clinical utility, comparison with conventional statistical and clinical methods, interpretability and explainability, randomness, transparent reporting, and sharing data.", "journal": "DIAGNOSTIC AND INTERVENTIONAL RADIOLOGY", "category": "Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": ["artificial intelligen", "machine learning", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000502639100001", "keywords": "batch similarity; kNN; process evaluation; quality investigation", "title": "Application of a kNN-based similarity method to biopharmaceutical manufacturing", "abstract": "Machine learning-based similarity analysis is commonly found in many artificial intelligence applications like the one utilized in e-commerce and digital marketing. In this study, a kNN-based (k-nearest neighbors) similarity method is proposed for rapid biopharmaceutical process diagnosis and process performance monitoring. Our proposed application measures the spatial distance between batches, identifies the most similar historical batches, and ranks them in order of similarity. The proposed method considers the similarity in both multivariate and univariate feature spaces and measures batch deviations to a benchmarking batch. The feasibility and effectiveness of the proposed method are tested on a drug manufacturing process at Biogen.", "journal": "BIOTECHNOLOGY PROGRESS", "category": "Biotechnology & Applied Microbiology; Food Science & Technology", "annotated_keywords": ["artificial intelligen"], "label": "1", "title_label": "1"}
{"id": "WOS:000709517800003", "keywords": "Plasma Disruption; Artificial Intelligence; Synthetic Data; Plasma Current Quench", "title": "An Application of Machine Learning for Plasma Current Quench Studies via Synthetic Data Generation", "abstract": "Electromagnetic forces, thermal loads, and radiation loads experienced by the in-vessel components or vacuum vessels at the time of the tokamak plasma current quench (CQ) significantly affect the overall plasma device's health. Thus the mitigation of plasma CQ is of paramount importance, which requires a proper identification of the disruption precursors. Using new Machine Learning (ML) and Artificial Intelligence (AI) approaches, it is possible to identify disruption precursors; however, such approaches require training the ML models. This training of models requires a massive amount of experimental data, which sometimes may not be available for different tokamaks. This necessitates the need for accurate synthetic disruption data generation presenting different types of the CQ profiles observed experimentally. A novel approach for synthetic CQ data generation, considering the experimental aspect of the CQ profile shape for a wide range of tokamak plasma discharges, is designed to train ML/AI models. The trained model results are also elaborated here, which includes identifying current before disruption and classification of CQ profile types in time-space.", "journal": "FUSION ENGINEERING AND DESIGN", "category": "Nuclear Science & Technology", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000614686200001", "keywords": "Intrusion detection systems; Security; Industry 4; 0; Artificial intelligence", "title": "Artificial intelligence, cyber-threats and Industry 4.0: challenges and opportunities", "abstract": "This survey paper discusses opportunities and threats of using artificial intelligence (AI) technology in the manufacturing sector with consideration for offensive and defensive uses of such technology. It starts with an introduction of Industry 4.0 concept and an understanding of AI use in this context. Then provides elements of security principles and detection techniques applied to operational technology (OT) which forms the main attack surface of manufacturing systems. As some intrusion detection systems (IDS) already involve some AI-based techniques, we focus on existing machine-learning and data-mining based techniques in use for intrusion detection. This article presents the major strengths and weaknesses of the main techniques in use. We also discuss an assessment of their relevance for application to OT, from the manufacturer point of view. Another part of the paper introduces the essential drivers and principles of Industry 4.0, providing insights on the advent of AI in manufacturing systems as well as an understanding of the new set of challenges it implies. AI-based techniques for production monitoring, optimisation and control are proposed with insights on several application cases. The related technical, operational and security challenges are discussed and an understanding of the impact of such transition on current security practices is then provided in more details. The final part of the report further develops a vision of security challenges for Industry 4.0. It addresses aspects of orchestration of distributed detection techniques, introduces an approach to adversarial/robust AI development and concludes with human-machine behaviour monitoring requirements.", "journal": "ARTIFICIAL INTELLIGENCE REVIEW", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["artificial intelligen"], "label": "1", "title_label": "1"}
{"id": "WOS:000504043500001", "keywords": "artificial intelligence; deep learning; echo; emergency medicine; emergency ultrasound; point-of-care ultrasound", "title": "Are All Deep Learning Architectures Alike for Point-of-Care Ultrasound?: Evidence From a Cardiac Image Classification Model Suggests Otherwise", "abstract": "Objectives-Little is known about optimal deep learning (DL) approaches for point-of-care ultrasound (POCUS) applications. We compared 6 popular DL architectures for POCUS cardiac image classification to determine whether an optimal DL architecture exists for future DL algorithm development in POCUS. Methods-We trained 6 convolutional neural networks (CNNs) with a range of complexities and ages (AlexNet, VGG-16, VGG-19, ResNet50, DenseNet201, and Inception-v4). Each CNN was trained by using images of 5 typical POCUS cardiac views. Images were extracted from 225 publicly available deidentified POCUS cardiac videos. A total of 750,018 individual images were extracted, with 90% used for model training and 10% for cross-validation. The training time and accuracy achieved were tracked. A real-world test of the algorithms was performed on a set of 125 completely new cardiac images. Descriptive statistics, Pearson R values, and kappa values were calculated for each CNN. Results-Accuracy ranged from 96% to 85.6% correct for the 6 CNNs. VGG-16, one of the oldest and simplest CNNs, performed best at 96% correct with 232 minutes to train (R = 0.97; kappa = 0.95; P < .00001). The worst-performing CNN was the newer DenseNet201, with 85.6% accuracy and 429 minutes to train (R = 0.92; kappa = 0.82; P < .00001). Conclusions-Six common image classification DL algorithms showed considerable variability in their accuracy and training time when trained and tested on identical data, suggesting that not all will perform optimally for POCUS DL applications. Contrary to well-established accuracies for CNNs, more modern and deeper algorithms yielded poorer results.", "journal": "JOURNAL OF ULTRASOUND IN MEDICINE", "category": "Acoustics; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": ["neural net", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000880090300003", "keywords": "Artificial neural network; Fuzzy; Adaptive neuro fuzzy inference system; Building thermal environment; Control algorithm", "title": "Neuromorphic quantum computing", "abstract": "Quantum computation builds on the use of correlations. Correlations could also play a central role for artificial intelligence, neuromorphic computing or \"biological computing.\" As a step toward a systematic exploration of \"correlated computing\" we demonstrate that neuromorphic computing can perform quantum operations. Spiking neurons in the active or silent states are connected to the two states of Ising spins. A quantum density matrix is constructed from the expectation values and correlations of the Ising spins. We show for a two qubit system that quantum gates can be learned as a change of parameters for neural network dynamics. These changes respect restrictions which ensure the quantum correlations. Our proposal for probabilistic computing goes beyond Markov chains and is not based on transition probabilities. Constraints on classical probability distributions relate changes made in one part of the system to other parts, similar to entangled quantum systems.", "journal": "PHYSICAL REVIEW E", "category": "Physics, Fluids & Plasmas; Physics, Mathematical", "annotated_keywords": ["artificial intelligen", "neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000641462000006", "keywords": "Artificial intelligence; Artificial neural network; Genetic algorithms; Pollutants; Wastewater treatment", "title": "Performance prediction of trace metals and cod in wastewater treatment using artificial neural network", "abstract": "Artificial intelligence is finding its ways into the mainstream of day-to-day operations. Novel AI application techniques such as the artificial neural network (ANN), fuzzy logic, genetic algorithms and expert systems have gained popularity in the fourth industrial revolution era. Due to the chemical composition, inherent complexity, incoherent flow rate and higher safety factor in the effective operation of the biological wastewater treatment process, the AI-based model was extensively tested in managing the wastewater treatment operations. The interrelationship between COD and trace metals was studied using an AI-based prediction model with ANNs incorporated in MATLAB. A supervised learning algorithm was used for training the ANNs and to relate input data to output data. The training was aimed at estimating, validating, predicting the parameters by an error function minimization. The goodness of the prediction was attained with the coefficient of determination (R-2) of 0.98-0.99, a sum of square error (SSE) 0.00029-0.1598, room mean-square error (RMSE) of 0.0049-0.8673, mean squared error (MSE) 2.7059e-14 to 2.3175e-15. The ANNs models were found to be a robust tool for predicting WWTP performance. The predictive approaches can be used in the prediction of environmental management and other emerging technologies. This will meet the cost-effectiveness, effective environmental and technical criteria with a wide range of big-data support and implementation of the sustainable development goals, circular bioeconomy and industry 4.0. (c) 2021 Elsevier Ltd. All rights reserved.", "journal": "COMPUTERS & CHEMICAL ENGINEERING", "category": "Computer Science, Interdisciplinary Applications; Engineering, Chemical", "annotated_keywords": ["neural net", "neural net", "expert system", "learning algorithm", "supervised learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000456671500036", "keywords": "Drug research and development; laboratory; preclinical and clinical trial; administration; management", "title": "Drug discovery and drug marketing with the critical roles of modern administration", "abstract": "Drug research and development is a long-term and complicated process with the involvement of multidisciplinary, multi-sector cooperation and regulations of the Food and Drug Administration (FDA). It is of high risk, high cost, high benefit and time-consuming. Therefore, the drug administration and management is extremely necessary and useful. We discussed the whole process including laboratory study, target determination, drug discovery and screening, leading compound and optimization, preclinical and clinical trials, FDA approval and marketing. Actively exploring and applying modern administration and innovative management technology, we can scientifically and effectively enhance the discovery of new drug research and development, and strengthen the supervision of drug market. In recent years, innovation such as artificial intelligence has been applied to drug discovery and drug administration. We further analyzed the possibility of applying management technology to reduce risks, generate profits and benefit patients in the whole process of new drug research and development. In conclusion, drug administration and management plays critical roles in modern drug research and development, and the new technology can be helpful for drug launching.", "journal": "AMERICAN JOURNAL OF TRANSLATIONAL RESEARCH", "category": "Oncology; Medicine, Research & Experimental", "annotated_keywords": ["artificial intelligen"], "label": "0", "title_label": "0"}
{"id": "WOS:000321689000001", "keywords": "data mining; decision-making; environmental informatics; integrated solid waste management; Internet of Things; knowledge engineering; sensors and sensing systems", "title": "Environmental Informatics for Solid and Hazardous Waste Management: Advances, Challenges, and Perspectives", "abstract": "Environmental informatics has experienced extraordinarily rapid progress in the past decade and has made an invaluable contribution to planning, design, and operations for waste management. In many cases, however, the roles of these information technologies have been limited to stand-alone projects without synergistic effects. This article presents a holistic view and an in-depth discussion of environmental informatics applied to solid and hazardous waste management from the onset to the present status, and to future trends aiming to advance the management potential. With regard to building, maintaining, and developing knowledge-based or artificial intelligence systems, the spectrum of environmental informatics for solid and hazardous waste management can be classified into five categories: database system, geographical information system, decision support system, expert system, and integrated environmental information system. Supporting technologies in the integrated environmental information system can be further divided into five classes in a logical order to enhance understanding: data acquisition, data communication, data and knowledge storage, data mining and knowledge discovery, and data and knowledge utilization. This critical review article may help create sustainable development strategies from a local solution to global opportunities that will elevate environmental informatics to a new level in dealing with more complex problems and large-scale applications for integrated solid and hazardous waste management.", "journal": "CRITICAL REVIEWS IN ENVIRONMENTAL SCIENCE AND TECHNOLOGY", "category": "Environmental Sciences", "annotated_keywords": ["artificial intelligen", "expert system"], "label": "1", "title_label": "0"}
{"id": "WOS:000661180100001", "keywords": "anomalous classes; anomaly detection; artificial intelligence; autonomous electric vehicles; deep learning; machine learning; reinforcement learning", "title": "Anomaly detection in autonomous electric vehicles using AI techniques: A comprehensive survey", "abstract": "The next wave in smart transportation is directed towards the design of renewable energy sources that can fuel automobile sector to shift towards the autonomous electric vehicles (AEVs). AEVs are sensor-driven and driverless that uses artificial intelligence (AI)-based interactions in Internet-of-vehicles (IoV) ecosystems. AEVs can reduce carbon footprints and trade energy with peer AEVs, smart grids (SG), and roadside units (RSUs). It supports green transportation vision. However, the sensor information, energy units, and user data are exchanged through open channels, and thus, are susceptible to various security and privacy attacks. Thus, AEVs can be remotely operated and directed by malicious entities that can propagate false updates to the peer nodes in IoV environment. This can cause the failure of components, congestion, as well as the entire disruption of IoV network. Globally researchers and security analysts have addressed solutions that pertain to specific security requirements, but still, the detection and classification of malicious AEVs is a widely studied topic. Malicious AEVs exhibit an anomaly behavior that differentiates them from normal AEVs, and thereby, the detection of anomalous AEVs and classification of anomaly type is required. Motivated from the aforementioned facts, the survey presents a systematic outlook of AI techniques in anomaly detection of AEVs. A solution taxonomy is proposed based on research gaps in the existing surveys, and the evaluation metrics for AI-based anomaly detection are discussed. The open challenges and issues in AI deployments are discussed and a case study is presented on anomaly classification through a weighted ensemble technique. Thus, the proposed survey is designed to guide the manufacturing industry, AI practitioners, and researchers worldwide to formulate and design accurate and precise mechanisms to detect anomalies.", "journal": "EXPERT SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Theory & Methods", "annotated_keywords": ["artificial intelligen"], "label": "1", "title_label": "1"}
{"id": "WOS:000657604400001", "keywords": "de novo design; deep learning; drug discovery; neural network; nuclear receptor", "title": "The Logic of Strategic Assets: From Oil to AI", "abstract": "What resources and technologies are strategic? Policy and theoretical debates often focus on this question, since the \"strategic\" designation yields valuable resources and elevated attention. The ambiguity of the very concept, however, frustrates these conversations. We offer a theory of when decision makers should designate assets as strategic based on the presence of important rivalrous externalities for which firms or military organizations will not produce socially optimal behavior on their own. We distill three forms of these externalities, which involve cumulative-, infrastructure-, and dependency-strategic logics. Although our framework cannot resolve debates about strategic assets, it provides a theoretically grounded conceptual vocabulary to make these debates more productive. To illustrate the analytic value of our framework for thinking about strategic technologies, we examine the US-Japan technology rivalry in the late 1980s and current policy discussions about artificial intelligence.", "journal": "SECURITY STUDIES", "category": "International Relations", "annotated_keywords": ["artificial intelligen"], "label": "1", "title_label": "1"}
{"id": "WOS:000876417800006", "keywords": "Defect detection; Deep learning; Artificial Intelligence of Things; Intelligence inspection", "title": "Development of an Intelligent Defect Detection System for Gummy Candy under Edge Computing", "abstract": "Gummy candies are one of the products of the food industry. It has invested more resources in all aspects of the food production chain to improve production processes. The defective candies cause the unevenness of the product that will cause the appearance, taste and flavor poor. That will lead to economic losses for the company. Most traditional candy companies set up product inspection personnel to eliminate defective product. In this paper, an intelligent defect detection system for gummy candy industry under edge computing environment is proposed. It can replace manual visual inspection, even shorten the processing time to reduce production costs, thereby improving product quality, the efficiency of the production line, and the number of inspections. The system includes: (1) The intelligent defect detection system by deep learning algorithms. (2) The edge computing architecture with AIoT. The proposed system adopted the YOLO deep learning algorithm. The results show that the Precision is 93%, Recall is 87% and the F1 Score is 90. It has certain empirical reference significance for the intelligent defect detection system of candies products. By adopting deep learning algorithm in the detection system, it can reduce the inspection man-power needs and long-term data collection.", "journal": "JOURNAL OF INTERNET TECHNOLOGY", "category": "Computer Science, Information Systems; Telecommunications", "annotated_keywords": ["deep learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000867303200001", "keywords": "multistep ahead; TN prediction; recursive feature elimination; wastewater treatment plant; machine learning", "title": "Investigating Machine Learning Applications for Effective Real-Time Water Quality Parameter Monitoring in Full-Scale Wastewater Treatment Plants", "abstract": "Environmental sensors are utilized to collect real-time data that can be viewed and interpreted using a visual format supported by a server. Machine learning (ML) methods, on the other hand, are excellent in statistically evaluating complicated nonlinear systems to assist in modeling and prediction. Moreover, it is important to implement precise online monitoring of complex nonlinear wastewater treatment plants to increase stability. Thus, in this study, a novel modeling approach based on ML methods is suggested that can predict the effluent concentration of total nitrogen (TNeff) a few hours ahead. The method consists of different ML algorithms in the training stage, and the best selected models are concatenated in the prediction stage. Recursive feature elimination is utilized to reduce overfitting and the curse of dimensionality by finding and eliminating irrelevant features and identifying the optimal subset of features. Performance indicators suggested that the multi-attention-based recurrent neural network and partial least squares had the highest accurate prediction performance, representing a 41% improvement over other ML methods. Then, the proposed method was assessed to predict the effluent concentration with multistep prediction horizons. It predicted 1-h ahead TNeff with a 98.1% accuracy rate, whereas 3-h ahead effluent TN was predicted with a 96.3% accuracy rate.", "journal": "WATER", "category": "Environmental Sciences; Water Resources", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000887115800001", "keywords": "posterior fossa tumor(s); neuro-oncology; artificial intelligence (AI); machine learning; neuroradiology", "title": "Machine Learning in the Classification of Pediatric Posterior Fossa Tumors: A Systematic Review", "abstract": "Simple Summary Diagnosis of posterior fossa tumors is challenging yet proper classification is imperative given that treatment decisions diverge based on tumor type. The aim of this systematic review is to summarize the current state of machine learning methods developed as diagnostic tools for these pediatric brain tumors. We found that, while individual algorithms were quite efficacious, the field is limited by its heterogeneity in methods, outcome reporting, and study populations. We identify common limitations in the study and development of these algorithms and make recommendations as to how they can be overcome. If incorporated into algorithm design, the practical guidelines outlined in this review could help to bridge the gap between theoretical algorithm diagnostic testing and practical clinical application for a wide variety of pathologies. Background: Posterior fossa tumors (PFTs) are a morbid group of central nervous system tumors that most often present in childhood. While early diagnosis is critical to drive appropriate treatment, definitive diagnosis is currently only achievable through invasive tissue collection and histopathological analyses. Machine learning has been investigated as an alternative means of diagnosis. In this systematic review and meta-analysis, we evaluated the primary literature to identify all machine learning algorithms developed to classify and diagnose pediatric PFTs using imaging or molecular data. Methods: Of the 433 primary papers identified in PubMed, EMBASE, and Web of Science, 25 ultimately met the inclusion criteria. The included papers were extracted for algorithm architecture, study parameters, performance, strengths, and limitations. Results: The algorithms exhibited variable performance based on sample size, classifier(s) used, and individual tumor types being investigated. Ependymoma, medulloblastoma, and pilocytic astrocytoma were the most studied tumors with algorithm accuracies ranging from 37.5% to 94.5%. A minority of studies compared the developed algorithm to a trained neuroradiologist, with three imaging-based algorithms yielding superior performance. Common algorithm and study limitations included small sample sizes, uneven representation of individual tumor types, inconsistent performance reporting, and a lack of application in the clinical environment. Conclusions: Artificial intelligence has the potential to improve the speed and accuracy of diagnosis in this field if the right algorithm is applied to the right scenario. Work is needed to standardize outcome reporting and facilitate additional trials to allow for clinical uptake.", "journal": "CANCERS", "category": "Oncology", "annotated_keywords": ["machine learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000445054700011", "keywords": "Depression; Automatic diagnosis; Median Robust extended Local Binary Patterns (MRELBP); Speech processing", "title": "Automated depression analysis using convolutional neural networks from speech", "abstract": "To help clinicians to efficiently diagnose the severity of a person's depression, the affective computing community and the artificial intelligence field have shown a growing interest in designing automated systems. The speech features have useful information for the diagnosis of depression. However, manually designing and domain knowledge are still important for the selection of the feature, which makes the process labor consuming and subjective. In recent years, deep-learned features based on neural networks have shown superior performance to hand-crafted features in various areas. In this paper, to overcome the difficulties mentioned above, we propose a combination of hand-crafted and deep-learned features which can effectively measure the severity of depression from speech. In the proposed method, Deep Convolutional Neural Networks (DCNN) are firstly built to learn deep-learned features from spectrograms and raw speech waveforms. Then we manually extract the state-of-the-art texture descriptors named median robust extended local binary patterns (MRELBP) from spectrograms. To capture the complementary information within the hand-crafted features and deep-learned features, we propose joint fine-tuning layers to combine the raw and spectrogram DCNN to boost the depression recognition performance. Moreover, to address the problems with small samples, a data augmentation method was proposed. Experiments conducted on AVEC2013 and AVEC2014 depression databases show that our approach is robust and effective for the diagnosis of depression when compared to state-of-the-art audio-based methods.", "journal": "JOURNAL OF BIOMEDICAL INFORMATICS", "category": "Computer Science, Interdisciplinary Applications; Medical Informatics", "annotated_keywords": ["artificial intelligen", "neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000569026100001", "keywords": "cardiovascular diseases; disease monitoring; liquid biopsy; multiomics; patient stratification", "title": "Liquid biopsy technologies for hematological diseases", "abstract": "Since the discovery of circulating tumor cells in 1869, technological advances in studying circulating biomarkers from patients' blood have made the diagnosis of nonhematologic cancers less invasive. Technological advances in the detection and analysis of biomarkers provide new opportunities for the characterization of other disease types. When compared with traditional biopsies, liquid biopsy markers, such as exfoliated bladder cancer cells, circulating cell-free DNA (cfDNA), and extracellular vesicles (EV), are considered more convenient than conventional biopsies. Liquid biopsy markers undoubtedly have the potential to influence disease management and treatment dynamics. Our main focuses of this review will be the cell-based, gene-based, and protein-based key liquid biopsy markers (including EV and cfDNA) in disease detection, and discuss the research progress of these biomarkers used in conjunction with liquid biopsy. First, we highlighted the key technologies that have been broadly adopted used in hematological diseases. Second, we introduced the latest technological developments for the specific detection of cardiovascular disease, leukemia, and coronavirus disease. Finally, we concluded with perspectives on these research areas, focusing on the role of microfluidic technology and artificial intelligence in point-of-care medical applications. We believe that the noninvasive capabilities of these technologies have great potential in the development of diagnostics and can influence treatment options, thereby advancing precision disease management.", "journal": "MEDICINAL RESEARCH REVIEWS", "category": "Chemistry, Medicinal; Pharmacology & Pharmacy", "annotated_keywords": ["artificial intelligen"], "label": "1", "title_label": "0"}
{"id": "WOS:000845893600001", "keywords": "artificial intelligence; respiratory exacerbation; multi-modal; sensor network; personalized medicine", "title": "A Personalized Respiratory Disease Exacerbation Prediction Technique Based on a Novel Spatio-Temporal Machine Learning Architecture and Local Environmental Sensor Networks", "abstract": "Chronic respiratory diseases, such as the Chronic Obstructive Pulmonary Disease (COPD) and asthma, are a serious health crisis, affecting a large number of people globally and inflicting major costs on the economy. Current methods for assessing the progression of respiratory symptoms are either subjective and inaccurate, or complex and cumbersome, and do not incorporate environmental factors to track individualized risks. Lacking predictive assessments and early intervention, unexpected exacerbations often lead to hospitalizations and high medical costs. This work presents a multi-modal solution for predicting the exacerbation risks of respiratory diseases, such as COPD, based on a novel spatio-temporal machine learning architecture for real-time and accurate respiratory events detection, and tracking of local environmental and meteorological data and trends. The proposed new neural network model blends key attributes of both convolutional and recurrent neural architectures, allowing extraction of the salient spatial and temporal features encoded in respiratory sounds, thereby leading to accurate classification and tracking of symptoms. Combined with the data from environmental and meteorological sensors, and a predictive model based on retrospective medical studies, this solution can assess and provide early warnings of respiratory disease exacerbations, thereby potentially reducing hospitalization rates and medical costs.", "journal": "ELECTRONICS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Physics, Applied", "annotated_keywords": ["neural net", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000512480900004", "keywords": "Machine learning; Cooperative spectrum sensing; Cognitive radio", "title": "Clustering formation in cognitive radio networks using machine learning", "abstract": "The goal of spectrum sensing is to elevate the detection performance of secondary users (SUs) in a cognitive radio network (CRN). In cooperative spectrum sensing, all secondary users (SUs) of the network deliver their sensing measurement to the fusion center (FC) for the final decision regarding the activity of primary user (PU). The collaboration among large number of SUs might create overhead for the FC. To improve the performance of cooperative spectrum sensing, a novel method is proposed, which segregates the network into clusters. We have used artificial intelligence to make the clusters. The formation of clusters is made based on machine learning affinity propagation algorithm. Using proposed method, SUs share local messages with their neighbors until a highest class of cluster heads are chosen and a corresponding clustering configuration is made. The messages are evaluated depend on measures of similarity between the SUs, which are selected based on the objective of the clustering process. The sensing message of delimited number of SUs is shared with their cluster heads, which is ultimately shared with the FC for final decision. The proposed approach obtains the highest energy and performance efficiency in comparison with conventional clustering schemes. (C) 2019 Elsevier GmbH. All rights reserved.", "journal": "AEU-INTERNATIONAL JOURNAL OF ELECTRONICS AND COMMUNICATIONS", "category": "Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": ["artificial intelligen", "machine learning", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000830333700001", "keywords": "artificial skin; intelligent robots; mechanoluminescence; user-interactive interface; visual tactile sensing", "title": "Self-Powered All-Optical Tactile Sensing Platform for User-Interactive Interface", "abstract": "User-interactive interfaces, converting tactile stimuli into readable signals to users and devices simultaneously, improve the communication and interaction between human and machines and therefore greatly contribute to the safety and dexterity during the interactions. However, the concomitant challenges of current user-interactive interfaces such as complex architecture, massive electrodes and fussy cable connections, bulky and unhandy power supply, and deficiency in multistimuli responses have yet to be solved. Herein, an all-optical tactile sensing platform consisting of heterogeneous mechanoluminescent materials and polymer matrix is proposed for the conversion of multiple tactile stimuli into heterochromatic lights in an untethered and self-powered manner. The all-optical tactile sensing platform can respond to tiny shear force such as fingertip slipping with a low limit of 2 N and wide range of strains ranging from 30% to 70% with appropriate discrimination. Most importantly, the visualization of tactile stimuli with human- and machine-readability and vividness permit remote and wireless user-interactive applications such as videogames and RC car control, assisted with the developed method of active optical signal recognition. This work presents a paradigm shift to user-interactive interfaces, boosting their implementation in multitudinous areas such as artificial intelligences and the Internet of Things.", "journal": "ADVANCED MATERIALS TECHNOLOGIES", "category": "Materials Science, Multidisciplinary", "annotated_keywords": ["artificial intelligen"], "label": "0", "title_label": "0"}
{"id": "WOS:000337382100001", "keywords": "signal strength; function approximation; propagation loss; neuro-fuzzy; empirical models", "title": "Application of Reinforcement Learning in Cognitive Radio Networks: Models and Algorithms", "abstract": "Cognitive radio (CR) enables unlicensed users to exploit the underutilized spectrum in licensed spectrum whilst minimizing interference to licensed users. Reinforcement learning (RL), which is an artificial intelligence approach, has been applied to enable each unlicensed user to observe and carry out optimal actions for performance enhancement in a wide range of schemes in CR, such as dynamic channel selection and channel sensing. This paper presents new discussions of RL in the context of CR networks. It provides an extensive review on how most schemes have been approached using the traditional and enhanced RL algorithms through state, action, and reward representations. Examples of the enhancements on RL, which do not appear in the traditional RL approach, are rules and cooperative learning. This paper also reviews performance enhancements brought about by the RL algorithms and open issues. This paper aims to establish a foundation in order to spark new research interests in this area. Our discussion has been presented in a tutorial manner so that it is comprehensive to readers outside the specialty of RL and CR.", "journal": "SCIENTIFIC WORLD JOURNAL", "category": "Multidisciplinary Sciences", "annotated_keywords": ["artificial intelligen"], "label": "1", "title_label": "1"}
{"id": "WOS:000769376300001", "keywords": "early arthritis; diagnosis; machine learning; artificial intelligence; Telehealth", "title": "Pilot study of a machine-learning tool to assist in the diagnosis of hand arthritis", "abstract": "Background Arthritis is a common condition, which frequently involves the hands. Patients with inflammatory arthritis have been shown to experience significant delays in diagnosis. Aim To develop and test a screening tool combining an image of a patient's hands, a short series of questions and a single examination technique to determine the most likely diagnosis in a patient presenting with hand arthritis. Machine learning techniques were used to develop separate algorithms for each component, which were combined to produce a diagnosis. Methods A total of 280 consecutive new patients presenting to a rheumatology practice with hand arthritis were enrolled. Each patient completed a nine-part questionnaire, had photographs taken of each hand and had a single examination result recorded. The rheumatologist diagnosis was recorded following a 45-min consultation. The photograph algorithm was developed from 1000 previous hand images and machine learning techniques were applied to the questionnaire results, training several models against the diagnosis from the rheumatologist. Results The combined algorithms in the present study were able to predict inflammatory arthritis with an accuracy, precision, recall and specificity of 96.8%, 97.2%, 98.6% and 90.5% respectively. Similar results were found when inflammatory arthritis was subclassified into rheumatoid arthritis and psoriatic arthritis. The corresponding figures for osteoarthritis were 79.6%, 85.9%, 61.9% and 92.6%. Conclusion The present study demonstrates a novel application combining image processing and a patient questionnaire with applied machine-learning methods to facilitate the diagnosis of patients presenting with hand arthritis. Preliminary results are encouraging for the application of such techniques in clinical practice.", "journal": "INTERNAL MEDICINE JOURNAL", "category": "Medicine, General & Internal", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000729796100001", "keywords": "AI; Ploidy status; time-lapse; PGT; Prediction", "title": "An artificial intelligence model (euploid prediction algorithm) can predict embryo ploidy status based on time-lapse data", "abstract": "Background For the association between time-lapse technology (TLT) and embryo ploidy status, there has not yet been fully understood. TLT has the characteristics of large amount of data and non-invasiveness. If we want to accurately predict embryo ploidy status from TLT, artificial intelligence (AI) technology is a good choice. However, the current work of AI in this field needs to be strengthened. Methods A total of 469 preimplantation genetic testing (PGT) cycles and 1803 blastocysts from April 2018 to November 2019 were included in the study. All embryo images are captured during 5 or 6 days after fertilization before biopsy by time-lapse microscope system. All euploid embryos or aneuploid embryos are used as data sets. The data set is divided into training set, validation set and test set. The training set is mainly used for model training, the validation set is mainly used to adjust the hyperparameters of the model and the preliminary evaluation of the model, and the test set is used to evaluate the generalization ability of the model. For better verification, we used data other than the training data for external verification. A total of 155 PGT cycles from December 2019 to December 2020 and 523 blastocysts were included in the verification process. Results The euploid prediction algorithm (EPA) was able to predict euploid on the testing dataset with an area under curve (AUC) of 0.80. Conclusions The TLT incubator has gradually become the choice of reproductive centers. Our AI model named EPA that can predict embryo ploidy well based on TLT data. We hope that this system can serve all in vitro fertilization and embryo transfer (IVF-ET) patients in the future, allowing embryologists to have more non-invasive aids when selecting the best embryo to transfer.", "journal": "REPRODUCTIVE BIOLOGY AND ENDOCRINOLOGY", "category": "Endocrinology & Metabolism; Reproductive Biology", "annotated_keywords": ["artificial intelligen", "artificial intelligen"], "label": "1", "title_label": "1"}
{"id": "WOS:000856945900006", "keywords": "Ultimate bearing capacity; Shallow foundations; Multi expression programming; Cohesionless soil", "title": "Determining ultimate bearing capacity of shallow foundations by using multi expression programming (MEP)", "abstract": "This study presents an artificial intelligence approach, namely multi expression programming (MEP), for determining ultimate bearing capacity of shallow foundations on cohesionless soils. Five governing parameters (i.e., internal friction angle, soil unit weight, the length to width ratio of foundation, foundation depth and foundation width) were used as input variables to develop the MEP model. Through the determination of the optimal parameter setting of MEP, a group of expressions were proposed. Then, the MEP model was compared with linear multiple regression, non-linear multiple regression and several previous models, and three statistical indices (i.e., coefficient of determination (R-2), root mean squared error (RMSE) and mean absolute error (MAE)) were employed to evaluate the prediction accuracy of these models. The results show that the proposed model has higher prediction precision than the other models, with higher R-2 value and lower RMSE and MAE values. Additionally, a monotonicity analysis was performed to verify the correct relationship between ultimate bearing capacity and various factors. From the monotonicity analysis, the ultimate bearing capacity increases with the increase of internal friction angle (Psi), soil unit weight (gamma), foundation width (B) and foundation depth (D), whereas it decreases with the increase of the length to width ratio of foundation (L/B). Then, a sensitivity analysis was performed. Through the sensitivity analysis, the effect rank of the five input parameters on ultimate bearing capacity is phi> B > D > gamma > L/B. Finally, a graphical user interface (GUI) of the MEP model is developed for practical application.", "journal": "ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE", "category": "Automation & Control Systems; Computer Science, Artificial Intelligence; Engineering, Multidisciplinary; Engineering, Electrical & Electronic", "annotated_keywords": ["artificial intelligen"], "label": "1", "title_label": "0"}
{"id": "WOS:000818943600008", "keywords": "WCE images; adversarial attacks; FGSM noise; salt and pepper noise; feature fusion; deep learning", "title": "A Two Stream Fusion Assisted Deep Learning Framework for Stomach Diseases Classification", "abstract": "Due to rapid development in Artificial Intelligence (AI) and Deep Learning (DL), it is difficult to maintain the security and robustness of these techniques and algorithms due to emergence of novel term adversary sampling. Such technique is sensitive to these models. Thus, fake samples cause AI and DL model to produce diverse results. Adversarial attacks that successfully implemented in real world scenarios highlight their applicability even further. In this regard, minor modifications of input images cause ???Adversarial Attacks??? that altered the performance of competing attacks dramatically. Recently, such attacks and defensive strategies are gaining lot of attention by the machine learning and security researchers. Doctors use different kinds of technologies to examine the patient abnormalities including Wireless Capsule Endoscopy (WCE). However, using WCE it is very difficult for doctors to detect an abnormality within images since it takes enough time while inspection and deciding abnormality. As a result, it took weeks to generate patients test report, which is tiring and strenuous for them. Therefore, researchers come out with the solution to adopt computerized technologies, which are more suitable for the classification and detection of such abnormalities. As far as the classification is concern, the adversarial attacks generate problems in classified images. Now days, to handle this issue machine learning is mainstream defensive approach against adversarial attacks. Hence, this research exposes the attacks by altering the datasets with noise including salt and pepper and Fast Gradient Sign Method (FGSM) and then reflects that how machine learning algorithms work fine to handle these noises in order to avoid attacks. Results obtained on the WCE images which are vulnerable to adversarial attack are 96.30% accurate and prove that the proposed defensive model is robust when compared to competitive existing methods.", "journal": "CMC-COMPUTERS MATERIALS & CONTINUA", "category": "Computer Science, Information Systems; Materials Science, Multidisciplinary", "annotated_keywords": ["machine learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000687044300001", "keywords": "Risk assessment; Coal mine gas accidents; The t-distributed stochastic neighbor embedding (t-SNE); Genetic algorithm (GA); Support vector machines (SVM)", "title": "Algorithms Acting Badly: A Solution from Corporate Law", "abstract": "Sometimes algorithms work against us. They offer many social benefits, but when they discriminate in lending, manipulate stock markets, or violate expectations of privacy, they can injure us on a massive scale. Only one-third of technologists predict that artificial intelligence will be a net positive for society. Law can help ensure that algorithms work for us by imposing liability when they work against us. The problem is that algorithms fit poorly into existing conceptions of liability. Liability requires injurious acts, but what does it mean for an algorithm to act? Only people act; and algorithms are not people. Some scholars have argued that the law should recognize sophisticated algorithms as people. However, the philosophical puzzles (are algorithms really people?), practical obstacles (how do you punish an algorithm?), and unexpected consequences (could algorithmic \"people\" sue us back?) have proven insurmountable. This Article proposes a more grounded approach to algorithmic liability. Corporations currently design and run the algorithms that have the most significant social impacts. Longstanding principles of corporate liability already recognize that corporations are \"people\" capable of acting injuriously. Corporate law stipulates that corporations act through their employees because corporations have control over and benefit from employee conduct. When employees misbehave, corporations are in the best position to discipline and correct them. This Article argues that the same control and benefit rationales extend to corporate algorithms. If the law were to recognize that algorithmic conduct could qualify as corporate action, the whole framework of corporate liability would kick in. By exercising the authority it already has over corporations, the law could help ensure that corporate algorithms work largely in our favor.", "journal": "GEORGE WASHINGTON LAW REVIEW", "category": "Law", "annotated_keywords": ["artificial intelligen"], "label": "1", "title_label": "0"}
{"id": "WOS:000754480800006", "keywords": "CNN; FPGA; Acceleration; Co-design; PYNQ-Z1", "title": "Deep convolutional neural networks-based Hardware-Software on-chip system for computer vision application", "abstract": "Embedded vision systems are the best solutions for high-performance and lightning-fast inspection tasks. As everyday life evolves, it becomes almost imperative to harness artificial intelligence (AI) in vision applications that make these systems intelligent and able to make decisions close to or similar to humans. In this context, the AI's integration on embedded systems poses many challenges, given that its performance depends on data volume and quality they assimilate to learn and improve. This returns to the energy consumption and cost constraints of the FPGA-SoC that have limited processing, memory, and communication capacity. Despite this, the AI algorithm implementation on embedded systems can drastically reduce energy consumption and processing times, while reducing the costs and risks associated with data transmission. Therefore, its efficiency and reliability always depend on the designed prototypes. Within this range, this work proposes two different designs for the Traffic Sign Recognition (TSR) application based on the convolutional neural network (CNN) model, followed by three implantations on PYNQ-Z1. Firstly, we propose to implement the CNN-based TSR application on the PYNQ-Z1 processor. Considering its runtime result of around 3.55 s, there is room for improvement using programmable logic (PL) and processing system (PS) in a hybrid architecture. Therefore, we propose a streaming architecture, in which the CNN layers will be accelerated to provide a hardware accelerator for each layer where direct memory access (DMA) interface is used. Thus, we noticed efficient power consumption, decreased hardware cost, and execution time optimization of 2.13 s, but, there was still room for design optimizations. Finally, we propose a second co-design, in which the CNN will be accelerated to be a single computation engine where BRAM interface is used. The implementation results prove that our proposed embedded TSR design achieves the best performances compared to the first proposed architectures, in terms of execution time of about 0.03 s, computation roof of about 36.6 GFLOPS, and bandwidth roof of about 3.2 GByte/s.", "journal": "COMPUTERS & ELECTRICAL ENGINEERING", "category": "Computer Science, Hardware & Architecture; Computer Science, Interdisciplinary Applications; Engineering, Electrical & Electronic", "annotated_keywords": ["artificial intelligen", "neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000429763800051", "keywords": "Waste tailings; Cemented paste backfill; Recycling; Strength prediction; Boosted regression trees; Particle swarm optimization", "title": "A strength prediction model using artificial intelligence for recycling waste tailings as cemented paste backfill", "abstract": "The recycling of waste tailings as cemented paste backfill (CPB) has attracted worldwide attention because of the increasing environmental awareness during mineral resources excavation. However, lots of mechanical tests are required to understand the strength development of CPB and its prediction under the combined effect of influencing variables is almost an unexplored field. This study proposes a strength prediction model integrating boosted regression trees (BRT) and particle swarm optimization (PSO), where the BRT algorithm was used for modelling the non-linear relationship between inputs and outputs and PSO was used for the BRT hyper-parameters tuning. An extensive mechanical experiment was performed to provide the dataset for the PSO-BRT model. This dataset contained unconfined compressive strength (UCS) results of 585 CPB specimens produced with a different combination of influencing variables, including the physical and chemical characteristics of tailings, the cement-tailings ratio, the solids content, and the curing time. 10-fold cross validation was used as the validation method, and performance measures were chosen as the mean squared error and the correlation coefficient. The results show that PSO was efficient in the hyper-parameters tuning of the BRT. The optimum BRT model was very accurate at predicting CPB strength. The relative importance of influencing variables was investigated, in which the cement-tailings ratio was found to be the most significant variable for CPB strength. This research indicates that more efficient reuse of waste tailings as CPB can be achieved by reducing the required number of mechanical experiments during engineering applications. (C) 2018 Elsevier Ltd. All rights reserved.", "journal": "JOURNAL OF CLEANER PRODUCTION", "category": "Green & Sustainable Science & Technology; Engineering, Environmental; Environmental Sciences", "annotated_keywords": ["artificial intelligen"], "label": "1", "title_label": "1"}
{"id": "WOS:000414961100023", "keywords": "Case-based reasoning; Artificial neural networks; Similarity measure; k-NN similarity measure; Data mining", "title": "Hybrid case-based reasoning system by cost-sensitive neural network for classification", "abstract": "Case-based reasoning (CBR) is an artificial intelligent approach to learning and problem-solving, which solves a target problem by relating past similar solved problems. But it faces the challenge of weights assignment to features to measure similarity between cases. There are many methods to overcome this feature weighting problem of CBR. However, neural network's pruning is one of the powerful and useful methods to overcome this feature weighting problem, which extracts feature weights from trained neural network without losing the generality of training set by four popular mechanisms: sensitivity, activity, saliency and relevance. It is habitually assumed that the training sets used for learning are balanced. However, this hypothesis is not always true in real-world applications, and hence, the tendency is to yield classification models that are biased toward the overrepresented class. Therefore, a hybrid CBR system is proposed in this paper to overcome this problem, which adopts a cost-sensitive back-propagation neural network (BPNN) in network pruning to find feature weights. These weights are used in CBR. A single cost parameter is used by the cost-sensitive BPNN to distinguish the importance of class errors. A balanced decision boundary is generated by the cost parameter using prior information. Thus, the class imbalance problem of network pruning is overcome to improve the accuracy of the hybrid CBR. From the empirical results, it is observed that the performance of the proposed hybrid CBR system is better than the hybrid CBR by standard neural network. The performance of the proposed hybrid system is validated with seven datasets.", "journal": "SOFT COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": ["artificial intelligen", "neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000528834600009", "keywords": "Light propagation simulation; Feature extraction; Monte-Carlo method; 3D model", "title": "3D Model Feature Extraction Based on Light Propagation Simulation with Monte Carlo Method", "abstract": "Three-dimensional model has been showing extensive demand and vitality in modern industrial design, artificial intelligence and software design fields. Traditional feature extraction methods merely depend on model surface feature, which could not sufficiently satisfy complex model feature extraction needs. In order to improve the accuracy of model feature extraction, a 3D model feature extraction method with high discrimination was proposed based on spectral analysis and light propagation attributes. Firstly, the probability of light transmission, scattering and reflection when light propagation in different medium was quantitatively analyzed with scattering coefficient, absorption coefficient and anisotropy parameters. Secondly, the Monte-Carlo method was used to simulate light propagation in complex 3D model, where different feature statistics including angle, distance and energy were obtained to complete feature extraction. Then, the influence factors of photon beam number and constrained space shape were tested for optimal parameters determination. Finally, the feature extraction effectiveness was evaluated on retrieval precision, recall and E-measure. The results showed that the feature extraction accuracy sensitively varied with constrained space shape and the optimal constrained space for photon propagation was sphere; The feature extraction efficiency decreased with more photon beams, and within basic accuracy requirement, 10 000 to 25 000 photon beams were the optimal simulation number; The feature extraction accuracy of proposed method was higher than the wavelet transform, distance-angle and D2 distribution methods, which is more suitable for offline feature extraction of complex 3D models. The proposed simulation method of feature extraction broadens spectral analysis application, which could extract the integrated feature between model surface and internal structure, promoting model feature extraction research.", "journal": "SPECTROSCOPY AND SPECTRAL ANALYSIS", "category": "Spectroscopy", "annotated_keywords": ["artificial intelligen"], "label": "1", "title_label": "1"}
{"id": "WOS:000881825800090", "keywords": "COVID-19; Chest CT; Artificial intelligence; Deep learning; External validation", "title": "Predicting health crises from early warning signs in patient medical records", "abstract": "The COVID-19 global pandemic has caused unprecedented worldwide changes in healthcare delivery. While containment and mitigation approaches have been intensified, the progressive increase in the number of cases has overwhelmed health systems globally, highlighting the need for anticipation and prediction to be the basis of an efficient response system. This study demonstrates the role of population health metrics as early warning signs of future health crises. We retrospectively collected data from the emergency department of a large academic hospital in the northeastern United States from 01/01/2019 to 08/07/2021. A total of 377,694 patient records and 303 features were included for analysis. Departing from a multivariate artificial intelligence (AI) model initially developed to predict the risk of high-flow oxygen therapy or mechanical ventilation requirement during the COVID-19 pandemic, a total of 19 original variables and eight engineered features showing to be most predictive of the outcome were selected for further analysis. The temporal trends of the selected variables before and during the pandemic were characterized to determine their potential roles as early warning signs of future health crises. Temporal analysis of the individual variables included in the high-flow oxygen model showed that at a population level, the respiratory rate, temperature, low oxygen saturation, number of diagnoses during the first encounter, heart rate, BMI, age, sex, and neutrophil percentage demonstrated observable and traceable changes eight weeks before the first COVID-19 public health emergency declaration. Additionally, the engineered rule-based features built from the original variables also exhibited a pre-pandemic surge that preceded the first pandemic wave in spring 2020. Our findings suggest that the changes in routine population health metrics may serve as early warnings of future crises. This justifies the development of patient health surveillance systems, that can continuously monitor population health features, and alarm of new approaching public health crises before they become devastating.", "journal": "SCIENTIFIC REPORTS", "category": "Multidisciplinary Sciences", "annotated_keywords": ["artificial intelligen"], "label": "1", "title_label": "1"}
{"id": "WOS:000663113000001", "keywords": "complex materials; soft materials; resonant soft x-ray scattering; in situ characterization; sample environment; multimodal; x-ray spectroscopy", "title": "Probing morphology and chemistry in complex soft materials with in situ resonant soft x-ray scattering", "abstract": "Small angle scattering methodologies have been evolving at fast pace over the past few decades due to the ever-increasing demands for more details on the complex nanostructures of multiphase and multicomponent soft materials like polymer assemblies and biomaterials. Currently, element-specific and contrast variation techniques such as resonant (elastic) soft/tender x-ray scattering, anomalous small angle x-ray scattering, and contrast-matching small angle neutron scattering, or combinations of above are routinely used to extract the chemical composition and spatial arrangement of constituent elements at multiple length scales and examine electronic ordering phenomena. Here we present some recent advances in selectively characterizing structural architectures of complex soft materials, which often contain multi-components with a wide range of length scales and multiple functionalities, where novel resonant scattering approaches have been demonstrated to decipher a higher level of structural complexity that correlates to functionality. With the advancement of machine learning and artificial intelligence assisted correlative analysis, high-throughput and autonomous experiments would open a new paradigm of material research. Further development of resonant x-ray scattering instrumentation with crossplatform sample environments will enable multimodal in situ/operando characterization of the system dynamics with much improved spatial and temporal resolution.", "journal": "JOURNAL OF PHYSICS-CONDENSED MATTER", "category": "Physics, Condensed Matter", "annotated_keywords": ["artificial intelligen", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000573099200003", "keywords": "Routing; baggage handling systems; deep reinforcement learning", "title": "Routing in congested baggage handling systems using deep reinforcement learning", "abstract": "The increasing number of people choosing to travel by airplane puts pressure on the baggage handling systems in airports. As the load increases, the risk of deadlocks in the systems increase as well. Therefore, it is increasingly important to find routing solutions which can handle the high loads. Currently this is achieved by using shortest path algorithms and hand engineered site-specific routing rules, based on the experience of the employees and on trial and error processes using complex emulators. This is a time-consuming and costly approach, as every airport needs its own set of routing rules. New development within machine learning, and especially reinforcement learning allows very complex control policies to be found in large environments. This could therefore potentially solve the need of manually creating site-specific routing rules. This paper proposes to use a single global deep reinforcement learning agent to route a fleet of baggage-totes to continuously pick up and deliver baggage in simple yet functionally realistic simulations of baggage handling systems. This is achieved using a Dueling DQN architecture with prioritized experience reply and a multi action approach. Training and testing are performed in three baggage handling system environments of different size and complexity. The results show that by training with a broad distribution of loads, it is possible to get a model, capable of routing in highly congested baggage handling systems. The results also show that the reinforcement learning agent can limit the number of deadlocks up until a higher load than both a static shortest path and a dynamic shortest path method, even if the dynamic shortest path method is using a naive deadlock avoidance add-on.", "journal": "INTEGRATED COMPUTER-AIDED ENGINEERING", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Engineering, Multidisciplinary", "annotated_keywords": ["machine learning", "reinforcement learning", "reinforcement learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000839719100001", "keywords": "deep reinforcement learning; entropy; model initialization; exploration", "title": "Entropy-Aware Model Initialization for Effective Exploration in Deep Reinforcement Learning", "abstract": "Effective exploration is one of the critical factors affecting performance in deep reinforcement learning. Agents acquire data to learn the optimal policy through exploration, and if it is not guaranteed, the data quality deteriorates, which leads to performance degradation. This study investigates the effect of initial entropy, which significantly influences exploration, especially in the early learning stage. The results of this study on tasks with discrete action space show that (1) low initial entropy increases the probability of learning failure, (2) the distributions of initial entropy for various tasks are biased towards low values that inhibit exploration, and (3) the initial entropy for discrete action space varies with both the initial weight and task, making it hard to control. We then devise a simple yet powerful learning strategy to deal with these limitations, namely, entropy-aware model initialization. The proposed algorithm aims to provide a model with high initial entropy to a deep reinforcement learning algorithm for effective exploration. Our experiments showed that the devised learning strategy significantly reduces learning failures and enhances performance, stability, and learning speed.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": ["reinforcement learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000357751200024", "keywords": "Machine learning; Reinforcement learning; Adaptive critic; Goal representation heuristic dynamic programming; Virtual reality", "title": "Data-driven heuristic dynamic programming with virtual reality", "abstract": "In this paper, we propose a virtual reality (VR) platform as a case study of machine learning, in this case applied to the goal representation heuristic dynamic programming (GrHDP) approach. In general, a VR platform normally includes a physical module, a control/learning module, and a VR module. It facilitates machine learning research, where scientists and engineers can participate in the simulation process to analyze dynamic experiments. The internal structure of the VR platform can be replaced according to different research targets, so the platform can be extended to other applications. In this paper, we present the detailed VR design strategy, with a number of applications, including a triple-link inverted pendulum balancing problem, a maze navigation problem, and a robot navigation with obstacle avoidance. (C) 2015 Elsevier B.V. All rights reserved.", "journal": "NEUROCOMPUTING", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000589001900001", "keywords": "Model-driven method; Intelligent assembly; Probabilistic tree; Adaptive reward mechanism; Reinforcement learning; Physical simulation engine", "title": "An adaptive adjustment strategy for bolt posture errors based on an improved reinforcement learning algorithm", "abstract": "Designing an intelligent and autonomous system remains a great challenge in the assembly field. Most reinforcement learning (RL) methods are applied to experiments with relatively small state spaces. However, the complicated situation and high-dimensional spaces of the assembly environment cause traditional RL methods to behave poorly in terms of their efficiency and accuracy. In this paper, a model-driven adaptive proximal proximity optimization (MAPPO) method was presented to make the assembly system autonomously rectify the bolt posture error. In the MAPPO method, a probabilistic tree and adaptive reward mechanism were used to improve the calculation efficiency and accuracy of the traditional PPO method. The size of the action space was reduced by establishing a hierarchical logical relationship for each parameter with a probabilistic tree. Based on an adaptive reward mechanism, the phenomenon that the algorithm easily falls into local minima could be improved. Finally, the proposed method was verified based on the Unity simulation engine. The advancement and robustness of the proposed model were also validated by comparing different cases in simulations and experiments. The results revealed that MAPPO has better algorithm efficiency and accuracy compared with other state-of-the-art algorithms.", "journal": "APPLIED INTELLIGENCE", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["reinforcement learning", "reinforcement learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000796480800005", "keywords": "Tamil character; TEAS; recognition; DCELM; NM; binarization; dataset; cropping", "title": "Newton Algorithm Based DELM for Enhancing Offline Tamil Handwritten Character Recognition", "abstract": "Numerous research based on offline Tamil recognition deals only with few Tamil characters since it becomes extremely complicated in distinguishing small variations in large handwritten document. The writer's complexity affects the overall formation of the characters. Such types of complexities are due to discontinuation of structures, unnecessary over loops, variation in shapes as well as irregular curves. This complex issue results in enhanced error value rate. Therefore, to conquer such issues, this paper proposes a novel approach to enhance the offline Tamil handwritten character recognition by utilizing four principal steps: pre-processing, segmentation, feature extraction and classification. For optimal segmentation of Tamil characters, this paper utilizes the Tsallis entropy approach-based atom search (TEAS) optimization algorithm. Then a Newton algorithm based deep convolution extreme learning (DELM) approach is utilized for the extraction and classification of input images. Finally, experiments are carried out for numerous Tamil handwritten recognition-based approaches. The proposed Tamil character recognition utilizes the datasets of isolated Tamil handwritten characters established by HP lab India to evaluate the efficiency of the system.", "journal": "INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000353551500007", "keywords": "Camera-based document recognition; whiteboard reading; mind map recognition; handwriting recognition; document layout analysis", "title": "Camera-Based Whiteboard Reading for Understanding Mind Maps", "abstract": "Mind maps, i.e. the spatial organization of ideas and concepts around a central topic and the visualization of their relations, represent a very powerful and thus popular means to support creative thinking and problem solving processes. Typically created on traditional whiteboards, they represent an important technique for collaborative brainstorming sessions. We describe a camera-based system to analyze hand-drawn mind maps written on a whiteboard. The goal of the presented system is to produce digital representations of such mind maps, which would enable digital asset management, i.e. storage and retrieval of manually created documents. Our system is based on image acquisition by means of a camera, followed by the segmentation of the particular whiteboard image focusing on the extraction of written context, i.e. the ideas captured by the mind map. The spatial arrangement of these ideas is recovered using layout analysis based on unsupervised clustering, which results in graph representations of mind maps. Finally, handwriting recognition derives textual transcripts of the ideas captured by the mind map. We demonstrate the capabilities of our mind map reading system by means of an experimental evaluation, where we analyze images of mind maps that have been drawn on whiteboards, without any further constraints other than the underlying topic. In addition to the promising recognition results, we also discuss training strategies, which effectively allow for system bootstrapping using out-of-domain sample data. The latter is important when addressing creative thinking processes where domain-related training data are difficult to obtain as they focus on novelty by definition.", "journal": "INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000440310900012", "keywords": "Artificial neural networks; Deep learning; Feature extraction; Feature selection; Human activity recognition", "title": "Efficiency investigation of artificial neural networks in human activity recognition", "abstract": "Nowadays, human activity recognition (HAR) is an important component of many ambient intelligent solutions where accelerometer and gyroscope signals give the information about the physical activity of an observed person. It has gained increasing attention by the availability of commercial wearable devices such as smartphones, smartwatches, etc. Previous studies have shown that HAR can be seen as a general machine learning problem with a particular data pre-processing stage. In the last decade, several researchers measured high recognition rates on public data sets with numerous \"shallow\" machine learning techniques. In some cases artificial neural networks (ANNs) produced better performance than other shallow techniques while in other cases it was less effective. After the appearance of deep learning, a significant part of HAR researches turned toward more complex solutions such as convolutional neural networks (CNNs) and they claimed that CNNs can substitute the feature extraction stage in shallow techniques and can outperform them. Therefore in the current state of the art, the efficiency of ANNs against CNNs and other machine learning techniques is unclear. The aim of this study is to investigate the performance of more ANN structures with different hyper-parameters and inputs on two public databases. The result will show that the two key factors in ANN design are the data-preprocessing and the hyper-parameter setup because the accuracy difference between a well and a badly parameterized ANN is huge. A well-tuned ANN with extracted features can outperform other machine learning methods in the HAR problem including CNN classifiers.", "journal": "JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems; Telecommunications", "annotated_keywords": ["neural net", "neural net", "machine learning", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000330628100010", "keywords": "Smartphone sensing; Activity recognition; Community-guided learning", "title": "Community Similarity Networks", "abstract": "Sensor-enabled smartphones are opening a new frontier in the development of mobile sensing applications. The recognition of human activities and context from sensor data using classification models underpins these emerging applications. However, conventional approaches to training classifiers struggle to cope with the diverse user populations routinely found in large-scale popular mobile applications. Differences between users (e.g., age, sex, behavioral patterns, lifestyle) confuse classifiers, which assume everyone is the same. To address this, we propose Community Similarity Networks (CSN), which incorporates inter-person similarity measurements into the classifier training process. Under CSN, every user has a unique classifier that is tuned to their own characteristics. CSN exploits crowd-sourced sensor data to personalize classifiers with data contributed from other similar users. This process is guided by similarity networks that measure different dimensions of inter-person similarity. Our experiments show CSN outperforms existing approaches to classifier training under the presence of population diversity.", "journal": "PERSONAL AND UBIQUITOUS COMPUTING", "category": "Computer Science, Information Systems; Telecommunications", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000523306100005", "keywords": "CSL; Recognition; Generation; RNN; Bidirectional communication; Probability model", "title": "Skeleton-based Chinese sign language recognition and generation for bidirectional communication between deaf and hearing people", "abstract": "Chinese sign language (CSL) is one of the most widely used sign language systems in the world. As such, the automatic recognition and generation of CSL is a key technology enabling bidirectional communication between deaf and hearing people. Most previous studies have focused solely on sign language recognition (SLR), which only addresses communication in a single direction. As such, there is a need for sign language generation (SLG) to enable communication in the other direction (i.e., from hearing people to deaf people). To achieve a smoother exchange of ideas between these two groups, we propose a skeleton-based CSL recognition and generation framework based on a recurrent neural network (RNN), to support bidirectional CSL communication. This process can also be extended to other sequence-to-sequence information interactions. The core of the proposed framework is a two-level probability generative model. Compared with previous techniques, this approach offers a more flexible approximate posterior distribution, which can produce skeletal sequences of varying styles that are recognizable to humans. In addition, the proposed generation method compensated for a lack of training data. A series of experiments in bidirectional communication were conducted on the large 500 CSL dataset. The proposed algorithm achieved high recognition accuracy for both real and synthetic data, with a reduced runtime. Furthermore, the generated data improved the performance of the discriminator. These results suggest the proposed bidirectional communication framework and generation algorithm to be an effective new approach to CSL recognition. (c) 2020 Elsevier Ltd. All rights reserved.", "journal": "NEURAL NETWORKS", "category": "Computer Science, Artificial Intelligence; Neurosciences", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000626775000033", "keywords": "Action recognition; feature fusion; depth motion maps; completed local binary pattern; polynormal", "title": "Fusing appearance and motion information for action recognition on depth sequences", "abstract": "With the advent of cost-efficient depth cameras, many effective feature descriptors have been proposed for action recognition from depth sequences. However, most of them are based on single feature and thus unable to extract the action information comprehensively, e.g., some kinds of feature descriptors can represent the area where the motion occurs while they lack the ability of describing the order in which the action is performed. In this paper, a new feature representation scheme combining different feature descriptors is proposed to capture various aspects of action cues simultaneously. First of all, a depth sequence is divided into a series of sub-sequences using motion energy based spatial-temporal pyramid. For each sub-sequence, on the one hand, the depth motion maps (DMMs) based completed local binary pattern (CLBP) descriptors are calculated through a patch-based strategy. On the other hand, each sub-sequence is partitioned into spatial grids and the polynormals descriptors are obtained for each of the grid sequences. Then, the sparse representation vectors of the DMMs based CLBP and the polynormals are calculated separately. After pooling, the ultimate representation vector of the sample is generated as the input of the classifier. Finally, two different fusion strategies are applied to conduct fusion. Through extensive experiments on two benchmark datasets, the performance of the proposed method is proved better than that of each single feature based recognition method.", "journal": "JOURNAL OF INTELLIGENT & FUZZY SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000780464900002", "keywords": "3D camera parameter calibration; Forensics; Gait energy image; Silhouette-based gait analysis; Practicality", "title": "Enhancing the robustness of forensic gait analysis against near-distance viewing direction differences", "abstract": "Gait analysis is a promising biometric technology to visually and quantitatively analyze an individual's walking style. In Japan, silhouette-based quantitative gait analyses have been implemented as a forensic tool; however, several challenges remain owing the narrow range of application. One of the yet-unsolved issues pertains to the existence of a 'slight' but critical viewing direction difference, which leads to the incorrect judgment in the analyses of a person even when using deep learning-based feature extraction. To alleviate the critical viewing direction difference problem, we developed a novel gait analysis technique involving three components: 3D calibration, gait energy image space registration, and regression of the distance vector. Results of the GUI development and mock appraisal tests indicated that the proposed method can help achieve practical improvements in the forensic science domain.", "journal": "MULTIMEDIA TOOLS AND APPLICATIONS", "category": "Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000859573300002", "keywords": "Gesture recognition; Deep convolutional neural networks; Self co -articulation; Human -computer interaction", "title": "Development of an intelligent recognition system for dynamic mid-air gesticulation of isolated alphanumeric keys", "abstract": "Detecting and recognizing naturally gesticulated characters in mid-air is difficult because of the complexity arising due to (i) gesticulation style; (ii) gesture's pattern and size. This worsens further when the gesticulation speed is faster causing the gesture object to be blurred in some of the frames. To ensure that the trajectory formed is equivalent to the intended gesticulation and be robust to the impostor's presence, we propose red marker -detection and tracking (RM-DT) approach. This uses channel subtraction, thresholding, compact (hand) criteria, and Kalman filter to detect and track the gesture object to form the gesture trajectory (accuracy of-96 %; mAP - 0.9419). Unlike handwritten characters, the gesticulated trajectory incorporates the self co-articulated strokes that increases the rate of misclassification at the recognition stage. Hence, we separate the gestures into three groups and apply group-specific pre-processing models to remove them (accuracy of-95 %). These tra-jectories were then clustered into two groups (based on the number of stroke segments and segments removed) using fuzzy inference system so to use two pre-trained InceptionV3 for recognition. An average accuracy of 96.81 % (precision -0.9597, recall -0.9484, Fscore -0.9550) and 98.95 % (precision -0.9767, recall -0.9631, Fscore -0.9666) across the two groups respectively. The overall accuracy of the proposed recognition model has a relative improvement of 9 % over the existing state-of-the-art model for dynamic character gesture recognition.", "journal": "EXPERT SYSTEMS WITH APPLICATIONS", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000426075500012", "keywords": "Opinion mining; Persian sentiment word miner; Feature engineering; Comprehensive Persian WordNet", "title": "The Impact of Sentiment Features on the Sentiment Polarity Classification in Persian Reviews", "abstract": "Natural language processing (NLP) techniques can prove relevant to a variety of specialties in the field of cognitive science, including sentiment analysis. This paper investigates the impact of NLP tools, various sentiment features, and sentiment lexicon generation approaches to sentiment polarity classification of internet reviews written in Persian language. For this purpose, a comprehensive Persian WordNet (FerdowsNet), with high recall and proper precision (based on Princeton WordNet), was developed. Using FerdowsNet and a generated corpus of reviews, a Persian sentiment lexicon was developed using (i) mapping to the SentiWordNet and (ii) a semi-supervised learning method, after which the results of both methods were compared. In addition to sentiment words, a set of various features were extracted and applied to the sentiment classification. Then, by employing various well-known feature selection approaches and state-of-the art machine learning methods, a sentiment classification for Persian text reviews was carried out. The obtained results demonstrate the critical role of sentiment lexicon quality in improving the quality of sentiment classification in Persian language.", "journal": "COGNITIVE COMPUTATION", "category": "Computer Science, Artificial Intelligence; Neurosciences", "annotated_keywords": ["machine learning", "supervised learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000523558800025", "keywords": "Natural language processing; Text mining; Condition mining; Neural networks", "title": "A deep-learning approach to mining conditions", "abstract": "A condition is a constraint that determines when a consequent holds. Mining them in text is paramount to understand many sentences properly. In the literature, there are a few pattern-based proposals that fall short regarding recall because it is not easy to characterise unusual ways to express conditions with hand-crafted patterns; there is one machine-learning proposal that is bound to the Japanese language, requires specific-purpose dictionaries, taxonomies, and heuristics, works on opinion sentences only, and was evaluated very shallowly. In this article, we present a deep-learning proposal to mine conditions that does not have any of the previous drawbacks; furthermore, we have performed a comprehensive experimental study on a large multi-lingual dataset on many common topics; our conclusion is that our proposals are similar to the state of the art in terms of precision, but improve recall enough to beat them in terms of F-1 score. (c) 2020 Elsevier B.V. All rights reserved.", "journal": "KNOWLEDGE-BASED SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000386436900001", "keywords": "POS tagging; SVM; feature set; vectorization; machine learning; tagger; punjabi; indian languages", "title": "Revisiting the ontologising of semantic relation arguments in wordnet synsets", "abstract": "Ontologising is the task of associating terms, in text, with an ontological representation of their meaning, in an ontology. In this article, we revisit algorithms that have previously been used to ontologise the arguments of semantic relations in a relationless thesaurus, resulting in a wordnet. For increased flexibility, the algorithms do not use the extraction context when selecting the most adequate synsets for each term argument. Instead, they exploit a term-based lexical network which can be established by knowledge extracted automatically, or obtained from the resource the relations are being ontologised to. On the latter idea, we made several experiments to conclude that the algorithms can be used both for wordnet creation and for their enrichment. Besides describing the algorithms with some detail, the aforementioned experiments, which target both English and Portuguese, and their results are reported and discussed.", "journal": "NATURAL LANGUAGE ENGINEERING", "category": "Computer Science, Artificial Intelligence; Linguistics; Language & Linguistics", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000357578000001", "keywords": "Natural language processing; Name-based Text Categorization; Semantic similarity", "title": "Text Categorization from category name in an industry-motivated scenario", "abstract": "In this work we suggest a novel Text Categorization (TC) scenario, motivated by an ad-hoc industrial need to assign documents to a set of predefined categories, while labeled training data for the categories is not available. The scenario is applicable in many industrial settings and is interesting from the academic perspective. We present a new dataset geared for the main characteristics of the scenario, and utilize it to investigate the name-based TC approach, which uses the category names as its only input and does not require training data. We evaluate and analyze the performance of state-of-the-art methods for this dataset to identify the shortcomings of these methods for our scenario, and suggest ways for overcoming these shortcomings. We utilize statistical correlation measured over a target corpus for improving the state-of-the-art, and offer a different classification scheme based on the characteristics of the setting. We evaluate our improvements and adaptations and show superior performance of our suggested method.", "journal": "LANGUAGE RESOURCES AND EVALUATION", "category": "Computer Science, Interdisciplinary Applications", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000687522700005", "keywords": "Structural features; Candidate fact ranking; Attention neural network; KBQA", "title": "SF-ANN: leveraging structural features with an attention neural network for candidate fact ranking", "abstract": "Candidate ranking is the process of selecting the candidate with the best matching probability to the question after generating candidates in the knowledge base question answering (KBQA) task. It is a representative problem in mining matching relationships between candidates and questions. Previous research works always model questions and candidate representations separately, ignoring their impact on each other. The text information is too short to capture rich features in the KBQA task. Therefore, our work presents an attention neural network (ANN) fused with structural features (SF-ANN) to rank candidate facts jointly. First, two types of attention mechanisms are used to capture the correlation between the question and the candidate fact: a mutual-attention mechanism that captures the correspondence between the sentence components of a question and each part of a candidate and an intra-attention mechanism that captures the self-dependency of the concatenation between a question and a candidate fact. Second, an ANN is designed for fusing these two types of attention mechanisms to deeply couple interactive information of the input. Finally, knowledge base structural features are introduced to supplement the semantic information to increase the richness of the information. Three mutual attention mechanisms are applied for fusing them into the ANN, resulting in higher information gain. The experimental results on the SimpleQuestions (SimpleQ) benchmark demonstrate that the proposed model achieves a higher ranking accuracy (82.9%) than the state-of-the-art models. Moreover, the ablation study on SimpleQ and WebQuestionsSP (WebQSP) shows that leveraged features and the propoesd ANN both contribute to performance improvement.", "journal": "APPLIED INTELLIGENCE", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000363309200020", "keywords": "Sentiment classification; Openion mining; Term weighting schemes", "title": "Detecting Hotspot Information Using Multi-Attribute Based Topic Model", "abstract": "Microblogging as a kind of social network has become more and more important in our daily lives. Enormous amounts of information are produced and shared on a daily basis. Detecting hot topics in the mountains of information can help people get to the essential information more quickly. However, due to short and sparse features, a large number of meaningless tweets and other characteristics of microblogs, traditional topic detection methods are often ineffective in detecting hot topics. In this paper, we propose a new topic model named multi-attribute latent dirichlet allocation (MA-LDA), in which the time and hashtag attributes of microblogs are incorporated into LDA model. By introducing time attribute, MA-LDA model can decide whether a word should appear in hot topics or not. Meanwhile, compared with the traditional LDA model, applying hashtag attribute in MA-LDA model gives the core words an artificially high ranking in results meaning the expressiveness of outcomes can be improved. Empirical evaluations on real data sets demonstrate that our method is able to detect hot topics more accurately and efficiently compared with several baselines. Our method provides strong evidence of the importance of the temporal factor in extracting hot topics.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000360835000032", "keywords": "Discourse analysis; single-document summarization; tree knapsack problem", "title": "Summarizing a Document by Trimming the Discourse Tree", "abstract": "Recent studies on extractive text summarization formulate it as a combinatorial optimization problem, extracting the optimal subset from a set of the textual units that maximizes an objective function without violating the length constraint. Although these methods successfully improve automatic evaluation scores, they do not consider the discourse structure in the source document. Thus, summaries generated by these methods may lack logical coherence. In previous work, we proposed a method that exploits a discourse tree structure to produce coherent summaries. By transforming a traditional discourse tree, namely a rhetorical structure theory-based discourse tree (RST-DT), into a dependency-based discourse tree (DEP-DT), we formulated the summarization procedure as a Tree Knapsack Problem whose tree corresponds to the DEP-DT. This paper extends the work with a detailed discussion of the approach together with a novel efficient dynamic programming algorithm for solving the Tree Knapsack Problem. Experiments show that our method not only achieved the highest score in both automatic and human evaluation, but also obtained good performance in terms of the linguistic qualities of the summaries.", "journal": "IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING", "category": "Acoustics; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000429324500001", "keywords": "Multi-view learning; Non-negative matrix factorization; Dual regularization; Clustering", "title": "Dual regularized multi-view non-negative matrix factorization for clustering", "abstract": "Many real-world datasets are described by multiple modalities or views, which can provide compatible and complementary information to each other. Synthesizing multi-view features for data representation can lead to more comprehensive data description, which may further allow us to find more effective solutions for multi-view data clustering. In this paper, a novel algorithm, called Dual-regularized Multi-view Non-negative Matrix Factorization (DMvNMF), is developed for multi-view data clustering, which is able to preserve the geometric structures of multi-view data in both the data space and the feature space. A parameter-free strategy is developed for constructing the data graph in the multi-view context. Firstly, the affinity graph is learned for each view adaptively by using the self-expressiveness property and the principle of sparsity, i.e., reconstructing each data instance by using a few most similar instances. Secondly, these affinity graphs for different views are linearly combined to generate the global data graph, where the combination weights (importance weights) are learned automatically and the views with better explanations for data reconstruction can get larger importance weights. The feature graph for each view is also constructed in a similar way and it is treated as the affinity graph. For model optimization, an iterative updating scheme is developed to support our DMvNMF algorithm and its convergence proof is also provided. Our experimental results on three real-world datasets have demonstrated the effectiveness of our DMvNMF algorithm for multi-view data clustering and it can significantly outperform other baseline methods. (C) 2017 Published by Elsevier B.V.", "journal": "NEUROCOMPUTING", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000337124200006", "keywords": "Iris image classification; Hierarchical Visual Codebook (HVC); iris liveness detection; race classification; coarse-to-fine iris identification", "title": "Iris Image Classification Based on Hierarchical Visual Codebook", "abstract": "Iris recognition as a reliable method for personal identification has been well-studied with the objective to assign the class label of each iris image to a unique subject. In contrast, iris image classification aims to classify an iris image to an application specific category, e. g. iris liveness detection (classification of genuine and fake iris images), race classification (e. g. classification of iris images of Asian and non-Asian subjects), coarse-to-fine iris identification (classification of all iris images in the central database into multiple categories). This paper proposes a general framework for iris image classification based on texture analysis. A novel texture pattern representation method called Hierarchical Visual Codebook (HVC) is proposed to encode the texture primitives of iris images. The proposed HVC method is an integration of two existing Bag-of-Words models, namely Vocabulary Tree (VT), and Locality-constrained Linear Coding (LLC). The HVC adopts a coarse-to-fine visual coding strategy and takes advantages of both VT and LLC for accurate and sparse representation of iris texture. Extensive experimental results demonstrate that the proposed iris image classification method achieves state-of-the-art performance for iris liveness detection, race classification, and coarse-to-fine iris identification. A comprehensive fake iris image database simulating four types of iris spoof attacks is developed as the benchmark for research of iris liveness detection.", "journal": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000447265500004", "keywords": "Illumination-reflectance model; Modality-Invariant feature; Illumination-invariant feature; Local maximum quotient; Circular quaternary pattern; Heterogeneous face recognition; Deep learning", "title": "A novel quaternary pattern of local maximum quotient for heterogeneous face recognition", "abstract": "One of the major applications of biometrics is in the field of forensics. Faces are the most important biometrics used in forensics. Various circumstances and requirements give birth to different modality or heterogeneous faces, such as near infrared (NIR) faces, face-sketches etc. In this paper, a novel methodology for heterogeneous face recognition, such as sketch-photo and near infrared (NIR)-visible (VIS) images is proposed. More importance is given to those biometric facial features, which are invariant in different modalities. We present a robust local image representation called local maximum quotient (LMQ) for capturing modality-invariant facial features. Finally, a local circular quaternary pattern (LCQP) is presented to capture the local variations of the maximum quotient representation and we call it a quaternary pattern of local maximum quotient (QPLMQ). We have tested the proposed methodology on different sketch-photo and NIR-VIS benchmark databases. In the case of viewed sketches, the rank-1 recognition accuracy of 98.95% and 92.88% are achieved on CUFSF and IIIT-D databases, respectively. QPLMQ gives a rank-1 accuracy of 77.83% on challenging IIIT-D Semi-forensic database. In the case of NIR-VIS matching, the rank-1 accuracy of 75.85% is achieved and which is superior to other state-of-the-art methods. A deep learning based convolutional neural network (CNN) is used to compare recent deep learning based state-of-theart method on CASIA NIR-VIS 2.0 database and other databases also. Our proposed QPLMQ combined with CNN gives better results than other state-of-the-art methods. (C) 2017 Elsevier B.V. All rights reserved.", "journal": "PATTERN RECOGNITION LETTERS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000397687900001", "keywords": "Biometrics; Ocular biometrics; Mobile biometrics; Iris; Conjunctival vasculature; Periocular biometrics; Visible spectrum", "title": "Ocular biometrics in the visible spectrum: A survey", "abstract": "Ocular biometrics encompasses the imaging and use of characteristic features extracted from the eyes for personal recognition. Ocular biometric modalities in visible light have mainly focused on iris, blood vessel structures over the white of the eye (mostly due to conjunctival and episcieral layers), and periocular region around eye. Most of the existing studies on iris recognition use the near infrared spectrum. However, conjunctival vasculature and periocular regions are imaged in the visible spectrum. Iris recognition in the visible spectrum is possible for light color irides or by utilizing special illumination. Ocular recognition in the visible spectrum is an important research area due to factors such as recognition at a distance, suitability for recognition with regular RGB cameras, and adaptability to mobile devices. Further these ocular modalities can be obtained from a single RGB eye image, and then fused together for enhanced performance of the system. Despite these advantages, the state-of-the-art related to ocular biometrics in visible spectrum is not well known. This paper surveys this topic in terms of computational image enhancement, feature extraction, classification schemes and designed hardware-based acquisition set-ups. Future research directions are also enumerated to identify the path forward. (C) 2016 Elsevier B.V. All rights reserved.", "journal": "IMAGE AND VISION COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic; Optics", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000370968700009", "keywords": "Attribute learning; dictionary learning; dictionary bases", "title": "Category Specific Dictionary Learning for Attribute Specific Feature Selection", "abstract": "Attributes, as mid-level features, have demonstrated great potential in visual recognition tasks due to their excellent propagation capability through different categories. However, existing attribute learning methods are prone to learning the correlated attributes. To discover the genuine attribute specific features, many feature selection methods have been proposed. However, these feature selection methods are implemented at the level of raw features that might be very noisy, and these methods usually fail to consider the structural information in the feature space. To address this issue, in this paper, we propose a label constrained dictionary learning approach combined with a multilayer filter. The feature selection is implemented at dictionary level, which can better preserve the structural information. The label constrained dictionary learning suppresses the intra-class noise by encouraging the sparse representations of intra-class samples to lie close to their center. A multilayer filter is developed to discover the representative and robust attribute specific bases. The attribute specific bases are only shared among the positive samples or the negative samples. The experiments on the challenging Animals with Attributes data set and the SUN attribute data set demonstrate the effectiveness of our proposed method.", "journal": "IEEE TRANSACTIONS ON IMAGE PROCESSING", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000395844700012", "keywords": "Fingerprint; Liveness detection; Biometric", "title": "Review of the Fingerprint Liveness Detection (LivDet) competition series: 2009 to 2015", "abstract": "A spoof attack, a subset of presentation attacks, is the use of an artificial replica of a biometric in an attempt to circumvent a biometric sensor. Liveness detection, or presentation attack detection, distinguishes between live and fake biometric traits and is based on the principle that additional information can be garnered above and beyond the data procured by a standard authentication system to determine if a biometric measure is authentic. The goals for the Liveness Detection (LivDet) competitions are to compare software-based fingerprint liveness detection and artifact detection algorithms (Part 1), as well as fingerprint systems which incorporate liveness detection or artifact detection capabilities (Part 2), using a standardized testing protocol and large quantities of spoof and live tests. The competitions are open to all academic and industrial institutions which have a solution for either software-based or system-based fingerprint liveness detection. The LivDet competitions have been hosted in 2009, 2011, 2013 and 2015 and have shown themselves to provide a crucial look at the current state of the art in liveness detection schemes. There has been a noticeable increase in the number of participants in LivDet competitions as well as a noticeable decrease in error rates across competitions. Participants have grown from four to the most recent thirteen submissions for Fingerprint Part 1. Fingerprints Part 2 has held steady at two submissions each competition in 2011 and 2013 and only one for the 2015 edition. The continuous increase of competitors demonstrates a growing interest in the topic. (C) 2016 Elsevier B.V. All rights reserved.", "journal": "IMAGE AND VISION COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic; Optics", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000461854100013", "keywords": "Affinity matrix; clustering; low-rank representation (LRR); sparse representation", "title": "Flexible Affinity Matrix Learning for Unsupervised and Semisupervised Classification", "abstract": "In this paper, we propose a unified model called flexible affinity matrix learning (FAML) for unsupervised and semisupervised classification by exploiting both the relationship among data and the clustering structure simultaneously. To capture the relationship among data, we exploit the self-expressiveness property of data to learn a structured matrix in which the structures are induced by different norms. A rank constraint is imposed on the Laplacian matrix of the desired affinity matrix, so that the connected components of data are exactly equal to the cluster number. Thus, the clustering structure is explicit in the learned affinity matrix. By making the estimated affinity matrix approximate the structured matrix during the learning procedure, FAML allows the affinity matrix itself to be adaptively adjusted such that the learned affinity matrix can well capture both the relationship among data and the clustering structure. Thus, FAML has the potential to perform better than other related methods. We derive optimization algorithms to solve the corresponding problems. Extensive unsupervised and semisupervised classification experiments on both synthetic data and real-world benchmark data sets show that the proposed FAML consistently outperforms the state-of-the-art methods.", "journal": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Hardware & Architecture; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000691542900020", "keywords": "Biometrics; Dorsal hand vein; Survey", "title": "A survey on dorsal hand vein biometrics", "abstract": "Biometrics technology is one of the most important and effective solutions for personal authentication. In recent years, as one of the emerging biometrics technologies, dorsal hand vein (DHV) biometrics has received a lot of attention. In fact, DHV biometrics has been studied for more than 30 years, during which different problems related to DHV recognition have been addressed. In this paper, we conduct a comprehensive survey on the state-of-the-art in DHV biometrics. Nearly all important aspects of DHV biometrics have been summarized, including the developmental history, data acquisition, databases, preprocessing algorithms, feature extraction and matching algorithms, information fusion schemes and commercial products. We also discuss the challenges and future directions in DHV biometrics research. (c) 2021 Elsevier Ltd. All rights reserved.", "journal": "PATTERN RECOGNITION", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000434382900048", "keywords": "3D face; Face analysis; Landmark localization; Differential geometry; Feature extraction", "title": "3D geometry-based automatic landmark localization in presence of facial occlusions", "abstract": "This study proposes a novel automatic method for facial landmark localization relying on geometrical properties of 3D facial surface working both on complete faces displaying different emotions and in presence of occlusions. In particular, 12 descriptors coming from Differential Geometry including the coefficients of the fundamental forms, Gaussian, mean, principal curvatures, shape index and curvedness are extracted as facial features and their local geometric properties are exploited to localize 13 soft-tissue landmarks from eye and nose areas. The method is deterministic and is backboned by a thresholding technique designed by studying the behaviour of each geometrical descriptor in correspondence to the locus of each landmark. Occlusions are managed by a detection algorithm based on geometrical properties which allows to proceed with the landmark localization avoiding the covered areas. Experimentations were carried out on 3132 faces of the Bosphorus database and of a 230-sized internal database, including expressive and occluded ones (mouth, eye, and eyeglasses occlusions), obtaining 4.75 mm mean localization error.", "journal": "MULTIMEDIA TOOLS AND APPLICATIONS", "category": "Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000375360900004", "keywords": "Biometric recognition; Feature extraction; Quadratic projection; Semidefinite programming; Lagrange duality", "title": "Quadratic projection based feature extraction with its application to biometric recognition", "abstract": "This paper presents a novel quadratic projection based feature extraction framework, where a set of quadratic matrices is learned to distinguish each class from all other classes. We formulate quadratic matrix learning (QML) as a standard semidefinite programming (SDP) problem. However, the conventional interior-point SDP solvers do not scale well to the problem of QML for high-dimensional data. To solve the scalability of QML, we develop an efficient algorithm, termed DualQML, based on the Lagrange duality theory, to extract nonlinear features. To evaluate the feasibility and effectiveness of the proposed framework, we conduct extensive experiments on biometric recognition. Experimental results on three representative biometric recognition tasks, including face, palmprint, and ear recognition, demonstrate the superiority of the DualQML-based feature extraction algorithm compared to the current state-of-the-art algorithms. (C) 2016 Elsevier Ltd. All rights reserved.", "journal": "PATTERN RECOGNITION", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000406987400004", "keywords": "Eye gazing Tracking; Virtual plane; Cross-ratio; Human Robot Interaction", "title": "Gazing point dependent eye gaze estimation", "abstract": "Cross-ratio invariant is used widely in projective transformations for eye gaze estimation. Establishing a virtual plane projection is an important step to use this property. Most of traditional cross-ratio approaches only used fixed parameters to calculate the gazing point. This paper proposes gazing point dependent eye gazing estimation approach. Our contributions are three-folded. First, we model a dynamic virtual plane projection, which is tangent to the cornea of pupil, to estimate the position of the gazing point. Second, we introduce a two stage approach consisting of rough-to-precise framework for gazing point estimation based on the gazing point dependent virtual plane projection. Third, a heuristic strategy which contains off-line and on-line parameter learning for gazing point estimation is proposed. The experiment results show that our approach can significantly improve the gazing estimation performance with an average accuracy of 0.70 degrees. (C) 2017 Elsevier Ltd. All rights reserved.", "journal": "PATTERN RECOGNITION", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000312844800010", "keywords": "Isometric mapping; manifold learning; nonlinear dimensionality reduction; tangent space", "title": "Local Coordinates Alignment With Global Preservation for Dimensionality Reduction", "abstract": "Dimensionality reduction is vital in many fields, and alignment-based methods for nonlinear dimensionality reduction have become popular recently because they can map the high-dimensional data into a low-dimensional subspace with the property of local isometry. However, the relationships between patches in original high-dimensional space cannot be ensured to be fully preserved during the alignment process. In this paper, we propose a novel method for nonlinear dimensionality reduction called local coordinates alignment with global preservation. We first introduce a reasonable definition of topology-preserving landmarks (TPLs), which not only contribute to preserving the global structure of datasets and constructing a collection of overlapping linear patches, but they also ensure that the right landmark is allocated to the new test point. Then, an existing method for dimensionality reduction that has good performance in preserving the global structure is used to derive the low-dimensional coordinates of TPLs. Local coordinates of each patch are derived using tangent space of the manifold at the corresponding landmark, and then these local coordinates are aligned into a global coordinate space with the set of landmarks in low-dimensional space as reference points. The proposed alignment method, called landmarks-based alignment, can produce a closed-form solution without any constraints, while most previous alignment-based methods impose the unit covariance constraint, which will result in the deficiency of global metrics and undesired rescaling of the manifold. Experiments on both synthetic and real-world datasets demonstrate the effectiveness of the proposed algorithm.", "journal": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Hardware & Architecture; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000372547400008", "keywords": "Deep learning; face verification; feature learning; mixture model; part-based representation", "title": "Convolutional Fusion Network for Face Verification in the Wild", "abstract": "Part-based methods have seen popular applications for face verification in the wild, since they are more robust to local variations in terms of pose, illumination, and so on. However, most of the part-based approaches are built on hand-crafted features, which may not be suitable for the specific face verification purpose. In this paper, we propose to learn a part-based feature representation under the supervision of face identities through a deep model that ensures that the generated representations are more robust and suitable for face verification. The proposed framework consists of the following two deliberate components: 1) a deep mixture model (DMM) to find accurate patch correspondence and 2) a convolutional fusion network (CFN) to extract the part-based facial features. Specifically, DMM robustly depicts the spatial-appearance distribution of patch features over the faces via several Gaussian mixtures, which provide more accurate patch correspondence even in the presence of local distortions. Then, DMM only feeds the patches which preserve the identity information to the following CFN. The proposed CFN is a two-layer cascade of convolutional neural networks: 1) a local layer built on face patches to deal with local variations and 2) a fusion layer integrating the responses from the local layer. CFN jointly learns and fuses multiple local responses to optimize the verification performance. The composite representation obtained possesses certain robustness to pose and illumination variations and shows comparable performance with the state-of-the-art methods on two benchmark data sets.", "journal": "IEEE TRANSACTIONS ON CIRCUITS AND SYSTEMS FOR VIDEO TECHNOLOGY", "category": "Engineering, Electrical & Electronic", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000446521700011", "keywords": "Face recognition; Demographic covariates; Race; Gender; Age", "title": "Review on the effects of age, gender, and race demographics on automatic face recognition", "abstract": "The performance of face recognition algorithms is affected by external factors and internal subject characteristics. Identifying these aspects and understanding their behaviors on performance can aid in predicting the performance of algorithms and in designing suitable acquisition settings at prospective locations to enhance performance. Factors that affect the performance of face recognition systems, such as pose, illumination, expression, and image resolution, are recognized as face recognition problems. These are substantially studied, and many algorithms have been developed to tackle these problems. However, the influence of population demographics (i.e., race, age, and gender) on face recognition performance has not received considerable attention. Early findings that deal with demographic influence give conflicting results. The studies conducted in the last decade resolve some of the contentions. Nonetheless, some findings have not reached consensus. Existing reviews on the influence of covariates are either outdated or do not cover the influence of demographic covariates on the performance of face recognition algorithms. This paper gives an intensive and focused review that covers recent research on demographic covariates. The effects of age, gender, and race covariates on face recognition are summarized based on these findings, and suggestions on the future direction of the field are given to have a significant understanding of these effects individually and their interactions with one another.", "journal": "VISUAL COMPUTER", "category": "Computer Science, Software Engineering", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000795795300001", "keywords": "machine vision; crack detection; image processing; image acquisition; subway tunnel", "title": "A crack detection system of subway tunnel based on image processing", "abstract": "For the images of crack defects of subway tunnel, traditional image processing algorithms is hardly effective for dealing with problems existing in the image like uneven illumination or severe noise interference. Based on pixel-level processing, an improved crack detection algorithm is proposed using structural analysis for improving the quality of tunnel images. Firstly, image preprocessing transforms the raw images of tunnel surface into binary images containing crack pixels and noise pixels. To extract crack information from binary images, three kinds of interference components are removed by structural analysis. With few interference components remaining in the image, the width of crack can be calculated according to the mean and standard deviation of the local area of the crack. Based on the algorithm, a crack detection system is designed, and a tunnel inspection experiment is conducted in a subway tunnel to capture tunnel surface images. Compared with popular image processing method, the crack recognition rate of the proposed method is 91.15% which is approximately 10% higher than others, and the measurement result of crack width based on the proposed method is closer to the ground truth. The experiment result indicates that the proposed method shows a better performance in crack detection.", "journal": "MEASUREMENT & CONTROL", "category": "Automation & Control Systems; Instruments & Instrumentation", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000789971300001", "keywords": "3D laser triangulation; artificial intelligence; deep convolutional neural networks; algorithms; change detection; safety; operational efficiency; inspection", "title": "A night pavement crack detection method based on image-to-image translation", "abstract": "Deep learning provides an efficient automated method for pavement condition surveys, but the datasets used for this model are usually images taken in good lighting conditions. If images are taken at night, this model cannot work effectively. This paper proposes a method for normalizing pavement images at night, which includes three main steps. First, the image feature point detection and matching method is used to process images taken during the day and night. Then, paired images of pavement during the day and night are obtained. Second, with the help of the image-to-image translation model, those paired images are used for training, and the best model for converting night images into day images is selected. Third, a convolutional neural network (CNN) based on VGGNet is constructed, and pavement images taken during the day are used for training. After that, six types of images are used and tested separately, namely, those taken during the day and the night, converted by the proposed method and converted by traditional methods. As evaluated by various evaluation indices and visualization methods, the detection performance of the CNN model can be significantly improved by using the proposed method of converted night-to-day images.", "journal": "COMPUTER-AIDED CIVIL AND INFRASTRUCTURE ENGINEERING", "category": "Computer Science, Interdisciplinary Applications; Construction & Building Technology; Engineering, Civil; Transportation Science & Technology", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000722555800009", "keywords": "Fabric defect; Textile defect; Anomaly detection; Neural network; Cross-patch similarity; Manhattan distance", "title": "Unsupervised textile defect detection using convolutional neural networks", "abstract": "In this study, we propose a novel motif-based approach for unsupervised textile anomaly detection that combines the benefits of traditional convolutional neural networks with those of an unsupervised learning paradigm. It consists of five main steps: preprocessing, automatic pattern period extraction, patch extraction, features selection and anomaly detection. This proposed approach uses a new dynamic and heuristic method for feature selection which avoids the drawbacks of initialization of the number of filters (neurons) and their weights, and those of the backpropagation mechanism such as the vanishing gradients, which are common practice in the state-of-the-art methods. The design and training of the network are performed in a dynamic and input domain-based manner and, thus, no ad-hoc configurations are required. Before building the model, only the number of layers and the stride are defined. We do not initialize the weights randomly nor do we define the filter size or number of filters as conventionally done in CNN-based approaches. This reduces effort and time spent on hyper-parameter initialization and fine-tuning. Only one defect-free sample is required for training and no further labeled data is needed. The trained network is then used to detect anomalies on defective fabric samples. We demonstrate the effectiveness of our approach on the Patterned Fabrics benchmark dataset. Our algorithm yields reliable and competitive results (on recall, precision, accuracy and f1-measure) compared to state-of-the-art unsupervised approaches, in less time, with efficient training in a single epoch and a lower computational cost. (C) 2021 Elsevier B.V. All rights reserved.", "journal": "APPLIED SOFT COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": ["neural net", "neural net", "supervised learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000554360400001", "keywords": "Concrete crack; detection and measurement; convolutional encoder-decoder network; deep learning", "title": "Automatic Crack Detection and Measurement of Concrete Structure Using Convolutional Encoder-Decoder Network", "abstract": "The detection and measurement of crack at pixel level is a challenge to existing methods. To overcome this challenge, this paper proposes a convolutional encoder-decoder network (CedNet) to detect crack from image, and the maximum widths and orientations of cracks are measured using image post-processing techniques. To realize this, a database including 1800 crack images (with $761\\times 569$ pixel resolution) taken from concrete structures is built. Then the CedNet is designed, trained and validated using the built database. The validating results show 98.90% accuracy, 93.58% precision, 94.73% recall, 93.18% F-measure, 87.23% intersection over union (IoU) of crack and 98.82% IoU of background. Subsequently, the robustness and adaptability of the trained model is tested. To measure true maximum widths and orientations of cracks, a laboratory experiment is carried out to calibrate a relation between ratio (pixel distance / real distance) and field of view (camera's view range on concrete surface included in image) and distance from the smartphone to concrete surface. In the post-processing techniques, the perspective transformation is employed to correct distorted images caused by the existence of the oblique angles between the smartphone and concrete surfaces. Then the maximum widths and orientations of cracks in predicted results are measured respectively using the Euclidean distance transformation and least squares principle. As comparison, two existing deep learning-based crack detection and measurement method are used to examine the performance of the proposed approach. The comparison results show that the proposed method substantiates quite good performance to detect cracks and measure maximum widths and orientations of cracks in our database.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000679683700002", "keywords": "connected components; defect detection; image analysis; region-based graph; titanium alloys", "title": "Automatic Microstructure Defect Detection of Ti6Al-4V Titanium Alloy by Regions-Based Graph", "abstract": "In this paper, we propose a simple and efficient approach for microstructure defect detection of Ti-6Al-4V titanium alloy based on image analysis. The proposed approach mimics the way that domain experts identify the defect area, by segmenting material grains via image preprocessing and detecting defects using region-based graph. The preprocessing step is a sequence of image processing techniques to produce potential defect regions. Next, a graph is constructed by considering the regions as nodes with connectivity determined by the pairwise distances. The connected components of this graph are the final detection result. An experiment involving 103 training and 517 testing microstructure images is carried out. The proposed method outperforms three benchmark methods with 0.919 G-mean score for the classification task. As to the performance of defect localization, the proposed approach largely outperforms two benchmark methods. In addition, the proposed method effectively detects the defect regions for 91 out of 96 defect images. Moreover, the implementation results also show that the proposed method has low computational cost. The processing time is on average 2.02 s per image, and 67.7 s for 517 images using parallel computation on a 32-core workstation.", "journal": "IEEE TRANSACTIONS ON EMERGING TOPICS IN COMPUTATIONAL INTELLIGENCE", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000433186700017", "keywords": "video surveillance; image classification; image sensors; multistage ranking approach; fast person reidentification; video-surveillance operators; forensic investigators; nonoverlapping cameras; image sorting; appearance descriptors; similarity measures; ranking quality; processing time; multistage classification approaches; reidentification ranking task; multistage system", "title": "Multi-stage ranking approach for fast person re-identification", "abstract": "One of the goals of person re-identification systems is to support video-surveillance operators and forensic investigators to find an individual of interest in videos acquired by a network of non-overlapping cameras. This is attained by sorting images of previously observed individuals for decreasing values of their similarity with a given probe individual. Existing appearance descriptors, together with their similarity measures, are mostly aimed at improving ranking quality. The authors address instead the issue of processing time, which is also relevant in practical applications involving interaction with human operators. They show how a trade-off between processing time and ranking quality, for any given descriptor, can be achieved through a multi-stage ranking approach inspired by multi-stage classification approaches, which they adapt to the re-identification ranking task. The authors analytically model the processing time of multi-stage system and discuss the corresponding accuracy, and derive from these results practical design guidelines. They then empirically evaluate their approach on three benchmark data sets and four state-of-the-art descriptors.", "journal": "IET COMPUTER VISION", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000851759700001", "keywords": "deep learning; video classification; accident detection; surveillance system; anomaly detection", "title": "Anomaly Detection in Traffic Surveillance Videos Using Deep Learning", "abstract": "In the recent past, a huge number of cameras have been placed in a variety of public and private areas for the purposes of surveillance, the monitoring of abnormal human actions, and traffic surveillance. The detection and recognition of abnormal activity in a real-world environment is a big challenge, as there can be many types of alarming and abnormal activities, such as theft, violence, and accidents. This research deals with accidents in traffic videos. In the modern world, video traffic surveillance cameras (VTSS) are used for traffic surveillance and monitoring. As the population is increasing drastically, the likelihood of accidents is also increasing. The VTSS is used to detect abnormal events or incidents regarding traffic on different roads and highways, such as traffic jams, traffic congestion, and vehicle accidents. Mostly in accidents, people are helpless and some die due to the unavailability of emergency treatment on long highways and those places that are far from cities. This research proposes a methodology for detecting accidents automatically through surveillance videos. A review of the literature suggests that convolutional neural networks (CNNs), which are a specialized deep learning approach pioneered to work with grid-like data, are effective in image and video analysis. This research uses CNNs to find anomalies (accidents) from videos captured by the VTSS and implement a rolling prediction algorithm to achieve high accuracy. In the training of the CNN model, a vehicle accident image dataset (VAID), composed of images with anomalies, was constructed and used. For testing the proposed methodology, the trained CNN model was checked on multiple videos, and the results were collected and analyzed. The results of this research show the successful detection of traffic accident events with an accuracy of 82% in the traffic surveillance system videos.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": ["neural net", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000435187500022", "keywords": "infrared moving target tracking; low-rank and sparse matrix representation; total variation regularization; particle filtering framework; Smart City", "title": "Total Variation Regularization Term-Based Low-Rank and Sparse Matrix Representation Model for Infrared Moving Target Tracking", "abstract": "Infrared moving target tracking plays a fundamental role in many burgeoning research areas of Smart City. Challenges in developing a suitable tracker for infrared images are particularly caused by pose variation, occlusion, and noise. In order to overcome these adverse interferences, a total variation regularization term-based low-rank and sparse matrix representation (TV-LRSMR) model is designed in order to exploit a robust infrared moving target tracker in this paper. First of all, the observation matrix that is derived from the infrared sequence is decomposed into a low-rank target matrix and a sparse occlusion matrix. For the purpose of preventing the noise pixel from being separated into the occlusion term, a total variation regularization term is proposed to further constrain the occlusion matrix. Then an alternating algorithm combing principal component analysis and accelerated proximal gradient methods is employed to separately optimize the two matrices. For long-term tracking, the presented algorithm is implemented using a Bayesien state inference under the particle filtering framework along with a dynamic model update mechanism. Both qualitative and quantitative experiments that were examined on real infrared video sequences verify that our algorithm outperforms other state-of-the-art methods in terms of precision rate and success rate.", "journal": "REMOTE SENSING", "category": "Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000658339800004", "keywords": "Swarm intelligence; Evolutionary algorithms; Meta-heuristic; Video tracking; Three-dimensional scene reconstruction", "title": "Automata design for honeybee search algorithm and its applications to 3D scene reconstruction and video tracking", "abstract": "Honeybees, as social insects, follow a modular strategy applied to dynamic environments to provide reasonable opportunities for partial solutions to evolve in the form of interacting coadapted subcomponents. The honeybee search algorithm combines concepts from the areas of evolutionary algorithms and swarm intelligence to solve optimization problems. This algorithm is mainly based on the foraging behavior of honeybees and the search power of evolution strategies, a type of evolutionary algorithm used for real-valued problems. This paper shows the integration between an automaton and the honeybee search algorithm to formalize the algorithm mathematically. The combination mentioned above is tested here with the innovative applications of three-dimensional scene reconstruction and video tracking. The experimental results for both applications show evidence that the honeybee search algorithm can be used to improve time costs in challenging computer vision tasks through controlled experiments and objective comparisons. Also, the validation of results demonstrates that the measured accuracy ranks top-tier among other algorithms in the ALOV++ benchmark.", "journal": "SWARM AND EVOLUTIONARY COMPUTATION", "category": "Computer Science, Artificial Intelligence; Computer Science, Theory & Methods", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000436420400016", "keywords": "Domain adaptation; feature transformation learning; object category recognition; online kernel learning", "title": "Online Feature Transformation Learning for Cross-Domain Object Category Recognition", "abstract": "In this paper, we introduce a new research problem termed online feature transformation learning in the context of multiclass object category recognition. The learning of a feature transformation is viewed as learning a global similarity metric function in an online manner. We first consider the problem of online learning a feature transformation matrix expressed in the original feature space and propose an online passive aggressive feature transformation algorithm. Then these original features are mapped to kernel space and an online single kernel feature transformation (OSKFT) algorithm is developed to learn a nonlinear feature transformation. Based on the OSKFT and the existing Hedge algorithm, a novel online multiple kernel feature transformation algorithm is also proposed, which can further improve the performance of online feature transformation learning in large-scale application. The classifier is trained with k nearest neighbor algorithm together with the learned similarity metric function. Finally, we experimentally examined the effect of setting different parameter values in the proposed algorithms and evaluate the model performance on several multiclass object recognition data sets. The experimental results demonstrate the validity and good performance of our methods on cross-domain and multiclass object recognition application.", "journal": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Hardware & Architecture; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000485922300051", "keywords": "Recurrent neural networks; Deep learning; Backpropagation through time; Optimization; Performance", "title": "Character-level recurrent neural networks in practice: comparing training and sampling schemes", "abstract": "Recurrent neural networks are nowadays successfully used in an abundance of applications, going from text, speech and image processing to recommender systems. Backpropagation through time is the algorithm that is commonly used to train these networks on specific tasks. Many deep learning frameworks have their own implementation of training and sampling procedures for recurrent neural networks, while there are in fact multiple other possibilities to choose from and other parameters to tune. In the existing literature, this is very often overlooked or ignored. In this paper, we therefore give an overview of possible training and sampling schemes for character-level recurrent neural networks to solve the task of predicting the next token in a given sequence. We test these different schemes on a variety of datasets, neural network architectures and parameter settings, and formulate a number of take-home recommendations. The choice of training and sampling scheme turns out to be subject to a number of trade-offs, such as training stability, sampling time, model performance and implementation effort, but is largely independent of the data. Perhaps the most surprising result is that transferring hidden states for correctly initializing the model on subsequences often leads to unstable training behavior depending on the dataset.", "journal": "NEURAL COMPUTING & APPLICATIONS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net", "neural net", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000562037600006", "keywords": "Three-dimensional displays; Feature extraction; Shape; Data mining; Task analysis; Sensitivity; Convolution; 3D Multimodal fusion; mesh; multi-view; hamming space", "title": "Hamming Embedding Sensitivity Guided Fusion Network for 3D Shape Representation", "abstract": "Three-dimensional multi-modal data are used to represent 3D objects in the real world in different ways. Features separately extracted from multimodality data are often poorly correlated. Recent solutions leveraging the attention mechanism to learn a joint-network for the fusion of multimodality features have weak generalization capability. In this paper, we propose a hamming embedding sensitivity network to address the problem of effectively fusing multimodality features. The proposed network called HamNet is the first end-to-end framework with the capacity to theoretically integrate data from all modalities with a unified architecture for 3D shape representation, which can be used for 3D shape retrieval and recognition. HamNet uses the feature concealment module to achieve effective deep feature fusion. The basic idea of the concealment module is to re-weight the features from each modality at an early stage with the hamming embedding of these modalities. The hamming embedding also provides an effective solution for fast retrieval tasks on a large scale dataset. We have evaluated the proposed method on the large-scale ModelNet40 dataset for the tasks of 3D shape classification, single modality and cross-modality retrieval. Comprehensive experiments and comparisons with state-of-the-art methods demonstrate that the proposed approach can achieve superior performance.", "journal": "IEEE TRANSACTIONS ON IMAGE PROCESSING", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000497434700010", "keywords": "Image segmentation; Probabilistic logic; Object detection; Optimization; Shape; Computer vision; Computational modeling; Instance segmentation; exemplar-based; Hough voting; plant leaf segmentation; plant phenotyping", "title": "Exemplar-Based Recursive Instance Segmentation With Application to Plant Image Analysis", "abstract": "Instance segmentation is a challenging computer vision problem which lies at the intersection of object detection and semantic segmentation. Motivated by plant image analysis in the context of plant phenotyping, a recently emerging application field of computer vision, this paper presents the exemplar-based recursive instance segmentation (ERIS) framework. A three-layer probabilistic model is first introduced to jointly represent hypotheses, voting elements, instance labels, and their connections. Afterward, a recursive optimization algorithm is developed to infer the maximum a posteriori (MAP) solution, which handles one instance at a time by alternating among the three steps of detection, segmentation, and update. The proposed ERIS framework departs from previous works mainly in two respects. First, it is exemplar-based and model-free, which can achieve instance-level segmentation of a specific object class given only a handful of (typically less than 10) annotated exemplars. Such a merit enables its use in case that no massive manually-labeled data is available for training strong classification models, as required by most existing methods. Second, instead of attempting to infer the solution in a single shot, which suffers from extremely high computational complexity, our recursive optimization strategy allows for reasonably efficient MAP-inference in full hypothesis space. The ERIS framework is substantialized for the specific application of plant leaf segmentation in this work. Experiments are conducted on public benchmarks to demonstrate the superiority of our method in both effectiveness and efficiency in comparison with the state-of-the-art.", "journal": "IEEE TRANSACTIONS ON IMAGE PROCESSING", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000673796900001", "keywords": "Deep learning framework; recommendation system; processing-in-memory; neural processing unit; FPGA prototyping; functionality; system verification", "title": "PIMCaffe: Functional Evaluation of a Machine Learning Framework for In-Memory Neural Processing Unit", "abstract": "The large amount of memory usage in recent machine learning applications imposes a significant system burden with respect to power and processing speed. To cope with such a problem, Processing-In-Memory (PIM) techniques can be applied as an alternative solution. Especially, the recommendation system, which is one of the major machine learning applications used in data centers, requires a large memory capacity and therefore represents a suitable candidate application that could be helped by the PIM technique. In this paper, we introduce a machine learning framework, PIMCaffe, designed for in-memory neural processing units and its evaluation environment. PIMCaffe consists of two components: a Caffe2-based deep learning framework that supports PIM acceleration and a PIM-emulating hardware platform. We develop a suite of functions, libraries, application programming interfaces, and a device driver to support the framework. In addition, we implement a prototype Neural Processing Unit (NPU) in PIMCaffe to evaluate the performance of our platform with machine learning applications. Our prototype NPU design includes a vector processor for parallel vector processing and a systolic array unit for matrix multiplication. Using the proposed software framework, we perform a detailed analysis of the in-memory neural processing unit. PIMCaffe supports evaluations of recommendation systems and various convolutional neural network models on the in-memory neural processing unit. PIMCaffe with the NPU shows up to 2.26 x, 5.99 x, and 1.71 x speedup, compared to the ARM Cortex-A53 CPU, for the recommendation system, AlexNet, and ResNet-50, respectively.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": ["neural net", "machine learning", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000525819400003", "keywords": "Convolutional Neural Network; Regularisation; Generalisation; Weight perturbation", "title": "A weight perturbation-based regularisation technique for convolutional neural networks and the application in medical imaging", "abstract": "A convolutional neural network has the capacity to learn multiple representation levels and abstraction in order to provide a better understanding of image data. In addition, a good multi-level representation of data typically results in a better generalisation capability. This fact emphasises the importance of concentrating on the regularity information of training data in order to improve generalisation. However, the training data contain erroneous information owing to noise and outliers. In this paper, we propose a new regularisation approach for convolutional neural networks with better generalisation properties. Specifically, the weights of the convolution layers are perturbed by additive noise in each learning iteration. The approach provides a better model for prediction, as shown by the experimental results on a number of medical benchmark data sets. Furthermore, the effectiveness and accuracy of the proposed convolutional neural network are demonstrated by comparing with several recent perturbation techniques. (C) 2020 Published by Elsevier Ltd.", "journal": "EXPERT SYSTEMS WITH APPLICATIONS", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000430770600010", "keywords": "Power line inspection; Vision-based inspection; Deep learning; UAVs", "title": "Automatic autonomous vision-based power line inspection: A review of current status and the potential role of deep learning", "abstract": "To maintain the reliability, availability, and sustainability of electricity supply, electricity companies regularly perform visual inspections on their transmission and distribution networks. These inspections have been typically carried out using foot patrol and/or helicopter-assisted methods to plan for necessary repair or replacement works before any major damage, which may cause power outage. This solution is quite slow, expensive, and potentially dangerous. In recent years, numerous researches have been conducted to automate the visual inspections by using automated helicopters, flying robots, and/or climbing robots. However, due to the high accuracy requirements of the task and its unique challenges, automatic vision-based inspection has not been widely adopted. In this paper, with the aim of providing a good starting point for researchers who are interested in developing a fully automatic autonomous vision-based power line inspection system, we conduct an extensive literature review. First, we examine existing power line inspection methods with special attention paid to highlight their advantages and disadvantages. Next, we summarize well-suited tasks and review potential data sources for automatic vision-based inspection. Then, we survey existing automatic vision-based power line inspection systems. Based on that, we propose a new automatic autonomous vision-based power line inspection concept that uses Unmanned Aerial Vehicle (UAV) inspection as the main inspection method, optical images as the primary data source, and deep learning as the backbone of data analysis and inspection. Then, we present an overview of possibilities and challenges of deep vision (deep learning for computer vision) approaches for both UAV navigation and UAV inspection and discuss possible solutions to the challenges. Finally, we conclude the paper with an outlook for the future of this field and propose potential next steps for implementing the concept.", "journal": "INTERNATIONAL JOURNAL OF ELECTRICAL POWER & ENERGY SYSTEMS", "category": "Engineering, Electrical & Electronic", "annotated_keywords": ["deep learning", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000808339300002", "keywords": "Monocular depth estimation; Plane and parallax geometry; Structure information; Joint prediction model", "title": "Joint prediction of monocular depth and structure using planar and parallax geometry", "abstract": "Supervised learning depth estimation methods can achieve good performance when trained on highquality ground-truth, like LiDAR data. However, LiDAR can only generate sparse 3D maps which causes losing information. Obtaining high-quality ground-truth depth data per pixel is difficult to acquire. In order to overcome this limitation, we propose a novel approach combining structure information from a promising Plane and Parallax geometry pipeline with depth information into a U-Net supervised learning network, which results in quantitative and qualitative improvement compared to existing popular learning-based methods.In particular, the model is evaluated on two large-scale and challenging datasets: KITTI Vision Benchmark and Cityscapes dataset and achieve the best performance in terms of relative error. Compared with pure depth supervision models, our model has impressive performance on depth prediction of thin objects and edges, and compared to structure prediction baseline, our model performs more robustly.(c) 2022 Elsevier Ltd. All rights reserved.", "journal": "PATTERN RECOGNITION", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": ["supervised learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000614909300003", "keywords": "Brain MRI; Artefact detection; Convolutional neural networks; Stroke segmentation", "title": "High-resolution radar road segmentation using weakly supervised learning", "abstract": "Self-driving vehicles must reliably detect the drivable area in front of them in any weather condition. An actively developed sensor approach is camera-based road segmentation, but it is limited by the visible spectrum. Radar-based approaches are a promising alternative and a new method extracts the drivable area from raw radar data by training a deep neural network using paired camera data, which can be labelled automatically using pretrained computer vision models. Autonomous driving has recently gained lots of attention due to its disruptive potential and impact on the global economy; however, these high expectations are hindered by strict safety requirements for redundant sensing modalities that are each able to independently perform complex tasks to ensure reliable operation. At the core of an autonomous driving algorithmic stack is road segmentation, which is the basis for numerous planning and decision-making algorithms. Radar-based methods fail in many driving scenarios, mainly as various common road delimiters barely reflect radar signals, coupled with a lack of analytical models for road delimiters and the inherit limitations in radar angular resolution. Our approach is based on radar data in the form of a two-dimensional complex range-Doppler array as input into a deep neural network (DNN) that is trained to semantically segment the drivable area using weak supervision from a camera. Furthermore, guided back propagation was utilized to analyse radar data and design a novel perception filter. Our approach creates the ability to perform road segmentation in common driving scenarios based solely on radar data and we propose to utilize this method as an enabler for redundant sensing modalities for autonomous driving.", "journal": "NATURE MACHINE INTELLIGENCE", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": ["neural net", "supervised learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000443244400057", "keywords": "Linear Convolution; Matching Points; Classification; Fashion images", "title": "Fashion image classification using matching points with linear convolution", "abstract": "Social image data related to fashion is flowing through the social networks in huge amount. Analysis of this data is a challenging task due to its characteristics like voluminous, unstructured, etc. Classification provides an easy and efficient way to deal with such data. In this paper, we proposed a new approach for classification of fashion images by incorporating the concepts of linear convolution and matching points using local features. Linear convolution is used to get the representative images with important features. Then, matching points between given image and class representative images are obtained. Maximum matching points are considered while assigning a class label to the given image. Proposed approach is useful further for various applications related to fashion such as fashion recommendation, fashion trend analysis, etc.", "journal": "MULTIMEDIA TOOLS AND APPLICATIONS", "category": "Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000482599100010", "keywords": "Multi-view learning; deep learning; regularization; normalization; canonical correlation analysis", "title": "Deep Multi-View Learning Using Neuron-Wise Correlation-Maximizing Regularizers", "abstract": "Many machine learning problems are concerned with discovering or associating common patterns in data of multiple views or modalities. Multi-view learning is one of the methods to achieve such goals. Recent methods propose deep multi-view networks via adaptation of generic deep neural networks (DNNs), which concatenate features of individual views at intermediate network layers (i.e., fusion layers). In this paper, we study the problem of multi-view learning in such end-to-end networks. We take a regularization approach via multi-view learning criteria, and propose a novel, effective, and efficient neuron-wise correlation-maximizing regularizer. We implement our proposed regularizers collectively as a correlation-regularized network layer (CorrReg). CorrReg can he applied to either fully-connected or convolutional fusion layers, simply by replacing them with their CorrReg counterparts. By partitioning neurons of a hidden layer in generic DNNs into multiple subsets, we also consider a multi-view feature learning perspective of generic DNNs. Such a perspective enables us to study deep multi-view learning in the context of regularized network training, for which we present control experiments of benchmark image classification to show the efficacy of our proposed CorrReg. To investigate how CorrReg is useful for practical multi-view learning problems, we conduct experiments of RCB-D object/scene recognition and multi-view-based 3D object recognition, using networks with fusion layers that concatenate intermediate features of individual modalities or views for subsequent classification. Applying CorrReg to fusion layers of these networks consistently improves classification performance. In particular, we achieve the new state of the art on the benchmark RGB-D object and RGB-D scene datasets. We make the implementation of CorrReg publicly available.", "journal": "IEEE TRANSACTIONS ON IMAGE PROCESSING", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": ["neural net", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000565858900019", "keywords": "Computer architecture; Estimation; Convolutional neural networks; Computational efficiency; Internet of Things; Visualization; Network compression; network pruning; convolutional neural networks", "title": "Discriminative Layer Pruning for Convolutional Neural Networks", "abstract": "The predictive ability of convolutional neural networks (CNNs) can be improved by increasing their depth. However, increasing depth also increases computational cost significantly, in terms of both floating point operations and memory consumption, hindering applicability on resource-constrained systems such as mobile and internet of things (IoT) devices. Fortunately, most networks have spare capacity, that is, they require fewer parameters than they actually have to perform accurately. This motivates network compression methods, which remove or quantize parameters to improve resource-efficiency. In this work, we consider a straightforward strategy for removing entire convolutional layers to reduce network depth. Since it focuses on depth, this approach not only reduces memory usage, but also reduces prediction time significantly by mitigating the serialization overhead incurred by forwarding through consecutive layers. We show that a simple subspace projection approach can be employed to estimate the importance of network layers, enabling the pruning of CNNs to a resource-efficient depth within a given network size constraint. We estimate importance on a subspace computed using Partial Least Squares, a feature projection approach that preserves discriminative information. Consequently, this importance estimation is correlated to the contribution of the layer to the classification ability of the model. We show that cascading discriminative layer pruning with filter-oriented pruning improves the resource-efficiency of the resulting network compared to using any of them alone, and that it outperforms state-of-the-art methods. Moreover, we show that discriminative layer pruning alone, without cascading, achieves competitive resource-efficiency compared to methods that prune filters from all layers.", "journal": "IEEE JOURNAL OF SELECTED TOPICS IN SIGNAL PROCESSING", "category": "Engineering, Electrical & Electronic", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000469878600008", "keywords": "Semantic segmentation; Instance detection; Lung cancer; Mediastinal lymph nodes; CT", "title": "Semantic segmentation and detection of mediastinal lymph nodes and anatomical structures in CT data for lung cancer staging", "abstract": "PurposeAccurate lung cancer diagnosis is crucial to select the best course of action for treating the patient. From a simple chest CT volume, it is necessary to identify whether the cancer has spread to nearby lymph nodes or not. It is equally important to know precisely where each malignant lymph node is with respect to the surrounding anatomical structures and the airways. In this paper, we introduce a new data-set containing annotations of fifteen different anatomical structures in the mediastinal area, including lymph nodes of varying sizes. We present a 2D pipeline for semantic segmentation and instance detection of anatomical structures and potentially malignant lymph nodes in the mediastinal area.MethodsWe propose a 2D pipeline combining the strengths of U-Net for pixel-wise segmentation using a loss function dealing with data imbalance and Mask R-CNN providing instance detection and improved pixel-wise segmentation within bounding boxes. A final stage performs pixel-wise labels refinement and 3D instance detection using a tracking approach along the slicing dimension. Detected instances are represented by a 3D pixel-wise mask, bounding volume, and centroid position.ResultsWe validated our approach following a fivefold cross-validation over our new data-set of fifteen lung cancer patients. For the semantic segmentation task, we reach an average Dice score of 76% over all fifteen anatomical structures. For the lymph node instance detection task, we reach 75% recall for 9 false positives per patient, with an average centroid position estimation error of 3 mm in each dimension.ConclusionFusing 2D networks' results increases pixel-wise segmentation results while enabling good instance detection. Better leveraging of the 3D information and station mapping for the detected lymph nodes are the next steps.", "journal": "INTERNATIONAL JOURNAL OF COMPUTER ASSISTED RADIOLOGY AND SURGERY", "category": "Engineering, Biomedical; Radiology, Nuclear Medicine & Medical Imaging; Surgery", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000744348500001", "keywords": "Traffic detection; Vehicle occlusion; Heatmaps; Roadside LiDAR", "title": "MAMask: Multi-feature aggregation instance segmentation with pyramid attention mechanism", "abstract": "Instance segmentation is a fundamental yet challenging vision task. Recently, many instance segmentation methods have attempted to use attention mechanisms to improve model efficiency. However, these methods still ignore the problem of information loss in lateral connection of Feature Pyramid Networks (the supplement operation of low-resolution, semantically strong features in FPN). The paper presents an effective detection-based approach named MAMask, which is closely tied to the one-stage method, Fully Convolutional One-Stage Object Detection (FCOS). In particular, it adopts the multi-feature aggregation decoder with pyramid integrate attention (PIA) to instance segmentation. The pyramid integrate attention block can prevent the loss of important channel information by learning richer multi-scale representation. Meanwhile, it also brings significant improvements in performance for existing FPN-based frameworks at slight additional computational costs. The proposed MAMask achieves 37.3% in box AP on the COCO dataset. The method outperforms a few recent methods without longer training time. Compared with the current typical algorithms, the proposed method has achieved excellent performance.", "journal": "IET IMAGE PROCESSING", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000856490500001", "keywords": "sonar target segmentation; location category; pyramid segmentation attention module; gated fusion; sonar target segmentation dataset", "title": "Attentive SOLO for Sonar Target Segmentation", "abstract": "Imaging sonar systems play an important role in underwater target detection and location. Due to the influence of reverberation noise on imaging sonar systems, the task of sonar target segmentation is a challenging problem. In order to segment different types of targets in sonar images accurately, we proposed the gated fusion-pyramid segmentation attention (GF-PSA) module. Specifically, inspired by gated full fusion, we improved the pyramid segmentation attention (PSA) module by using gated fusion to reduce the noise interference during feature fusion and improve segmentation accuracy. Then, we improved the SOLOv2 (Segmenting Objects by Locations v2) algorithm with the proposed GF-PSA and named the improved algorithm Attentive SOLO. In addition, we constructed a sonar target segmentation dataset, named STSD, which contains 4000 real sonar images, covering eight object categories with a total of 7077 target annotations. The experimental results show that the segmentation accuracy of Attentive SOLO on STSD is as high as 74.1%, which is 3.7% higher than that of SOLOv2.", "journal": "ELECTRONICS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Physics, Applied", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000637533800018", "keywords": "Visualization; Training; Semantics; Medical diagnostic imaging; Neural networks; Task analysis; Multimodal neural networks; text-guided network training; attention mechanisms; vision recognition; medical images", "title": "Text-Guided Neural Network Training for Image Recognition in Natural Scenes and Medicine", "abstract": "Convolutional neural networks (CNNs) are widely recognized as the foundation for machine vision systems. The conventional rule of teaching CNNs to understand images requires training images with human annotated labels, without any additional instructions. In this article, we look into a new scope and explore the guidance from text for neural network training. We present two versions of attention mechanisms to facilitate interactions between visual and semantic information and encourage CNNs to effectively distill visual features by leveraging semantic features. In contrast to dedicated text-image joint embedding methods, our method realizes asynchronous training and inference behavior: a trained model can classify images, irrespective of the text availability. This characteristic substantially improves the model scalability to multiple (multimodal) vision tasks. We also apply the proposed method onto medical imaging, which learns from richer clinical knowledge and achieves attention-based interpretable decision-making. With comprehensive validation on two natural and two medical datasets, we demonstrate that our method can effectively make use of semantic knowledge to improve CNN performance. Our method performs substantial improvement on medical image datasets. Meanwhile, it achieves promising performance for multi-label image classification and caption-image retrieval as well as excellent performance for phrase-based and multi-object localization on public benchmarks.", "journal": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000652047500001", "keywords": "Feature extraction; Computer architecture; Fuses; Convolutional neural networks; Visualization; Training; Task analysis; Convolutional neural networks; performance; multi-scale features fusion", "title": "Improving the Performance of Convolutional Neural Networks by Fusing Low-Level Features With Different Scales in the Preceding Stage", "abstract": "The width of convolutional neural networks (CNNs) is crucial for improving performance. Many wide CNNs use a convolutional layer to fuse multiscale features or fuse the preceding features to subsequent features. However, these CNNs rarely use blocks, which consist of a series of successive convolutional layers, to fuse multiscale features. In this paper, we propose an approach for improving performance by fusing the low-level features extracted from different blocks. We utilize five different convolutions, including 3 x 3, 5 x 5, 7 x 7,5 x 3 boolean OR 3 x 5 and 7 x 3 boolean OR 3 x 7 , to generate five low-level features, and we design two fusion strategies: low-level feature fusion (L-Fusion) and high-level feature fusion (H-Fusion). Experimental results show that the L-Fusion is more helpful for improving the performance of CNNs, and the 5 x 5 convolution is more suitable for multiscale feature fusion. We summarize the conclusion as a strategy that fuses multiscale features in the preceding stage of CNNs. Furthermore, we propose a new architecture to perceive the input of CNNs by using two self-governed blocks based on the strategy. Finally, we modify five off-the-shelf networks, DenseNet-BC (depth = 40), ALL-CNN-C (depth = 9), Darknet 19 (depth = 19), Resnet 18 (depth = 18) and Resnet 50 (depth = 50), by utilizing the proposed architecture to verify the conclusion, and these updated networks provide more competitive results.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000861108300027", "keywords": "Orientation adaptive kernel; rotation invariance; image transformation; feature extraction", "title": "OACNNs: Orientation adaptive convolutional neural networks", "abstract": "Geometric invariant feature representation plays an indispensable role in the field of image processing and computer vision. Recently, convolution neural networks (CNNs) have witnessed a great research progress, however CNNs do not excel at dealing with geometrically transformed images. Existing methods enhancing the ability of CNNs learning invariant feature representation rely partly on data augmentation or have a relatively weak generalization ability. This paper proposes orientation adaptive kernels (OA kernels) and orientation adaptive max pooling (OA max pooling) that comprise a new topological structure, orientation adaptive neural networks (OACNNs). OA kernels output the orientation feature maps which encode the orientation information of images. OA max pooling max-pools the orientation feature maps by automatically rotating the pooling windows according to their orientation. OA kernels and OA max pooling together allow for the eight orientation response of images to be computed, and then the max orientation response is obtained, which is proved to be a robust rotation invariant feature representation. OACNNs are compared with state-of-the-art methods and consistently outperform them in various experiments. OACNNs demonstrate a better generalization ability, yielding a test error rate 3.14 on the rotated images but only trained on \"up-right\" images, which outperforms all state-of-the-art methods by a large margin.", "journal": "JOURNAL OF INTELLIGENT & FUZZY SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000724261200001", "keywords": "GAN; Data augmentation; Semantic segmentation", "title": "Learning an augmentation strategy for sparse datasets", "abstract": "The limited quantity of training data can hamper supervised machine learning methods, that generally need large amounts of data to avoid overfitting. Data augmentation has a long history of use with machine learning algorithms and is a straightforward method to overcome overfitting and improve model generalisation. However, data augmentation schemes are typically designed by hand and demand substantial domain knowledge to create suitable data transformations. This paper introduces a GAN based method that automatically learns an augmentation strategy appropriate for sparse datasets and can improve pixel-level semantic segmentation accuracy by filling the gaps in the training set. Our method can also be combined with other augmentation techniques to further improve performance. We evaluate the proposed method's feasibility on four datasets and three semantic segmentation models, leading to improvement in the mean intersection-over-union (mIoU) score of between 0.5 and 14 percentage points, under different circumstances. (c) 2021 Elsevier B.V. All rights reserved.", "journal": "IMAGE AND VISION COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic; Optics", "annotated_keywords": ["machine learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000541768000001", "keywords": "FPGA implementation; High-speed vision; Fast-object detection; Convolutional neural network", "title": "A novel hardware-oriented ultra-high-speed object detection algorithm based on convolutional neural network", "abstract": "This paper describes a hardware-oriented two-stage algorithm that can be deployed in a resource-limited field-programmable gate array (FPGA) for fast-object detection and recognition with out external memory. The first stage is the bounding boxes proposal with a conventional object detection method, and the second is convolutional neural network (CNN)-based classification for accuracy improvement. Frequently accessing external memories significantly affects the execution efficiency of object classification. Unfortunately, the existing CNN models with a large number of parameters are difficult to deploy in FPGAs with limited on-chip memory resources. In this study, we designed a compact CNN model and performed the hardware-oriented quantization for parameters and intermediate results. As a result, CNN-based ultra-fast-object classification was realized with all parameters and intermediate results stored on chip. Several evaluations were performed to demonstrate the performance of the proposed algorithm. The object classification module consumes only 163.67 Kbits of on-chip memories for ten regions of interest (ROIs), this is suitable for low-end FPGA devices. In the aspect of accuracy, our method provides a correctness rate of 98.01% in open-source data set MNIST and over 96.5% in other three self-built data sets, which is distinctly better than conventional ultra-high-speed object detection algorithms.", "journal": "JOURNAL OF REAL-TIME IMAGE PROCESSING", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000788730900006", "keywords": "Contrastive learning; Self-supervised learning; Deep knowledge tracing; Graph neural network; Intelligent tutoring systems", "title": "Bi-CLKT: Bi-Graph Contrastive Learning based Knowledge Tracing", "abstract": "The goal of Knowledge Tracing (KT) is to estimate how well students have mastered a concept based on their historical learning of related exercises. The benefit of knowledge tracing is that students' learning plans can be better organised and adjusted, and interventions can be made when necessary. With the recent rise of deep learning, Deep Knowledge Tracing (DKT) has utilised Recurrent Neural Networks (RNNs) to accomplish this task with some success. Other works have attempted to introduce Graph Neural Networks (GNNs) and redefine the task accordingly to achieve significant improvements. However, these efforts suffer from at least one of the following drawbacks: (1) they pay too much attention to details of the nodes rather than to high-level semantic information; (2) they struggle to effectively establish spatial associations and complex structures of the nodes; and (3) they represent either concepts or exercises only, without integrating them. Inspired by recent advances in self-supervised learning, we propose a Bi-Graph Contrastive Learning based Knowledge Tracing (Bi-CLKT) to address these limitations. Specifically, we design a two-layer comparative learning scheme based on an \"exercise-to-exercise \"(E2E) relational subgraph. It involves node-level contrastive learning of subgraphs to obtain discriminative representations of exercises, and graph-level contrastive learning to obtain discriminative representations of concepts. Moreover, we designed a joint contrastive loss to obtain better representations and hence better prediction performance. Also, we explored two different variants, using RNN and memory-augmented neural networks as the prediction layer for comparison to obtain better representations of exercises and concepts respectively. Extensive experiments on four real-world datasets show that the proposed Bi-CLKT and its variants outperform other baseline models. (c) 2022 Elsevier B.V. All rights reserved.", "journal": "KNOWLEDGE-BASED SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net", "deep learning", "supervised learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000435456100001", "keywords": "Dropout; deep neural networks; regularization; learning theory", "title": "Surprising properties of dropout in deep networks", "abstract": "We analyze dropout in deep networks with rectified linear units and the quadratic loss. Our results expose surprising differences between the behavior of dropout and more traditional regularizers like weight decay. For example, on some simple data sets dropout training produces negative weights even though the output is the sum of the inputs. This provides a counterpoint to the suggestion that dropout discourages co-adaptation of weights. We also show that the dropout penalty can grow exponentially in the depth of the network while the weight-decay penalty remains essentially linear, and that dropout is insensitive to various re-scalings of the input features, outputs, and network weights. This last insensitivity implies that there are no isolated local minima of the dropout training criterion. Our work uncovers new properties of dropout, extends our understanding of why dropout succeeds, and lays the foundation for further progress.", "journal": "JOURNAL OF MACHINE LEARNING RESEARCH", "category": "Automation & Control Systems; Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000782844000029", "keywords": "Few-shot learning; Object detection; Self-attention; Feature aggregation; Orthogonal loss", "title": "Deep learning-based approach for identification of diseases of maize crop", "abstract": "In recent years, deep learning techniques have shown impressive performance in the field of identification of diseases of crops using digital images. In this work, a deep learning approach for identification of in-field diseased images of maize crop has been proposed. The images were captured from experimental fields of ICAR-IIMR, Ludhiana, India, targeted to three important diseases viz. Maydis Leaf Blight, Turcicum Leaf Blight and Banded Leaf and Sheath Blight in a non-destructive manner with varied backgrounds using digital cameras and smartphones. In order to solve the problem of class imbalance, artificial images were generated by rotation enhancement and brightness enhancement methods. In this study, three different architectures based on the framework of 'Inception-v3' network were trained with the collected diseased images of maize using baseline training approach. The best-performed model achieved an overall classification accuracy of 95.99% with average recall of 95.96% on the separate test dataset. Furthermore, we compared the performance of the best-performing model with some pre-trained state-of-the-art models and presented the comparative results in this manuscript. The results reported that best-performing model performed quite better than the pre-trained models. This demonstrates the applicability of baseline training approach of the proposed model for better feature extraction and learning. Overall performance analysis suggested that the best-performed model is efficient in recognizing diseases of maize from in-field images even with varied backgrounds.", "journal": "SCIENTIFIC REPORTS", "category": "Multidisciplinary Sciences", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000838362300008", "keywords": "Remote sensing; Image segmentation; Feature extraction; Minerals; Satellites; Head; Benchmark testing; Atrous convolution; mining area; pixel-adaptive; remote sensing image segmentation", "title": "Pixel-Adaptive Field-of-View for Remote Sensing Image Segmentation", "abstract": "Mineral segmentation of satellite imagery is crucial to mining surveying and monitoring. Conventional deep segmentation networks extract features at every position with a fixed field-of-view. Nevertheless, the rich content in large mineral scenes, which causes dramatically different local characteristics across regions, may require features with spatially varying field-of-view to achieve accurate segmentation. In light of this, we propose a novel pixel-adaptive field-of-view (PA-FoV) module to adjust the field-of-view of a given feature map in a pixel-wise manner. Specifically, it refines the features at each position by a weighted aggregation of the outputs from atrous convolutions with different dilation rates adaptively depending on the position-specific content. The module works in a plug-and-play manner and can be flexibly inserted into any arbitrary backbone network or segmentation head, to boost feature representation and in turn improve the result of segmentation. Moreover, in order to mitigate the scarcity of labeled data, we further establish a benchmark remote sensing mineral dataset, dubbed RSMI, to facilitate research in this field. Extensive experiments show a simple addition of our PA-FoV module provides solid improvements on top of strong baselines, achieving state-of-the-art performance.", "journal": "IEEE GEOSCIENCE AND REMOTE SENSING LETTERS", "category": "Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000800780600001", "keywords": "Quantization (signal); Binary codes; Image retrieval; Semantics; Feature extraction; Deep learning; Convolutional neural networks; Deep supervised hashing (DSH); discriminative ability; geometric structure; image retrieval", "title": "Discriminative Geometric-Structure-Based Deep Hashing for Large-Scale Image Retrieval", "abstract": "Deep hashing reaps the benefits of deep learning and hashing technology, and has become the mainstream of large-scale image retrieval. It generally encodes image into hash code with feature similarity preserving, that is, geometric-structure preservation, and achieves promising retrieval results. In this article, we find that existing geometric-structure preservation manner inadequately ensures feature discrimination, while improving feature discrimination of hash code essentially determines hash learning retrieval performance. This fact principally spurs us to propose a discriminative geometric-structure-based deep hashing method (DGDH), which investigates three novel loss terms based on class centers to induce the so-called discriminative geometrical structure. In detail, the margin-aware center loss assembles samples in the same class to the corresponding class centers for intraclass compactness, then a linear classifier based on class center serves to boost interclass separability, and the radius loss further puts different class centers on a hypersphere to tentatively reduce quantization errors. An efficient alternate optimization algorithm with guaranteed desirable convergence is proposed to optimize DGDH. We theoretically analyze the robustness and generalization of the proposed method. The experiments on five popular benchmark datasets demonstrate superior image retrieval performance of the proposed DGDH over several state of the arts.", "journal": "IEEE TRANSACTIONS ON CYBERNETICS", "category": "Automation & Control Systems; Computer Science, Artificial Intelligence; Computer Science, Cybernetics", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000615340300017", "keywords": "radiography; deep learning; anteroposterior; posteroanterior; error correction; X-ray", "title": "Differentiation Between Anteroposterior and Posteroanterior Chest X-Ray View Position With Convolutional Neural Networks", "abstract": "Purpose Detection and validation of the chest X-ray view position with use of convolutional neural networks to improve meta-information for data cleaning within a hospital data infrastructure. Material and Methods Within this paper we developed a convolutional neural network which automatically detects the anteroposterior and posteroanterior view position of a chest radiograph. We trained two different network architectures (VGG variant and ResNet-34) with data published by the RSNA (26 684 radiographs, class distribution 46 % AP, 54 % PA) and validated these on a self-compiled dataset with data from the University Hospital Essen (4507, radiographs, class distribution 55% PA, 45% AP) labeled by a human reader. For visualization and better understanding of the network predictions, a Grad-CAM was generated for each network decision. The network results were evaluated based on the accuracy, the area under the curve (AUC), and the F1-score against the human reader labels. Also a final performance comparison between model predictions and DICOM labels was performed. Results The ensemble models reached accuracy and F1-scores greater than 95%. The AUC reaches more than 0.99 for the ensemble models. The Grad-CAMs provide insight as to which anatomical structures contributed to a decision by the networks which are comparable with the ones a radiologist would use. Furthermore, the trained models were able to generalize over mislabeled examples, which was found by comparing the human reader labels to the predicted labels as well as the DICOM labels. Conclusion The results show that certain incorrectly entered meta-information of radiological images can be effectively corrected by deep learning in order to increase data quality in clinical application as well as in research.", "journal": "ROFO-FORTSCHRITTE AUF DEM GEBIET DER RONTGENSTRAHLEN UND DER BILDGEBENDEN VERFAHREN", "category": "Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": ["neural net", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000761758600001", "keywords": "Crowd counting; Crowd density estimation; Convolutional neural network; Deep learning", "title": "A survey of crowd counting and density estimation based on convolutional neural network", "abstract": "Crowd counting and crowd density estimation methods are of great significance in the field of public security. Estimating crowd density and counting from single image or video frame has become an essential part of a computer vision system in various scenarios. In this paper, we comprehensively review the recent research advancement on crowd counting and density estimation. First of all, we introduce the background of crowd counting and crowd density estimation. Second, the traditional crowd counting methods are summarized. Third, we focus on reviewing the crowd counting and crowd density methods based on convolutional neural network (CNN) models. Next, we report and discuss the experimental results of a number of typical methods on benchmark datasets. Finally, we present the promising future directions of crowd counting and crowd density. (c) 2021 Elsevier B.V. All rights reserved.", "journal": "NEUROCOMPUTING", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000332245500010", "keywords": "Gamma-Semihypergroup; intuitionistic fuzzy bi-(left, right, two-sided)Gamma-hyperideal; (lambda, mu)-intuitionistic fuzzy Gamma-hyperideal; intuitionistic fuzzy M(resp N)-hypersystems", "title": "A study on intuitionistic fuzzy sets in Gamma-semihypergroups", "abstract": "The notion of intuitionistic fuzzy sets was introduced by Atanassov as a generalization of the notion of fuzzy sets. In this paper, using Atanassov idea, we continue the study on intuitionistic fuzzy sets in Gamma-semihypergroups initiated recently by Ersoy and Davvaz [25]. We give some further properties of intuitionistic fuzzy Gamma-hyperideals and intuitionistic fuzzy bi-Gamma-hyperideals in a Gamma-semihypergroup and use the intuitionistic fuzzy left, right, two-sided and bi-Gamma-hyperideals to characterize some classes of Gamma-semihypergroups. We introduce and study (lambda, mu)-intuitionistic fuzzy Gamma-hyperideals. We also introduce the notion of an intuitionistic fuzzy M(resp. N)-hypersystem of a Gamma-semihypergroup and some properties of them are investigated.", "journal": "JOURNAL OF INTELLIGENT & FUZZY SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000464625000052", "keywords": "Wireless network; intrusion interference signal; detection", "title": "AN OPTIMIZATION DETECTION ALGORITHM FOR COMPLEX INTRUSION INTERFERENCE SIGNAL IN MOBILE WIRELESS NETWORK", "abstract": "At present, when detecting intrusive interference signals in classified form, the effect of channel denoising is very poor, and the characteristics of the extracted signals are not clear, which can not achieve effective detection of intrusion signals. An algorithm based on wavelet packet frequency hopping estimation for complex network intrusion detection is proposed in this paper. The soft and hard threshold method is used for wavelet coefficient decomposition, threshold processing, and signal reconstruction; according to probability statistics, a new sequence is composed of the spectral amplitude corresponding to the same frequency of each random variable in a random process and the spectrum matrix of intrusion interference signal is formed, so as to extract the characteristic spectrum of intrusion interference signal; by using the energy balance method, Gauss stochastic wavelet characteristics of intrusion signal can be simulated. The results of network intrusion detection are obtained by the Gauss additivity of the high-order cumulants of the network intrusion. The three edge centroid positioning method is applied to achieve the high-precision location of the intrusion point. Experiments show that the algorithm effectively improves the network channel denoising and the feature extraction effect of the intrusion signal, and it is also better than the current algorithm for the detection and location of the interference signals.", "journal": "DISCRETE AND CONTINUOUS DYNAMICAL SYSTEMS-SERIES S", "category": "Mathematics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000465122600004", "keywords": "Android malware; malware families; dimensionality reduction; artificial neural networks", "title": "Gaining deep knowledge of Android malware families through dimensionality reduction techniques", "abstract": "This research proposes the analysis and subsequent characterisation of Android malware families by means of low dimensional visualisations using dimensional reduction techniques. The well-known Malgenome data set, coming from the Android Malware Genome Project, has been thoroughly analysed through the following six dimensionality reduction techniques: Principal Component Analysis, Maximum Likelihood Hebbian Learning, Cooperative Maximum Likelihood Hebbian Learning, Curvilinear Component Analysis, Isomap and Self Organizing Map. Results obtained enable a clear visual analysis of the structure of this high-dimensionality data set, letting us gain deep knowledge about the nature of such Android malware families. Interesting conclusions are obtained from the real-life data set under analysis.", "journal": "LOGIC JOURNAL OF THE IGPL", "category": "Mathematics, Applied; Mathematics; Logic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000323803500012", "keywords": "Estimation of distribution algorithms; Mutation; Bayesian network; Structure learning; Optimization", "title": "Structure Learning of Bayesian Networks by Estimation of Distribution Algorithms with Transpose Mutation", "abstract": "Estimation of distribution algorithms (EDAs) constitute a new branch of evolutionary optimization algorithms that were developed as a natural alternative to genetic algorithms (GAs). Several studies have demonstrated that the heuristic scheme of EDAs is effective and efficient for many optimization problems. Recently, it has been reported that the incorporation of mutation into EDAs increases the diversity of genetic information in the population, thereby avoiding premature convergence into a suboptimal solution. In this study, we propose a new mutation operator, a transpose mutation, designed for Bayesian structure learning. It enhances the diversity of the offspring and it increases the possibility of inferring the correct arc direction by considering the arc directions in candidate solutions as bi-directional, using the matrix transpose operator. As compared to the conventional EDAs, the transpose mutation-adopted EDAs are superior and effective algorithms for learning Bayesian networks.", "journal": "JOURNAL OF APPLIED RESEARCH AND TECHNOLOGY", "category": "Mathematics, Applied; Mathematics", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000459582900001", "keywords": "Function-motion-action decomposition method; meta-action unit; fault diagnosis; fault tree analysis approach; evidential network", "title": "Designing of a Risk Assessment Model for Issuing Credit Card Using Parallel Social Spider Algorithm", "abstract": "The financial creditability of the customer needs to be verified by the lender/bank before issuing a credit card. This involves assessment of factors like the economic, social or social-economic background of the person. Thus, the features incorporated into the analysis are mixed data type ex. Income (numerical) and Property Owned (Categorical). In this manuscript, a credit card lending model is designed using a recently proposed parallel social spider algorithm by Shukla and Nanda in 2016. Suitable modifications have been introduced in the coding scheme and mating procedure to efficiently solve the credit assessment problem. Experiments are carried out on various standard credit card data available like German, Australian and Japanese credit card datasets. The superior performance of proposed algorithm is reported as compared to that achieved by K-means, parallel real genetic algorithm and parallel particle swarm optimization (PPSO). The Silhouette Index obtained by various algorithms specifically for Germen dataset are 0.56% by K-means, 0.86% by parallel Real Coded Genetic algorithm, 0.71% by PPSO and 0.84% by proposed method.", "journal": "APPLIED ARTIFICIAL INTELLIGENCE", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000429584400006", "keywords": "Data streams; Metalearning; Adaptive ensemble size; Return on investment; OzaBag", "title": "Anomaly behavior analysis for IoT sensors", "abstract": "The Internet of Things (IoT) will not only connect computers and mobile devices but also interconnect smart buildings, homes, and cities, as well as electrical grids, gas, and water networks, automobiles, airplanes, etc. IoT will lead to the development of a wide range of advanced information services that need to be processed in real time and require large storage and computational power. The integration of IoT with fog and cloud computing not only brings the computational requirements but also enables IoT services to be pervasive, cost-effective, and accessible from anywhere and at anytime. In any IoT application, sensors are indispensable to bring the physical world into the digital world that can be implemented by leveraging fog computing. However, IoT sensors will introduce major security challenges as they contribute to a significant increase in the IoT attack surface. In this paper, we present a methodology to develop an intrusion detection system on the basis of anomaly behavior analysis to detect when a sensor has been compromised and used to provide misinformation. Our preliminary experimental results show that our approach can accurately authenticate sensors on the basis of their behavior and can detect known and unknown sensor attacks with high detection rate and low false alarms.", "journal": "TRANSACTIONS ON EMERGING TELECOMMUNICATIONS TECHNOLOGIES", "category": "Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000345492900017", "keywords": "Classification; Coefficient regularization; Polynomial kernels; Bernstein-Kantorovich polynomial; Learning rates", "title": "CLASSIFICATION WITH POLYNOMIAL KERNELS AND l(1)-COEFFICIENT REGULARIZATION", "abstract": "In this paper we investigate a class of learning algorithms for classification generated by regularization schemes with polynomial kernels and l(1)-regularizer. The novelty of our analysis lies in the estimation of the hypothesis error. A Bernstein-Kantorovich polynomial is introduced as a regularizing function. Although the hypothesis spaces and the regularizers in the schemes are sample dependent, we prove the hypothesis error can be removed from the error decomposition with confidence. As a result, we derive some explicit learning rates for the produced classifiers under some assumptions.", "journal": "TAIWANESE JOURNAL OF MATHEMATICS", "category": "Mathematics", "annotated_keywords": ["learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000323015000004", "keywords": "Clustering; Fuzzy clustering; Noise; Outlier; Constraint; Trimming", "title": "Robust constrained fuzzy clustering", "abstract": "It is well-known that outliers and noisy data can be very harmful when applying clustering methods. Several fuzzy clustering methods which are able to handle the presence of noise have been proposed. In this work, we propose a robust clustering approach called F-TCLUST based on trimming a fixed proportion of observations that are (\"impartially\") determined by the data set itself. The proposed approach also considers an eigenvalue ratio constraint that makes it a mathematically well-defined problem and serves to control the allowed differences among cluster scatters. A computationally feasible algorithm is proposed for its practical implementation. Some guidelines about how to choose the parameters controlling the performance of the fuzzy clustering procedure are also given. (C) 2013 Elsevier Inc. All rights reserved.", "journal": "INFORMATION SCIENCES", "category": "Computer Science, Information Systems", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000415684800009", "keywords": "Offshore wind farm; site selection; interval number with probability distribution; stochastic dominance degree", "title": "An innovative method for offshore wind farm site selection based on the interval number with probability distribution", "abstract": "There is insufficient research relating to offshore wind farm site selection in China. The current methods for site selection have some defects. First, information loss is caused by two aspects: the implicit assumption that the probability distribution on the interval number is uniform; and ignoring the value of decision makers' (DMs') common opinion on the criteria information evaluation. Secondly, the difference in DMs' utility function has failed to receive attention. An innovative method is proposed in this article to solve these drawbacks. First, a new form of interval number and its weighted operator are proposed to reflect the uncertainty and reduce information loss. Secondly, a new stochastic dominance degree is proposed to quantify the interval number with a probability distribution. Thirdly, a two-stage method integrating the weighted operator with stochastic dominance degree is proposed to evaluate the alternatives. Finally, a case from China proves the effectiveness of this method.", "journal": "ENGINEERING OPTIMIZATION", "category": "Engineering, Multidisciplinary; Operations Research & Management Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000778057700001", "keywords": "Magnetic resonance imaging; Brain image segmentation; Gravitational search algorithm; Fuzzy clustering; Fuzzy inference rules", "title": "Fuzzy clustering using gravitational search algorithm for brain image segmentation", "abstract": "Clustering is a key activity in numerous data mining applications such as information retrieval, text mining, image segmentation. Clustering also plays a major role in medical image processing. Manual image segmentation is very tedious and time consuming task and the results of manual segmentation are subjected to errors due to huge and varying data. Therefore, automated segmentation systems are gaining enormous importance nowadays. This paper presents an automated system for segmentation of brain tissues namely white matter, gray matter and cerebrospinal fluid from brain MRI images. In this work, we propose a novel clustering approach, Fuzzy-Gravitational Search Algorithm(GSA) for MRI brain image segmentation. The proposed approach is based on GSA, and uses fuzzy inference rules for controlling the parameter alpha as search progresses. The results of the system are compared with GSA and recent work on brain image segmentation algorithms for both real and simulated database on the basis of Dice Coefficient values. The performance of the Fuzzy-GSA algorithm is also evaluated against four benchmark datasets from the UC Irvine repository. The results illustrate that the Fuzzy-GSA approach attains the highest quality clustering over the selected datasets when compared with several other clustering algorithms.", "journal": "MULTIMEDIA TOOLS AND APPLICATIONS", "category": "Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000317744300023", "keywords": "Type 2 diabetes mellitus; Artificial neural network; Multivariate logistic regression; Classification model", "title": "Evaluating the risk of type 2 diabetes mellitus using artificial neural network: An effective classification approach", "abstract": "Aim: To develop and evaluate an effective classification approach without biochemical parameters to identify those at high risk of T2DM in rural adults. Methods: A cross-sectional survey was conducted. Of 8640 subjects who met inclusion criteria, 75% (N-1 = 6480) were randomly selected to provide training set for constructing artificial neural network (ANN) and multivariate logistic regression (MLR) models. The remaining 25% (N-2 = 2160) were assigned to validation set for performance comparisons of the ANN and MLR models. Predictive performance of different models was analyzed by the receiver operating characteristic (ROC) curve using the validation set. Results: The prevalence rates of T2DM were 8.66% (n = 561) and 9.21% (n = 199) in training and validation sets, respectively. For ANN model, the sensitivity, specificity, positive and negative predictive value for identifying T2DM were 86.93%, 79.14%, 31.86%, and 98.18%, respectively, while MLR model were only 60.80%, 75.48%, 21.78%, and 94.52%, respectively. Area under the ROC curve (AUC) value for identifying T2DM when using the ANN model was 0.891, showing more accurate predictive performance than the MLR model (AUC = 0.744) (P = 0.0001). Conclusion: The ANN model is an effective classification approach for identifying those at high risk of T2DM based on demographic, lifestyle and anthropometric data. (C) 2013 Elsevier Ireland Ltd. All rights reserved.", "journal": "DIABETES RESEARCH AND CLINICAL PRACTICE", "category": "Endocrinology & Metabolism", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000741363900016", "keywords": "Multi-criteria decision-making; spherical fuzzy; intuitionistic fuzzy; TOPSIS; physician selection", "title": "A spherical fuzzy TOPSIS method for solving the physician selection problem", "abstract": "The membership functions of the intuitionistic fuzzy sets, Pythagorean fuzzy sets, neutrosophic sets and spherical fuzzy sets are based on three dimensions. The aim is to collect the expert's judgments. Physicians serve patients in the physician selection problem. It is difficult to measure the service's quality due to the variability in patients' preferences. The patients physician preference criteria is differing and uncertainties. Thus, solving this problem with fuzzy method is more appropriate. In this study, we considered the physician selection as a multi-criteria decision-making problem. Solving this problem, we proposed a spherical fuzzy TOPSIS method. We used the five alternatives and eight criteria. The application was performed in the neurology clinics of Konya city state hospitals. In addition, we solved the same problem by the intuitionistic fuzzy TOPSIS method. We compared the solutions of two methods with each other. We found that the spherical fuzzy TOPSIS method is effective for solving the physician selection problem.", "journal": "JOURNAL OF INTELLIGENT & FUZZY SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000424188400002", "keywords": "Three-way decision spaces; Three-way decisions; Semi-three-way decision spaces; Triangular norms", "title": "On transformations from semi-three-way decision spaces to three-way decision spaces based on triangular norms and triangular conorms", "abstract": "Recently, three-way decisions on three-way decision spaces based on partially ordered sets have been proposed by the axiomatic definitions. And, in the theory of three-way decision spaces, the decision evaluation function (which is defined by the so-called minimum element axiom, monotonicity axiom and complement axiom) is a vital notion and plays pretty important role in practical decision-making problems. However, there are many meaningful handy functions which can not become decision evaluation functions because they only satisfy minimum element axiom and monotonicity axiom in general. Therefore, there arises one discussion in which the author introduced the concept of semi-decision evaluation function without complement axiom lately. And three-way decisions based on semi three-way decision spaces are proposed. This paper continues this research topic and gives several transformation methods from semi-three-way decision spaces to three-way decision spaces based on triangular norms and triangular conorms. Based on them, decision makers can obtain more useful decision evaluation functions thereby having more choices in realistic decision-making problems. In addition, as an application, we analyse a reality example of an evaluation problem of credit card applicants by using the results obtained in this paper. (C) 2017 Elsevier Inc. All rights reserved.", "journal": "INFORMATION SCIENCES", "category": "Computer Science, Information Systems", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000324663000014", "keywords": "Bankruptcy prediction; Logit-regression; Artificial neural networks; Classification and regression trees; AdaBoost", "title": "Bankruptcy prediction for Russian companies: Application of combined classifiers", "abstract": "The problem of bankruptcy forecasting is one of the most actively studied nowadays, posing the task of building effective classifiers as well as the task of dealing with dataset imbalance. In this paper, we apply different combinations of modern learning algorithms (MDA, LR, CRT, and ANNs) in order to try to identify the most effective approach to bankruptcy prediction for Russian manufacturing companies. Simultaneously, we try to find out whether the financial indicators stipulated by Russian legislation provide an effective set of indicators for bankruptcy prediction. (C) 2013 Elsevier Ltd. All rights reserved.", "journal": "EXPERT SYSTEMS WITH APPLICATIONS", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science", "annotated_keywords": ["learning algorithm"], "label": "1", "title_label": "0"}
{"id": "WOS:000470263700066", "keywords": "Salmonella; Antimicrobial resistance; Multi-drug resistance; Patterns of antimicrobial resistance", "title": "DCG plus plus : A data-driven metric for geometric pattern recognition", "abstract": "Clustering large and complex data sets whose partitions may adopt arbitrary shapes remains a difficult challenge. Part of this challenge comes from the difficulty in defining a similarity measure between the data points that captures the underlying geometry of those data points. In this paper, we propose an algorithm, DCG++ that generates such a similarity measure that is data-driven and ultrametric. DCG++ uses Markov Chain Random Walks to capture the intrinsic geometry of data, scans possible scales, and combines all this information using a simple procedure that is shown to generate an ultrametric. We validate the effectiveness of this similarity measure within the context of clustering on synthetic data with complex geometry, on a real-world data set containing segmented audio records of frog calls described by mel-frequency cepstral coefficients, as well as on an image segmentation problem. The experimental results show a significant improvement on performance with the DCG-based ultrametric compared to using an empirical distance measure.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000447535400001", "keywords": "Cotton fibre; selection; MCDM; axiomatic design; fuzzy", "title": "When Collective Knowledge Meets Crowd Knowledge in a Smart City: A Prediction Method Combining Open Data Keyword Analysis and Case-Based Reasoning", "abstract": "One of the significant issues in a smart city is maintaining a healthy environment. To improve the environment, huge amounts of data are gathered, manipulated, analyzed, and utilized, and these data might include noise, uncertainty, or unexpected mistreatment of the data. In some datasets, the class imbalance problem skews the learning performance of the classification algorithms. In this paper, we propose a case-based reasoning method that combines the use of crowd knowledge from open source data and collective knowledge. This method mitigates the class imbalance issues resulting from datasets, which diagnose wellness levels in patients suffering from stress or depression. We investigate effective ways to mitigate class imbalance issues in which the datasets have a higher proportion of one class over another. The results of this proposed hybrid reasoning method, using a combination of crowd knowledge extracted from open source data (i.e., a Google search, or other publicly accessible source) and collective knowledge (i.e., case-based reasoning), were that it performs better than other traditional methods (e.g., SMO, BayesNet, IBk, Logistic, C4.5, and crowd reasoning). We also demonstrate that the use of open source and big data improves the classification performance when used in addition to conventional classification algorithms.", "journal": "JOURNAL OF HEALTHCARE ENGINEERING", "category": "Health Care Sciences & Services", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000416793400044", "keywords": "business sustainability; research and development (R&D); multiple criteria decision-making (MCDM); financial objective; variable-consistency dominance-based rough set approach (VC-DRSA); internetwork relationship map (INRM); directional flow graph (DFG)", "title": "Exploring R&D Influences on Financial Performance for Business Sustainability Considering Dual Profitability Objectives", "abstract": "The importance of research and development (R&D) for business sustainability have gained increasing interests, especially in the high-tech sector. However, the efforts of R&D might cause complex and mixed impacts on the financial results considering the associated expenses. Thus, this study aims to examine how R&D efforts may influence business to improve its financial performance considering the dual objectives: the gross and the net profitability. This research integrates a rough-set-based soft computing technique and multiple criteria decision-making (MCDM) methods to explore this complex and yet valuable issue. A group of public listed companies from Taiwan, all in the semiconductor sector, is analyzed as a case study. More than 30 variables are considered, and the adopted soft computing technique retrieves 14 core attributesfor the dual profitability objectivesto form the evaluation model. The importance of R&D for pursuing superior financial prospects is confirmed, and the empirical case demonstrates how to guide an individual company to plan for improvements to achieve its long-term sustainability by this hybrid approach.", "journal": "SUSTAINABILITY", "category": "Green & Sustainable Science & Technology; Environmental Sciences; Environmental Studies", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000382312200024", "keywords": "Naive Bayes; Test-cost sensitive learning; Test cost; Classification accuracy; Greedy search", "title": "Beyond accuracy: Learning selective Bayesian classifiers with minimal test cost", "abstract": "Some existing test-cost sensitive learning algorithms are about balancing act of the misclassification cost and the total test cost, and the others focus on the balance between the classification accuracy and the total test cost. By far, however, few works reduce the total test cost, yet at the same time maintain the high classification accuracy. In order to achieve this goal, this paper modifies the backward greedy search strategy employed in selective Bayesian classifiers (SBC), which is a state-of-the-art improved naive Bayes algorithm pursuing the high classification accuracy but ignoring the total test cost. We call the resulting model test-cost sensitive naive Bayes (TCSNB). TCSNB conducts a modified backward greedy search strategy to select an optimal attribute subset with the minimal total test cost, yet at the same time maintains the high classification accuracy that characterizes SBC. Extensive empirical study validates its effectiveness and efficiency. (C) 2016 Elsevier B.V. All rights reserved.", "journal": "PATTERN RECOGNITION LETTERS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000461208300012", "keywords": "multiple attribute group decision-making problem; Pythagorean fuzzy Einstein prioritized weighted average operator; Pythagorean fuzzy Einstein prioritized weighted geometric operator; Pythagorean fuzzy sets", "title": "Multiattribute group decision-making based on Pythagorean fuzzy Einstein prioritized aggregation operators", "abstract": "Pythagorean fuzzy set (PFS) is a powerful tool to deal with the imprecision and vagueness. Many aggregation operators have been proposed by many researchers based on PFSs. But the existing methods are under the hypothesis that the decision-makers (DMs) and the attributes are at the same priority level. However, in real group decision-making problems, the attribute and DMs may have different priority level. Therefore, in this paper, we introduce multiattribute group decision-making (MAGDM) based on PFSs where there exists a prioritization relationship over the attributes and DMs. First we develop Pythagorean fuzzy Einstein prioritized weighted average operator and Pythagorean fuzzy Einstein prioritized weighted geometric operator. We study some of its desirable properties such as idempotency, boundary, and monotonicity in detail. Moreover we propose a MAGDM approach based on the developed operators under Pythagorean fuzzy environment. Finally, an illustrative example is provided to illustrate the practicality of the proposed approach.", "journal": "INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000754373600005", "keywords": "Coal industry; fuzzy knowledge; creditworthiness; applied&nbsp; informatics; decision-making", "title": "Fuzzy Model for Assessing the Creditworthiness of Ukrainian Coal Industry Enterprises", "abstract": "The actual scientific research of development of information models of representation of fuzzy knowledge at an estimation of creditworthiness of the enterprises of the coal industry of Ukraine and perfection of a fuzzy mathematical model of an estimation of creditworthiness of the enterprises is carried out. Based on set theoretic analysis, a set of 11 criteria for assessing the creditworthiness of enterprises is formed and divided into 3 groups: indicators of financial stability, analysis of profits and losses, the efficiency of enterprise management. A membership function is constructed for each criterion, which will reveal the uncertainty in the input data, leading to a normalized form for comparison. To construct membership functions, a study was conducted to determine their type and type, as well as the parameters of membership functions based on the experience of credit experts and data sets on indicators, using real financial reports of Ukrainian coal industry enterprises for 2020. The improved fuzzy mathematical model for assessing the creditworthiness of enterprises, which reveals the vagueness of input data, derives the assessment of creditworthiness based on the reasoning of the decision-maker (DM), determines the linguistic level of ability to repay financial obligations on time. Based on the built model, a general six-step algorithm is constructed, which can be quickly implemented in the software product. The developed information model and the improved fuzzy mathematical model were tested on real data of credit assessment of the Lvivugol State Enterprise of Ukraine. Outcome, models, and approaches to presenting fuzzy knowledge for indicators of creditworthiness assessment of Ukrainian coal industry enterprises, which will be a model for other countries to follow.", "journal": "ACTA MONTANISTICA SLOVACA", "category": "Geosciences, Multidisciplinary; Mining & Mineral Processing", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000340533300024", "keywords": "Analytic network process (ANP); Group decision making; Water transfer projects", "title": "A New Integrated MADM Technique Combined with ANP, FTOPSIS and Fuzzy Max-Min Set Method for Evaluating Water Transfer Projects", "abstract": "In sustainable water resources management, it is essential to rank inter-basin water transfer projects. This task is difficult due to many different conflict criteria, complex relations among criteria and various judgments of decision makers. In this paper, an integrated multiple attribute group decision making method consists of ANP (Analytical Network Process), fuzzy TOPSIS and fuzzy max-min set methods is proposed for evaluating water transfer projects. A set of over 60 criteria in social, environmental and economic sectors are used for ranking four water transfer projects in Karun River based on three decision maker judgments. A key novelty of the proposed methodology is its ability to model both complex relations among different criteria in water management and the influence of decision maker judgments' weights on the final ranking in group decision making problem. The procedure starts by obtaining the priority of water transfer projects and the weight of each decision maker judgments by employing ANP and fuzzy TOPSIS, respectively. These weights are used as inputs in the fuzzy max-min set method. Then the effects of decision maker weights on the final ranking are determined in fuzzy environment. Finally, the sensitivity analysis of decision makers' weights has been conducted. The results show that the proposed method is an effective tool for group decision making problems by considering different criteria and decision makers' weights.", "journal": "WATER RESOURCES MANAGEMENT", "category": "Engineering, Civil; Water Resources", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000460171000002", "keywords": "Credit scoring; Gustafson-Kessel algorithm; Feature subset selection problem; Binary particle swarm optimization", "title": "A new algorithm of modified binary particle swarm optimization based on the Gustafson-Kessel for credit risk assessment", "abstract": "To increase the quality of loans provision and reduce the risk involved in this process, several credit scoring models have been developed and utilized to improve the process of assessing credit worthiness. Credit scoring is an evaluation of the risk connected with lending to clients (consumers) or an organization. The Gustafson-Kessel (GK) algorithm has become one of the most valuable tools for credit scoring. However, this algorithm demonstrates a relatively poor capability to identify a subset of features from a large dataset. Most methods that use the GK algorithm require a predefined number of clusters. This paper presents a new GK-based modified binary particle swarm optimization (MBPSO) approach to increase the classification accuracy of the GK algorithm. The proposed MBPSO consists of three parts. First, the figure of particles is utilized to determine the optimal number of clusters automatically and overcome the drawback of the GK algorithm that requires a predefined number of clusters. A subset of features is identified because the same dataset may contain influencing features or a high level of noise. The two procedures are then combined in the same optimization method to increase the classification accuracy of the GK algorithm. Second, the updating function uses velocity and position to update the next position for every particle in the swarm. Third, a kernel fuzzy clustering method (KFCM) is used as the fitness function because this function can analyze high- dimensional data. These modifications are utilized as preprocessing steps before the classification of credit data is performed. Internal measures of clustering are conducted on Australian, German, and Taiwan standard datasets that contain 690, 1,000, and 30,000 instances, respectively, with several feature properties. Results show that the GK algorithm is good at separating the data into clusters. Furthermore, the fuzzy Rand validity measures of the three credit datasets derived by using the proposed method of combining the GK algorithm with a MBPSO are greater than the values of the two other compared methods. This finding means that fuzzy partitioning (classification) is robust therefore, the risk associated with loans provision can be reduced when the proposed method is used.", "journal": "NEURAL COMPUTING & APPLICATIONS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000498834900080", "keywords": "Data mining; spatial colocation; tensors; image-objects", "title": "Mining spatial colocations from image-objects: A tensor factorization approach", "abstract": "The spatial colocation problem is totally different from the traditional association rule problem, as it operates on spatial data and not on conventional transaction data. In this work, a spatial colocation mining framework is proposed that mines spatial colocation of image-objects present in images using a tensor factorization approach. The framework takes in image data directly, tensorize it and perform the mining task, thus eliminating the need of converting into a transaction based approach. An interestingness measure called, spatial dominance is also proposed in this work. This measure is an indicator of the prevalence of the mined colocation pattern. Algorithms are designed in this framework, first to map the classified pixels as members of image-objects, which is a pre-stage before mining and second to find spatial colocation patterns. Experiment results are provided to show the strength of the spatial colocation mining algorithm.", "journal": "JOURNAL OF INTELLIGENT & FUZZY SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000431950400007", "keywords": "Fuzzy time series; Forecasting; Artificial fish swarm algorithm; Levy flight; HAFSA", "title": "A novel fuzzy time series forecasting method based on the improved artificial fish swarm optimization algorithm", "abstract": "Recently, many forecasting methods have been proposed for the analysis of fuzzy time series. The main factors that affect the results of the forecasting of these models are partition universe of discourse and determination of fuzzy relations. In this paper, a novel fuzzy time series forecasting method which uses a hybrid artificial fish swarm optimization algorithm for the determination of interval lengths is proposed. Firstly, we introduce the chemotactic behavior of Bacterial foraging optimization into foraging behavior. Secondly, the Levy flight is used as the mutation operator for a mutation strategy. Finally, the new proposed method is applied to a fuzzy time series forecasting and the experimental results show that the proposed model obtain better forecasting results than those of other existing models. It proves the feasibility and validity of above-mentioned approaches.", "journal": "SOFT COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000388777700013", "keywords": "Fuzzy predicates; Interval type-2 fuzzy logic; Clustering; Knowledge-discovering; Vagueness", "title": "Discovering knowledge from data clustering using automatically-defined interval type-2 fuzzy predicates", "abstract": "In data clustering fuzzy predicates act as cluster descriptors providing linguistically expressed knowledge which indicates how features are related to each cluster. Fuzzy predicates directly and automatically obtained from data enable discovering knowledge inside clusters, even when there is no prior-information about the clustering problem. In this work a new method for automatic discovering of interval type-2 fuzzy predicates in data clustering is proposed, called Type-2 Data-based Fuzzy Predicate Clustering (T2-DFPC). In a first stage, a data analysis is performed by making a random partition of the original data and running a clustering scheme that automatically determines the suitable number of clusters. From this stage, interval type-2 fuzzy predicates are discovered. Results obtained on very different clustering datasets show that the T2-DFPC method was consistently one of the best in terms of accuracy. The method preserves all known advantages of the interval type-2 FL to deal with problems with vagueness, quantifying the degree of truth of the fuzzy predicates and modelling the variability of the data inside the clusters. The proposed method is a fast, useful, general, and unsupervised approach for interpretable data clustering, being the knowledge-extracting capabilities one of the main contributions. Linguistic expressions can be easily adapted to match the terminology used in the field the data are related to. The predicates are able to generalize the knowledge for new cases (new data), as an intelligent system. This new approach might be surprisingly useful in contexts where, besides the clustering partition, summary information from data is of interest. (C) 2016 Elsevier Ltd. All rights reserved.", "journal": "EXPERT SYSTEMS WITH APPLICATIONS", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000209204000008", "keywords": "k-nearest neighbor; KNNModel; Subspace; Classification", "title": "Optimal subspace classification method for complex data", "abstract": "KNNModel algorithm is an improved version for k-nearest neighbor method. However, it has the problem of high time complexity and lower performance when dealing with complex data. An optimal subspace classification method called IKNNModel is proposed in this paper by projecting different training samples onto their own optimal subspace and constructing the corresponding class cluster and pure cluster as the basis of classification. For datasets with complex structure, that is, the training samples from different categories are overlapped with one another on the original space or have a high dimensionality, the proposed method can construct the corresponding clusters for the overlapped samples on their own subspaces easily. Experimental results show that compared with KNNModel, the proposed method not only significantly improves the classification performance on datasets with complex structure, but also improves the efficiency of the classification.", "journal": "INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000749244600004", "keywords": "Information volume; Information quality; Deng entropy; Mass function; Deng distribution", "title": "Measure information quality of basic probability assignment: An information volume method", "abstract": "Information quality is a concept that can be used to measure the information of probability distribution. Dempster-Shafer evidence theory can describe uncertain information more reasonably than probability theory. Therefore, it is a research hot spot to propose information quality applicable to evidence theory. Recently, Deng proposed the concept of information volume based on Deng entropy. It is worth noting that, compared with the Deng entropy, the information volume of the Deng entropy contains more information. Obviously, it may be more reasonable to use the information volume of Deng entropy to represent uncertain information. Therefore, this article proposes a new information quality, which is based on the information volume of Deng entropy. In addition, when the basic probability assignment (BPA) degenerates into a probability distribution, the proposed information quality is consistent with the information quality proposed by Yager and Petry. What's more, based on the information quality of information volume, a correlation coefficient is proposed. After that, several numerical examples illustrate the effectiveness of this new method. Finally, a weight average fusion method based on information quality of information volume is proposed, and a target recognition task and a pattern recognition experiment are used to illustrate the efficiency of the method.", "journal": "APPLIED INTELLIGENCE", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000712030600001", "keywords": "Decentralized nonparametric learning; reproducing kernel Hilbert space; random features; ADMM; communication censoring", "title": "COKE: Communication-Censored Decentralized Kernel Learning", "abstract": "This paper studies the decentralized optimization and learning problem where multiple interconnected agents aim to learn an optimal decision function defined over a reproducing kernel Hilbert space by jointly minimizing a global objective function, with access to their own locally observed dataset. As a non-parametric approach, kernel learning faces a major challenge in distributed implementation: the decision variables of local objective functions are data-dependent and thus cannot be optimized under the decentralized consensus framework without any raw data exchange among agents. To circumvent this major challenge, we leverage the random feature (RF) approximation approach to enable consensus on the function modeled in the RF space by data-independent parameters across different agents. We then design an iterative algorithm, termed DKLA, for fast-convergent implementation via ADMM. Based on DKLA, we further develop a communication-censored kernel learning (COKE) algorithm that reduces the communication load of DKLA by preventing an agent from transmitting at every iteration unless its local updates are deemed informative. Theoretical results in terms of linear convergence guarantee and generalization performance analysis of DKLA and COKE are provided. Comprehensive tests on both synthetic and real datasets are conducted to verify the communication efficiency and learning effectiveness of COKE.(1)", "journal": "JOURNAL OF MACHINE LEARNING RESEARCH", "category": "Automation & Control Systems; Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000419814400006", "keywords": "big data; parallel computing; MAPREDUCE; decision trees; editing distance; tree similarit", "title": "INTERPRETABLE DECISION-TREE INDUCTION IN A BIG DATA PARALLEL FRAMEWORK", "abstract": "When running data-mining algorithms on big data platforms, a parallel, distributed framework, such asMAPREDUCE, may be used. However, in a parallel framework, each individual model fits the data allocated to its own computing node without necessarily fitting the entire dataset. In order to induce a single consistent model, ensemble algorithms such as majority voting, aggregate the local models, rather than analyzing the entire dataset directly. Our goal is to develop an efficient algorithm for choosing one representative model from multiple, locally induced decision-tree models. The proposed SySM (syntactic similarity method) algorithm computes the similarity between the models produced by parallel nodes and chooses the model which is most similar to others as the best representative of the entire dataset. In 18.75% of 48 experiments on four big datasets, SySM accuracy is significantly higher than that of the ensemble; in about 43.75% of the experiments, SySM accuracy is significantly lower; in one case, the results are identical; and in the remaining 35.41% of cases the difference is not statistically significant. Compared with ensemble methods, the representative tree models selected by the proposed methodology are more compact and interpretable, their induction consumes less memory, and, as confirmed by the empirical results, they allow faster classification of new records.", "journal": "INTERNATIONAL JOURNAL OF APPLIED MATHEMATICS AND COMPUTER SCIENCE", "category": "Automation & Control Systems; Computer Science, Artificial Intelligence; Mathematics, Applied", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000404204100009", "keywords": "Multiobjective evolutionary algorithms; Clustering; Scaling-Up; Stratification", "title": "Scaling-up multiobjective evolutionary clustering algorithms using stratification", "abstract": "Multiobjective evolutionary clustering algorithms are based on the optimization of several objective functions that guide the search following a cycle based on evolutionary algorithms. Their capabilities allow them to find better solutions than with conventional clustering algorithms when more than one criterion is necessary to obtain understandable patterns from the data. However, these kind of techniques are expensive in terms of computational time and memory usage, and specific strategies are required to ensure their successful scalability when facing large-scale data sets. This work proposes the application of a data subset approach for scaling-up multiobjective clustering algorithms and it also analyzes the impact of three stratification methods. The experiments show that the use of the proposed data subset approach improves the performance of multiobjective evolutionary clustering algorithms without considerably penalizing the accuracy of the final clustering solution. (C) 2016 Elsevier B.V. All rights reserved.", "journal": "PATTERN RECOGNITION LETTERS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000700513700001", "keywords": "Classification; Ant colony optimization; Data mining; Semi-supervised learning; Sekf-training; Pseudo labeling; Associative classification", "title": "Semi-supervised associative classification using ant colony optimization algorithm", "abstract": "Labeled data is the main ingredient for classification tasks. Labeled data is not always available and free. Semi-supervised learning solves the problem of labeling the unlabeled instances through heuristics. Self-training is one of the most widely-used comprehensible approaches for labeling data. Traditional self-training approaches tend to show low classification accuracy when the majority of the data is unlabeled. A novel approach named Self-Training using Associative Classification using Ant Colony Optimization (ST-AC-ACO) has been proposed in this article to label and classify the unlabeled data instances to improve self-training classification accuracy by exploiting the association among attribute values (terms) and between a set of terms and class labels of the labeled instances. Ant Colony Optimization (ACO) has been employed to construct associative classification rules based on labeled and pseudo-labeled instances. Experiments demonstrate the superiority of the proposed associative self-training approach to its competing traditional self-training approaches.", "journal": "PEERJ COMPUTER SCIENCE", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods", "annotated_keywords": ["supervised learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000415142000009", "keywords": "OWA functions; orness; Choquet integrals; symmetric capacities; k-additivity; binomial decomposition", "title": "The binomial decomposition of OWA functions, the 2-additive and 3-additive cases in n dimensions", "abstract": "In the context of the binomial decomposition of ordered weighted averaging (OWA) functions, we investigate the constraints associated with the 2-additive and 3-additive cases in n dimensions. The 2-additive case depends on one coefficient whose feasible region does not depend on the dimension n. On the other hand, the feasible region of the 3-additive case depends on two coefficients and is explicitly dependent on the dimension n. This feasible region is a convex polygon with n vertices and n edges, which is strictly expanding in the dimension n. The orness of the OWA functions within the feasible region is linear in the two coefficients, and the vertices associated with maximum and minimum orness are identified. Finally, we discuss the 3-additive binomial decomposition in the asymptotic infinite dimensional limit.", "journal": "INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000865423600010", "keywords": "Self-organizing maps; Graphical optimization; Decision processes; Machine learning; Unsupervised learning", "title": "A novel data-driven visualization of n-dimensional feasible region using interpretable self-organizing maps (iSOM)", "abstract": "Graphical optimization allows solving one or two dimensional optimization problems visually by merely plotting the objective function and constraint function contours. In addition to the discovery of optima, such a visualization-based approach enables understanding and interpretation of design variable and objective behavior with respect to feasibility and optimality, permitting intuitive decision making for designers. However, visualization of optimization problems in higher dimensions is challenging, though it is desirable. Interpretable self-organizing map (iSOM) is an artificial neural network that enables visualization of many dimensions via two-dimensional representations. We introduce iSOM to solve multidimensional optimization problems graphically. In the current work, a novel graphical representation of the n-dimensional feasible region, called B-matrix is constructed using iSOM. B-matrix is used to represent feasible range of design variables and objective function on separate plots. Consequently, dimension-wise shrinkage in the search space is also obtained. The proposed approach is demonstrated on various benchmark analytical examples and engineering examples with dimensions ranging from 2 to 30. (c) 2022 Elsevier Ltd. All rights reserved.", "journal": "NEURAL NETWORKS", "category": "Computer Science, Artificial Intelligence; Neurosciences", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000837794400001", "keywords": "automated machine learning; AutoML; hyperparameter optimization; pipeline; time series forecasting", "title": "Review of automated time series forecasting pipelines", "abstract": "Time series forecasting is fundamental for various use cases in different domains such as energy systems and economics. Creating a forecasting model for a specific use case requires an iterative and complex design process. The typical design process includes five sections (1) data preprocessing, (2) feature engineering, (3) hyperparameter optimization, (4) forecasting method selection, and (5) forecast ensembling, which are commonly organized in a pipeline structure. One promising approach to handle the ever-growing demand for time series forecasts is automating this design process. The article, thus, reviews existing literature on automated time series forecasting pipelines and analyzes how the design process of forecasting models is currently automated. Thereby, we consider both automated machine learning (AutoML) and automated statistical forecasting methods in a single forecasting pipeline. For this purpose, we first present and compare the identified automation methods for each pipeline section. Second, we analyze these automation methods regarding their interaction, combination, and coverage of the five pipeline sections. For both, we discuss the reviewed literature that contributes toward automating the design process, identify problems, give recommendations, and suggest future research. This review reveals that the majority of the reviewed literature only covers two or three of the five pipeline sections. We conclude that future research has to holistically consider the automation of the forecasting pipeline to enable the large-scale application of time series forecasting. This article is categorized under: Technologies > Machine Learning Technologies > Prediction Algorithmic Development > Spatial and Temporal Data Mining", "journal": "WILEY INTERDISCIPLINARY REVIEWS-DATA MINING AND KNOWLEDGE DISCOVERY", "category": "Computer Science, Artificial Intelligence; Computer Science, Theory & Methods", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000861108300022", "keywords": "PFS; q-ROFS; Soft Sets; q-ROFStS; Dombi Operators; q-ROFSt DWA; q-ROFSt DOWA; q-ROFSt DHA; q-ROFSt DWG; q-ROFSt DOWG and q-ROFSt DHG Operator; MCDM", "title": "q-Rung orthopair fuzzy soft aggregation operators based on Dombi t-norm and t-conorm with their applications in decision making", "abstract": "Recently, some improvement has been made in the dominant notion of fuzzy set that is Yager investigated the generalized concept of fuzzy set, Intuitionistic fuzzy set (IFS) and Pythagorean fuzzy set (PFS) and called it q-rung orthopair fuzzy (q-ROF) set (q-ROFS). The aim of this manuscript is to present the concept of q-ROF soft (q-ROFSt) set (q-ROFSt S) based on the Dombi operations. Since Dombi operational parameter possess natural flexibility with the resilience of variability. Some new operational laws are defined based on hybrid study of soft sets and q-ROFS. The advantage of Dombi operational parameter is very important to express the experts' attitude in decision making. In this paper, we present q-ROFSt Dombi average (q-ROFSt DA) aggregation operators including q-ROFSt Dombi weighted average (q-ROFSt DWA), q-ROFSt Dombi ordered weighted average (q-ROFSt DOWA) and q-ROFSt Dombi hybrid average (q-ROFSt DHA) operators. Moreover, we investigate q-ROFSt Dombi geometric (q-ROFSt DG) aggregation operators including q-ROFSt Dombi weighted geometric (q-ROFSt DWG), q-ROFSt Dombi ordered weighted geometric (q-ROFSt DOWG), and q-ROFSt Dombi hybrid geometric (qROFS(t) DHG) operators. The basic properties of these operators are presented with detail such us Idempotency, Boundedness, Monotonicity, Shift invariance, and Homogeneity. Thus from the analysis and advantages of proposed model, it is clear that the investigated q-ROFSt DWA operator is the generalized form of IF St DWA, PFSt DWA and q-ROFDWA operators. Similarly, the investigated q-ROFSt DWG operator is the generalized form of IF S-t DWG, PFSt DWG and q-ROFDWG operators. By applying the develop approach, this manuscript contains the technique and algorithm for multicriteria decision making (MCDM). Further a numerical example is developed to illustrate the flexibility and applicability of the developed operators.", "journal": "JOURNAL OF INTELLIGENT & FUZZY SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000329926600031", "keywords": "Interval-valued fuzzy set; interval-valued fuzzy information system; incomplete information system; rough fuzzy set; fuzzy rough set", "title": "Rough set theory for the incomplete interval valued fuzzy information systems", "abstract": "Considering the incompleteness and uncertainty of information systems, this paper combines the incomplete information systems with interval-valued fuzzy sets, and defines the incomplete interval-valued fuzzy information systems. The basic rough set theory for incomplete interval-valued fuzzy information systems is also discussed in this paper.", "journal": "JOURNAL OF INTELLIGENT & FUZZY SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000461208300007", "keywords": "aggregation operator; distance measure; fermatean fuzzy linguistic term set (FFLTS); fermatean fuzzy set (FFS); TOPSIS method", "title": "Fermatean fuzzy linguistic set and its application in multicriteria decision making", "abstract": "In this paper, we propose the concept of Fermatean fuzzy linguistic term sets based on linguistic term sets and Fermatean fuzzy sets. The basic operational laws, the score function, and the accuracy function of Fermatean fuzzy linguistic numbers are provided. Then we propose the Fermatean fuzzy linguistic weighted aggregation operator, the Fermatean fuzzy linguistic weighted geometric operator, and the Fermatean fuzzy linguistic distance measures. Furthermore, we extend the TOPSIS method to the proposed distance measures, and the ranking of alternatives is obtained under a Fermatean fuzzy linguistic environment. An example is provided to illustrate the feasibility and effectiveness of the proposed method, and a comparison with the existing method is also analyzed. Finally, the sensitivity analysis of the parameter lambda in the proposed distance measure is also discussed.", "journal": "INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000327110400022", "keywords": "MEMS; Real-time; Active control; Turbulent flow; CFD; Neural network", "title": "Active MEMS-based flow control using artificial neural network", "abstract": "These last years several research works have studied the application of Micro-Electro-Mechanical Systems (MEMS) for aerodynamic active flow control. Controlling such MEMS-based systems remains a challenge. Among the several existing control approaches for time varying systems, many of them use a process model representing the dynamic behavior of the process to be controlled. The purpose of this paper is to study the suitability of an artificial neural network first to predict the flow evolution induced by MEMS, and next to optimize the flow w.r.t a numerical criterion. To achieve this objective, we focus on a dynamic flow over a backward facing step where MEMS actuators velocities are adjusted to maximize the pressure over the step surface. The first effort has been to establish a baseline database provided by computational fluid dynamics simulations for training the neural network. Then we investigate the possibility to control the flow through MEMS configuration changes. Results are promising, despite slightly high computational times for real time application. (C) 2013 Elsevier Ltd. All rights reserved.", "journal": "MECHATRONICS", "category": "Automation & Control Systems; Engineering, Electrical & Electronic; Engineering, Mechanical; Robotics", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000526527800015", "keywords": "Neural networks; Prediction algorithms; Forecasting; Time series analysis; Mathematical model; Learning systems; Real-time systems; Metacognitive network; octonion-valued neural networks (OVNNs); prediction; renewable energy", "title": "Metacognitive Octonion-Valued Neural Networks as They Relate to Time Series Analysis", "abstract": "In this paper, a metacognitive octonion-valued neural network (Mc-OVNN) learning algorithm and its application to diverse time series prediction are presented. The Mc-OVNN is comprised of two components: the octonion-valued neural network that represents the cognitive component and the metacognitive component that serves to self-regulate the learning algorithm. At each epoch, the metacognitive component decides if, how, and when learning occurs. The algorithm deletes unneeded samples and only stores those that will be used. This decision is determined by the octonion magnitude and the seven phases. To evaluate the Mc-OVNN algorithm's performance, it is applied to five real-world forecasting problems: the power consumption of a home in Honolulu, HI, USA, Box and Jenkins J series, Euro to Algerian Dinar (DZ) real-time conversion rates, the Mackey-Glass equation, and Europe Brent oil price prediction in a time series. When comparing the Mc-OVNN to other relevant techniques, Mc-OVNN displays its capability for efficient time series prediction. The real-time evaluation of the proposed algorithm is presented using the power consumption of a home in Boumerdes, Algeria, as a case study.", "journal": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Hardware & Architecture; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": ["neural net", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000353746800015", "keywords": "Group decision making; Fuzzy linguistic preference relations; Cooperative games method; Weighting algorithm; Optimization model", "title": "Group decision making with fuzzy linguistic preference relations via cooperative games method", "abstract": "When we consider the weighting approach for group decision making with fuzzy linguistic preference relations, the groupment of experts has merely been studied. In this paper, a novel weighting approach on the basis of cooperative games method is developed. The group decision error matrix is built to reflect the deviations of all experts with given initial weighting vector. An iterative algorithm is designed to lower the sum of the decision error so that a final convergence result can be obtained. The advantage of the weighting algorithm is that it can consider the contribution of each expert and reduce the sum of decision error with increasing iteration numbers. Then an optimization model using triangular fuzzy numbers as alternatives' weights is constructed, whose results are used to rank the alternatives. Finally, a numerical example of subjective evaluation of vehicle sound quality is considered to illustrate the feasibility and validity of the proposed weighting approach in the group decision making problem. (c) 2015 Elsevier Ltd. All rights reserved.", "journal": "COMPUTERS & INDUSTRIAL ENGINEERING", "category": "Computer Science, Interdisciplinary Applications; Engineering, Industrial", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000422740900004", "keywords": "fuzzy inference system; fuzzy VIKOR; risk assessment; risk management; wastewater reuse", "title": "Development of an algorithm for risk-based management of wastewater reuse alternatives", "abstract": "Due to water resources limitations, special attention has been paid to wastewater reuse in recent years. The risks associated with wastewater reuse alternatives should be considered in decision-making. Even when selecting the alternative with the least risk, risk management issues are of high importance. This study aims to develop an algorithm for risk-based management of wastewater reuse alternatives. This algorithm uses a three-step risk assessment and management approach. Risks are identified, then risks of alternatives are assessed, and, finally, risk management measures are proposed for risk reduction in the selected alternative. In risk identification, economic, social, health, and environmental aspects are taken into account. In risk assessment, its three components of likelihood, severity, and vulnerability are considered through a fuzzy inference system. Alternatives are prioritized based on calculated risks using a fuzzy VIKOR method. A case study is presented in which the proposed algorithm is used to select the best alternative for reuse of treated wastewater from Ekbatan Town, located in the western part Tehran in Iran. The results showed that the proposed approach provides the users with an easier understanding of risks and increases the relative confidence of decision-makers about the selection of the best alternatives for wastewater reuse and their risk control methods.", "journal": "JOURNAL OF WATER REUSE AND DESALINATION", "category": "Engineering, Environmental; Water Resources", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000366272000007", "keywords": "Bayesian Network; Locally Weighted Learning; Ranking; Classification", "title": "Locally Weighted Learning: How and When Does it Work in Bayesian Networks?", "abstract": "Bayesian network (BN), a simple graphical notation for conditional independence assertions, is promised to represent the probabilistic relationships between diseases and symptoms. Learning the structure of a Bayesian network classifier (BNC) encodes conditional independence assumption between attributes, which may deteriorate the classification performance. One major approach to mitigate the BNC's primary weakness (the attributes independence assumption) is the locally weighted approach. And this type of approach has been proved to achieve good performance for naive Bayes, a BNC with simple structure. However, we do not know whether or how effective it works for improving the performance of the complex BNC. In this paper, we first do a survey on the complex structure models for BNCs and their improvements, then carry out a systematically experimental analysis to investigate the effectiveness of locally weighted method for complex BNCs, e.g., tree-augmented naive Bayes (TAN), averaged one-dependence estimators AODE and hidden naive Bayes (HNB), measured by classification accuracy (ACC) and the area under the ROC curve ranking (AUC). Experiments and comparisons on 36 benchmark data sets collected from University of California, Irvine (UCI) in Weka system demonstrate that locally weighting technologies just slightly outperforms unweighted complex BNCs on ACC and AUC. In other words, although locally weighting could significantly improve the performance of NB (a BNC with simple structure), it could not work well on BNCs with complex structures. This is because the performance improvements of BNCs are attributed to their structures not the locally weighting.", "journal": "INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000464301300012", "keywords": "Mutual neighbors; Single link; Density; Distance; Clustering", "title": "Munec: a mutual neighbor-based clustering algorithm", "abstract": "It is expected for new clustering algorithms to find the appropriate number of clusters when dealing with complex data, meaning various shapes and densities. They also have to be self-tuning and adaptive for the input parameters to differentiate only between acceptable solutions. This work addresses this challenge. At the beginning mutual nearest neighbors are merged without any constraint until the number of groups including at least two items reaches a maximum. Subsequent mergings are only possible for mutual neighbor groups with a similar distance between neighbors. Finally, to manage more nuanced situations, heuristics that combine local density and distance are defined. The whole strategy aims to progressively consolidate the data representation structures. Munec requires some parameters. Most of them were integrated as constants and a single user parameter controls the process: the higher its value, the more constraints there are on the merging and the higher the number of clusters. Tests carried out using 2-dimensional datasets showed that Munec proved to be highly effective in matching a ground truth target. Moreover, with the same input configuration it can identify clusters of various densities, arbitrary shape and including a large amount of noise. These results hold for spaces of moderate dimension. (C) 2019 Elsevier Inc. All rights reserved.", "journal": "INFORMATION SCIENCES", "category": "Computer Science, Information Systems", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000382776800007", "keywords": "Interval-valued hesitant fuzzy set; multiple criteria group decision making; power geometric operator; variable power geometric operator; interval-valued hesitant fuzzy power geometric operator", "title": "On Extending Power-Geometric Operators to Interval-Valued Hesitant Fuzzy Sets and Their Applications to Group Decision Making", "abstract": "Developing aggregation operators for interval-valued hesitant fuzzy sets (IVHFSs) is a technological task we are faced with, because they are specifically important in many problems related to the fusion of interval-valued hesitant fuzzy information. This paper develops several novel kinds of power geometric operators, which are referred to as variable power geometric operators, and extends them to interval-valued hesitant fuzzy environments. A series of generalized interval-valued hesitant fuzzy power geometric (GIVHFG) operators are also proposed to aggregate the IVHFSs to model mandatory requirements. One of the important characteristics of these operators is that objective weights of input arguments are variable with the change of a non-negative parameter. By adjusting the exact value of the parameter, the influence caused by some \"false\" or \"biased\" arguments can be reduced. We demonstrate some desirable and useful properties of the proposed aggregation operators and utilize them to develop techniques for multiple criteria group decision making with IVHFSs considering the heterogeneous opinions among individual decision makers. Furthermore, we propose an entropy weights-based fitting approach for objectively obtaining the appropriate value of the parameter. Numerical examples are provided to illustrate the effectiveness of the proposed techniques.", "journal": "INTERNATIONAL JOURNAL OF INFORMATION TECHNOLOGY & DECISION MAKING", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Interdisciplinary Applications; Operations Research & Management Science", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000350273000003", "keywords": "biometrics; network security; fuzzy analytic hierarchy process; best non-fuzzy performance", "title": "Evaluating and selecting the biometrics in network security", "abstract": "Since Apple merged with AuthenTec, a leading fingerprint recognition company, in 2012, biometrics has widely been considered to strengthen security and privacy in the network security field. Although biometrics has been applied in specific areas for decades, it has gradually proliferated in customer and mobile electronic products to enhance security and privacy. This study aims to evaluate biometrics through conventional technology assessment considerations combined with viewpoints on the specifics of biometric technologies and then to provide suggestions for selection. To conduct the biometric technology assessment, the fuzzy analytic hierarchy process and non-fuzzy best performance approaches are used. Although the outcomes first indicate that technology assessment should be the key object in selecting biometric technologies, that object is followed by biometric competence and key elements of biometrics. The outcomes also indicate that features of the target technologies should be considered when evaluating them. Additionally, fingerprint recognition, iris recognition, and face recognition are the preferred biometrics in evaluation and selection. Copyright (c) 2014 John Wiley & Sons, Ltd.", "journal": "SECURITY AND COMMUNICATION NETWORKS", "category": "Computer Science, Information Systems; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000367291500017", "keywords": "crowdfunding; peer-to-peer lending; social finance; disintermediation; marketplace finance; screening", "title": "Peer-to-Peer Crowdfunding: Information and the Potential for Disruption in Consumer Lending", "abstract": "TCan peer-to-peer lending (P2P) disintermediate and mitigate information frictions in lending so that choices and outcomes for at least some borrowers and investors are improved? I offer a framing of issues and survey the nascent literature on P2P. On the investor side, P2P disintermediates an asset class of consumer loans, and investors may be able to capture rents associated with the removal of a layer of financial intermediation. Risk and portfolio choice questions linger prior to any inference. On the borrower side, evidence suggests that proximate knowledge (direct or inferred) unearths soft information. Thus, P2P may be able to offer pricing and/or access benefits to potential borrowers. Early research suggests that the future of consumer lending will involve more big data and reintermediation of underwriting by all types of financial institutions. I ask many more questions than current research can answer, hoping to motivate future research.", "journal": "ANNUAL REVIEW OF FINANCIAL ECONOMICS, VOL 7", "category": "Business, Finance; Economics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000383769500004", "keywords": "Co-location mining; Uncertain data mining; Association rule and frequent pattern mining; Air pollutant and environmental health", "title": "On discovering co-location patterns in datasets: a case study of pollutants and child cancers", "abstract": "We intend to identify relationships between cancer cases and pollutant emissions by proposing a novel co-location mining algorithm. In this context, we specifically attempt to understand whether there is a relationship between the location of a child diagnosed with cancer with any chemical combinations emitted from various facilities in that particular location. Co-location pattern mining intends to detect sets of spatial features frequently located in close proximity to each other. Most of the previous works in this domain are based on transaction-free apriori-like algorithms which are dependent on user-defined thresholds, and are designed for boolean data points. Due to the absence of a clear notion of transactions, it is nontrivial to use association rule mining techniques to tackle the co-location mining problem. Our proposed approach is focused on a grid based transactionization? of the geographic space, and is designed to mine datasets with extended spatial objects. It is also capable of incorporating uncertainty of the existence of features to model real world scenarios more accurately. We eliminate the necessity of using a global threshold by introducing a statistical test to validate the significance of candidate co-location patterns and rules. Experiments on both synthetic and real datasets reveal that our algorithm can detect a considerable amount of statistically significant co-location patterns. In addition, we explain the data modelling framework which is used on real datasets of pollutants (PRTR/NPRI) and childhood cancer cases.", "journal": "GEOINFORMATICA", "category": "Computer Science, Information Systems; Geography, Physical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000439697800015", "keywords": "Criteria weights determination; hybrid evaluation information; knowledge granularity; multi-criteria decision making; TOPSIS", "title": "Granularity Approach for Multi-Criteria Decision Making About Hybrid Evaluation Information", "abstract": "In this study, a new version of TOPSIS method is reconstructed to deal with the problem of multi-criteria decision making. Here, the data representation of all alternatives is varied according to different criteria, such as real number, interval-valued number, set-valued number and intuitionistic fuzzy-valued number, etc. Because the distinguishing ability of each criterion can be reflected by its knowledge granularity, naturally, a knowledge granularity method is constructed to measure the criteria weights. Besides, the approach of how to select the ideal solution is redefined, especially for the case that the content of criterion according to all alternatives is not a totally ordered set anymore. What is more, the decision maker's personal preference is considered, and the concrete indicator value can be calculated by the convex combination of the distance from possible alternatives to ideal solutions. Finally, the validity of the proposed decision-making algorithm is illustrated by a synthetic example.", "journal": "INTERNATIONAL JOURNAL OF PATTERN RECOGNITION AND ARTIFICIAL INTELLIGENCE", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000461896100004", "keywords": "fuzzy logic; FMEA; Taguchi method; groupthink; fuzzy inference system", "title": "Fuzzy logic-based FMEA robust design: a quantitative approach for robustness against groupthink in group/team decision-making", "abstract": "Group/team decision-making is an integral part of almost all failure mode and effects analysis (FMEA) projects. A dysfunctional aspect of this decision-making fashion in fuzzy FMEA is that group/team members' designs for membership functions and IF-THEN rules may be overshadowed by a member's design. This problem is caused by groupthink, a pitfall known by the Organisational Behaviour science. This study aims to develop a fuzzy FMEA approach which is robust to the problem. We applied the Taguchi's robust parameter design and investigated the effects of various control parameters namely Defuzzification, Aggregation, And and Implication operators for the fuzzy inference system (FIS). Our experiments illustrate that the control parameters, in the above-mentioned order, have the most effect on the signal-to-noise ratio (SNR). These factors' optimal setting consists of the Centroid, Sum, Minimum and Minimum levels, respectively.", "journal": "INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH", "category": "Engineering, Industrial; Engineering, Manufacturing; Operations Research & Management Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000399632500019", "keywords": "Clustering; Natural neighbor; Local representatives", "title": "Natural neighbor-based clustering algorithm with local representatives", "abstract": "Clustering by identifying cluster centers is important for detecting patterns in a data set. However, many center-based clustering algorithms cannot process data sets containing non-spherical clusters. In this paper, we propose a novel clustering algorithm called NaNLORE based on natural neighbor and local representatives. Natural neighbor is a new neighbor concept and introduced to compute local density and find local representatives which are points with local maximum density. We first find local representatives and then select cluster centers from the local representatives. The density-adaptive distance is introduced to measure the distance between local representatives, which helps to solve the problem of clustering data sets with complex manifold structure. Cluster centers are characterized by higher density than their neighbors and a relatively large density-adaptive distance from any local representatives with higher density. In experiments, we compare the proposed algorithm NaNLORE with existing algorithms on synthetic and real data sets. Results show that NaNLORE performs better than existing algorithm, especially on clustering non-spherical data and manifold data. (C) 2017 Elsevier B.V. All rights reserved.", "journal": "KNOWLEDGE-BASED SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000329229100008", "keywords": "Data mining algorithms; Pattern mining; Statistical significance testing", "title": "A statistical significance testing approach to mining the most informative set of patterns", "abstract": "Hypothesis testing using constrained null models can be used to compute the significance of data mining results given what is already known about the data. We study the novel problem of finding the smallest set of patterns that explains most about the data in terms of a global p value. The resulting set of patterns, such as frequent patterns or clusterings, is the smallest set that statistically explains the data. We show that the newly formulated problem is, in its general form, NP-hard and there exists no efficient algorithm with finite approximation ratio. However, we show that in a special case a solution can be computed efficiently with a provable approximation ratio. We find that a greedy algorithm gives good results on real data and that, using our approach, we can formulate and solve many known data-mining tasks. We demonstrate our method on several data mining tasks. We conclude that our framework is able to identify in various settings a small set of patterns that statistically explains the data and to formulate data mining problems in the terms of statistical significance.", "journal": "DATA MINING AND KNOWLEDGE DISCOVERY", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000343519500001", "keywords": "visual compliance; high-speed visual feedback; dynamic compensation; fast peg-and-hole alignment", "title": "Three-Step Epipolar-Based Visual Servoing for Nonholonomic Robot with FOV Constraint", "abstract": "Image-based visual servoing for nonholonomic mobile robots using epipolar geometry is an efficient technology for visual servoing problem. An improved visual servoing strategy, namely, three-step epipolar-based visual servoing, is developed for the nonholonomic robot in this paper. The proposed strategy can keep the robot meeting FOV constraint without any 3D reconstruction. Moreover, the trajectory planned by this strategy is shorter than the existing strategies. The mobile robot can reach the desired configuration with exponential converge. The control scheme in this paper is divided into three steps. Firstly, by using the difference of epipoles as feedback, the robot rotates to make the current configuration and desired configuration in the same orientation. Then, by using a linear input-output feedback, the epipoles are zeroed so as to align the robot with the goal. Finally, by using the difference of feature points, the robot reaches the desired configuration. Simulation results and experimental results are given to illustrate the effectiveness of the proposed control scheme.", "journal": "JOURNAL OF APPLIED MATHEMATICS", "category": "Mathematics, Applied; Mathematics, Interdisciplinary Applications", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000644525100001", "keywords": "fuzzy logic; fuzzy arithmetic; extension principle; economic models; national income", "title": "A Fuzzy Economic Dynamic Model", "abstract": "In the study presented here, fuzzy logic was used to analyze the behavior of a model of economic dynamics that assumes income to be in equilibrium when it is composed of consumption and investment, that is, when savings and investment are equal. The study considered that consumption and savings depend on the income of the previous period through uncertain factors, and, at the same time, that investment is an uncertain magnitude across various periods, represented as a fuzzy number with a known membership function. Under these conditions, the model determines the factor of income growth and investments required to maintain equilibrium, as well as the uncertain values of income for the different periods, expressed through fuzzy numbers. The study also analyzes the conditions for their convergence and the fuzzy value that income represents in equilibrium.", "journal": "MATHEMATICS", "category": "Mathematics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000832835500006", "keywords": "Relation extraction; Polysemy rethinking mechanism; character-based method; Chinese relation extraction", "title": "A novel chinese relation extraction method using polysemy rethinking mechanism", "abstract": "The methods of Chinese relation extraction(CRE) based on the neural network can be divided into two categories according to the input mode(word-based and character-based). The performance of word-based models depends on the accuracy of word segmentation. Unfortunately, there are still errors in existing word segmentation tools (methods). Among the character-based models, Lattice LSTM-based models have been successful in CRE. However, such RNN-based models cannot meet the requirements of parallel computing and thus have natural drawbacks in model training and inference. There is much word polysemy in Chinese that constrains the performance of CRE. Most CRE models are built on English datasets, which often perform poorly on Chinese datasets. To address the above issues, we propose a method for CRE with the Polysemy Rethinking Mechanism. In this method, (1) we use a CNN-based architecture in which input is characters. It can incorporate word-level information through the lexicon to correct the error caused by word segmentation. (2) We propose a Polysemy Rethinking Mechanism, which can alleviate the problems caused by multiple meanings of one word by adding multiple sense information to the model. (3) Compared with the Lattice LSTM-based model, our model improves computational efficiency to gain results. We conduct experiments on two real-world datasets of CRE. The results show that our method achieves better performance than the state-of-the-art ones.", "journal": "APPLIED INTELLIGENCE", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000369379100002", "keywords": "Time series; classification; distance measures", "title": "Query-sensitive distance measure selection for time series nearest neighbor classification", "abstract": "Many distance or similarity measures have been proposed for time series similarity search. However, none of these measures is guaranteed to be optimal when used for 1-Nearest Neighbor (NN) classification. In this paper we study the problem of selecting the most appropriate distance measure, given a pool of time series distance measures and a query, so as to perform NN classification of the query. We propose a framework for solving this problem, by identifying, given the query, the distance measure most likely to produce the correct classification result for that query. From this proposed framework, we derive three specific methods, that differ from each other in the way they estimate the probability that a distance measure correctly classifies a query object. In our experiments, our pool of measures consists of Dynamic TimeWarping (DTW), Move-Split-Merge (MSM), and Edit distance with Real Penalty (ERP). Based on experimental evaluation with 45 datasets, the best-performing of the three proposed methods provides the best results in terms of classification error rate, compared to the competitors, which include using the Cross Validation method for selecting the distance measure in each dataset, as well as using a single specific distance measure (DTW, MSM, or ERP) across all datasets.", "journal": "INTELLIGENT DATA ANALYSIS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000335401900009", "keywords": "sliding puzzle; cooperative path-finding; BIBOX-theta; makespan optimization; motion coordination; NxN-puzzle; multirobot path-planning; BIBOX; (N2- 1)-puzzle; 15-puzzle; domain dependent planning", "title": "SOLVING ABSTRACT COOPERATIVE PATH-FINDING IN DENSELY POPULATED ENVIRONMENTS", "abstract": "The problem of cooperative path-finding is addressed in this work. A set of agents moving in a certain environment is given. Each agent needs to reach a given goal location. The task is to find spatial temporal paths for agents such that they eventually reach their goals by following these paths without colliding with each other. An abstraction where the environment is modeled as an undirected graph is adopted-vertices represent locations and edges represent passable regions. Agents are modeled as elements placed in the vertices while at most one agent can be located in a vertex at a time. At least one vertex remains unoccupied to allow agents to move. An agent can move into unoccupied neighboring vertex or into a vertex being currently vacated if a certain additional condition is satisfied. Two novel scalable algorithms for solving cooperative path-finding in bi-connected graphs are presented. Both algorithms target environments that are densely populated by agents. A theoretical and experimental evaluation shows that the suggested algorithms represent a viable alternative to search based techniques as well as to techniques employing permutation groups on the studied class of the problem.", "journal": "COMPUTATIONAL INTELLIGENCE", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000345817600011", "keywords": "Product development; Challenges; Knowledge management; Empirical evidence; Classification", "title": "Industrial challenges in managing product development knowledge", "abstract": "To systematically create and share product development knowledge creates challenges for engineering companies. This paper presents an extensive study regarding the process of identifying such challenges in managing product development knowledge from the perspective of designers and engineers. This research is part of the LeanPPD, a project funded by the EU-PF7 (www.leanppd.eu), to address the need of European manufacturing companies for a new model, which extends beyond lean manufacturing and incorporates lean thinking into the product design and development process. A rigorous research methodology has been employed, which included the use of questionnaires and focused interviews with key informants from industry, involving forty-two product development engineers from nine different companies. The most significant concerns raised during the study concerned knowledge life cycle activities, product development environment and management. Thirty-eight challenges were identified, classified and discussed in order to provide the knowledge management community with practical evidence, and also to inform future research directions in managing product development knowledge. (C) 2014 Elsevier B.V. All rights reserved.", "journal": "KNOWLEDGE-BASED SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000351140600018", "keywords": "Laser alloying; hybrid adaptive fuzzy and neural network control; on-line learning", "title": "Design of a hybrid adaptive fuzzy and neural network controller for nonlinear laser alloying process", "abstract": "The laser surface alloying quality may vary significantly with respect to the process parameters variation. Hence, a feedback control system is required to monitor the operating parameters for yielding a good quality control. Since this multi-input and multi-output system has nonlinear coupling and time varying dynamic characteristics, it is very difficult to establish an accurate process model for designing model-based controller. Hence a hybrid model-free adaptive fuzzy and neural network controller (HAFNC), which combines an adaptive rule with fuzzy and neural network control, is employed in this study to overcome the difficulty. It has on-line learning ability for responding to the system's nonlinear and time-varying behaviors. Since this model-free controller has simple control structure and small number of control parameters, it is easy to implement. Two HAFNC controllers are designed for tuning the laser power and the traverse velocity simultaneously to tackle the absorptivity and geometrical variations of the work pieces. Simulation results based on semi-experimental system dynamic model and parameters show that the good surface lapping performance is achieved by using this intelligent control strategy.", "journal": "JOURNAL OF INTELLIGENT & FUZZY SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000372173700010", "keywords": "Digital signal processors; fuzzy systems; intelligent control; mamdani inference; programmable control; reconfigurable architectures; Sys/BIOS; web and internet services", "title": "Design of real-time reconfigurable fuzzy logic controller with M-FRHC rule reduction technique", "abstract": "This paper presents a generic hardware architecture based type-I Mamdani fuzzy logic controller (FLC) implemented on a programmable device, which can be remotely configured in real-time over Ethernet. This feature of reconfigurability enables a user to change fuzzy parameters in real-time, eliminating repeated hardware programming. Realization of these systems is generally difficult as the computational requirement is exponentially related to the number of inputs. This is achieved by significantly reducing the Rulebase to make inference time perceivable for real-time applications. An algorithm for Rulebase reduction is proposed and implemented which reduces effective rules without compromising system accuracy leading to improved cycle time in terms of fuzzy logic operations per second (FzLOPS). A hardware software co-design architecture for the proposed generic FLC is developed on TI C6748 DSP with Sys/BIOS RTOS and seamlessly integrated with a web based user interface (WebUI) for reconfigurability. The WebUI acquires the fuzzy parameters from users and a server application is dedicated to data communication between the hardware and the server. Analysis of this design is carried out by using hardware-in-loop (HIL) test to control various plant models in Simulink/Matlab environment. Performance of the proposed system is compared to Fuzzy Toolbox of Matlab and PID controllers.", "journal": "JOURNAL OF INTELLIGENT & FUZZY SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000379266300020", "keywords": "Face recognition; Super-resolution; Sparse domain; Dictionary; Edginess", "title": "Explicit and implicit employment of edge-related information in super-resolving distant faces for recognition", "abstract": "Edges and related features play significant role in discriminating face images. But those features are not enough informative when the face images are captured from a distance (e.g., video surveillance). Traditionally, those features are enhanced by super-resolving low-resolution grayscale face images. In this paper, we demonstrate a superior performance by directly considering such features (continuous gradient value, also known as edginess) in the super-resolution process. Edginess features are extracted using 1-D processing of image. This process is carried out along different directions to obtain partial evidences, which are combined to detect the person's identity. Here, super-resolution of the face image and its recognition has been performed in sparse domain framework. The explicit usage of edginess feature in the proposed approach shows considerable improvement in both recognition performance as well as computational time, as only the patches related to strong edges are considered for super-resolution. In addition to that, the edginess feature gives improved recognition rate when it is preserved implicitly during super-resolution in grayscale domain.", "journal": "PATTERN ANALYSIS AND APPLICATIONS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000404286400026", "keywords": "Smoke detection; dynamic region; choquet fuzzy integral; dynamic texture; centroid; gray level co-occurrence matrix", "title": "Video smoke detection using shape, color and dynamic features", "abstract": "Video smoke detection benefits life safety and environment protection. A method of video smoke detection using shape, color and dynamic texture features is presented in this paper. Firstly, an algorithm identifying cone geometry feature is used to distinguish conical region from dynamic regions. Secondly, conical regions are filtered by a color filtering algorithm to further test the candidate smoke region. Finally, a texture filtering algorithm is used to differentiate true smoke from candidate smoke regions. Experiments show that the proposed method is effective and it results in earlier and more reliable detection than the other two methods reported in the literature. The processing rate of the smoke detection method achieves 25 frames per second with an image size of 320 x 240 pixels.", "journal": "JOURNAL OF INTELLIGENT & FUZZY SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000461770000014", "keywords": "Four-degree of freedom robot arm; two-dimensional vision sensing method; forward and inverse kinematics; dynamic proportional-fuzzy controller; two-dimension limitation using a single CCD", "title": "Dynamic proportional-fuzzy grip control for robot arm by two-dimensional vision sensing method", "abstract": "In this study, a four-degree of freedom (4-DOF) robot arm uses an innovative two-dimensional vision sensing method to grip a moving target on a moving platform. This study utilizes forward and inverse kinematics to establish a dynamic model of the 4-DOF robot arm. A computer as a controller and a single charge-coupled device (CCD) calculates the two-dimensional vision sensing method and sends commands to an Arduino Uno microcontroller to drive the robot arm. According to simulation results of transient and steady states in MATLAB SimMechanics, the response of the dynamic proportional-fuzzy controller is better than the response of proportional-integral-derivative controller. To demonstrate a precise control of the point-contact grip, this study utilizes a ping pong ball as a target on a moving platform. Using the dynamic proportional-fuzzy controller based on the two-dimensional vision sensing method, the 4-DOF robot arm can position, grip, and carry a moving ping pong ball to a designated place in three-dimensional space, which breaks through the previous two-dimensional limitation using a single CCD. This breakthrough can reduce the weight and cost of the robot arm. Therefore, this study aims to utilize the technology to grip moving targets on a moving platform for manpower cost reduction in the industry or agriculture domain in the future.", "journal": "JOURNAL OF INTELLIGENT & FUZZY SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000440330300026", "keywords": "Variable structure control; hypersonic aircraft; neural network; adaptive; stability", "title": "Attack angle tracking for high speed vehicle based on variable structure and Taylor type FLNN neural network", "abstract": "Considering the fast time varying characteristic of pitch channel model of hypersonic model with coupling of engine, a kind of hybrid attack angle tracking controller is designed based on variable structure and Taylor type FLNN neural network method. And the adoption of variable structure control method make the system response very fast and robust. The Taylor type FLNN neural network is constructed with the format of fitting function which is used to describe the force and moment caused by attack angle, so it can compensate the interruption caused by the uncertainty and unconsidered factors effectively. At last, detailed numerical simulation is done to testify the rightness of the proposed hybrid method.", "journal": "JOURNAL OF INTELLIGENT & FUZZY SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000351310600006", "keywords": "Neural networks; Exponential synchronization; Discontinuous neuron activations; Adaptive controller; Lyapunov stability theory", "title": "Adaptive exponential synchronization of delayed Cohen-Grossberg neural networks with discontinuous activations", "abstract": "This paper treats of the exponential synchronization issue of delayed Cohen-Grossberg neural networks with discontinuous activations. By utilizing Lyapunov stability theory, an adaptive controller is designed such that the response system can be exponentially synchronized with a drive system. Our synchronization criteria are easily verified and the obtained results are also applicable to neural networks with continuous activations since they are a special case of neural networks with discontinuous activations. Results of this paper improve a few previous known results. Finally, numerical simulations are given to verify the effectiveness of the theoretical results.", "journal": "INTERNATIONAL JOURNAL OF MACHINE LEARNING AND CYBERNETICS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000412618300022", "keywords": "Constrained optimisation; Multi-objective; Swarm intelligence; Particle swarm optimisation; Dynamic economic emission dispatch; Power generation", "title": "Multi-objective dynamic economic emission dispatch using particle swarm optimisation variants", "abstract": "Particle swarm optimisation (PSO) is a bio-inspired swarm based approach to solving optimisation problems. The algorithm functions as a result of particles traversing and evaluating the problem space, eventually converging on the optimum solution. This paper applies a number of PSO variants to the dynamic economic emission dispatch (DEED) problem. The DEED problem is a multi-objective optimisation problem in which the goal is to optimise two conflicting objectives: cost and emissions. The PSO variants tested include: the standard PSO (SPSO), the PSO with avoidance of worst locations (PSO AWL), and also a selection of different topologies including the PSO with a gradually increasing directed neighbourhood (PSO GIDN). The aim of the paper is to test the performance of different variants of the PSO AWL against variants of the SPSO on the DEED problem. The results show that the PSO AWL outperforms the SPSO for every topology implemented. The results are also compared to state of the art genetic algorithm (NSGA-II) and multi-agent eeinforcement learning (MARL). This paper then examines the performance of each PSO algorithm when the power demand is modified to form a triangle wave. The purpose of this experiment was to analyse the performance of different PSO variants on an increasingly constrained problem. (C) 2017 Elsevier B.V. All rights reserved.", "journal": "NEUROCOMPUTING", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000465453100047", "keywords": "Metaheuristics; Bat algorithm; Global optimization problem; Mutation operator", "title": "An enhanced Bat algorithm with mutation operator for numerical optimization problems", "abstract": "This article introduces a new variation of a known metaheuristic method for solving global optimization problems. The proposed algorithm is based on the Bat algorithm (BA), which is inspired by the micro-bat echolocation phenomenon, and addresses the problems of local-optima trapping using a special mutation operator that enhances the diversity of the standard BA, hence the name enhanced Bat algorithm (EBat). The design of EBat is introduced and its performance is evaluated against 24 of the standard benchmark functions, and compared to that of the standard BA, as well as to several well-established metaheuristic techniques. We also analyze the impact of different parameters on the EBat algorithm and determine the best combination of parameter values in the context of numerical optimization. The obtained results show that the new EBat method is indeed a promising addition to the arsenal of metaheuristic algorithms and can outperform several existing ones, including the original BA algorithm.", "journal": "NEURAL COMPUTING & APPLICATIONS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000428818000009", "keywords": "Sparse component analysis; Hyperplane clustering; Underdetermined blind source separation; Kernel density function", "title": "A Hyperplane Clustering Algorithm for Estimating the Mixing Matrix in Sparse Component Analysis", "abstract": "The method of sparse component analysis in general has two steps: the first step is to identify the mixing matrix in the linear model , and the second step is to recover the sources . To improve the first step, we propose a novel hyperplane clustering algorithm under some sparsity assumptions of the latent components . We apply an existing clustering function with some modifications to detect the normal vectors of the hyperplanes concentrated by observed data , then those normal vectors are clustered to identify the mixing matrix . An adaptive gradient method is developed to optimize the clustering function. The experimental results indicate that our algorithm is faster and more effective than the existing algorithms. Moreover, our algorithm is robust to the insufficient sparse sources, and can be used in a sparser source assumption.", "journal": "NEURAL PROCESSING LETTERS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000405265900004", "keywords": "Structure from motion; Line segments; Line segment matching; Visual odometry", "title": "Structure from Motion with Line Segments Under Relaxed Endpoint Constraints", "abstract": "We present a novel structure from motion pipeline, which estimates motion and wiry 3D structure from imaged line segments across multiple views. Although the position and orientation of line segments can be determined more accurately than point features, the instability of their endpoints and the fact that lines are not constrained by epipolar geometry diverted most research focus away to point-based methods. In our approach, we tackle the problem of instable endpoints by utilizing relaxed constraints on their positions, both during matching and as well in the following bundle adjustment stage. Furthermore, we gain efficiency in estimating trifocal image relations by decoupling rotation and translation. To this end, a novel linear solver for relative translation estimation given rotations from five line correspondences in three views is introduced. Extensive experiments on long image sequences show that our line-based structure from motion pipeline advantageously complements point-based methods, giving more meaningful 3D representation for indoor scenarios.", "journal": "INTERNATIONAL JOURNAL OF COMPUTER VISION", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000323360400010", "keywords": "Scene classification; Dominant color; Roughness; Openness", "title": "Human-inspired features for natural scene classification", "abstract": "Scene classification has been the target of much research. Most psychological studies have agreed that humans perceive a scene first globally recognizing its category and then they localize and recognize objects. In previous work the same feature set were used in classifying both natural scenes and manmade scenes simultaneously. We suggest the use of different features for each. In this paper the proposed features for natural scenes classification are presented. The new proposed features are inspired from the way humans perceive and recognize scenes at a glance. Outdoor scenes global features such as openness, roughness, and dominant directions have been investigated and translated into a new feature set, focusing on characteristics that efficiently differentiate between natural scene sub-classes. The effectiveness of the proposed features is tested using two datasets consists of 4 natural scenes (coast, mountain, forest, and open country) and 6 natural scenes (the previous 4 scenes plus desert and waterfall scenes), the first dataset is a benchmark data set used for testing scene classification techniques. Results showed that a classification accuracy of up to 95% could be achieved using the proposed feature set. (c) 2013 Elsevier B.V. All rights reserved.", "journal": "PATTERN RECOGNITION LETTERS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000369195100007", "keywords": "Web documents; Information extraction; Ranking method; Automatisation", "title": "ARIEX: Automated ranking of information extractors", "abstract": "Information extractors are used to transform the user-friendly information in a web document into structured information that can be used to feed a knowledge-based system. Researchers are interested in ranking them to find out which one performs the best. Unfortunately, many rankings in the literature are deficient. There are a number of formal methods to rank information extractors, but they also have many problems and have not reached widespread popularity. In this article, we present ARIEX, which is an automated method to rank web information extraction proposals. It does not have any of the problems that we have identified in the literature. Our proposal shall definitely help authors make sure that they have advanced the state of the art not only conceptually, but from an empirical point of view; it shall also help practitioners make informed decisions on which proposal is the most adequate for a particular problem. (c) 2015 Elsevier B.V. All rights reserved.", "journal": "KNOWLEDGE-BASED SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000332805700024", "keywords": "Stereo matching; Occlusion handling; Edge constraint; Graph cuts", "title": "Stereo matching by using the global edge constraint", "abstract": "Stereo matching, the key problem in the field of computer vision has long been researched for decades. However, constructing an accurate dense disparity map is still very challenging for both local and global algorithms, especially when dealing with the occlusions and disparity discontinuities. In this paper, by exploring the characteristics of the color edges, a novel constraint named the global edge constraint (GEC) is proposed to discriminate the locations of potential occlusions and disparity discontinuities. The initial disparity map is estimated by using a local algorithm, in which the GEC could guarantee that the optimal support windows would not cross the occlusions. Then a global optimization framework is adopted to improve the accuracy of the disparity map. The data term of the energy function is constructed by using the reliable correspondences selected from the initial disparity map; and the smooth term incorporates the GEC as a soft constraint to handle the disparity discontinuities. Optimal solution can be approximated via existing energy minimization approaches such as Graph cuts used in this paper. Experimental results using the Middlebury Stereo test bed demonstrate the superior performance of the proposed approach. (C) 2013 Elsevier B.V. All rights reserved.", "journal": "NEUROCOMPUTING", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000343806200029", "keywords": "Fuzzy set; fuzzy mapping; alpha-fuzzy fixed point; beta(F)-admissible", "title": "Common alpha-fuzzy fixed point theorems for fuzzy mappings via beta(F)-admissible pair", "abstract": "The object of this paper is to extend the concept of beta-admissible for multivalued mappings due to Mohammadi et al. [B. Mohammadi, S. Rezapour and N. Shahzad, Some results on fixed points of alpha-psi-Ciric generalized multifunctions, Fixed Point Theory and Applications 2013 (2013), 24] to fuzzy mappings and utilize this concept to prove a common alpha-fuzzy fixed point theorem for a pair of fuzzy mappings. We also give illustrative example which demonstrate the validity of the hypotheses of our results. Some related results to common fixed point for multivalued mappings are also derived from the main result.", "journal": "JOURNAL OF INTELLIGENT & FUZZY SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000341094000012", "keywords": "Web service; Trust; Bayesian network; Online learning; Dirichlet; Beta-Liouville", "title": "Probabilistic approach for QoS-aware recommender system for trustworthy web service selection", "abstract": "We present a QoS-aware recommender approach based on probabilistic models to assist the selection of web services in open, distributed, and service-oriented environments. This approach allows consumers to maintain a trust model for each service provider they interact with, leading to the prediction of the most trustworthy service a consumer can interact with among a plethora of similar services. In this paper, we associate the trust in a service to its performance denoted by QoS ratings instigated by the amalgamation of various QoS metrics. Since the quality of a service is contingent, which renders its trustworthiness uncertain, we adopt a probabilistic approach for the prediction of the quality of a service based on the evaluation of past experiences (ratings) of each of its consumers. We represent the QoS ratings of services using different statistical distributions, namely multinomial Dirichlet, multinomial generalized Dirichlet, and multinomial Beta-Liouville. We leverage various machine learning techniques to compute the probabilities of each web service to belong to different quality classes. For instance, we use the Bayesian inference method to estimate the parameters of the aforementioned distributions, which presents a multidimensional probabilistic embodiment of the quality of the corresponding web services. We also employ a Bayesian network classifier with a Beta-Liouville prior to enable the classification of the QoS of composite services given the QoS of its constituents. We extend our approach to function in an online setting using the Voting EM algorithm that enables the estimation of the probabilities of the QoS after each interaction with a web service. Our experimental results demonstrate the effectiveness of the proposed approaches in modeling, classifying and incrementally learning the QoS ratings.", "journal": "APPLIED INTELLIGENCE", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000326889800055", "keywords": "Additive increase/multiplicative decrease (AIMD); Congestion control; Genetic algorithm (GA); Covariance control; Robust stochastic AIMD (RS-AIMD)", "title": "Robust stochastic moment control via genetic-pole placement in communication network parameter setting", "abstract": "In this paper, the problems of stochastic robust approximate covariance assignment and robust covariance feedback stabilization, which are applied to variable parameters of additive increase/multiplicative decrease (AIMD) networks, are considered. The main idea of the developed algorithm is to use the parameter settings of an AIMD network congestion control scheme, where parameters may assign the desired network's window covariance, with respect to the current network conditions. The aim is to search for the optimal AIMD parameters of a feedback gain matrix such that the objective functions defined via appropriate robustness measures and covariance assignment constraints can be optimized using an adaptive genetic algorithm (AGA). It is shown that the results can be used to develop tools for analyzing the behavior of AIMD communication networks. Quality of service (QoS) and other performance measures of the network have been improved by using the proposed congestion control. The accuracy of the controller is demonstrated by using MATLAB and NS software programs.", "journal": "NEURAL COMPUTING & APPLICATIONS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000378588600001", "keywords": "Extended filter; n-fold strongly integral filter (resp., pseudo-residuated lattice); logic; n-fold pseudo-residuated logic", "title": "Some new axiomatic extensions of residuated logics", "abstract": "In this paper, we introduce the notions of extended filter and n-fold strongly integral filter (resp., pseudo-residuated lattice). We give the characterizations of n-fold filters by a extended filter. We construct a new logical system of a pseudo-residuated logic. Afterwards, we show that the classes of n-fold strongly integral (resp., boolean, implicative, fantastic, involutive, strong) pseudo-residuated lattices are subvarieties of the variety of all pseudo-residuated lattices. Finally, we give logics which have the above varieties as models and the connections among them.", "journal": "JOURNAL OF INTELLIGENT & FUZZY SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000375165900017", "keywords": "Self-structuring algorithm; Hierarchical type-2 fuzzy neural network; Adaptive sliding mode control; Synchronization; Fractional-order; Hyperchaotic systems; Chaotic systems", "title": "A modified sliding mode approach for synchronization of fractional-order chaotic/hyperchaotic systems by using new self-structuring hierarchical type-2 fuzzy neural network", "abstract": "This paper presents a new adaptive sliding mode control approach for the synchronization of the uncertain fractional-order chaotic systems. A self-structuring hierarchical type-2 fuzzy neural network (SHT2FNN) is proposed for estimation of uncertainties. Also the switching control action in the conventional sliding mode scheme is replaced by combination type-2 fuzzy neural network (T2FNN) with hyperbolic tangent function. In SHT2FNN, the number of rules is determined by a proposed algorithm. Adaptation laws of all trainable parameters of T2FNN and the consequent parameters of SHT2FNN, are derived based on Lyapunov stability analysis. The simulation results on two kind systems: Genio-Tesi and Coullet System and fractional-order hyper-chaotic Lorenz system, confirm the efficacy of the proposed scheme in synchronization of the uncertain fractional-order hyperchaotic and fractional-order chaotic systems. The proposed controller is robust against the approximation error and external disturbance. The proposed self-structuring algorithm in this paper is simple and it can be applied in the high dimensional problems. Furthermore, the proposed algorithm can delete unimportant rules. Adjusting the structure of the T2FNN in the hierarchical form ensures that the estimation error is very small so it can be negligible. Furthermore, the proposed strategy guarantees the robustness of controller. (C) 2016 Elsevier B.V. All rights reserved.", "journal": "NEUROCOMPUTING", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000404286400032", "keywords": "MV-algebra; MV-topological space; Lukasiewicz semantic MV-topology; compactness; Hausdorff MV-space; Stone MV-space", "title": "Lukasiewicz semantic MV-topology for MV-algebra and its application to Lukasiewicz propositional logic", "abstract": "We introduce an MV-topology on the set of all valuations of MV-algebra and then establish Lukasiewicz semantic MV-topological space. We study the topological properties of Lukasiewicz semantic MV-topology, and prove that the Lukasiewicz semantic MV-topological space is a compact zero dimension Hausdorff MV-topological space and a N-compact space. We also establish a classical topology D on the valuations set of MV-algebra, and prove that topology D is finer than the cut topology C generated by Lukasiewicz semantic MV-topology. We prove that a s-complete lattice is an MV-algebra if and only if it is isomorphic to an MV-clopen lattice of a Stone MV-space. As an application, we use the compactness of topology D to prove the compactness of Lukasiewicz semantic and Lukasiewicz propositional logic system.", "journal": "JOURNAL OF INTELLIGENT & FUZZY SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000366705100006", "keywords": "Global Lyapunov stability; Optimization; Recurrent neural networks", "title": "Stability of discrete time recurrent neural networks and nonlinear optimization problems", "abstract": "We consider the method of Reduction of Dissipativity Domain to prove global Lyapunov stability of Discrete Time Recurrent Neural Networks. The standard and advanced criteria for Absolute Stability of these essentially nonlinear systems produce rather weak results. The method mentioned above is proved to be more powerful. It involves a multi-step procedure with maximization of special nonconvex functions over polytopes on every step. We derive conditions which guarantee an existence of at most one point of local maximum for such functions over every hyperplane. This nontrivial result is valid for wide range of neuron transfer functions. (C) 2015 Elsevier Ltd. All rights reserved.", "journal": "NEURAL NETWORKS", "category": "Computer Science, Artificial Intelligence; Neurosciences", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000426936500017", "keywords": "Nonlinear control; Energy shaping; Lie group; Self-balancing robot; Posture stabilization", "title": "Self-Balancing Controlled Lagrangian and Geometric Control of Unmanned Mobile Robots", "abstract": "This work presents a novel geometric framework for self-balancing as well as planar motion control of wheeled vehicles with two fewer control inputs than the configuration variables. For self-balancing control, we shape the kinetic energy in such a way that the upright direction of the robot's body becomes a nonlinearly stable equilibrium for the corresponding controlled Lagrangian which is inherently a saddle point. Then for planar motion control of the robot, we set its position and attitude as an element of the special Euclidean group SE(2) and apply a logarithmic feedback control taking advantage of the Lie group exponential coordinates. For simulation and evaluating the controllers, the unified dynamic model of the self-balancing mobile robot (SMR) is developed using the constrained Euler-Lagrange equations.", "journal": "JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS", "category": "Computer Science, Artificial Intelligence; Robotics", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000318770500003", "keywords": "Optimization; Ecologically inspired algorithm; Mussel wandering; Levy walk", "title": "New Supervised Learning Theory Applied to Cerebellar Modeling for Suppression of Variability of Saccade End Points", "abstract": "A new supervised learning theory is proposed for a hierarchical neural network with a single hidden layer of threshold units, which can approximate any continuous transformation, and applied to a cerebellar function to suppress the end-point variability of saccades. In motor systems, feedback control can reduce noise effects if the noise is added in a pathway from a motor center to a peripheral effector; however, it cannot reduce noise effects if the noise is generated in the motor center itself: a new control scheme is necessary for such noise. The cerebellar cortex is well known as a supervised learning system, and a novel theory of cerebellar cortical function developed in this study can explain the capability of the cerebellum to feedforwardly reduce noise effects, such as end-point variability of saccades. This theory assumes that a Golgi-granule cell system can encode the strength of a mossy fiber input as the state of neuronal activity of parallel fibers. By combining these parallel fiber signals with appropriate connection weights to produce a Purkinje cell output, an arbitrary continuous input-output relationship can be obtained. By incorporating such flexible computation and learning ability in a process of saccadic gain adaptation, a new control scheme in which the cerebellar cortex feedforwardly suppresses the end-point variability when it detects a variation in saccadic commands can be devised. Computer simulation confirmed the efficiency of such learning and showed a reduction in the variability of saccadic end points, similar to results obtained from experimental data.", "journal": "NEURAL COMPUTATION", "category": "Computer Science, Artificial Intelligence; Neurosciences", "annotated_keywords": ["neural net", "supervised learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000346762700006", "keywords": "Spike-timing-dependent plasticity; Phase oscillators; Adaptive networks; Synchronization", "title": "Toward a Multisubject Analysis of Neural Connectivity", "abstract": "Directed acyclic graphs (DAGs) and associated probability models are widely used to model neural connectivity and communication channels. In many experiments, data are collected from multiple subjects whose connectivities may differ but are likely to share many features. In such circumstances, it is natural to leverage similarity among subjects to improve statistical efficiency. The first exact algorithm for estimation of multiple related DAGs was recently proposed by Oates, Smith, Mukherjee, and Cussens (2014). In this letter we present examples and discuss implications of the methodology as applied to the analysis of fMRI data from a multisubject experiment. Elicitation of tuning parameters requires care, and we illustrate how this may proceed retrospectively based on technical replicate data. In addition to joint learning of subject-specific connectivity, we allow for heterogeneous collections of subjects and simultaneously estimate relationships between the subjects themselves. This letter aims to highlight the potential for exact estimation in the multisubject setting.", "journal": "NEURAL COMPUTATION", "category": "Computer Science, Artificial Intelligence; Neurosciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000336313700004", "keywords": "Semantic memory; Feature sharedness; Artificial neural networks", "title": "A Computational Model of Semantic Memory Categorization: Identification of a Concept's Semantic Level from Feature Sharedness", "abstract": "Recent studies have shown that members of superordinate concepts share less features than members of basic-level concepts. An artificial neural network model was implemented to evaluate whether feature sharedness could distinguish between these two types of concepts and whether lesioning the network would particularly affect less shared features and superordinate categorization. The model was successful in the semantic categorization test, supporting the idea that superordinate and basic-level concepts can be distinguished on the basis of feature sharedness. In contrast, lesion results proved that the model structure was not adequate to evaluate the relation between feature sharedness, processing requirements, and patient performance. Limitations and future directions for modeling semantic memory and for semantic computing are discussed.", "journal": "COGNITIVE COMPUTATION", "category": "Computer Science, Artificial Intelligence; Neurosciences", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000459807800005", "keywords": "Virtual reality; Presence; Training; Embodied conversational agent; Health domain; Breaking bad news", "title": "Training doctors' social skills to break bad news: evaluation of the impact of virtual environment displays on the sense of presence", "abstract": "The way doctors deliver bad news has a significant impact on the therapeutic process. In order to facilitate doctor's training, we have developed an embodied conversational agent simulating a patient to train doctors to break bad news. In this article, we present an evaluation of the virtual reality training platform comparing the users' experience depending on the virtual environment displays: a PC desktop, a virtual reality headset, and four wall fully immersive systems. The results of the experience, including both real doctors and naive participants, reveal a significant impact of the environment display on the perception of the user (sense of presence, sense of co-presence, perception of the believability of the virtual patient), showing, moreover, the different perceptions of the participants depending on their level of expertise.", "journal": "JOURNAL ON MULTIMODAL USER INTERFACES", "category": "Computer Science, Artificial Intelligence; Computer Science, Cybernetics", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000461250600003", "keywords": "Bayesian analysis; human factors; human-robot interaction; machine learning", "title": "Age and Gender Differences in Performance for Operating a Robotic Manipulator", "abstract": "This paper examines the performance differences across gender and age when operating a robotic manipulator arm and also seeks to determine which human factors are considered important predictors of performance for each group. To examine these differences, 93 participants were recruited and divided up into both male (46) and female (47) as well as young (46) and old (47). While men and women had nearly identical human factors, except for women exhibiting better dexterity, different navigation strategies were utilized by the genders leading men to perform the tasks quicker and with less overall moves than women. While task completion speed was affected most by working memory (WM) and spatial abilities for men, it was seen to be mostly dependent on physical abilities for women. Substantial differences were seen between the age cohorts in WM and dexterity which resulted in the younger cohort completing tasks quicker and with a higher rate of commands than the older cohort; no difference was observed in the total number of moves which provided evidence of a similar navigation strategy across the age groups. To improve task speed performance, older adults used all facets of their information processing and spatial abilities as compared to the younger group who used a narrower subset. To compensate for the aforementioned variations in important human factors, human-computer interface design considerations are suggested.", "journal": "IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Cybernetics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000324110600014", "keywords": "Artificial bee colony; Opposition based learning; Software effort estimation; Intermediate population", "title": "Enhancing the food locations in an artificial bee colony algorithm", "abstract": "Artificial bee colony or ABC is one of the newest additions to the class of population based Nature Inspired Algorithms. In the present study we suggest some modifications in the structure of basic ABC to further improve its performance. The corresponding algorithms proposed in the present study are named Intermediate ABC (I-ABC) and I-ABC greedy. In I-ABC, the potential food sources are generated by using the intermediate positions between the uniformly generated random numbers and random numbers generated by opposition based learning (OBL). I-ABC greedy is a variation of I-ABC, where the search is always forced to move towards the solution vector having the best fitness value in the population. While the use of OBL provides a priori information about the search space, the component of greediness improves the convergence rate. The performance of proposed I-ABC and I-ABC greedy are investigated on a comprehensive set of 13 classical benchmark functions, 25 composite functions included in the special session of CEC 2005 and eleven shifted functions proposed in the special session of CEC 2008, ISDA 2009, CEC 2010 and SOCO 2010. Also, the efficiency of the proposed algorithms is validated on two real life problems; frequency modulation sound parameter estimation and to estimate the software cost model parameters. Numerical results and statistical analysis demonstrates that the proposed algorithms are quite competent in dealing with different types of problems.", "journal": "SOFT COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000330827800001", "keywords": "Location-routing; Column generation; Metaheuristic", "title": "A GRASP + ILP-based metaheuristic for the capacitated location-routing problem", "abstract": "In this paper we present a three-phase heuristic for the Capacitated Location-Routing Problem. In the first stage, we apply a GRASP followed by local search procedures to construct a bundle of solutions. In the second stage, an integer-linear program (ILP) is solved taking as input the different routes belonging to the solutions of the bundle, with the objective of constructing a new solution as a combination of these routes. In the third and final stage, the same ILP is iteratively solved by column generation to improve the solutions found during the first two stages. The last two stages are based on a new model, the location-reallocation model, which generalizes the capacitated facility location problem and the reallocation model by simultaneously locating facilities and reallocating customers to routes assigned to these facilities. Extensive computational experiments show that our method is competitive with the other heuristics found in the literature, yielding the tightest average gaps on several sets of instances and being able to improve the best known feasible solutions for some of them.", "journal": "JOURNAL OF HEURISTICS", "category": "Computer Science, Artificial Intelligence; Computer Science, Theory & Methods", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000435358300006", "keywords": "Big data; Complexity class; Preprocessing; Query; Tractability", "title": "Tractable queries on big data via preprocessing with logarithmic-size output", "abstract": "To provide a dichotomy between those queries that are feasible on big data after appropriate preprocessing and those for which preprocessing does not help, Fan et al. developed the -tractability theory, which provides a formal foundation on the tractability of query classes in the context of big data. Inspired by some technologies used to deal with big data, we introduce a novel notion of -tractability in this paper. We place a restriction on preprocessing functions, which limits the functions to produce relatively short outputs, at most logarithmic-size of the inputs. We set a complexity class to denote the classes of Boolean queries that are -tractable and conclude that it is properly contained in that of -tractable query classes, after discovering that a -tractable query class is not -tractable. With an existing reduction, which does not allow re-factorizing data and query parts, we define complete query classes for the complexity class and give an efficient way to detect such query classes. We also investigate the query classes that can be made -tractable and prove that all PTIME classes of Boolean queries can be made -tractable.", "journal": "KNOWLEDGE AND INFORMATION SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000366105600007", "keywords": "Evolutionary computation; multidimensional scaling (MDS); variation operators; visualization", "title": "Evolutionary Nonlinear Projection", "abstract": "This paper examines evolutionary nonlinear projection (NLP), a form of multidimensional scaling (MDS) performed with an evolutionary algorithm. MDS is a family of techniques for producing a low dimensional data set whose points have a one-to-one correspondence with the points of a higher dimensional data set with the added property that distances or dissimilarities in the higher dimensional space are preserved as much as possible in the lower dimensional space. The goal is typically visualization but may also be clustering or other forms of analysis. In this paper, we review current methods of NLP and go on to characterize NLP as an evolutionary computation problem, gaining insight into MDS as an optimization problem. Two different mutation operators, one introduced in this paper, are compared and parameter studies are performed on mutation rate and population size. The new mutation operator is found to be superior. NLP is found to be a problem where small population sizes exhibit superior performance. It is demonstrated experimentally that NLP is a multimodal optimization problem. Two broad classes of projection problems are identified, one of which yields consistent high-quality results and the other of which has many optima, all of low quality. A number of applications of the technique are presented, including projections of feature vectors for polyominos, of vectors that are members of an error correcting code, of behavioral assessments of a collection of agents, and of features derived from DNA sequences.", "journal": "IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION", "category": "Computer Science, Artificial Intelligence; Computer Science, Theory & Methods", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000397302900045", "keywords": "museum visitor routing problem; algorithm; lower bound; encoding", "title": "An efficient encoding scheme for a new multiple-type museum visitor routing problem with must-see and select-see exhibition rooms", "abstract": "We present a new multiple-type museum visitor routing problem (MT-MVRP) in which a museum's exhibition rooms are classified into must-see and select-see rooms. A novel encoding scheme is proposed to simultaneously determine the scheduling of rooms for all of the groups and an immune based evolutionary algorithm is developed to solve the MT-MVRP. Additionally, the lower bound of the makespan for the MT-MVRP is derived. The numerical results of the immune based algorithm on three museums in Taiwan are discussed and compared with those by the other approaches.", "journal": "INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000360416900002", "keywords": "Comprehensive learning (CL); Exploration; Exploitation; Particle swarm optimization (PSO); Heterogeneous", "title": "Heterogeneous comprehensive learning particle swarm optimization with enhanced exploration and exploitation", "abstract": "This paper presents a comprehensive learning particle swarm optimization algorithm with enhanced exploration and exploitation, named as \"heterogeneous comprehensive learning particle swarm optimization\" (HCLPSO). In this algorithm, the swarm population is divided into two subpopulations. Each subpopulation is assigned to focus solely on either exploration or exploitation. Comprehensive learning (CL) strategy is used to generate the exemplars for both subpopulations. In the exploration-subpopulation, the exemplars are generated by using personal best experiences of the particles in the exploration-subpopulation itself. In the exploitation-subpopulation, the personal best experiences of the entire swarm population are used to generate the exemplars. As the exploration-subpopulation does not learn from any particles in the exploitation-subpopulation, the diversity in the exploration-subpopulation can be retained even if the exploitation-subpopulation converges prematurely. The heterogeneous comprehensive learning particle swarm optimization algorithm is tested on shifted and rotated benchmark problems and compared with other recent particle swarm optimization algorithms to demonstrate superior performance of the proposed algorithm over other particle swarm optimization variants. (C) 2015 Elsevier B.V. All rights reserved.", "journal": "SWARM AND EVOLUTIONARY COMPUTATION", "category": "Computer Science, Artificial Intelligence; Computer Science, Theory & Methods", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000335565200006", "keywords": "IPOP-CMA-ES; feasible solutions; bound constraints; MA-LSch-CMA; Continuous optimization", "title": "A Note on Bound Constraints Handling for the IEEE CEC'05 Benchmark Function Suite", "abstract": "The benchmark functions and some of the algorithms proposed for the special session on real parameter optimization of the 2005 IEEE Congress on Evolutionary Computation (CEC'05) have played and still play an important role in the assessment of the state of the art in continuous optimization. In this article, we show that if bound constraints are not enforced for the final reported solutions, state-of-the-art algorithms produce infeasible best candidate solutions for the majority of functions of the IEEE CEC'05 benchmark function suite. This occurs even though the optima of the CEC'05 functions are within the specified bounds. This phenomenon has important implications on algorithm comparisons, and therefore on algorithm designs. This article's goal is to draw the attention of the community to the fact that some authors might have drawn wrong conclusions from experiments using the CEC'05 problems.", "journal": "EVOLUTIONARY COMPUTATION", "category": "Computer Science, Artificial Intelligence; Computer Science, Theory & Methods", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000343988900001", "keywords": "Particle swarm optimization; Monte Carlo method; The joint spectral radius; Convergence analysis", "title": "A new joint spectral radius analysis of random PSO algorithm", "abstract": "The existing stability analysis of particle swarm optimization (PSO) algorithm is chiefly concluded by the assumption of constant transfer matrix or time-varying random transfer matrix. Firstly, one counterexample is provided to show that the existing convergence analysis is not possibly valid for PSO system involving random variables. Secondly, the joint spectral radius, mainly calculated by the maximum eigenvalue of the product of all asymmetric random transfer matrices, is introduced to analyze and discuss convergence condition and convergence rate from numerical viewpoint with the aid of Monte Carlo method. Numerical results show that there is one major discrepancy between some preview convergence results and our corresponding results, helping us to deeply understand the tradeoff between exploration ability and exploitation ability as well as providing certain guideline for parameter selection.", "journal": "INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000463472000011", "keywords": "Evolutionary algorithms (EAs); multiobjective optimization; robust optimization", "title": "Robust Multiobjective Optimization via Evolutionary Algorithms", "abstract": "Uncertainty inadvertently exists in most real-world applications. In the optimization process, uncertainty poses a very important issue and it directly affects the optimization performance. Nowadays, evolutionary algorithms (EAs) have been successfully applied to various multiobjective optimization problems (MOPs). However, current researches on EAs rarely consider uncertainty in the optimization process and existing algorithms often fail to handle the uncertainty, which have limited EAs' applications in real-world problems. When MOPs come with uncertainty, they are referred to as robust MOPs (RMOPs). In this paper, we aim at solving RMOPs using EA-based optimization search. We propose a novel robust multiobjective optimization EA (RMOEA) with two distinct, yet complement, parts: 1) multiobjective optimization finding global Pareto optimal front ignoring disturbance at first and 2) robust optimization searching for the robust optimal front afterward. Furthermore, a comprehensive performance evaluation method is proposed to quantify the performance of RMOEA in solving RMOPs. Experimental results on a group of benchmark functions demonstrate the superiority of the proposed design in terms of both solutions' quality under the disturbance and computational efficiency in solving RMOPs.", "journal": "IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION", "category": "Computer Science, Artificial Intelligence; Computer Science, Theory & Methods", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000389108500005", "keywords": "Decision making; Web user; Eye-tracking; Pupil dilation; EEG", "title": "Combining eye tracking, pupil dilation and EEG analysis for predicting web users click intention", "abstract": "In this paper a novel approach for analyzing web user behavior and preferences on a web site is introduced, consisting of a physiological-based analysis for the assessment of a web users' click intention, by merging pupil dilation and electroencephalogram (EEG) responses. First, we conducted an empirical study using five real web sites from which the gaze position, pupil dilation and EEG of 21 human subjects were recorded while performing diverse information foraging tasks. We found the existence of a statistical differentiation between choice and not-choice pupil dilation curves, specifically that fixations corresponding to clicks had greater pupil size than fixations without a click. Then 7 classification models were proposed using 15 out of 789 pupil dilation and EEG features obtained from a Random Lasso feature selection process. Although good results were obtained for Accuracy (71,09% using Logistic Regression), the results for Precision, Recall and F-Measure remained low, which indicates that the behaviour we were studying was not well classified. The above results show that it is possible to create a classifier for web user click intention behaviour based on merging features extracted from pupil dilation and EEG responses. However we conclude that it is necessary to use better quality instruments for capturing the data. (C) 2016 Elsevier B.V. All rights reserved.", "journal": "INFORMATION FUSION", "category": "Computer Science, Artificial Intelligence; Computer Science, Theory & Methods", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000329159900005", "keywords": "ANFIS models; Condition monitoring; Wind turbine; SCADA data; Normal behavior models", "title": "Introduction to the Special Issue on Social Web Mining", "abstract": "This paper is part two of a two part series. The originality of part one was the proposal of a novelty approach for wind turbine supervisory control and data acquisition (SCADA) data mining for condition monitoring purposes. The novelty concerned the usage of adaptive neuro-fuzzy interference system (ANFIS) models in this context and the application of a proposed procedure to a wide range of different SCADA signals. The applicability of the set up ANFIS models for anomaly detection was proven by the achieved performance of the models. In combination with the fuzzy interference system (FIS) proposed the prediction errors provide information about the condition of the monitored components. Part two presents application examples illustrating the efficiency of the proposed method. The work is based on continuously measured wind turbine SCADA data from 18 modern type pitch regulated wind turbines of the 2 MW class covering a period of 35 months. Several real life faults and issues in this data are analyzed and evaluated by the condition monitoring system (CMS) and the results presented. It is shown that SCADA data contain crucial information for wind turbine operators worth extracting. Using full signal reconstruction (FSRC) adaptive neuro-fuzzy interference system (ANFIS) normal behavior models (NBM) in combination with fuzzy logic (FL) a setup is developed for data mining of this information. A high degree of automation can be achieved. It is shown that FL rules established with a fault at one turbine can be applied to diagnose similar faults at other turbines automatically via the CMS proposed. A further focus in this paper lies in the process of rule optimization and adoption, allowing the expert to implement the gained knowledge in fault analysis. The fault types diagnosed here are: (1) a hydraulic oil leakage; (2) cooling system filter obstructions; (3) converter fan malfunctions; (4) anemometer offsets and (5) turbine controller malfunctions. Moreover, the graphical user interface (GUI) developed to access, analyze and visualize the data and results is presented. (C) 2013 Elsevier B.V. All rights reserved.", "journal": "ACM TRANSACTIONS ON INTELLIGENT SYSTEMS AND TECHNOLOGY", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000313011600001", "keywords": "Scheduling; Batching machines, Arbitrary job sizes; Ant colony optimization", "title": "An improved ant colony optimization for scheduling identical parallel batching machines with arbitrary job sizes", "abstract": "In this paper we consider the problem of scheduling parallel batching machines with jobs of arbitrary sizes. The machines have identical capacity of size and processing velocity. The jobs are processed in batches given that the total size of jobs in a batch cannot exceed the machine capacity. Once a batch starts processing, no interruption is allowed until all the jobs are completed. First we present a mixed integer programming model of the problem. We show the computational complexity of the problem and optimality properties. Then we propose a novel ant colony optimization method where the Metropolis Criterion is used to select the paths of ants to overcome the immature convergence. Finally, we generate different scales of instances to test the performance. The computational results show the effectiveness of the algorithm, especially for large-scale instances. Crown Copyright (C) 2012 Published by Elsevier B.V. All rights reserved.", "journal": "APPLIED SOFT COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000316762000006", "keywords": "Heterogeneous Fleet Vehicle Routing Problem; Fleet size and mix; Metaheuristic; Iterated Local Search", "title": "An Iterated Local Search heuristic for the Heterogeneous Fleet Vehicle Routing Problem", "abstract": "This paper deals with the Heterogeneous Fleet Vehicle Routing Problem (HFVRP). The HFVRP is -hard since it is a generalization of the classical Vehicle Routing Problem (VRP), in which clients are served by a heterogeneous fleet of vehicles with distinct capacities and costs. The objective is to design a set of routes in such a way that the sum of the costs is minimized. The proposed algorithm is based on the Iterated Local Search (ILS) metaheuristic which uses a Variable Neighborhood Descent procedure, with a random neighborhood ordering (RVND), in the local search phase. To the best of our knowledge, this is the first ILS approach for the HFVRP. The developed heuristic was tested on well-known benchmark instances involving 20, 50, 75 and 100 customers. These test-problems also include dependent and/or fixed costs according to the vehicle type. The results obtained are quite competitive when compared to other algorithms found in the literature.", "journal": "JOURNAL OF HEURISTICS", "category": "Computer Science, Artificial Intelligence; Computer Science, Theory & Methods", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000334125500004", "keywords": "Process control; software tool; Neural Network; Direct Inverse Control; pH Neutralization Process", "title": "Advanced control software framework for process control applications", "abstract": "Industries are now moving towards PC-based control from PLC (Programmable Logic Control) based control as the PC (personal computer) is easily available and capable of implementing various control strategies to improve productivity. Advances in both hardware and software technology are expediting this move. Absence of a user friendly, cost effective and easy to apply advanced control strategy based software is the motivation for this work. In this paper we reviewed issues of software application in process control systems. We present the development of an advance process control software tool and its application on a pH neutralization plant. Neural network model based DIC (Direct Inverse Control) strategy is implemented to control the process. The application and results shows the viability and robustness of the advanced control software and its superiority over the conventional PID (proportional-integral-derivative) controller.", "journal": "INTERNATIONAL JOURNAL OF COMPUTATIONAL INTELLIGENCE SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000355262900009", "keywords": "Walking pattern classification; Human gait model; Linguistic modeling", "title": "Walking pattern classification using a granular linguistic analysis", "abstract": "Classifying walking patterns helps the diagnosis of health status, disease progression and the effect of interventions. In this paper, we develop previous research on human gait to extract a meaningful set of parameters that allow us to design a highly interpretable system capable of identifying different gait styles with linguistic fuzzy if-then rules. The model easily discriminates among five different walking patterns, namely: normal walk, on tiptoes, dragging left limb, dragging right limb, and dragging both limbs. We have carried out a complete experimentation to test the performance of the extracted parameters to correctly classify these five chosen gait styles. (C) 2015 Elsevier B.V. All rights reserved.", "journal": "APPLIED SOFT COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000387108600001", "keywords": "Supply chain network design; Value-at-risk; Variable possibility distribution; Parameter decomposition method", "title": "Robust optimization of supply chain network design in fuzzy decision system", "abstract": "This paper presents a new robust optimization method for supply chain network design problem by employing variable possibility distributions. Due to the variability of market conditions and demands, there exist some impreciseness and ambiguousness in developing procurement and distribution plans. The proposed optimization method incorporates the uncertainties encountered in the manufacturing industry. The main motivation for building this optimization model is to make tools available for producers to develop robust supply chain network design. The modeling approach selected is a fuzzy value-at-risk (VaR) optimization model, in which the uncertain demands and transportation costs are characterized by variable possibility distributions. The variable possibility distributions are obtained by using the method of possibility critical value reduction to the secondary possibility distributions of uncertain demands and costs. We also discuss the equivalent parametric representation of credibility constraints and VaR objective function. Furthermore, we take the advantage of structural characteristics of the equivalent optimization model to design a parameter-based domain decomposition method. Using the proposed method, the original optimization problem is decomposed to two equivalent mixed-integer parametric programming sub-models so that we can solve the original optimization problem indirectly by solving its sub-models. Finally, we present an application example about a food processing company with four suppliers, five plants, five distribution centers and five customer zones. We formulate our application example as parametric optimization models and conduct our numerical experiments in the cases when the input data (demands and costs) are deterministic, have fixed possibility distributions and have variable possibility distributions. Experimental results show that our parametric optimization method can provide an effective and flexible way for decision makers to design a supply chain network.", "journal": "JOURNAL OF INTELLIGENT MANUFACTURING", "category": "Computer Science, Artificial Intelligence; Engineering, Manufacturing", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000456588800015", "keywords": "Tolerance design; Quality loss function; Hierarchical product; Taguchi method", "title": "Optimal tolerance design of hierarchical products based on quality loss function", "abstract": "Taguchi's loss function has been used for optimal tolerance design, but the traditional quadratic quality loss function is inappropriate in the tolerance design of hierarchical products, which are ubiquitous in industrial production. This study emphasizes hierarchical products and extends the traditional quality loss function on the basis of Taguchi's quadratic loss function; the modified formulas are subsequently used to establish quality loss function models of the nominal-the-best, larger-the-better, and smaller-the-better characteristics of hierarchical products. An example is presented to demonstrate the application of the extended smaller-the-better characteristic loss function model to the optimal tolerance design of hierarchical products. Furthermore, the problem associated with selecting materials of various grades in the design process is discussed. The results show that the extended quality loss function model demonstrates good operability in the tolerance design of hierarchical products.", "journal": "JOURNAL OF INTELLIGENT MANUFACTURING", "category": "Computer Science, Artificial Intelligence; Engineering, Manufacturing", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000435198600024", "keywords": "Electroencephalography (EEG); emotion recognition; feedforward neural network; subnetwork nodes", "title": "EEG-Based Emotion Recognition Using Hierarchical Network With Subnetwork Nodes", "abstract": "Emotions play a crucial role in decision-making, brain activity, human cognition, and social intercourse. This paper proposes a hierarchical network structure with subnetwork nodes to discriminate three human emotions: 1) positive; 2) neutral; and 3) negative. Each subnetwork node embedded in the network that are formed by hundreds of hidden nodes, could be functional as an independent hidden layer for feature representation. The top layer of the hierarchical network, like the mammal cortex in the brain, combine such features generated from subnetwork nodes, but simultaneously, recast these features into a mapping space so that the network can be performed to produce more reliable cognition. The proposed method is compared with other state-of-the-art methods. The experimental results from two different EEG datasets show that a promising result is obtained when using the proposed method with both single and multiple modality.", "journal": "IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS", "category": "Computer Science, Artificial Intelligence; Robotics; Neurosciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000412985300006", "keywords": "", "title": "Optimization of the occupational safety compliance control (OSCC) system at a coal mining enterprise", "abstract": "The causes for occupational safety violations at coal mining enterprises are considered. The fixed fines control model and the model with occupational safety violations punished by dismissal are described. The theory & game approach is used to create an efficient controlling body that makes it possible to exclude any chance of conspiracy. The optimal probabilities of examinations and inspector salaries that minimize expenditures are found. A single-level control system is compared with a twolevel control system. The region of parameter values for each variant, where a given variant is preferable, is specified.", "journal": "JOURNAL OF COMPUTER AND SYSTEMS SCIENCES INTERNATIONAL", "category": "Computer Science, Artificial Intelligence; Computer Science, Cybernetics; Computer Science, Theory & Methods", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000323367000007", "keywords": "generalized degrees of freedom; grouping; K-means clustering; Lasso; penalized regression; truncated Lasso penalty (TLP)", "title": "Cluster Analysis: Unsupervised Learning via Supervised Learning with a Non-convex Penalty", "abstract": "Clustering analysis is widely used in many fields. Traditionally clustering is regarded as unsupervised learning for its lack of a class label or a quantitative response variable, which in contrast is present in supervised learning such as classification and regression. Here we formulate clustering as penalized regression with grouping pursuit. In addition to the novel use of a non-convex group penalty and its associated unique operating characteristics in the proposed clustering method, a main advantage of this formulation is its allowing borrowing some well established results in classification and regression, such as model selection criteria to select the number of clusters, a difficult problem in clustering analysis. In particular, we propose using the generalized cross-validation (GCV) based on generalized degrees of freedom (GDF) to select the number of clusters. We use a few simple numerical examples to compare our proposed method with some existing approaches, demonstrating our method's promising performance.", "journal": "JOURNAL OF MACHINE LEARNING RESEARCH", "category": "Automation & Control Systems; Computer Science, Artificial Intelligence", "annotated_keywords": ["supervised learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000428930600010", "keywords": "3D reconstruction; road condition assessment; subpixel disparity estimation; parabola interpolation; Markov random fields; fast bilateral stereo", "title": "Road Surface 3D Reconstruction Based on Dense Subpixel Disparity Map Estimation", "abstract": "Various 3D reconstruction methods have enabled civil engineers to detect damage on a road surface. To achieve the millimeter accuracy required for road condition assessment, a disparity map with subpixel resolution needs to be used. However, none of the existing stereo matching algorithms are specially suitable for the reconstruction of the road surface. Hence in this paper, we propose a novel dense subpixel disparity estimation algorithm with high computational efficiency and robustness. This is achieved by first transforming the perspective view of the target frame into the reference view, which not only increases the accuracy of the block matching for the road surface but also improves the processing speed. The disparities are then estimated iteratively using our previously published algorithm, where the search range is propagated from three estimated neighboring disparities. Since the search range is obtained from the previous iteration, errors may occur when the propagated search range is not sufficient. Therefore, a correlation maxima verification is performed to rectify this issue, and the subpixel resolution is achieved by conducting a parabola interpolation enhancement. Furthermore, a novel disparity global refinement approach developed from the Markov random fields and fast bilateral stereo is introduced to further improve the accuracy of the estimated disparity map, where disparities are updated iteratively by minimizing the energy function that is related to their interpolated correlation polynomials. The algorithm is implemented in C language with a near real-time performance. The experimental results illustrate that the absolute error of the reconstruction varies from 0.1 to 3 mm.", "journal": "IEEE TRANSACTIONS ON IMAGE PROCESSING", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000440049800008", "keywords": "Vertical handover necessity estimation; unnecessary handover; handover failure; probabilistic model", "title": "A novel model for minimizing unnecessary handover in heterogeneous networks", "abstract": "Over the years, vertical handover necessity estimation has attracted the interest of numerous researchers. Despite the attractive benefits of integrating different wireless platforms, mobile users are confronted with the issue of detrimental handover. This paper uses extensive geometric and probabilistic techniques to develop a realistic and novel model for the coverage area of a wireless local area network (WLAN) cell with an aim to minimize unnecessary handover and handover failure of a mobile node (MN) traversing the WLAN cell from a third-generation network. The dwell time is estimated along with the threshold values to ensure an optimal handover decision by the MN, while the probability of unnecessary handover and probability of handover failure are kept within tolerable bounds. Monte Carlo simulations were carried out to assess the behavior of the proposed and existing models. Simulation results showed that our proposed model is more robust and capable of keeping unnecessary handover probability and handover failure probability closest to a predefined probability benchmark. For model validation, the Nash-Sutcliffe coefficient was used to compute the efficiency of the proposed model, which achieved a value of 98.82% indicating the accuracy of the model.", "journal": "TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000412341500011", "keywords": "Multi-agent systems; Agents; Centralized; Decentralized; Empirical; Evaluation; Dynamism; Urgency; Scale; Operational research; Logistics", "title": "When do agents outperform centralized algorithms? A systematic empirical evaluation in logistics", "abstract": "Multi-agent systems (MAS) literature often assumes decentralized MAS to be especially suited for dynamic and large scale problems. In operational research, however, the prevailing paradigm is the use of centralized algorithms. Present paper empirically evaluates whether a multi-agent system can outperform a centralized algorithm in dynamic and large scale logistics problems. This evaluation is novel in three aspects: (1) to ensure fairness both implementations are subject to the same constraints with respect to hardware resources and software limitations, (2) the implementations are systematically evaluated with varying problem properties, and (3) all code is open source, facilitating reproduction and extension of the experiments. Existing work lacks a systematic evaluation of centralized versus decentralized paradigms due to the absence of a real-time logistics simulator with support for both paradigms and a dataset of problem instances with varying properties. We extended an existing logistics simulator to be able to perform real-time experiments and we use a recent dataset of dynamic pickup-and-delivery problem with time windows instances with varying levels of dynamism, urgency, and scale. The OptaPlanner constraint satisfaction solver is used in a centralized way to compute a global schedule and used as part of a decentralized MAS based on the dynamic contract-net protocol (DynCNET) algorithm. The experiments show that the DynCNET MAS finds solutions with a relatively lower operating cost when a problem has all following three properties: medium to high dynamism, high urgency, and medium to large scale. In these circumstances, the centralized algorithm finds solutions with an average cost of 112.3% of the solutions found by the MAS. However, averaged over all scenario types, the average cost of the centralized algorithm is 94.2%. The results indicate that the MAS performs best on very urgent problems that are medium to large scale.", "journal": "AUTONOMOUS AGENTS AND MULTI-AGENT SYSTEMS", "category": "Automation & Control Systems; Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000440049800037", "keywords": "Backlight; current balancing error; light-emitting diode driver; single-inductor multiple-output", "title": "A novel single-inductor eight-channel light-emitting diode driver for low power display backlight applications", "abstract": "A novel decoder-based single-inductor eight-channel light-emitting diode (LED) driver circuit for low power display backlight applications has been proposed. Uniform brightness in the display provides better picture clarity, which can be achieved by providing uniform DC currents to all channels in the backlight arrangement. Existing systems use individual current regulators for each channel, which fails to provide uniform current to individual channels. Instead, uniform current is provided to all eight channels in the proposed system as the same current is distributed to all channels using time-multiplexing by a 3 x 8 decoder and a 3-bit binary up-counter. A digital pulse width modulator is used in the existing system, which has unwanted switching activities and electromagnetic interference (EMI). Unwanted switching activities and EMI are completely eliminated in the proposed system by applying a novel switching technique. The proposed LED driver is designed and implemented using the 180-nm CMOS process. Each channel is designed to conduct 300 mA of current through it when the supply is 12 V. The proposed LED driver consumes total power of 8.746 W at 27 degrees C while the peak power efficiency is 96.26%. The minimum current balancing error of the proposed LED driver is 0.019%, whereas it is 0.12% for the existing system. The existing method exhibits an average current balancing error of 2.04% whereas the proposed technique exhibits 1.33% as the average current balancing error, which is 34.82% less than the existing system.", "journal": "TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000406993300074", "keywords": "AlGaN channel; sheet carrier concentration model; Ga-face; polarization; high breakdown; total induced net interface polarization", "title": "Relaxation rate and polarization charge density model for AlN/Al-x,Ga1-x,N/AlN heterostructures", "abstract": "This work describes the strain-relaxation dependent carrier concentration (n(s)) profile model using spontaneous and piezoelectric polarization for AlN/AlxGa1-x N/AlN HEMTs in all mole fraction (x) interpolations. As x varies, the Aluminum Gallium Nitride (AlGaN) channel shows strain relaxation with the Aluminum Nitride (AlN) barrier. The degree of relaxation is modeled from AlN to GaN regions in the channel. It shows that the AlN barrier and buffer relaxation and strain recovery occurs due to the gradual crystal quality degradation from barrier/buffer to the channel interface. These combination devices show less drain current degradation with temperature variation from 300 K to 573 K. This model shows a good agreement with experimental data with a carrier density of n(s) = 2.8 x 10(13) /cm(2)", "journal": "TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000435627400001", "keywords": "graphical model selection; nonparanormal graph; time-varying network analysis; hypothesis test; regularized rank-based estimator", "title": "Post-Regularization Inference for Time-Varying Nonparanormal Graphical Models", "abstract": "We propose a novel class of time-varying nonparanormal graphical models, which allows us to model high dimensional heavy-tailed systems and the evolution of their latent network structures. Under this model we develop statistical tests for presence of edges both locally at a fixed index value and globally over a range of values. The tests are developed for a high-dimensional regime, are robust to model selection mistakes and do not require commonly assumed minimum signal strength. The testing procedures are based on a high dimensional, debiasing-free moment estimator, which uses a novel kernel smoothed Kendall's tau correlation matrix as an input statistic. The estimator consistently estimates the latent inverse Pearson correlation matrix uniformly in both the index variable and kernel bandwidth. Its rate of convergence is shown to be minimax optimal. Our method is supported by thorough numerical simulations and an application to a neural imaging data set.", "journal": "JOURNAL OF MACHINE LEARNING RESEARCH", "category": "Automation & Control Systems; Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000455183800006", "keywords": "Image binarization; Background subtraction; Robust regression; Document image analysis; Microscopy image analysis", "title": "Multivariate Correlation Entropy and Law Discovery in Large Data Sets", "abstract": "Over the past several centuries, many important natural laws have been discovered by scientists, which have not only changed our viewpoints about nature but also affected our lives significantly. Today, automatic discovery of meaningful laws from data beyond two variables becomes an important task of our time. Here, we propose two multivariate correlation measures, namely, the multivariate correlation entropy (MCE) and the multivariate incorrelation entropy (MIE), which can be used to measure the strength of the correlation among multiple variables. Using MIE makes it possible to directly detect linear relations existing in large data sets. In addition, more complicated nonlinear multivariate laws can be discovered using a function dictionary.", "journal": "IEEE INTELLIGENT SYSTEMS", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000412571400060", "keywords": "Sphere decoding; distributed antenna system; sphere radius; multiuser detection", "title": "Sphere decoding algorithm for multiuser detection in a distributed antenna system", "abstract": "In this paper, the impact of initial search radius on the complexity and performance of a sphere decoding algorithm is investigated for different user positions within a distributed antenna system. In a distributed antenna system, users can take up random positions within the cell clusters. The channel matrix can therefore take up infinitely different forms. In the presented work, a distributed antenna system with three different user positions in the cooperating cells is considered by employing different channel matrices. The effect on the complexity and performance of the sphere decoder due to the choice of the initial sphere radius is investigated for these user positions. It is shown that the signal lattice volume changes considerably for different user positions within the cells. A dynamic radius allocation algorithm is proposed in which the behavior is exploited by dynamically adjusting the initial sphere radius based on the knowledge of the channel path gain matrix. The simulation results show that the proposed algorithm results in a considerable reduction in the complexity of the sphere decoder in a distributed antenna system. Additionally, the performance of the sphere decoder in different coupling scenarios within the distributed antenna system has been investigated for a different number of candidates. It is shown that the performance of cell edge users can be considerably enhanced with high channel diversity, which otherwise could severely deteriorate the overall system performance.", "journal": "TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000428721400017", "keywords": "Pulse-width modulation rectifier; repetitive control; sliding mode control", "title": "Improved discrete sliding mode control strategy for pulse-width modulation rectifier", "abstract": "An improved sliding mode control utilizing repetitive control (ISMRC) is proposed for a three-phase pulse width modulation (PWM) rectifier. The proposed controller integrates the advantages of both sliding mode control (SMC) and repetitive control (RC) by implementing a structure that embeds an RC controller into the equivalent control branch of an SMC controller. Both a simulation and an experiment are conducted to compare the proposed ISMRC controller with a conventional SMC controller. It is demonstrated that the fifth harmonic distortion of the current of the PWM rectifier system is controlled at 3.3%, the power factor is close to the unit, and the effect on the DC bus voltage is effectively restrained. Therefore, the proposed control strategy can improve both the steady-state performance and the dynamic transient response of a PWM rectifier control system effectively, as well as increase the robustness of the system to load disturbances and parametric uncertainties.", "journal": "TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000369325300008", "keywords": "Recommendation system; incremental weighted bipartite algorithm; personalized point of learning interests", "title": "Incremental weighted bipartite algorithm for large-scale recommendation systems", "abstract": "Personalized recommendation systems are a solution for information overload. In the real recommendation system, the data are dynamic; thus, the real time of the system will be seriously affected if we recalculate after every change, and, in time, sensitive situation of the delayed updated calculation will affect the accuracy of the recommendation. The incremental calculation methods of collaborative filtering, clustering, singular value decomposition, and bipartite algorithm have achieved some progress; however, there is limited research on the incremental computation of the weighted bipartite algorithm. This paper proposes a personalized recommendation algorithm. In this algorithm, the rank of the recommendations is effectively influenced by the rate and times of the same choice. This paper uses the borrowing records of students in college libraries to model the recommendation system and achieve a rank of personalized recommendations for college students. This system, combined with the autonomic learning resources platform, will improve student learning efficiency.", "journal": "TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000450150900008", "keywords": "Brain storm algorithm; Bilevel optimization; Sales territory design; Latency", "title": "A discrete bilevel brain storm algorithm for solving a sales territory design problem: a case study", "abstract": "A sales territory design problem faced by a manufacturing company that supplies products to a group of customers located in a service region is addressed in this paper. The planning process of designing the territories has the objective to minimizing the total dispersion of the customers without exceeding a limited budget assigned to each territory. Once territories have been determined, a salesperson has to define the day-by-day routes to satisfy the demand of customers. Currently, the company has established a service level policy that aims to minimize total waiting times during the distribution process. Also, each territory is served by a single salesperson. A novel discrete bilevel optimization model for the sales territory design problem is proposed. This problem can be seen as a bilevel problem with a single leader and multiple independent followers, in which the leader's problem corresponds to the design of territories (manager of the company), and the routing decision for each territory corresponds to each follower. The hierarchical nature of the current company's decision-making process triggers some particular characteristics of the bilevel model. A brain storm algorithm that exploits these characteristics is proposed to solve the discrete bilevel problem. The main features of the proposed algorithm are that the workload is used to verify the feasibility and to cluster the leader's solutions. In addition, four discrete mechanisms are used to generate new solutions, and an elite set of solutions is considered to reduce computational cost. This algorithm is used to solve a real case study, and the results are compared against the current solution given by the company. Results show a reduction of more than 20% in the current costs with the solution obtained by the proposed algorithm. Furthermore, a sensitivity analysis is performed, providing interesting managerial insights to improve the current operations of the company.", "journal": "MEMETIC COMPUTING", "category": "Computer Science, Artificial Intelligence; Operations Research & Management Science", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000416682900005", "keywords": "Community detection; Parallel distributed algorithms; Big data; Social networks", "title": "Community Detection Algorithm for Big Social Networks Using Hybrid Architecture", "abstract": "One of the most relevant and widely studied structural properties of networks is their community structure. Detecting communities is of great importance in social networks where systems are often represented as graphs. With the advent of web-based social networks like Twitter, Facebook and LinkedIn. community detection became even more difficult due to the massive network size, which can reach up to hundreds of millions of vertices and edges. This large graph structured data cannot be processed without using distributed algorithms due to memory constraints of one machine and also the need to achieve high performance. In this paper, we present a novel hybrid (shared + distributed memory) parallel algorithm to efficiently detect high quality communities in massive social networks. For our simulations, we use synthetic graphs ranging from 100K to 16M vertices to show the scalability and quality performance of our algorithm. We also use two massive real world networks: (a) section of Twitter-2010 network having approximate to 41M vertices and approximate to 1.4 Bedges (b) UK-2007 (.uk web domain) having approximate to 105M vertices and approximate to 3.3B edges. Simulation results on MPI setup with 8 compute nodes having 16 cores each show that, upto approximate to 6X speedup is achieved for synthetic graphs in detecting communities without compromising the quality of the results. (C) 2017 Elsevier Inc. All rights reserved.", "journal": "BIG DATA RESEARCH", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000345629000049", "keywords": "Adaptive synchronization; M-matrix; Markovian switching; neutral-type neural network", "title": "Adaptive Synchronization for Neutral-Type Neural Networks with Stochastic Perturbation and Markovian Switching Parameters", "abstract": "In this paper, the problem of adaptive synchronization is investigated for stochastic neural networks of neutral-type with Markovian switching parameters. Using the M-matrix approach and the stochastic analysis method, some sufficient conditions are obtained to ensure three kinds of adaptive synchronization for the stochastic neutral-type neural networks. These three kinds of adaptive synchronization include the almost sure asymptotical synchronization, exponential synchronization in pth moment and almost sure exponential synchronization. Some numerical examples are provided to illustrate the effectiveness and potential of the proposed design techniques.", "journal": "IEEE TRANSACTIONS ON CYBERNETICS", "category": "Automation & Control Systems; Computer Science, Artificial Intelligence; Computer Science, Cybernetics", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000402307600002", "keywords": "binary regression; data mining; logistic; penalized maximum likelihood; precision; probit; sensitivity testing analysis", "title": "Panning for gold: Enhancing the precision of sensitivity test data", "abstract": "Sensitivity tests apply a range of stimulus values to experimental subjects and record binary responses in order to estimate the distribution of threshold values in the subject population, where thresholds delineate responses from nonresponses. In many applications, such as explosives engineering, individual tests are expensive and are conducted in small runs. Scarcity of data results in nonexistence of estimates, or estimates with low precision. We discuss various methods, such as combining test runs, covariate analysis, and penalized maximum likelihood, for enhancing precision and mining more gold from expensive test results.", "journal": "STATISTICAL ANALYSIS AND DATA MINING", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Statistics & Probability", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000209525200001", "keywords": "extreme value theory; cluster analysis; discrimination; logistic regression; ROC curve; misuse; evaluation", "title": "Discrimination of Psychotropic Drugs Over-Consumers Using a Threshold Exceedance Based Approach", "abstract": "Use of some medication, such as tranquilizers or hypnotics may carry important risks for patients including the emergence of abuse and/or dependence. The problem we tackle consists of identifying and discriminating the group most \"at risk\" of abuse and/or dependence for a given drug, in order to provide an estimation of its prevalence and to develop preventive measures targeted toward the corresponding drug prescription. A criterion, currently employed to characterize patients' consumption of a drug, is the ratio between their daily average consumption and the maximum recommended daily dose as specified in the drug monograph, called the F factor. In theory, any patient having an F factor greater than 1 should be classified as an over-consumer for the corresponding drug, but in practice this threshold might not be very relevant for all drugs. The proposed approach, combining different statistical methods (extreme value theory with the Peaks Over Threshold Model, logistic regression, ROC curve), is an innovative way to study consumption behaviors of psychotropic drugs. Two drugs are studied: an antidepressant, tianeptine and a hypnotic, zolpidem. From one drug to another, different thresholds for the F factor and patient's characteristics associated with the risk of extreme consumption are found, revealing different consumption behaviors. (C) 2012 Wiley Periodicals, Inc.", "journal": "STATISTICAL ANALYSIS AND DATA MINING", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Statistics & Probability", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000342435800006", "keywords": "Mixed-resolution multi-view images; Super-resolution; Depth estimation", "title": "A new closed loop method of super-resolution for multi-view images", "abstract": "In this paper, we propose a closed loop method to resolve the multi-view super-resolution problem. For the mixed-resolution multi-view case, where the input is one high-resolution view along with its neighboring low-resolution views, our method can give the super-resolution results and obtain a high-quality depth map simultaneously. The closed loop method consists of two parts: part I, stereo matching and depth maps fusion; and part II, super-resolution. Under the guidance of the estimated depth information, the super-resolution problem can be formulated as an optimization problem. It can be solved approximately by a three-step method, which involves disparity-based pixel mapping, nonlocal construction and final fusion. Based on the super-resolution results, we can update the disparity maps and fuse them into a more reliable depth map. We repeat the loop several times until obtaining stable super-resolution results and depth maps simultaneously. The experimental results on public dataset show that the proposed method can achieve high-quality performance at different scale factors.", "journal": "MACHINE VISION AND APPLICATIONS", "category": "Computer Science, Artificial Intelligence; Computer Science, Cybernetics; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000418291400014", "keywords": "Adaptive optimal control; frequency selective learning (FSL); linear quadratic regulator (LQR); model reference adaptive control (MRAC)", "title": "Adaptive Optimal Control Using Frequency Selective Information of the System Uncertainty With Application to Unmanned Aircraft", "abstract": "A new efficient adaptive optimal control approach is presented in this paper based on the indirect model reference adaptive control (MRAC) architecture for improvement of adaptation and tracking performance of the uncertain system. The system accounts here for both matched and unmatched unknown uncertainties that can act as plant as well as input effectiveness failures or damages. For adaptation of the unknown parameters of these uncertainties, the frequency selective learning approach is used. Its idea is to compute a filtered expression of the system uncertainty using multiple filters based on online instantaneous information, which is used for augmentation of the update law. It is capable of adjusting a sudden change in system dynamics without depending on high adaptation gains and can satisfy exponential parameter error convergence under certain conditions in the presence of structured matched and unmatched uncertainties as well. Additionally, the controller of the MRAC system is designed using a new optimal control method. This method is a new linear quadratic regulator-based optimal control formulation for both output regulation and command tracking problems. It provides a closed-form control solution. The proposed overall approach is applied in a control of lateral dynamics of an unmanned aircraft problem to show its effectiveness.", "journal": "IEEE TRANSACTIONS ON CYBERNETICS", "category": "Automation & Control Systems; Computer Science, Artificial Intelligence; Computer Science, Cybernetics", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000327647500010", "keywords": "Analysis of variance; Bayesian methods; nonlinear filters; parametric methods", "title": "Parametric Bayesian Filters for Nonlinear Stochastic Dynamical Systems: A Survey", "abstract": "Nonlinear stochastic dynamical systems are commonly used to model physical processes. For linear and Gaussian systems, the Kalman filter is optimal in minimum mean squared error sense. However, for nonlinear or non-Gaussian systems, the estimation of states or parameters is a challenging problem. Furthermore, it is often required to process data online. Therefore, apart from being accurate, the feasible estimation algorithm also needs to be fast. In this paper, we review Bayesian filters that possess the aforementioned properties. Each filter is presented in an easy way to implement algorithmic form. We focus on parametric methods, among which we distinguish three types of filters: filters based on analytical approximations (extended Kalman filter, iterated extended Kalman filter), filters based on statistical approximations (unscented Kalman filter, central difference filter, Gauss-Hermite filter), and filters based on the Gaussian sum approximation (Gaussian sum filter). We discuss each of these filters, and compare them with illustrative examples.", "journal": "IEEE TRANSACTIONS ON CYBERNETICS", "category": "Automation & Control Systems; Computer Science, Artificial Intelligence; Computer Science, Cybernetics", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000320351800006", "keywords": "Survival analysis; Genetic algorithms; Artificial neural networks; Concordance index; Breast cancer recurrence", "title": "Training artificial neural networks directly on the concordance index for censored data using genetic algorithms", "abstract": "Objective: The concordance index (c-index) is the standard way of evaluating the performance of prognostic models in the presence of censored data. Constructing prognostic models using artificial neural networks (ANNs) is commonly done by training on error functions which are modified versions of the c-index. Our objective was to demonstrate the capability of training directly on the c-index and to evaluate our approach compared to the Cox proportional hazards model. Method: We constructed a prognostic model using an ensemble of ANNs which were trained using a genetic algorithm. The individual networks were trained on a non-linear artificial data set divided into a training and test set both of size 2000, where 50% of the data was censored. The ANNs were also trained on a data set consisting of 4042 patients treated for breast cancer spread over five different medical studies, 2/3 used for training and 1/3 used as a test set. A Cox model was also constructed on the same data in both cases. The two models' c-indices on the test sets were then compared. The ranking performance of the models is additionally presented visually using modified scatter plots. Results: Cross validation on the cancer training set did not indicate any non-linear effects between the covariates. An ensemble of 30 ANNs with one hidden neuron was therefore used. The ANN model had almost the same c-index score as the Cox model (c-index = 0.70 and 0.71, respectively) on the cancer test set. Both models identified similarly sized low risk groups with at most 10% false positives, 49 for the ANN model and 60 for the Cox model, but repeated bootstrap runs indicate that the difference was not significant. A significant difference could however be seen when applied on the non-linear synthetic data set. In that case the ANN ensemble managed to achieve a c-index score of 0.90 whereas the Cox model failed to distinguish itself from the random case (c-index = 0.49). Conclusions: We have found empirical evidence that ensembles of ANN models can be optimized directly on the c-index. Comparison with a Cox model indicates that near identical performance is achieved on a real cancer data set while on a non-linear data set the ANN model is clearly superior. (C) 2013 Elsevier B.V. All rights reserved.", "journal": "ARTIFICIAL INTELLIGENCE IN MEDICINE", "category": "Computer Science, Artificial Intelligence; Engineering, Biomedical; Medical Informatics", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000423639400010", "keywords": "Big damping nonholonomic constraints; Underactuated control; Energy pumping; Turn-back angle feedback; Lyapunov stability theory; Position-orientation control", "title": "Underactuated control of a bionic-ape robot based on the energy pumping method and big damping condition turn-back angle feedback", "abstract": "This paper studies a reliable control strategy for a bionic-ape brachiation robot with dual-arm hands, a four-link brachiate mechanism, which swings and grasps branches like a gibbon. Based on the analysis of the brachiation of gibbons and humans, the big damping nonholonomic constraints model is introduced. A control strategy which combines the energy pumping control with the big damping turn-back angle feedback control is proposed. The grasping motion was divided into several processes: the swing-up self-starting phase, the energy rising phase, the transition phase and the grasping phase. A self-starting controller is designed for swing-up motion from a downward stable position. In the energy rising phase, an energy pumping control law is deduced based on Lyapunov stability theory, and the singularity and convergence of the system are analyzed. The system energy is calculated periodically to determine if the robot gets enough energy for grasping, so that it could shift into the transition phase. A position orientation control method is adopted to realize that the gripper grasps target bars in the desired position, orientation and speed. By means of ADAMS-MATLAB co-simulation, the bionic-ape robot achieves smooth swing-grasp motions under different heights and horizontal distances. The simulation results indicate that this method has advantages of universality and reliability in a grasping target bar. Therefore, the effectiveness of the proposed control strategy is validated. (C) 2017 Elsevier B.V. All rights reserved.", "journal": "ROBOTICS AND AUTONOMOUS SYSTEMS", "category": "Automation & Control Systems; Computer Science, Artificial Intelligence; Robotics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000433011400019", "keywords": "Earthquake; Crustal motion velocities; Spatial prediction; Variogram; Kriging; Fuzzy logic; Adaptive fuzzy neural network", "title": "A New Spatial Algorithm Based on Adaptive Fuzzy Neural Network for Prediction of Crustal Motion Velocities in Earthquake Research", "abstract": "In earthquake studies, different methods are used in modeling of the crustal motions. In case of obscurity data structure, different approaches are needed in solving motion problems. In this paper, a new spatial algorithm has been developed which is based on adaptive fuzzy neural network (AFNN) approach for the prediction of the crustal motion velocities. In order to find the fuzzy class numbers regarding the network model formed by the fuzzification of the studied area, subtractive clustering algorithm is used. In determining the membership function, utilization of the variogram function which models the relationship that depends on distance among spatial data is proposed. The Marmara Region, Turkey, is used as the case for this study. In order to evaluate the performance of the approach, the kriging method is also utilized in the prediction and the results obtained from both methods are compared based on the mean-square-error criteria. It is observed that the AFNN approach yields results which are as effective as those of kriging. Consequently, it is shown that the AFNN approach will contribute to earthquake studies.", "journal": "INTERNATIONAL JOURNAL OF FUZZY SYSTEMS", "category": "Automation & Control Systems; Computer Science, Artificial Intelligence; Computer Science, Information Systems", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000324934400008", "keywords": "Big data; skyline; pruning; SSPL", "title": "Efficient Skyline Computation on Big Data", "abstract": "Skyline is an important operation in many applications to return a set of interesting points from a potentially huge data space. Given a table, the operation finds all tuples that are not dominated by any other tuples. It is found that the existing algorithms cannot process skyline on big data efficiently. This paper presents a novel skyline algorithm SSPL on big data. SSPL utilizes sorted positional index lists which require low space overhead to reduce I/O cost significantly. The sorted positional index list L-j is constructed for each attribute A(j) and is arranged in ascending order of A(j). SSPL consists of two phases. In phase 1, SSPL computes scan depth of the involved sorted positional index lists. During retrieving the lists in a round-robin fashion, SSPL performs pruning on any candidate positional index to discard the candidate whose corresponding tuple is not skyline result. Phase 1 ends when there is a candidate positional index seen in all of the involved lists. In phase 2, SSPL exploits the obtained candidate positional indexes to get skyline results by a selective and sequential scan on the table. The experimental results on synthetic and real data sets show that SSPL has a significant advantage over the existing skyline algorithms.", "journal": "IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000352608000003", "keywords": "Hidden databases; checkbox; aggregate estimation; weight adjustment", "title": "Aggregate Estimation in Hidden Databases with Checkbox Interfaces", "abstract": "A large number of web data repositories are hidden behind restrictive web interfaces, making it an important challenge to enable data analytics over these hidden web databases. Most existing techniques assume a form-like web interface which consists solely of categorical attributes (or numeric ones that can be discretized). Nonetheless, many real-world web interfaces (of hidden databases) also feature checkbox interfaces-e.g., the specification of a set of desired features, such as A/C, navigation, etc., for a car-search website like Yahoo! Autos. We find that, for the purpose of data analytics, such checkbox-represented attributes differ fundamentally from the categorical/numerical ones that were traditionally studied. In this paper, we address the problem of data analytics over hidden databases with checkbox interfaces. Extensive experiments on both synthetic and real datasets demonstrate the accuracy and efficiency of our proposed algorithms.", "journal": "IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000335514900003", "keywords": "Defect causal analysis; root cause classification; in-process feedback; team capability index; software measurement", "title": "AN ANALYSIS OF THE ROOT CAUSES OF DEFECTS INJECTED INTO THE SOFTWARE BY THE SOFTWARE TEAM: AN INDUSTRIAL STUDY OF THE DISTRIBUTED HEALTH-CARE SYSTEM", "abstract": "A root cause is a source of software defect, whose removal decreases or removes the defect. A root cause of software defect is injected into the software by software engineers during the development process. One of the main concerns of the software team leader, such as the project manager, is to determine who injected various root causes of the defects into the software and when these have been injected. In this paper, a cost-benefit scheme is presented, which allows a software team to determine skill weakness and improve team capability. The scheme provides effective in-process feedback based on the causal analysis of software defects. The proposed analysis scheme includes orthogonal root cause definitions, role-based root cause types, and gradational correction actions. In the experiment, the projects of a distributed health-care system are used to verify the efficiency of the proposed scheme. The results show that the root cause ratios (RCR) are 33.8%, 30.6%, 21.9%, 10.7%, and 3.0% in design, implementation, analysis, business and deployment, respectively. The defects in the projects mainly occurred during the design and implementation phases of the projects. Correction activities to enhance the designers' skills, such as exception handling (40.5%) and DB/data schema (25.0%), are the top priorities that must be addressed by the software team. The findings can help the team leader to determine methods to improve these weaknesses.", "journal": "INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING", "category": "Computer Science, Artificial Intelligence; Computer Science, Software Engineering; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000381112900006", "keywords": "Aspect-oriented programming; code smells; object-oriented programming; characterization of cross-cutting concerns; re-factoring; formal concept analysis", "title": "Aspect Oriented Re-engineering of Legacy Software Using Cross-Cutting Concern Characterization and Significant Code Smells Detection", "abstract": "Although object-oriented programming (OOP) methodologies immensely promote reusable and well-factored decomposition of complex source code, legacy software systems often show symptoms of deteriorating design over time due to lack of maintenance. Software systems may have different business and application contexts, but most of these systems require similar maintenance mechanism of understanding, analysis and transformation. As a consequence, intensive re-engineering efforts based on the model driven approach can be effective ensuring that best practices are followed during maintenance and eventually reducing the development cost. In this paper, we suggest detailed framework of re-engineering which includes: (i) rigorous and automated source code analysis technique for identification, characterization and prioritization of most prominent and threatening design flaws in legacy software, (ii) migration of existing the code to aspect-oriented programming (AOP) code by exploiting current state of art for aspect mining mechanism and incorporating behavioral knowledge of cross-cutting concerns. To exemplify how the approach works a case study has been conducted to experimentally validate the idea and analyze the effect of process on specific software quality spectrum. An explicit analysis of prevalent work on the subject and their critical reviews are also presented to further enhance the recognition of proposed re-engineering framework.", "journal": "INTERNATIONAL JOURNAL OF SOFTWARE ENGINEERING AND KNOWLEDGE ENGINEERING", "category": "Computer Science, Artificial Intelligence; Computer Science, Software Engineering; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000332817900008", "keywords": "Cryptographic key management; secret sharing; key recovery; KRAs", "title": "High-Availability Decentralized Cryptographic Multi-Agent Key Recovery", "abstract": "This paper proposes two versions for the implementation of a novel High-Availability Decentralized cryptographic Multi-agent Key Recovery System (HADM-KRS) that do not require a key recovery centre: HADM-KRSv1 and HADM-KRSv2. They have been enhanced from our previous work and entirely comply with the latest key recovery system in the National Institute of Standards and Technologies (NIST's) framework. System administrators can specify the minimum number of Key Recovery Agents (KRAs) according to security policies and requirements while maintaining compliance with legal requirements. This feature is achieved by applying the concept of secret sharing and power set to distribute the session key to participating KRAs. It uses the principle of secure session key management with an appropriate design of key recovery function. The system is designed to achieve high availability despite the failure of some KRAs. The performance evaluation results show that the proposed systems incur little processing times. They provide a security platform with good performance, fault tolerance, and robustness in terms of secrecy and availability.", "journal": "INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000320493400036", "keywords": "Information federation; Service orientation; Semantic web; Information security", "title": "Secure federation of semantic information services", "abstract": "fundamental challenge for product-lifecycle management in collaborative value networks is to utilize the vast amount of product information available from heterogeneous sources in order to improve business analytics, decision support, and processes. This becomes even more challenging if those sources are distributed across multiple organizations. Federations of semantic information services, combining service-orientation and semantic technologies, provide a promising solution for this problem. However, without proper measures to establish information security, companies will be reluctant to join an information federation, which could lead to serious adoption barriers. Following the design science paradigm, this paper presents general objectives and a process for designing a secure federation of semantic information services. Furthermore, new as well as established security measures are discussed. Here, our contributions include an access-control enforcement system for semantic information services and a process for modeling access-control policies across organizations. In addition, a comprehensive security architecture is presented. An implementation of the architecture in the context of an application scenario and several performance experiments demonstrate the practical viability of our approach. (C) 2012 Elsevier B.V. All rights reserved.", "journal": "DECISION SUPPORT SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems; Operations Research & Management Science", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000325980900005", "keywords": "Communication delays; input uncertainties; neural network; teleoperation", "title": "Neural-Adaptive Control of Single-Master-Multiple-Slaves Teleoperation for Coordinated Multiple Mobile Manipulators With Time-Varying Communication Delays and Input Uncertainties", "abstract": "In this paper, adaptive neural network control is investigated for single-master-multiple-slaves teleoperation in consideration of time delays and input dead-zone uncertainties for multiple mobile manipulators carrying a common object in a cooperative manner. Firstly, concise dynamics of teleoperation systems consisting of a single master robot, multiple coordinated slave robots, and the object are developed in the task space. To handle asymmetric time-varying delays in communication channels and unknown asymmetric input dead zones, the nonlinear dynamics of the teleoperation system are transformed into two subsystems through feedback linearization: local master or slave dynamics including the unknown input dead zones and delayed dynamics for the purpose of synchronization. Then, a model reference neural network control strategy based on linear matrix inequalities (LMI) and adaptive techniques is proposed. The developed control approach ensures that the defined tracking errors converge to zero whereas the coordination internal force errors remain bounded and can be made arbitrarily small. Throughout this paper, stability analysis is performed via explicit Lyapunov techniques under specific LMI conditions. The proposed adaptive neural network control scheme is robust against motion disturbances, parametric uncertainties, time-varying delays, and input dead zones, which is validated by simulation studies.", "journal": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Hardware & Architecture; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000362417500010", "keywords": "image restoration; ISTA algorithm; FISTA algorithm; convergence; computational cost; peak signal-to-noise ratio", "title": "An Improved Fast Iterative Shrinkage Thresholding Algorithm for Image Deblurring", "abstract": "An improved fast iterative shrinkage thresholding algorithm (IFISTA) for image deblurring is proposed. The IFISTA algorithm uses a positive definite weighting matrix in the gradient function of the minimization problem of the known fast iterative shrinkage thresholding (FISTA) image restoration algorithm. A convergence analysis of the IFISTA algorithm shows that due to the weighting matrix, the IFISTA algorithm has an improved convergence rate and improved restoration capability of the unknown image over that of the FISTA algorithm. The weighting matrix is predetermined and fixed, and hence, like the FISTA algorithm, the IFISTA algorithm requires only one matrix vector product operation in each iteration. As a result, the computational burden per iteration of the IFISTA algorithm remains the same as in the FISTA algorithm. Numerical examples are presented that demonstrate the improved performance of the IFISTA algorithm over that of the FISTA and iterative shrinkage thresholding (ISTA) algorithms in terms of the convergence speed and the peak signal-to-noise ratio.", "journal": "SIAM JOURNAL ON IMAGING SCIENCES", "category": "Computer Science, Artificial Intelligence; Computer Science, Software Engineering; Mathematics, Applied; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000391852100014", "keywords": "Teichmfiller extremal mapping; Teichmfiller metric; point clouds; shape analysis; face recognition; quasi-conformal theory", "title": "TEMPO: Feature-Endowed Teichmiiller Extremal Mappings of Point Clouds", "abstract": "In recent decades, the use of three-dimensional point clouds has been widespread in the computer industry. The development of techniques for analyzing point clouds is increasingly important. In particular, mapping of point clouds has been a challenging problem. In this paper, we develop a discrete analogue of the Teichmfiller extremal mappings, which guarantees uniform conformality distortions on point cloud surfaces. Based on the discrete analogue, we propose a novel method called TEMPO for computing Teichmfiller extremal mappings between feature-endowed point clouds. Using our proposed method, the Teichmfiller metric is introduced for evaluating the dissimilarity of point clouds. Consequently, our algorithm enables accurate recognition and classification of point clouds. Experimental results demonstrate the effectiveness of our proposed method.", "journal": "SIAM JOURNAL ON IMAGING SCIENCES", "category": "Computer Science, Artificial Intelligence; Computer Science, Software Engineering; Mathematics, Applied; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000400228900011", "keywords": "Gaussian mutation; Cauchy mutation; Adaptive Levy mutation; Mean mutation; Adaptive mean mutation; Combined mutation; Flower pollination algorithm", "title": "Application of mutation operators to flower pollination algorithm", "abstract": "Flower pollination algorithm (FPA) is a recent addition to the field of nature inspired computing. The algorithm has been inspired from the pollination process in flowers and has been applied to a large spectra of optimization problems. But it has certain drawbacks which prevents its applications as a standard algorithm. This paper proposes new variants of FPA employing new mutation operators, dynamic switching and improved local search. A comprehensive comparison of proposed algorithms has been done for different population sizes for optimizing seventeen benchmark problems. The best variant among these is adaptive-Levy flower pollination algorithm (ALFPA) which has been further compared with the well-known algorithms like artificial bee colony (ABC), differential evolution (DE), firefly algorithm (FA), bat algorithm (BA) and grey wolf optimizer (GWO). Numerical results show that ALFPA gives superior performance for standard benchmark functions. The algorithm has also been subjected to statistical tests and again the performance is better than the other algorithms. (c) 2017 Elsevier Ltd. All rights reserved.", "journal": "EXPERT SYSTEMS WITH APPLICATIONS", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000342250300004", "keywords": "Social network; Service recommendation; Trust-enhanced; Random walk", "title": "Social network-based service recommendation with trust enhancement", "abstract": "Given the increasing applications of service computing and cloud computing, a large number of Web services are deployed on the Internet, triggering the research of Web service recommendation. Despite of service QoS, the use of user feedback is becoming the current trend in service recommendation. Likewise in traditional recommender systems, sparsity, cold-start and trustworthiness are major issues challenging service recommendation in adopting similarity-based approaches. Meanwhile, with the prevalence of social networks, nowadays people become active in interacting with various computers and users, resulting in a huge volume of data available, such as service information, user-service ratings, interaction logs, and user relationships. Therefore, how to incorporate the trust relationship in social networks with user feedback for service recommendation motivates this work. In this paper, we propose a social network-based service recommendation method with trust enhancement known as RelevantTrustWalker. First, a matrix factorization method is utilized to assess the degree of trust between users in social network. Next, an extended random walk algorithm is proposed to obtain recommendation results. To evaluate the accuracy of the algorithm, experiments on a real-world dataset are conducted and experimental results indicate that the quality of the recommendation and the speed of the method are improved compared with existing algorithms. (C) 2014 Elsevier Ltd. All rights reserved.", "journal": "EXPERT SYSTEMS WITH APPLICATIONS", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000322051600041", "keywords": "Automated negotiation; BATNA; Resistance force; Concession force; Search; Outside options", "title": "Automated negotiation in open and distributed environments", "abstract": "Automated negotiation is one of the most common approaches used to make decisions and manage disputes between computational entities leading them to optimal agreements. Many existing works tackle single-issue negotiations and the negotiation environment is assumed to be static so that the agents can make decisions based solely on the proposals of the counterparts and their own fixed parameters. Most real-world scenarios, however, involve complex domains and dynamic environments. In such cases, it is no longer sufficient to consider negotiation as an isolated activity in a static environment. Therefore, a more general framework for automated negotiation is needed in which the negotiation agents can be very flexible and adaptive. In this paper, we describe a generic framework for automated negotiation, which captures descriptively the social dynamics of the negotiation process. The proposed framework enables the agents to behave responsively to the changes in the environment. Their strategies can adapt as the conditions outside of the negotiation change to ensure that their decisions remain rational. And the agents are proactive and responsive by searching for options, which are outside of the negotiation and which may improve their outcomes. The key ideas and the overall system architecture together with a specific negotiation instance in a basic bilateral setting are described, along with two illustrative examples. The first example is in the context of e-commerce, and the second example is an application scenario of service level agreement negotiation in service computing. We also describe a prototypical implementation of the proposed negotiation framework. (C) 2013 Elsevier Ltd. All rights reserved.", "journal": "EXPERT SYSTEMS WITH APPLICATIONS", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000315509600002", "keywords": "Differential pulse coded modulation; Capsule endoscopic application; Golomb coding; Image compression", "title": "Subsample-based image compression for capsule endoscopy", "abstract": "The key design challenge in capsule endoscopic system is to reduce the area and power consumption while maintaining acceptable video quality. In this paper, a subsample-based image compressor for such endoscopic application is presented. The algorithm is developed around some special features of endoscopic images. It consists of a differential pulse coded modulation followed by Golomb-rice coding. Based on the nature of endoscopic images, several subsampling schemes on the chrominance components are applied. This scheme is particularly suitable to work with any commercial low-power image sensors that outputs image pixels in a raster scan fashion, eliminating the need of memory buffer, as well as temporary storage (as needed in transform coding schemes). An image corner clipping algorithm is also presented. The reconstructed images have been verified by medical doctors for acceptability. The proposed algorithm has a very low complexity and is suitable for the VLSI implementation. Compared to other transform-based algorithms targeted to capsule endoscopy, the proposed raster-scan-based scheme performs very strongly with a compression ratio of 80% and a very high reconstruction PSNR (over 45 dB).", "journal": "JOURNAL OF REAL-TIME IMAGE PROCESSING", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000326028200008", "keywords": "brightness; gradient methods; image fusion; stochastic processes; Poisson solver; gradient domain; luminance reversion artifact; real scene luminance; input image exposure parameter; multiexposure image fusion; high contrast scene; single still image fusion; layered-based exposure fusion algorithm", "title": "Layered-based exposure fusion algorithm", "abstract": "Owing to the limitation of dynamic range, a single still image is usually insufficient to describe a high contrast scene. Fusing multi-exposure images of the same scene can produce a resulting image with details both in the bright and the dark regions. However, they may be sensitive to the exposure parameters of the input images. In this study, a global layer is introduced to improve the robustness of the fusion method. The global layer is employed to preserve the overall luminance of a real scene and avoid possible luminance reversion artefacts. Then, details are recovered in the gradient domain by a Poisson solver. Experimental results show the superior performance of our approach in terms of robustness and details preservation.", "journal": "IET IMAGE PROCESSING", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000335101300015", "keywords": "Computational anatomy; Deformation momenta; Kernel Partial Least Squares (PLS); Alzheimer's disease; Prediction", "title": "Quantifying anatomical shape variations in neurological disorders", "abstract": "We develop a multivariate analysis of brain anatomy to identify the relevant shape deformation patterns and quantify the shape changes that explain corresponding variations in clinical neuropsychological measures. We use kernel Partial Least Squares (PLS) and formulate a regression model in the tangent space of the manifold of diffeomorphisms characterized by deformation momenta. The scalar deformation momenta completely encode the diffeomorphic changes in anatomical shape. In this model, the clinical measures are the response variables, while the anatomical variability is treated as the independent variable. To better understand the \"shape-clinical response\" relationship, we also control for demographic confounders, such as age, gender, and years of education in our regression model. We evaluate the proposed methodology on the Alzheimer's Disease Neuroimaging Initiative (ADNI) database using baseline structural MR imaging data and neuropsychological evaluation test scores. We demonstrate the ability of our model to quantify the anatomical deformations in units of clinical response. Our results also demonstrate that the proposed method is generic and generates reliable shape deformations both in terms of the extracted patterns and the amount of shape changes. We found that while the hippocampus and amygdala emerge as mainly responsible for changes in test scores for global measures of dementia and memory function, they are not a determinant factor for executive function. Another critical finding was the appearance of thalamus and putamen as most important regions that relate to executive function. These resulting anatomical regions were consistent with very high confidence irrespective of the size of the population used in the study. This data-driven global analysis of brain anatomy was able to reach similar conclusions as other studies in Alzheimer's disease based on predefined ROIs, together with the identification of other new patterns of deformation. The proposed methodology thus holds promise for discovering new patterns of shape changes in the human brain that could add to our understanding of disease progression in neurological disorders. (C) 2014 Elsevier B.V. All rights reserved.", "journal": "MEDICAL IMAGE ANALYSIS", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Engineering, Biomedical; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000401383800011", "keywords": "Intraoperative tool tracking; Interventional radiology; Fluoroscopy; Guidewire", "title": "Robust guidewire tracking under large deformations combining segment-like features (SEGlets)", "abstract": "Robust tracking of interventional tools, such as guidewires and catheters, in X-ray fluoroscopic video sequences has a wide range of clinical applications for endovascular procedures. Thus far, the tracking is usually achieved by finding the optimal displacement of the control points of a spline, which models the guidewire, between consecutive frames. The displacement of the control points is typically driven by a data term and smoothed by a regularization term. In the presence of large deformation and changes in length of the tool, the current tracking methods may fail to recover the guidewire motion. This can occur because of the limitation of the data and regularization terms, and the absence of an explicit solution for coping with elongations of the guidewire. The purpose of this paper is to present an algorithm that can robustly track guidewires under these challenging conditions. The algorithm is based on two main contributions: (a) new robust features termed SEGlets for segment-like features are introduced to overcome the limitations of the current data terms; (b) a tracking formulation based on the generation of tracking hypotheses by organizing the SEGlets in plausible guidewire shapes. The proposed method allows high flexibility of the guidewire between consecutive frames in contrast to the spline model, which can suffer from the limitations of the regularization terms. Furthermore, the technique models elongations of the guidewire which makes it possible for robust tracking under motion. A tool model which is recursively updated by employing a Kalman filter, is also proposed for modelling the regularization term. A detailed evaluation and a comparative study with three state-of-the-art guidewire tracking methods have been performed to demonstrate the potential clinical value of the technique. The proposed method achieves an overall guidewire tracking precision of 2.40 pixels, tip precision of 25.55 pixels, false tracking rate of 5.73%, missing tracking rate of 9.69%, and F-1 score of 0.92. The implementation of the proposed technique and the three tracking methods will be made publicly available as software libraries. (C) 2017 Published by Elsevier B.V.", "journal": "MEDICAL IMAGE ANALYSIS", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Engineering, Biomedical; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000379631100014", "keywords": "Space-time delay neural network; Space-time autocorrelation; London road traffic network; Travel time prediction", "title": "A space-time delay neural network model for travel time prediction", "abstract": "Research on space-time modelling and forecasting has focused on integrating space-time autocorrelation into statistical models to increase the accuracy of forecasting. These models include space-time autoregressive integrated moving average (STARIMA) and its various extensions. However, they are inadequate for the cases when the correlation between data is dynamic and heterogeneous, such as traffic network data. The aim of the paper is to integrate spatial and temporal autocorrelations of road traffic network by developing a novel space-time delay neural network (STDNN) model that capture the autocorrelation locally and dynamically. Validation of the space-time delay neural network is carried out using real data from London road traffic network with 22 links by comparing benchmark models such as Naive, ARIMA, and STARIMA models. Study results show that STDNN outperforms the Naive, ARIMA, and STARIMA models in prediction accuracy and has considerable advantages in travel time prediction. (C) 2016 Elsevier Ltd. All rights reserved.", "journal": "ENGINEERING APPLICATIONS OF ARTIFICIAL INTELLIGENCE", "category": "Automation & Control Systems; Computer Science, Artificial Intelligence; Engineering, Multidisciplinary; Engineering, Electrical & Electronic", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000316428400012", "keywords": "Multivariate; Sparse; Truncation; Variable selection; Partial least squares; Relevant components", "title": "Distribution based truncation for variable selection in subspace methods for multivariate regression", "abstract": "Analysis of data containing a vast number of features, but only a limited number of informative ones, requires methods that can separate true signal from noise variables. One class of methods attempting this is the sparse partial least squares methods for regression (sparse PLS). This paper aims at improving the theoretical foundation, speed and robustness of such methods. A general justification of truncation of PLS loading weights is achieved through distribution theory and the central limit theorem. We also introduce a quick plug-in based truncation procedure based on a novel application of theory intended for analysis of variance for experiments without replicates. The result is a versatile and intuitive method that performs component-wise variable selection very efficiently and in a less ad hoc manner than existing methods. Prediction performance is on par with existing methods, while robustness is ensured through a better theoretical foundation. (C) 2013 Elsevier B.V. All rights reserved.", "journal": "CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS", "category": "Automation & Control Systems; Chemistry, Analytical; Computer Science, Artificial Intelligence; Instruments & Instrumentation; Mathematics, Interdisciplinary Applications; Statistics & Probability", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000346398500012", "keywords": "Multimode; Process monitoring; Clustering; Model alignment", "title": "Neighborhood based global coordination for multimode process monitoring", "abstract": "A novel framework named neighborhood based global coordination (NBGC) for model alignment and multimode process monitoring is proposed in this paper. To identify the different patterns in the training database, a new clustering method is derived by utilizing the serial correlations between adjacent samples. With local outlier probability (LoOP) which can exhibit the novelty of the augmented samples, the fracture parts between multiple modes can be located. Then, an arrangement approach is conducted to piece together the similar but disconnecting segments of samples. Next, conventional principal component analysis (PCA) is applied for each separated cluster. Different from the traditional approaches where process monitoring will be performed individually and results from all local models will be summarized, the proposed method aims at involving the inter-mode correlations by aligning the local models together into a global model. A new objective function is proposed to ensure that both the local and nonlocal information can be included. Finally the utility and feasibility of NBGC are demonstrated through a numerical example and TE benchmark process. (C) 2014 Elsevier B.V. All rights reserved.", "journal": "CHEMOMETRICS AND INTELLIGENT LABORATORY SYSTEMS", "category": "Automation & Control Systems; Chemistry, Analytical; Computer Science, Artificial Intelligence; Instruments & Instrumentation; Mathematics, Interdisciplinary Applications; Statistics & Probability", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000739778900004", "keywords": "Motor imagery; Real-time brain computer interface; Convolutional neural network; Common spatial patterns; Continuous wavelet transform", "title": "A CNN-based modular classification scheme for motor imagery using a novel EEG sampling protocol suitable for IoT healthcare systems", "abstract": "The implementation of brain-computer interfaces (BCI) for real-time has become a paramount technology. Implementation of real-time BCI systems requires of methodologies that achieve high performance on classification over general brain signals of different subjects. Therefore, this work presents two simple and efficient methodologies to classify two and four motor imageries. The methodology to classify two motor imageries (MC-TM) includes an analysis of feature extraction methods based on spatial patterns and time-frequency transforms; and a convolutional neural network that preserves the information of the magnitude in the frequency bands of the sensorimotor rhythms (CNN-PIM). Besides, the methodology to classify four motor imageries (MC-FM) includes a modular classification scheme that instances 6 CNN-PIM; a new algorithm that uses the output of the softmax of each CNN-PIM to enhance the performance of the MC-FM methodology; an electroencephalogram sampling protocol that includes a specific procedure for 4 MIs classes; and a new dataset with the brain signals of 15 subjects. The MC-TM methodology achieved an accuracy of 94.44 +/- 02.18% evaluated in BCI Competition IV dataset 2a (BCI-IV-2a), and accuracy of 97.67 +/- 02.06% when evaluated in the EEGdataset. Meanwhile, the MC-FM achieved accuracies of 91.37 +/- 3.29% and 86.48 +/- 4.74% when evaluated in BCI-IV-2a and in the proposed dataset, respectively. These results situate our methodologies in a competitive position in comparison with the state-of-the-art methods. Moreover, the approximate processing time that the MC-FM methodology takes to classify EEG signals is 270 ms. Thus, it is suitable to be implemented in a real-time BCI system.", "journal": "NEURAL COMPUTING & APPLICATIONS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000831233800030", "keywords": "Cloud model; fuzzy C-mean clustering; regret theory; large group decision-making", "title": "A method based on cloud model and FCM clustering for risky large group decision making", "abstract": "A risky large group decision-making method based on FCM clustering and cloud models is proposed for risky large group decision-making problems with linguistic evaluation scales, unknown attribute weights, and many decision members with unknown weights, considering the psychological behavioral characteristics of decision makers' regret avoidance. The method first uses the golden partition method to improve the cloud model to transform the uncertain linguistic evaluation matrix into a comprehensive cloud model, which quantifies the fuzziness and randomness of linguistic values. The cloud model expectation values are then extracted to determine the attribute weights using the entropy weighting method. Secondly, the three numerical features of the cloud model are extracted as sample features for FCM clustering to obtain the decision maker's preference clustering information, and the initial weights of decision-makers are determined according to the majority principle, which improves the existing studies that simply use the expected value of the cloud model for clustering analysis, ignoring the entropy and super entropy for portraying the ambiguity and randomness. On this basis, the Hamming distance is introduced to calculate the closeness to adjust the initial weights of decision-makers, improving the way that the weights of aggregation members are equally distributed in previous studies. Finally, considering the influence of the decision maker's psychological behavior on decision information in the risky decision-making process, regret theory is introduced to construct a decision maker's perceived utility matrix, which is combined with the decision maker's weights to determine and rank the combined perceived utility. Through comparison with existing methods, it is found that the proposed method of recalibration of decision-maker preference clustering, while considering the psychological behavior of decision-maker regret avoidance, not only solves the situation of large group decision making in which expert information is easily distorted but also satisfies the convenience of the calculation process and is more suitable for the situation where there are many decision-makers and their preferences are complicated.", "journal": "JOURNAL OF INTELLIGENT & FUZZY SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000670425000002", "keywords": "neural network; feedforward neural network; function approximation; fitness function; feasibility function; data preselection; data filtering; 2D manipulator; redundant manipulator; redundant manipulator kinematics; inverted kinematics; obstacle avoidance", "title": "NEURAL NETWORK FOR THE IDENTIFICATION OF A FUNCTIONAL DEPENDENCE USING DATA PRESELECTION", "abstract": "A neural network can be used in the identification of a given functional dependency. An undetermined problem (with more degrees of freedom) has to be converted to a determined one by adding other conditions. This is easy for a well-defined problem, described by a theoretical functional dependency; in this case, no identification (using a neural network) is necessary. The article describes how to apply a fitness (or a penalty) function directly to the data, before a neural network is trained. As a result, the trained neural network is near to the best possible solution according to the selected fitness function. In comparison to implementing the fitness function during the training of the neural network, the method described here is simpler and more reliable. The new method is demonstrated on the kinematics control of a redundant 2D manipulator.", "journal": "NEURAL NETWORK WORLD", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000806653700001", "keywords": "Semantic scene completion; 3D Vision; Semantic segmentation; Scene understanding; Scene reconstruction; Point cloud", "title": "3D Semantic Scene Completion: A Survey", "abstract": "Semantic scene completion (SSC) aims to jointly estimate the complete geometry and semantics of a scene, assuming partial sparse input. In the last years following the multiplication of large-scale 3D datasets, SSC has gained significant momentum in the research community because it holds unresolved challenges. Specifically, SSC lies in the ambiguous completion of large unobserved areas and the weak supervision signal of the ground truth. This led to a substantially increasing number of papers on the matter. This survey aims to identify, compare and analyze the techniques providing a critical analysis of the SSC literature on both methods and datasets. Throughout the paper, we provide an in-depth analysis of the existing works covering all choices made by the authors while highlighting the remaining avenues of research. SSC performance of the SoA on the most popular datasets is also evaluated and analyzed.", "journal": "INTERNATIONAL JOURNAL OF COMPUTER VISION", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000510955100010", "keywords": "Algorithms; Data processing; Hyperspectral image classification; Residual learning; Convolutional neural network", "title": "Cascaded dual-scale crossover network for hyperspectral image classification", "abstract": "In recent years, deep neural networks have exhibited numerous advantages in hyperspectral image classification (HIC). However, owing to the limited number of training samples of hyperspectral images (HSIs), the network structure should not be designed too deep to retard the overfitting phenomenon. This study proposes a cascaded dual-scale crossover network for HIC, which not only could extract rich features, but also does not make the network deeper. It continuously connects two different cascaded dual-scale crossover blocks, and automatically extracts the spectral-spatial features of HSIs. Moreover, for the limited training samples, the proposed network could flexibly capture more discriminant contextual features by using different spectral-size and spatial-size convolution kernels. Furthermore, two different cross-merge methods are designed to improve the information flow and contrast of the images to obtain parts of interest for the images. Two skip structures are also used for alleviating overfitting and accelerating the network training. Additional experimental results on some datasets, including Indian Pines, Kennedy Space Center, and University of Pavia, verify the feasibility of the proposed network. Namely, the classification accuracy of the proposed network is superior to that of other existing networks. (C) 2019 Elsevier B.V. All rights reserved.", "journal": "KNOWLEDGE-BASED SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000469348400018", "keywords": "System identification; TV-NARX model; Locally regularized orthogonal forward; regression; Non-stationary dynamical processes; Neuronal dynamics", "title": "Neural activity inspired asymmetric basis function TV-NARX model for the identification of time-varying dynamic systems", "abstract": "Inspired by the unique neuronal activities, a new time-varying nonlinear autoregressive with exogenous input (TV-NARX) model is proposed for modelling nonstationary processes. The NARX nonlinear process mimics the action potential initiation and the time-varying parameters are approximated with a series of postsynaptic current like asymmetric basis functions to mimic the ion channels of the inter-neuron propagation. In the model, the time-varying parameters of the process terms are sparsely represented as the superposition of a series of asymmetric alpha basis functions in an over-complete frame. Combining the alpha basis functions with the model process terms, the system identification of the TV-NARX model from observed input and output can equivalently be treated as the system identification of a corresponding time-invariant system. The locally regularised orthogonal forward regression (LROFR) algorithm is then employed to detect the sparse model structure and estimate the associated coefficients. The excellent performance in both numerical studies and modelling of real physiological signals showed that the TV-NARX model with asymmetric basis function is more powerful and efficient in tracking both smooth trends and capturing the abrupt changes in the time-varying parameters than its symmetric counterparts. Crown Copyright (C) 2019 Published by Elsevier B.V. All rights reserved.", "journal": "NEUROCOMPUTING", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000540481000004", "keywords": "Code-switching; Textual features; Factored language modeling", "title": "Novel textual features for language modeling of intra-sentential code-switching data", "abstract": "Code-switching refers to the frequent use of non-native language words/phrases by speakers while conversating in their native languages. Traditionally, for training a language model (LM) for code-switching data, one is required to tediously collect a large amount of text corpus in the respective code-switching domain. Alternately, we recently proposed a more viable approach that adapts an existing native LM to handle the code-switching data. In this work, we present our efforts for language modeling of code-switching data following both the traditional and the proposed approaches. The salient contributions of this paper includes: (i) creation of the Hindi-English code-switching text corpus, (ii) an improved parts-of-speech (POS) labeling scheme for accurate tagging of non-native words embedded in the code-switching data, and (iii) the proposal of a novel textual feature referred to as the code-switching location (CSL) feature, that allows LMs to predict the code-switching instances. The evaluation of the proposed features has been done on two code-switching datasets: Hindi-English and Mandarin-English. On experimental evaluation, a substantial reduction in the perplexity is achieved with the use of the improvised POS features. It is also observed that the proposed CSL features provide an independent and additive improvement over the POS features in terms of perplexity. (c) 2020 Elsevier Ltd. All rights reserved.", "journal": "COMPUTER SPEECH AND LANGUAGE", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000886972200098", "keywords": "Quantum key distribution; fuzzy logic; SQKD; Q-bits and quantum cryptography", "title": "Enhancing the cloud security using side channel attack free QKD with entangled fuzzy logic", "abstract": "As technology advances, it becomes easier to share large amounts of data over the internet. Cloud computing is one of the technologies that allows for easy data sharing over the internet. It is critical to provide security for this data when they are being shared across the internet. The security of data saved in cloud storage, as well as data transport and transmitting a key required to encrypt data between two parties, has been a source of concern for the industry, as a result of the growing use of cloud services in recent years. Collective attacks are significantly more powerful than individual strikes, according to our research. Despite the fact that additional research works were studied in the previous literature review, there are some study concerns for not correcting third-party data hacking. Therefore, this paper focuses on the design of Secured Quantum Key Distribution (SQKD) with Fuzzy logic to improve the security of the shared key. Quantum Key Distribution, Post Quantum Key Distribution, and the EPR Proto-col are technologies that increase the security of data sharing. We have incorporated the Secured Quantum Key Distribution (SQKD) with Fuzzy logic in our proposed work to improve the security of the shared key. The proposed systems include some additional characteristics in addition to the existing approaches. The proposed model uses shifting algorithms and the fuzzification procedure to assure the security of the secret key in the Fuzzification of Quantum Key approach. The experimental results states that the mean value of security losses in SFQ is 1.8306051, and the mean value of QKD is 14.6448416, with standard deviations of 1.7329 and 13.863 for SFQ and QKD, respectively.", "journal": "JOURNAL OF INTELLIGENT & FUZZY SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000788427900007", "keywords": "Liquidation; Foreign exchange rates; Stochastic process; Reinforcement learning; Stochastic dynamic programming", "title": "Optimal liquidation of foreign currencies when FX rates follow a generalised Ornstein-Uhlenbeck process", "abstract": "In this article, we consider the case of a multinational company realizing profits in a country other than its base country. The currencies used in the base and foreign countries are referred to as the domestic and foreign currencies respectively. For its quarterly and yearly financial statements, the company transfers its profits from a foreign bank account to a domestic bank account. Thus, the foreign currency liquidation task consists formally in exchanging over a period T a volume V of cash in the foreign currency f for a maximum volume of cash in the domestic currency d. The foreign exchange (FX) rate that prevails at time t is denoted X-d/f(t) and is defined as the worth of one unit of currency d in the currency f. We assume in this article that the natural logarithm of the FX rate x(t) = log X-d/f (t) follows a discrete generalized Ornstein-Uhlenbeck (OU) process, a process which generalizes the Brownian motion and mean-reverting processes. We also assume minimum and maximum volume constraints on each transaction. Foreign currency liquidation exposes the multinational company to financial risks and can have a significant impact on its final revenues, since FX rates are hard to predict and often quite volatile. We introduce a Reinforcement Learning (RL) framework for finding the liquidation strategy that maximizes the expected total revenue in the domestic currency. Despite the huge success of Deep Reinforcement Learning (DRL) in various domains in the recent past, existing DRL algorithms perform sub-optimally in this task and the Stochastic Dynamic Programming (SDP) algorithm - which yields the optimal strategy in the case of discrete state and action spaces - is rather slow. Thus, we propose here a novel algorithm that addresses both issues. Using SDP, we first determine numerically the optimal solution in the case where the state and decision variables are discrete. We analyse the structure of the computed solution and derive an analytical formula for the optimal trading strategy in the general continuous case. Quasi-optimal parameters of the analytical formula can then be obtained via grid search. This method, simply referred to as \"Estimated Optimal Liquidation Strategy\" (EOLS) is validated experimentally using the Euro as domestic currency and 3 foreign currencies, namely USD (US Dollar), CNY(Chinese Yuan) and GBP(Great British Pound). We introduce a liquidation optimality measure based on the gap between the average transaction rate captured by a strategy and the minimum rate over the liquidation period. The metric is used to compare the performance of EOLS to the Time Weighted Average Price (TWAP), SDP and the DRL algorithms Deep Q-Network (DQN) and Proximal Policy Optimization (PPO). The results show that EOLS outperforms TWAP by 54%, and DQN and PPO by 15 - 27%. EOLS runs in average 20 times faster than DQN and PPO. It has a performance on par with SDP but runs 44 times faster. EOLS is the first algorithm that utilizes a closed-form solution of the SDP strategy to achieve quasi-optimal decisions in a liquidation task. Compared with state-of-the-art DRL algorithms, it exhibits a simpler structure, superior performance and significantly reduced compute time, making EOLS better suited in practice.", "journal": "APPLIED INTELLIGENCE", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000488323000015", "keywords": "Classification; Data imputation; Gradient decent; Missing attribute value; Neural network", "title": "Imputation of missing data with neural networks for classification", "abstract": "We propose a mechanism to use data with missing values for designing classifiers which is different from predicting missing values for classification. Our imputation method uses an auto-encoder neural network. We make an innovative use of the training data without missing values to train the autoen-coder so that it is better equipped to predict missing values. It is a two-stage training scheme. Unlike most of the existing auto-encoder based methods which use a bottleneck layer for missing data handling, we justify and use a latent space of much higher dimension than that of the input. Now to design a classifier using a training set with missing values, we use the trained auto-encoder to predict missing values based on the hypothesis that a good choice for a missing value would be the one which can reconstruct itself via the auto-encoder. For this we make an initial guess of the missing value using the nearest neighbor rule and then refine the missing value minimizing the reconstruction error. We train several classifiers using the union of the imputed instances and the remaining training instances without missing values. We also train another classifier of the same type with the same configuration using the corresponding complete dataset. The performances of these classifiers are compared. We compare the proposed method with eight state-of-the-art imputation techniques using fourteen datasets and eight classification strategies. (C) 2019 Elsevier B.V. All rights reserved.", "journal": "KNOWLEDGE-BASED SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000849316900001", "keywords": "automatic parameter tuning; evolutionary computation; parameter control; reinforcement learning", "title": "General parameter control framework for evolutionary computation", "abstract": "This study proposes a general multiple parameter control framework by leveraging the ability of a reinforcement learning system to learn empirical knowledge for evolutionary computation. We design a feedback evaluation mechanism to define the rewards offered to agents, using which they can learn to choose appropriate parameters in formulated action sets. Moreover, a learning strategy is proposed to utilize the parameter selection-related knowledge that is gained during training episodes. Three famous evolutionary computation (EC) methods (i.e., particle swarm optimization, artificial bee colony, and differential evolution) are selected as the baseline algorithms and applied to the proposed framework. The aforementioned redesigned algorithms are tested on 15 common benchmark functions, as well as the CEC2017 benchmarks. In addition, the robustness of the algorithms is demonstrated through parameter sensitivity analysis. The results of the comparative analysis reveal that the three improved algorithms exhibit a faster overall convergence and higher accuracy than their state-of-the-art variants. It is also confirmed that our proposed framework has the capability to improve the performance of EC approach.", "journal": "INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["reinforcement learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000718134800007", "keywords": "Optimization; Chimp Optimization Algorithm; Swarm-intelligence; Levy Flight; Dynamic search", "title": "Dynamic Levy Flight Chimp Optimization", "abstract": "Background: The Chimp Optimization Algorithm (ChOA) is a hunting-based model and can be utilized as a set of optimization rules to tackle optimization problems. Due to agents' insufficient diversity in some complex problems, this algorithm is sometimes exposed to local optima stagnation. Objective: This paper introduces a Dynamic Levy Flight (DLF) technique to smoothly and gradually transit the search agents from the exploration phase to the exploitation phase. Methods: To investigate the efficiency of the DLFChOA, this paper evaluates the performance of DLFChOA on twenty-three standard benchmark functions, twenty challenging functions of CEC-2005, ten suit tests of IEEE CEC06-2019, and twelve real-world optimization problems. The results are compared to benchmark optimization algorithms, including CMA-ES, SHADE, ChOA, HGSO, LGWO and ALEP (as the best benchmark Levy-based algorithms), and eighteen state-of-the-art algorithms (as the winners of the CEC2019, the GECCO2019, and the SEMCCO2019). Result and conclusion: Among forty-three numerical test functions, DLFChOA and CMA-ES gain the first and second rank with thirty and eleven best results. In the 100-digit challenge, jDE100 with a score of 100 provides the best results, followed by DISHchain1e+12, and DLFChOA with a score of 85.68 is ranked fifth among eighteen state-of-the-art algorithms achieved the best score in seven out of ten problems. Finally, DLFChOA and CMA-ES respectively gain the best results in five and four real-world engineering problems. (C) 2021 Elsevier B.V. All rights reserved.", "journal": "KNOWLEDGE-BASED SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000527939100006", "keywords": "Structured argumentation; Dung's semantics; Assumption-based argumentation; Deductive argumentation; Defeasible reasoning; Inconsistency management", "title": "Simple contrapositive assumption-based argumentation frameworks", "abstract": "Assumption-based argumentation is one of the most prominent formalisms for logical (or structured) argumentation, with tight links to different forms of defeasible reasoning. In this paper we study the Dung semantics for extended forms of assumption-based argumentation frameworks (ABFs), based on any contrapositive propositional logic, and whose defeasible assumptions are expressed by arbitrary formulas in that logic. We show that unless the falsity propositional constant is part of the defeasible assumptions, the grounded and the well-founded semantics for ABFs lack most of the desirable properties they have in abstract argumentation frameworks (AAFs), and that for simple definitions of the contrariness operator and the attacks relations, preferred and stable semantics are reduced to naive semantics. We also show the redundancy of the closure condition in the standard definition of Dung's semantics for ABFs, and investigate the use of disjunctive attacks in this setting. Finally, we show some close relations of reasoning with ABFs to reasoning with maximally consistent sets of premises, and consider some properties of the induced entailments, such as being cumulative, preferential, or rational relations that satisfy non-interference. (C) 2020 Elsevier Inc. All rights reserved.", "journal": "INTERNATIONAL JOURNAL OF APPROXIMATE REASONING", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000587897100003", "keywords": "Traffic volume prediction; Attention mechanism; BDLSTM; Spatial-temporal dependence; External features", "title": "An effective dynamic spatiotemporal framework with external features information for traffic prediction", "abstract": "Traffic prediction is necessary for management departments to dispatch vehicles and for drivers to avoid congested roads. Many traffic forecasting methods based on deep learning have been proposed in recent years, and their main aim is to solve the problem of spatial dependencies and temporal dynamics. This paper proposes a useful dynamic model to predict the urban traffic volume by combining fully bidirectional LSTM, a complex attention mechanism, and external features, including weather conditions and events. First, we adopt bidirectional LSTM to obtain temporal dependencies of traffic volume dynamically in each layer, which is different from the hybrid methods combining bidirectional and unidirectional approaches. Second, we use a more elaborate attention mechanism to learn short-term and long-term periodic temporal dependencies. Finally, we collect weather condition and event information as external features to further improve the prediction precision. The experimental results show that the proposed model improves the prediction precision by approximately 3-7 percent on the NYC-Taxi and NYC-Bike datasets compared to the most recently developed method and is therefore a useful tool for urban traffic prediction.", "journal": "APPLIED INTELLIGENCE", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000570466300008", "keywords": "Approximate nearest neighbor search; Locality-sensitive hashing; LSM-tree; Streaming data", "title": "Efficient locality-sensitive hashing over high-dimensional streaming data", "abstract": "Approximate nearest neighbor (ANN) search in high-dimensional spaces is fundamental in many applications. Locality-sensitive hashing (LSH) is a well-known methodology to solve the ANN problem. Existing LSH-based ANN solutions typically employ a large number of individual indexes optimized for searching efficiency. Updating such indexes might be impractical when processing high-dimensional streaming data. In this paper, we present a novel disk-based LSH index that offers efficient support for both searches and updates. The contributions of our work are threefold. First, we use the write-friendly LSM-trees to store the LSH projections to facilitate efficient updates. Second, we develop a novel estimation scheme to estimate the number of required LSH functions, with which the disk storage and access costs are effectively reduced. Third, we exploit both the collision number and the projection distance to improve the efficiency of candidate selection, improving the search performance with theoretical guarantees on the result quality. Experiments on four real-world datasets show that our proposal outperforms the state-of-the-art schemes.", "journal": "NEURAL COMPUTING & APPLICATIONS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000686932100001", "keywords": "health assessment; hidden Markov model (HMM); robot mechanical axis; state prediction; temporal convolutional network (TCN)", "title": "HMM-TCN-based health assessment and state prediction for robot mechanical axis", "abstract": "Aiming at the problems of high manual cost, low efficiency, and low precision of the mechanical axis health management in industrial robot applications, this paper proposes a health assessment and state prediction algorithm based on hidden Markov model (HMM) and temporal convolutional networks (TCN). First, the MPdist similarity comparison algorithm is used to construct the mechanical axis health index. Then the hidden Markov model is trained with observable sensor data. After that, the temporal convolution neural network is used to predict state transition time iteratively, and the predicted results are decoded by HMM. The experimental results show that the HMM-TCN model can accurately assess the health state of the mechanical axis and predict the state transition in real-time. The prediction accuracy of this method reaches 87.5%, and the error interval locates in [-3,9] time steps. The accuracy, early/late prediction indicators are better than HMM-RNN, HMM-LSTM, and HMM-GRU.", "journal": "INTERNATIONAL JOURNAL OF INTELLIGENT SYSTEMS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000521971700038", "keywords": "Method of moments; Alternating gradient descent; Online learning; Tensor analysis", "title": "Estimation of Gaussian mixture models via tensor moments with application to online learning", "abstract": "In this paper, we present an alternating gradient descent algorithm for estimating parameters of a spherical Gaussian mixture model by the method of moments (AGD-MoM). We formulate the problem as a constrained optimisation problem which simultaneously matches the third order moments from the data, represented as a tensor, and the second order moment, which is the empirical covariance matrix. We derive the necessary gradients (and second derivatives), and use them to implement alternating gradient search to estimate the parameters of the model. We show that the proposed method is applicable in both a batch as well as in a streaming (online) setting. Using synthetic and benchmark datasets, we demonstrate empirically that the proposed algorithm outperforms the more classical algorithms like Expectation Maximisation and variational Bayes. (c) 2020 Elsevier B.V. All rights reserved.", "journal": "PATTERN RECOGNITION LETTERS", "category": "Computer Science, Artificial Intelligence", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000523306100015", "keywords": "Reachable set; Maximal Lyapunov functional; Mixed delays systems; Polytopic uncertainties; Reciprocally convex", "title": "Reachable set bounding for neural networks with mixed delays: Reciprocally convex approach", "abstract": "This paper discusses the reachable set estimation problem of neural networks with mixed delays. Firstly, by means of the maximal Lyapunov-Krasovskii functional, we obtain a non-ellipsoid form of the reachable set. Further more, when calculating the derivative of the maximum Lyapunov functional, the lower bound lemma and reciprocally convex approach method are used to solve the reciprocally convex combination term, which reduce the related decision variables. Secondly, we extend the results to polytopic uncertainties neural networks and consider the case of uncertain differentiable parameters. Finally, two numerical examples and one application example are listed to show the validity of our methods. (c) 2020 Elsevier Ltd. All rights reserved.", "journal": "NEURAL NETWORKS", "category": "Computer Science, Artificial Intelligence; Neurosciences", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000634927000004", "keywords": "Locomotion; Stereo vision; Soft robots; Bioinspired robots; SMER", "title": "A Rhythmic Activation Mechanism for Soft Multi-legged Robots", "abstract": "Compared to standard solutions, soft robotics presents enhanced adaptability to unpredictable and unstructured environments, encompassing advances in fabrication, modeling, and control. The absence of a general theory for the latter is one of the biggest challenges in the field, which constrains these robots' employment in real-world applications. This research proposes the application of Scheduling by Multiple Edge Reversal (SMER) in the activation of soft legs to be applied in multi-legged robots. A soft device was developed to be tested as a robot's leg to evaluate the proposed application. A logic controller for this device was designed using the SMER technique. Image processing techniques were used to assess the functionality of the proposed strategy, which demands limited resources. The vision tracking system is composed of a set of infrared-reflective patches, an infrared illuminator, and a pair of cameras with no infrared filters. Results revealed that it is possible to use SMER techniques to activate soft robotics systems and that the methods employed to develop and test the device were appropriate.", "journal": "JOURNAL OF INTELLIGENT & ROBOTIC SYSTEMS", "category": "Computer Science, Artificial Intelligence; Robotics", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000548474800002", "keywords": "Unified kinematics; Hybrid robots; Optimization and control", "title": "A unified kinematics modeling, optimization and control of universal robots: from serial and parallel manipulators to walking, rolling and hybrid robots", "abstract": "The paper develops a unified kinematics modeling, optimization and control that is applicable to a wide range of autonomous and non-autonomous robots. These include hybrid robots that combine two or more modes of operations, such as combination of walking and rolling, or rolling and manipulation, as well as parallel robots in various configurations. The equations of motion are derived in compact forms that embed an optimization criterion. These equations are used to obtain various useful forms of the robot kinematics such as recursive, body and limb-end kinematic forms. Using the modeling, actuation and control equations are derived that ensure traversing a desired path while maintaining balanced operations and tip-over avoidance. Various simulation results are provided for a hybrid rolling-walking robot, which demonstrate the capabilities and effectiveness of the developed methodologies.", "journal": "AUTONOMOUS ROBOTS", "category": "Computer Science, Artificial Intelligence; Robotics", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000505021700031", "keywords": "Spiking neural network; Biophysical model; Synchronism; Burst activity; Convergence", "title": "Realistic spiking neural network: Non-synaptic mechanisms improve convergence in cell assembly", "abstract": "Learning in neural networks inspired by brain tissue has been studied for machine learning applications. However, existing works primarily focused on the concept of synaptic weight modulation, and other aspects of neuronal interactions, such as non-synaptic mechanisms, have been neglected. Non-synaptic interaction mechanisms have been shown to play significant roles in the brain, and four classes of these mechanisms can be highlighted: (i) electrotonic coupling; (ii) ephaptic interactions; (iii) electric field effects; and iv) extracellular ionic fluctuations. In this work, we proposed simple rules for learning inspired by recent findings in machine learning adapted to a realistic spiking neural network. We show that the inclusion of non-synaptic interaction mechanisms improves cell assembly convergence. By including extracellular ionic fluctuation represented by the extracellular electrodiffusion in the network, we showed the importance of these mechanisms to improve cell assembly convergence. Additionally, we observed a variety of electrophysiological patterns of neuronal activity, particularly bursting and synchronism when the convergence is improved. (c) 2019 Elsevier Ltd. All rights reserved.", "journal": "NEURAL NETWORKS", "category": "Computer Science, Artificial Intelligence; Neurosciences", "annotated_keywords": ["neural net", "neural net", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000477943300012", "keywords": "Graph partitioning; Neural network; Combinatorial optimization; NP-hard problem; Deterministic annealing neural network algorithm", "title": "An approximation algorithm for graph partitioning via deterministic annealing neural network", "abstract": "Graph partitioning, a classical NP-hard combinatorial optimization problem, is widely applied to industrial or management problems. In this study, an approximated solution of the graph partitioning problem is obtained by using a deterministic annealing neural network algorithm. The algorithm is a continuation method that attempts to obtain a high-quality solution by following a path of minimum points of a barrier problem as the barrier parameter is reduced from a sufficiently large positive number to 0. With the barrier parameter assumed to be any positive number, one minimum solution of the barrier problem can be found by the algorithm in a feasible descent direction. With a globally convergent iterative procedure, the feasible descent direction could be obtained by renewing Lagrange multipliers red. A distinctive feature of it is that the upper and lower bounds on the variables will be automatically satisfied on the condition that the step length is a value from 0 to 1. Four wellknown algorithms are compared with the proposed one on 100 test samples. Simulation results show effectiveness of the proposed algorithm. (C) 2019 Elsevier Ltd. All rights reserved.", "journal": "NEURAL NETWORKS", "category": "Computer Science, Artificial Intelligence; Neurosciences", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000500922700027", "keywords": "Perceptual learning; Behavioral interference; Expert skill; Early visual cortex; Tuning curves; Recurrent neural network", "title": "Interfering with a memory without erasing its trace", "abstract": "Previous research has shown that performance of a novice skill can be easily interfered with by subsequent training of another skill. We address the open questions whether extensively trained skills show the same vulnerability to interference as novice skills and which memory mechanism regulates interference between expert skills. We developed a recurrent neural network model of V1 able to learn from feedback experienced over the course of a long-term orientation discrimination experiment. After first exposing the model to one discrimination task for 3480 consecutive trials, we assessed how its performance was affected by subsequent training in a second, similar task. Training the second task strongly interfered with the first (highly trained) discrimination skill. The magnitude of interference depended on the relative amounts of training devoted to the different tasks. We used these and other model outcomes as predictions for a perceptual learning experiment in which human participants underwent the same training protocol as our model. Specifically, over the course of three months participants underwent baseline training in one orientation discrimination task for 15 sessions before being trained for 15 sessions on a similar task and finally undergoing another 15 sessions of training on the first task (to assess interference). Across all conditions, the pattern of interference observed empirically closely matched model predictions. According to our model, behavioral interference can be explained by antagonistic changes in neuronal tuning induced by the two tasks. Remarkably, this did not stem from erasing connections due to earlier learning but rather from a reweighting of lateral inhibition. (C) 2019 Elsevier Ltd. All rights reserved.", "journal": "NEURAL NETWORKS", "category": "Computer Science, Artificial Intelligence; Neurosciences", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000592167800004", "keywords": "Self-tuning neural network; Unsupervised feature learning; Unsupervised transfer learning; Bagged clustering; Ranking violation for triplet sampling", "title": "Synchrony and Complexity in State-Related EEG Networks: An Application of Spectral Graph Theory", "abstract": "The brain may be considered as a synchronized dynamic network with several coherent dynamical units. However, concerns remain whether synchronizability is a stable state in the brain networks. If so, which index can best reveal the synchronizability in brain networks? To answer these questions, we tested the application of the spectral graph theory and the Shannon entropy as alternative approaches in neuroimaging. We specifically tested the alpha rhythm in the resting-state eye closed (rsEC) and the resting-state eye open (rsEO) conditions, a well-studied classical example of synchrony in neuroimaging EEG. Since the synchronizability of alpha rhythm is more stable during the rsEC than the rsEO, we hypothesized that our suggested spectral graph theory indices (as reliable measures to interpret the synchronizability of brain signals) should exhibit higher values in the rsEC than the rsEO condition. We performed two separate analyses of two different datasets (as elementary and confirmatory studies). Based on the results of both studies and in agreement with our hypothesis, the spectral graph indices revealed higher stability of synchronizability in the rsEC condition. The k-mean analysis indicated that the spectral graph indices can distinguish the rsEC and rsEO conditions by considering the synchronizability of brain networks. We also computed correlations among the spectral indices, the Shannon entropy, and the topological indices of brain networks, as well as random networks. Correlation analysis indicated that although the spectral and the topological properties of random networks are completely independent, these features are significantly correlated with each other in brain networks. Furthermore, we found that complexity in the investigated brain networks is inversely related to the stability of synchronizability. In conclusion, we revealed that the spectral graph theory approach can be reliably applied to study the stability of synchronizability of state-related brain networks.", "journal": "NEURAL COMPUTATION", "category": "Computer Science, Artificial Intelligence; Neurosciences", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000824725400001", "keywords": "Real-time systems; Force; Haptic interfaces; Visualization; Task analysis; Electroencephalography; Bars; Brain-computer interface (BCI); pseudohaptics; virtual reality (VR); weight perception", "title": "Real-Time Adjustment of Tracking Offsets Through a Brain-Computer Interface for Weight Perception in Virtual Reality", "abstract": "Provision of the perception of pseudo-weight through tracking offsets in virtual reality (VR) allows users to estimate the weight of virtual objects. However, the contribution of the user's real-time perception to such illusions is unknown. Here, we focus on this issue using a brain-computer interface (BCI), through which the user's perception of the weight of virtual objects can be detected in real-time and used to adjust the tracking offset in a closed loop. We first trained a computational model with electroencephalography (EEG) data by asking users to imagine lifting a heavy or a light ball. With this model, the user's perception of the object weight could be detected through the BCI in real-time to adjust the tracking offset, thereby enabling further generation of a more realistic visual sensation. Then, we evaluated the effects of the BCI tracking offset on the perception of the weights of three virtual objects used to simulate real objects, namely, tennis, billiard, and bowling balls. Our results showed that the BCI tracking offset could assist participants in generating perceived weights for virtual objects in VR. We further showed that our approach can provide weight perception through real-time adjustment of the tracking offset, which might be useful for new virtual objects that appear suddenly in the virtual environment. Additionally, most participants (78%) preferred this BCI tracking offset system for weight perception. This article provides the first quantification of weight perception for a virtual object with a BCI that can be used to adjust the tracking offset in real-time for pseudo-weight perception in VR.", "journal": "IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Cybernetics", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000502284100007", "keywords": "Task analysis; Automation; Manufacturing systems; Adaptive systems; Smart manufacturing; Human computer interaction; Adaptive automation; formal methods; human-machine interaction; human supervisory control; manufacturing system", "title": "Formalizing Human-Machine Interactions for Adaptive Automation in Smart Manufacturing", "abstract": "Human-machine interaction is one of the most crucial aspects of advanced manufacturing systems that have advanced to so-called smart manufacturing systems. In this regard, this paper presents a framework for formalizing human-machine manufacturing systems. The human-machine system considered in this paper consists of the following three main components: a human supervisor; several cells, each of which is composed of a human operator and a machine; and interfaces. A human operator interacts with a machine in a cell and performs manufacturing tasks based on commands given by the supervisor. Meanwhile, the supervisor is responsible for performing exception handling tasks in response to unanticipated events reported by the cells. With the proposed model, desirable specifications are constructed, which include a condition free of mode confusion, manufacturing task goal reachability, and exception handling task supportability in human-machine manufacturing systems. It is also suggested that adaptive automation with varying levels of information abstraction to humans can be accommodated by the proposed framework. As an illustrative example, we demonstrate the formal models and specifications and the applicability of adaptive automation with a case study of a simple chair assembly system.", "journal": "IEEE TRANSACTIONS ON HUMAN-MACHINE SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Cybernetics", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000582762000079", "keywords": "Multi-linear M-tensor equation; Modified Newton integration (MNI) neural algorithm; Noise-suppression ability", "title": "Modified Newton integration neural algorithm for solving the multi-linear M-tensor equation", "abstract": "This paper attends to solve the multi-linear equations with special structure, e.g., the multi-linear M-tensor equation, which frequently appears in engineering applications such as deep learning and hypergraph. For its critical and promising role, there are numbers of resolving schemes devoting to obtain a high-performing solution of the multi-linear M-tensor equation. However, few investigations are discovered with noise-suppression ability till now. To be proper with digital devices and further improve the solving effectiveness, it is desirable to design a discrete-time computational algorithm with noise-suppression ability and high-performing property. Inspired by the aforementioned requirements, this paper proposes a modified Newton integration (MNI) neural algorithm for solving the multilinear M-tensor equation with noise-suppression ability. Additionally, the corresponding robustness analyses on the proposed MNI neural algorithm are provided. Simultaneously, computer simulative experiments are generated to explain the capabilities and availabilities of the MNI neural algorithm in noise suppression. As a result, in terms of noise suppression, the proposed MNI neural algorithm is superior to other related algorithms, such as Newton-Raphson iterative (NRI) algorithm (Ding and Wei, 2016), discrete time neural network (DTNN) algorithm (Wang et al., 2019), and sufficient descent nonlinear conjugate gradient (SDNCG) algorithm (Liu et al., 2020). (C) 2020 Elsevier B.V. All rights reserved.", "journal": "APPLIED SOFT COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": ["neural net", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000528484400009", "keywords": "Differential evolution; Combinatorial optimization; Quasirandom sequences; Discrepancy", "title": "Differential evolution for the optimization of low-discrepancy generalized Halton sequences", "abstract": "Halton sequences are d-dimensional quasirandom sequences that fill the d-dimensional hyperspace in a uniform way. They can be used in a variety of applications such as multidimensional integration, uniform sampling, and, e.g., quasi-Monte Carlo simulations. Generalized Halton sequences improve the space-filling properties of original Halton sequences in higher dimensions by digit scrambling. In this work, an evolutionary optimization algorithm, the differential evolution, is used to optimize scrambling permutations of a cl-dimensional generalized Halton sequence so that the discrepancy of the generated point set is minimized.", "journal": "SWARM AND EVOLUTIONARY COMPUTATION", "category": "Computer Science, Artificial Intelligence; Computer Science, Theory & Methods", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000485039900044", "keywords": "Optimal block selection; Distinct discrete firefly algorithm; Entropy; Robustness; Transparency", "title": "Selecting optimal blocks for image watermarking using entropy and distinct discrete firefly algorithm", "abstract": "Image watermarking is the most promising method for preserving image copyright and its ownership identification. Watermark should have two contradictory properties of transparency and robustness. Location of embedding watermark in the image plays an important role in balancing these two properties. In this paper, a novel robust image watermarking scheme is proposed which uses combination of entropy and distinct discrete firefly algorithm (DDFA) for selecting suitable blocks to balance transparency and robustness. As image blocks numbers are distinct and discrete values, DDFA which is the modified version of firefly algorithm for optimizing distinct and discrete values is proposed and used to select optimal blocks. Hadamard transform applies on each selected block and watermark bits are embedded in Hadamard coefficients using average neighboring coefficients. Using Hadamard transform domain caused more robustness against signal processing attacks. The proposed method has been investigated by various standard metrics and experimental results showed its high robustness and imperceptibility; furthermore, there is a significant balance between these two properties.", "journal": "SOFT COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000839992900001", "keywords": "Parameter identification; Particle Swarm Optimization; Spectral Richness; Servomechanism; Closed -loop control; Thermoelectric cooler; Persistency of Excitation condition", "title": "Reinforcement learning supercharges redox flow batteries", "abstract": "Designing viable molecular candidates is pivotal to devising low-cost and sustainable storage systems. A reinforcement learning framework has been developed that can identify stable candidates for redox flow batteries in the large search space of organic radicals.", "journal": "NATURE MACHINE INTELLIGENCE", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": ["reinforcement learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000736986700003", "keywords": "Carbon price vertical forecast; ICEEMD-SSA-BP model; Interval estimation; Three-stage framework", "title": "Scheduling software updates for connected cars with limited availability (vol 114, 105575, 2019)", "abstract": "In the current context of pursuing carbon neutrality and carbon peaking, many countries are accelerating the construction of carbon trading markets. Accurate prediction of carbon prices can enable national carbon trading markets to play a role in carbon emission reduction as soon as possible. However, current research is limited mostly to point forecasting of carbon prices, which makes it difficult to guarantee the stability of forecasting results in an increasingly complex market. Therefore, this paper proposes a three-stage vertical carbon price interval prediction framework. The contributions of this paper are as follows: the selection process of the decomposition model is regarded as an important process of prediction; a backpropagation neural network optimized by the sparrow search algorithm (SSA-BPNN) is used for the point prediction of carbon prices as a first attempt; and the kernel density estimation (KDE) model is used for interval estimation based on the point prediction results, which improves the confidence of the prediction. To validate the framework, this paper uses Shenzhen SZA-2014 products as the sample. The results show that the root mean square error of the predicted result with the improved complete ensemble empirical mode decomposition model (ICEEMD) is reduced by 29.7% and the use of SSA increases the predicted R-2 by 8.5% compared with other optimization algorithms. In addition, the prediction interval coverage probability of interval prediction reaches 86% under 70% confidence. These results show that the proposed framework is not only more effective in point prediction but also performs well in interval prediction. (C) 2021 Elsevier B.V. All rights reserved.", "journal": "APPLIED SOFT COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000508744100009", "keywords": "Hybrid flowshop; Lot-streaming scheduling; Multi-objective optimization; Variable sub-lots", "title": "Efficient multi-objective algorithm for the lot-streaming hybrid flowshop with variable sub-lots", "abstract": "Recent years, the multi-objective evolutionary algorithm based on decomposition (MOEA/D) has been researched and applied for numerous optimization problems. In this study, we propose an improved version of MOEA/D with problem-specific heuristics, named PH-MOEAD, to solve the hybrid flowshop scheduling (HFS) lot-streaming problems, where the variable sub-lots constraint is considered to minimize four objectives, i.e., the penalty caused by the average sojourn time, the energy consumption in the last stage, as well as the earliness and the tardiness values. For solving this complex scheduling problem, each solution is coded by a two-vector-based solution representation, i.e., a sub-lot vector and a scheduling vector. Then, a novel mutation heuristic considering the permutations in the sub-lots is proposed, which can improve the exploitation abilities. Next, a problem-specific crossover heuristic is developed, which considered solutions with different sub-lot size, and therefore can make a solution feasible and enhance the exploration abilities of the algorithm as well. Moreover, several problem-specific lemmas are proposed and a right-shift heuristic based on them is subsequently developed, which can further improve the performance of the algorithm. Lastly, a population initialization mechanism is embedded that can assign a fit reference vector for each solution. Through comprehensive computational comparisons and statistical analysis, the highly effective performance of the proposed algorithm is favorably compared against several presented algorithms, both in solution quality and population diversity.", "journal": "SWARM AND EVOLUTIONARY COMPUTATION", "category": "Computer Science, Artificial Intelligence; Computer Science, Theory & Methods", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000835741700002", "keywords": "Gear fault diagnosis; Limited samples; Parameter adaptation; Prototypical network; Frequency slice wavelet transform", "title": "An adaptive anti-noise gear fault diagnosis method based on attention residual prototypical network under limited samples", "abstract": "Deep learning networks are widely used to realize the intelligent diagnosis of gear faults. However, the problem of the insufficient number of typical fault samples and strong noise often occur in practical applications. Thence, this paper proposes an adaptive anti-noise gear fault diagnosis method based on attention residual prototypical network (ARPN) under the limited sample. In order to maximize the characterization of implicit classification information under fewer samples, frequency slice wavelet transform is applied to convert the vibration signal into a time-frequency image. Then, the Bayesian optimization algorithm is introduced to automatically adjust hyperparameters to meet different application conditions. And the feature embedding stage combined with the improved NonLocal-Pooling-Attention module is constructed to capture the effective feature information better under strong noise conditions. Finally, the internal principle of the proposed model is analyzed based on the visualization process. Meanwhile, the initial application of an interpretable deep learning network in the classification of gear health status has been realized. The ARPN is verified on the Connecticut standard gear data set and the data set collected actually by the laboratory. The results show that the diagnostic accuracy of the ARPN is higher and has stronger recognition ability under different loads. (C) 2022 Elsevier B.V. All rights reserved.", "journal": "APPLIED SOFT COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000538132300002", "keywords": "Coevolution; digital evolution; evolutionary stasis; Red Queen; temporal intransitivity", "title": "Alteration of (Frequency-Dependent) Fitness in Time-Shift Experiments Reveals Cryptic Coevolution and Uncoordinated Stasis in a Virtual Jurassic Park", "abstract": "Among the major unresolved questions in ecosystem evolution are whether coevolving multispecies communities are dominated more by biotic or by abiotic factors, and whether evolutionary stasis affects performance as well as ecological profile; these issues remain difficult to address experimentally. Digital evolution, a computer-based instantiation of Darwinian evolution in which short self-replicating computer programs compete, mutate, and evolve, is an excellent platform for investigating such topics in a rigorous experimental manner. We evolved model communities with ecological interdependence among community members, which were subjected to two principal types of mass extinction: a pulse extinction that killed randomly, and a selective press extinction involving an alteration of the abiotic environment to which the communities had to adapt. These treatments were applied at two different strengths (Strong and Weak), along with unperturbed Control experiments. We performed several kinds of competition experiments using simplified versions of these communities to see whether long-term stability that was implied previously by ecological and phylogenetic metrics was also reflected in performance, namely, whether fitness was static over long periods of time. Results from Control and Weak treatment communities revealed almost completely transitive evolution, while Strong treatment communities showed higher incidences of temporal intransitivity, with pre-treatment ecotypes often able to displace some of their post-recovery successors. However, pre-treatment carryovers more often had lower fitness in mixed communities than in their own fully native conditions. Replacement and invasion experiments pitting single ecotypes against pre-treatment reference communities showed that many of the invading ecotypes could measurably alter the fitnesses of one or more residents, usually with depressive effects, and that the strength of these effects increased over time even in the most stable communities. However, invaders taken from Strong treatment communities often had little or no effect on resident performance. While we detected periods of time when the fitness of a particular evolving ecotype remained static, this stasis was not permanent and never affected an entire community at once. Our results lend support to the fitness-deterioration interpretation of the Red Queen hypothesis, and highlight community context dependence in determining fitness, the shaping of communities by both biotic factors and abiotic forcing, and the illusory nature of evolutionary stasis. Our results also demonstrate the potential of digital evolution studies to illuminate many aspects of evolution in interacting multispecies communities.", "journal": "ARTIFICIAL LIFE", "category": "Computer Science, Artificial Intelligence; Computer Science, Theory & Methods", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000487038100002", "keywords": "Quasi-coincidence; Multiset; Multi point; Multiset topology", "title": "Multiset mixed topological space", "abstract": "In this article based on the concept of quasi-coincidence of a multipoint and a multiset, we introduce the concept of multiset mixed topological space. We have investigated some of the properties of the introduced multiset mixed topological space.", "journal": "SOFT COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000872896300001", "keywords": "dynamic target detection; VSLAM; YOLOv3; GMM; Kalman filter", "title": "VSLAM method based on object detection in dynamic environments", "abstract": "Augmented Reality Registration field now requires improved SLAM systems to adapt to more complex and highly dynamic environments. The commonly used VSLAM algorithm has problems such as excessive pose estimation errors and easy loss of camera tracking in dynamic scenes. To solve these problems, we propose a real-time tracking and mapping method based on GMM combined with YOLOv3. The method utilizes the ORB-SLAM2 system framework and improves its tracking thread. It combines the affine transformation matrix to correct the front and back frames, and employs GMM to model the background image and segment the foreground dynamic region. Then, the obtained dynamic region is sent to the YOLO detector to find the possible dynamic target. It uses the improved Kalman filter algorithm to predict and track the detected dynamic objects in the tracking stage. Before building a map, the method filters the feature points detected in the current frame and eliminates dynamic feature points. Finally, we validate the proposed method using the TUM dataset and conduct real-time Augmented Reality Registration experiments in a dynamic environment. The results show that the method proposed in this paper is more robust under dynamic datasets and can register virtual objects stably and in real time.", "journal": "FRONTIERS IN NEUROROBOTICS", "category": "Computer Science, Artificial Intelligence; Robotics; Neurosciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000810192500001", "keywords": "STS; auxiliary stand robot; center of gravity; optical gait acquisition system; AMTI", "title": "Sit-to-Stand (STS) Movement Analysis of the Center of Gravity for Human-Robot Interaction", "abstract": "As the aging process is springing up around the whole world, the problem of sit-to-stand (STS) for the elderly has become the focal point of older people themselves, their families, and society. The key challenge in solving this problem is developing and applying the technology of auxiliary robots for standing up and auxiliary stands. The research is not only to find the appropriate fundamentals from a healthcare perspective in the center of gravity movement (COM) curve of the elderly during STS but also to meet the psychological needs of the elderly during STS in a comfortable, pleasant, and safe way. To obtain the skeleton tracking technology used in this study, we used the Vicon optical motion capture system and automatic moving target indicator (AMTI) 3-D force measuring table to obtain a COM curve of the elderly during STS. The stationary process, speed, and sitting posture analysis during STS were combined in this paper to seek a solution to the psychological needs of the elderly. The analysis is conducted with the integration of medicine, engineering, art, and science into this research. Finally, a movement curve and related dynamic parameters that can truly reflect the STS of the elderly are obtained, and the discussion is provided. The research result can provide theoretical and technical support for the later development of auxiliary robots for standing up and auxiliary stands technology products, so that the elderly can STS comfortably, happily, and safely with the assistance of human-robot interaction.", "journal": "FRONTIERS IN NEUROROBOTICS", "category": "Computer Science, Artificial Intelligence; Robotics; Neurosciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000481781800002", "keywords": "Manipulation in sports; Knockout tournaments; Monte Carlo algorithm; Control; Manipulation; Voting; King player", "title": "Controlling sub-tournaments: easy or hard problem? Theoretical vs. practical analysis", "abstract": "Is it possible for the organizers of a sports tournament to influence the identity of the final winner by manipulating the initial seeding of the tournament? Is it possible to ensure a specific good (i.e. king) player will win at least a certain number of rounds in the tournament? This paper investigates these questions both by means of a theoretical method and a practical approach. The theoretical method focuses on the attempt to identify sufficient conditions to ensure a king player will win at least a pre-defined number of rounds in the tournament. It seems that the tournament must adhere to very strict conditions to ensure the outcome, suggesting that this is a hard problem. The practical approach, on the other hand, uses the Monte Carlo method to demonstrate that these problems are solvable in realistic computational time. A comparison of the results lead to the realization that players with equivalent representation might relax the actual complexity of the problem, and enable manipulation of tournaments that can be controlled in reality.", "journal": "ANNALS OF MATHEMATICS AND ARTIFICIAL INTELLIGENCE", "category": "Computer Science, Artificial Intelligence; Mathematics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000916821100031", "keywords": "Brain-computer interface (BCI); electroen-cephalography (EEG); feature extraction; lexical tone; machine learning; Mandarin; speech recognition", "title": "Recognizing Tonal and Nontonal Mandarin Sentences for EEG-Based Brain-Computer Interface", "abstract": "Most current research has focused on nontonal languages such as English. However, more than 60% of the world's population speaks tonal languages. Mandarin is the most spoken tonal languages in the world. Interestingly, the use of tone in tonal languages may represent different meanings of words and reflect feelings, which is very different from nontonal languages. The objective of this study is to determine whether a spoken Mandarin sentence with or without tone can be distinguished by analyzing electroencephalographic (EEG) signals. We first constructed a new Brain Research Center Speech (BRCSpeech) database to recognize Mandarin. The EEG data of 14 participants were recorded, while they articulated preselected sentences. To the best of our knowledge, this is the first study to apply the method of asymmetric feature extraction method for speech recognition using EEG signals. This study shows that the feature extraction method of rational asymmetry (RASM) can achieve the best accuracy in the classification of cross-subjects. In addition, our proposed binomial variable algorithm methodology can achieve 98.82% accuracy in cross-subject classification. Furthermore, we demonstrate that the use of eight channels [(F7, F8), (C5, C6), (P5, P6), and (O1, O2)] can achieve an accurate of 94.44%. This study explores the neurophysiological correlation of Mandarin pronunciation, which can help develop a tonal language synthesis system based on BCI in the future.", "journal": "IEEE TRANSACTIONS ON COGNITIVE AND DEVELOPMENTAL SYSTEMS", "category": "Computer Science, Artificial Intelligence; Robotics; Neurosciences", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000764447300001", "keywords": "Electronic patient information (EPI); Privacy protection; Generalized prediction error expansion; Deep neural network; Symmetric encryption; Reversible data hiding (RDH)", "title": "An enhanced reversible data hiding algorithm using deep neural network for E-healthcare", "abstract": "E-healthcare requires communication of patient report to a specialized doctor in a real time scenario. Therefore, any harm to patient medical data can lead to a faulty diagnosis that can be lethal for the patient. Some of the earlier state-of-the-art reversible data hiding methods make use of prediction for embedding data reversibly. To ensure secure and safe communication in encrypted domain, predictors has attained an extraordinary consideration from the research community nowadays. This paper presents an enhanced reversible data hiding method that gives a high embedding capacity by embedding m, (m >= 1) binary bits of electronic patient information (EPI) at embeddable pixels of cover image respectively. In addition to EPI, a fragile watermark has been embedded for observing any tamper to the patient data during transmission phase. In proposed method, deep neural network is used for prediction and data is embedded through generalized prediction error expansion scheme. Through combination of stream ciphers and logistic map, proposed method generated encrypted stego image to improve resistance against various attacks such as cipher-only attack and differential attack respectively. The experimental study reveals that for all types of test images, proposed method has an embedding capacity nearly twice larger than the compared methods, at receiver end precisely recover EPI (Bit Error Rate value is zero) with a PSNR value of 8 dB between the cover image and reconstructed image successfully. For all test images, the proposed method altogether beat all the compared methods in its ability to embed secret information and precisely recover it with maintaining the visual quality of stego images too.", "journal": "JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems; Telecommunications", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000853875300053", "keywords": "Feature extraction; Detectors; Computer architecture; Measurement; Image sensors; Image edge detection; Training; Deep learning; multisensor images; image matching; feature points detection", "title": "Joint Detection and Matching of Feature Points in Multimodal Images", "abstract": "In this work, we propose a novel Convolutional Neural Network (CNN) architecture for the joint detection and matching of feature points in images acquired by different sensors using a single forward pass. The resulting feature detector is tightly coupled with the feature descriptor, in contrast to classical approaches (SIFT, etc.), where the detection phase precedes and differs from computing the descriptor. Our approach utilizes two CNN subnetworks, the first being a Siamese CNN and the second, consisting of dual non-weight-sharing CNNs. This allows simultaneous processing and fusion of the joint and disjoint cues in the multimodal image patches. The proposed approach is experimentally shown to outperform contemporary state-of-the-art schemes when applied to multiple datasets of multimodal images. It is also shown to provide repeatable feature points detections across multi-sensor images, outperforming state-of-the-art detectors. To the best of our knowledge, it is the first unified approach for the detection and matching of such images.", "journal": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000658333500005", "keywords": "Histograms; Brightness; Visualization; Neurons; Computer architecture; Image enhancement; Transfer functions; Contrast enhancement; color image; context modelling; energy curve; image enhancement", "title": "A Context-Based Image Contrast Enhancement Using Energy Equalization With Clipping Limit", "abstract": "In this paper, a new context-based image contrast enhancement process using energy curve equalization (ECE) with a clipping limit has been proposed. In a fundamental anomaly to the existing contrast enhancement practice using histogram equalization, the projected method uses the energy curve. The computation of the energy curve utilizes a modified Hopfield neural network architecture. This process embraces the image's spatial adjacency information to the energy curve. For each intensity level, the energy value is calculated and the overall energy curve appears to be smoother than the histogram. A clipping limit applies to evade the over enhancement and is chosen as the average of the mean and median value. The clipped energy curve is subdivided into three regions based on the standard deviation value. Each part of the subdivided energy curve is equalized individually, and the final enhanced image is produced by combining transfer functions computed by the equalization process. The projected scheme's qualitative and quantitative efficiency is assessed by comparing it with the conventional histogram equalization techniques with and without the clipping limit.", "journal": "IEEE TRANSACTIONS ON IMAGE PROCESSING", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000774599800035", "keywords": "Microblogs; natural language processing; text similarity; text preprocessing; tweets; word embedding", "title": "Event-related microblog retrieval in Turkish", "abstract": "Microblogs, such as tweets, are short messages in which users are able to share any opinion and information. Microblogs are mostly related to real-life events reported in news articles. Finding event-related microblogs is important to analyze online social networks and understand public opinion on events. However, finding such microblogs is a challenging task due to the dynamic nature of microblogs and their limited length. In this study, assuming that news articles are given as queries and microblogs as documents, we find event-related microblogs in Turkish. In order to represent news articles and microblogs, we examine encoding methods, namely traditional bag-of-words and word embeddings provided by BERT and FastText pretrained language models based on deep learning. We find the distance between the encoded news article and microblog to measure text similarity or relatedness between them. We then rank microblogs according to their relatedness to the input query. The experimental results show that (i) BERT-based model outperforms other encoding methods in Turkish, though bag-of-words with Dice similarity has a challenging performance in short text; (ii) news title is successful to represent event as query, and (iii) preprocessing Turkish microblogs has positive impact in bag-of-words and also FastText embeddings, while BERT embeddings are robust to noise in Turkish.", "journal": "TURKISH JOURNAL OF ELECTRICAL ENGINEERING AND COMPUTER SCIENCES", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000484312800012", "keywords": "Distributed parameter system (DPS); fuzzy control (FC); linear matrix inequality (LMI); spatially point measurements; time-varying delay", "title": "Fuzzy Control for Nonlinear Time-Delay Distributed Parameter Systems Under Spatially Point Measurements", "abstract": "This paper introduces a fuzzy control (FC) under spatially point measurements for nonlinear time-delay distributed parameter systems (DPSs) described by parabolic partial differential-difference equations (PDdEs). First, a Takagi-Sugeno (T-S) fuzzy PDdE model is employed to represent the nonlinear time-delay DPSs. Second, with the aid of the T-S fuzzy PDdE model, an FC design under spatially point measurements is developed in the formulation of linear matrix inequalities by constructing an appropriate Lyapunov functional, which can stabilize exponentially the time-delay DPSs. This stabilization condition can be applied to either slowing-varying time delay or fast-varying one. Finally, simulation results of a numerical example are provided to illustrate the effectiveness of the proposed method.", "journal": "IEEE TRANSACTIONS ON FUZZY SYSTEMS", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000518876100006", "keywords": "Multi-label image retrieval; Hashing; Deep learning", "title": "Deep code operation network for multi-label image retrieval", "abstract": "Deep hashing methods have been extensively studied for large-scale image search and achieved promising results in recent years. However, there are two major limitations of previous deep hashing methods for multilabel image retrieval: the first one concerns the flexibility for users to express their query intention (so-called the intention gap), and the second one concerns the exploitation of rich similarity structures of the semantic space (so-called the semantic gap). To address these issues, we propose a novel Deep Code Operation Network (CoNet), in which a user is allowed to simultaneously present multiple images instead of a single one as his/her query, and then the system triggers a series of code operators to extract the hidden relations among them. In this way, a set of new queries are automatically constructed to cover users' real complex query intention, without the need of explicitly stating them. The CoNet is trained with a newly proposed margin-adaptive triplet loss function, which effectively encourages the system to incorporate the hierarchical similarity structures of the semantic space into the learning procedure of the code operations. The whole system has an end-to-end differentiable architecture, equipped with an adversarial mechanism to further improve the quality of the final intention representation. Experimental results on four multi-label image datasets demonstrate that our method significantly improves the state-of-the-art in performing complex multi-label retrieval tasks with multiple query images.", "journal": "COMPUTER VISION AND IMAGE UNDERSTANDING", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000853875300087", "keywords": "Image reconstruction; Image color analysis; Sensors; Image coding; Gray-scale; Cameras; Noise reduction; Compressive sensing; deep learning; computational imaging; coded aperture; image processing; video processing; coded aperture compressive temporal imaging (CACTI); plug-and-play (PnP) algorithms", "title": "Plug-and-Play Algorithms for Video Snapshot Compressive Imaging", "abstract": "We consider the reconstruction problem of video snapshot compressive imaging (SCI), which captures high-speed videos using a low-speed 2D sensor (detector). The underlying principle of SCI is to modulate sequential high-speed frames with different masks and then these encoded frames are integrated into a snapshot on the sensor and thus the sensor can be of low-speed. On one hand, video SCI enjoys the advantages of low-bandwidth, low-power and low-cost. On the other hand, applying SCI to large-scale problems (HD or UHD videos) in our daily life is still challenging and one of the bottlenecks lies in the reconstruction algorithm. Existing algorithms are either too slow (iterative optimization algorithms) or not flexible to the encoding process (deep learning based end-to-end networks). In this paper, we develop fast and flexible algorithms for SCI based on the plug-and-play (PnP) framework. In addition to the PnP-ADMM method, we further propose the PnP-GAP (generalized alternating projection) algorithm with a lower computational workload. We first employ the image deep denoising priors to show that PnP can recover a UHD color video with 30 frames from a snapshot measurement. Since videos have strong temporal correlation, by employing the video deep denoising priors, we achieve a significant improvement in the results. Furthermore, we extend the proposed PnP algorithms to the color SCI system using mosaic sensors, where each pixel only captures the red, green or blue channels. A joint reconstruction and demosaicing paradigm is developed for flexible and high quality reconstruction of color video SCI systems. Extensive results on both simulation and real datasets verify the superiority of our proposed algorithm.", "journal": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000537517400001", "keywords": "Adversarial optimization; Black-box optimization; Computer simulations", "title": "Adaptive divergence for rapid adversarial optimization", "abstract": "Adversarial Optimization provides a reliable, practical way to match two implicitly defined distributions, one of which is typically represented by a sample of real data, and the other is represented by a parameterized generator. Matching of the distributions is achieved by minimizing a divergence between these distribution, and estimation of the divergence involves a secondary optimization task, which, typically, requires training a model to discriminate between these distributions. The choice of the model has its tradeoff: high-capacity models provide good estimations of the divergence, but, generally, require large sample sizes to be properly trained. In contrast, low-capacity models tend to require fewer samples for training; however, they might provide biased estimations. Computational costs of Adversarial Optimization becomes significant when sampling from the generator is expensive. One of the practical examples of such settings is finetuning parameters of complex computer simulations. In this work, we introduce a novel family of divergences that enables faster optimization convergence measured by the number of samples drawn from the generator. The variation of the underlying discriminator model capacity during optimization leads to a significant speed-up. The proposed divergence family suggests using low-capacity models to compare distant distributions (typically, at early optimization steps), and the capacity gradually grows as the distributions become closer to each other. Thus, it allows for a significant acceleration of the initial stages of optimization. This acceleration was demonstrated on two fine-tuning problems involving Pythia event generator and two of the most popular black-box optimization algorithms: Bayesian Optimization and Variational Optimization. Experiments show that, given the same budget, adaptive divergences yield results up to an order of magnitude closer to the optimum than Jensen-Shannon divergence. While we consider physics-related simulations, adaptive divergences can be applied to any stochastic simulation.", "journal": "PEERJ COMPUTER SCIENCE", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000482658500001", "keywords": "Bak-Sneppen model; Particle swarm optimization; Velocity update strategy", "title": "Steady state particle swarm", "abstract": "This paper investigates the performance and scalability of a new update strategy for the particle swarm optimization (PSO) algorithm. The strategy is inspired by the Bak-Sneppen model of co-evolution between interacting species, which is basically a network of fitness values (representing species) that change over time according to a simple rule: the least fit species and its neighbors are iteratively replaced with random values. Following these guidelines, a steady state and dynamic update strategy for PSO algorithms is proposed: only the least fit particle and its neighbors are updated and evaluated in each time-step; the remaining particles maintain the same position and fitness, unless they meet the update criterion. The steady state PSO was tested on a set of unimodal, multimodal, noisy and rotated benchmark functions, significantly improving the quality of results and convergence speed of the standard PSOs and more sophisticated PSOs with dynamic parameters and neighborhood. A sensitivity analysis of the parameters confirms the performance enhancement with different parameter settings and scalability tests show that the algorithm behavior is consistent throughout a substantial range of solution vector dimensions.", "journal": "PEERJ COMPUTER SCIENCE", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000609166100011", "keywords": "Diagnosis estimation; Daily life cycle; Deep learning; Personal health data analysis; Data analysis", "title": "Improving Diagnosis Estimation by Considering the Periodic Span of the Life Cycle Based on Personal Health Data", "abstract": "With the surge in popularity of wearable devices, collection of personal health data has become quite easy. Many studies have been conducted using health data to estimate the onset and progression of illness. However, life habits may vary among individuals. By analyzing the life cycle from health-related data, conventional studies may be improved. This study proposes a new approach to improving diagnosis estimation by considering the life cycle analyzed from health-related data. The periodic span of the life cycle is estimated via autocorrelation analysis. In the range of the periodic span, dimension reduction for health data is performed by principal component analysis, and health features are extracted and used for diagnosis estimation. In our experiment, we used personal health data and pulse diagnosis data collected by a traditional Chinese medicine doctor. Using six multi-label classification methods, we verified that a combination of pulse and health features could improve the accuracy of diagnosis estimation compared with that using only pulse features. (C) 2020 Elsevier Inc. All rights reserved.", "journal": "BIG DATA RESEARCH", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Theory & Methods", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000811499800001", "keywords": "Data validation; SWRL; SPARQL; Model-driven engineering; Water supply and distribution systems", "title": "An MDE-based methodology for closed-world integrity constraint checking in the semantic web", "abstract": "Ontology-based data-centric systems support open-world reasoning. Therefore, for these systems, Web Ontology Language (OWL) and Semantic Web Rule Language (SWRL) are not suitable for expressing integrity constraints based on the closed-world assumption. Thus, the requirement of integrating the open-world assumption of OWL/SWRL with closed-world integrity constraint checking is inevitable. SPARQL, recommended by World Wide Web (W3C), is a query language for RDF graphs, and many research studies have shown that it is a perfect candidate for closed-world constraint checking for ontology-based data-centric applications. In this regard, many research studies have been performed to transform integrity constraints into SPARQL queries where some studies have shown the limitations of partial expressivity of knowledge bases while performing the indirect transformations, whereas others are limited to a platform-specific implementation. To address these issues, this paper presents a flexible and formal methodology that employs Model-Driven Engineering (MDE) to model closed-world integrity constraints for open-world reasoning. The proposed approach offers semantic validation of data by expressing integrity constraints at both the model level and the code level. Moreover, straightforward transformations from OWL/SWRL to SPARQL can be performed. Finally, the methodology is demonstrated via a real-world case study of water observations data. (c) 2022 The Author(s). Published by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).", "journal": "JOURNAL OF WEB SEMANTICS", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems; Computer Science, Software Engineering", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000761683500001", "keywords": "Bayesian inference; Cholesky decomposition; continuous shrinkage prior; nonparanormal graphical models", "title": "Regression-based Bayesian estimation and structure learning for nonparanormal graphical models", "abstract": "A nonparanormal graphical model is a semiparametric generalization of a Gaussian graphical model for continuous variables in which it is assumed that the variables follow a Gaussian graphical model only after some unknown smooth monotone transformations. We consider a Bayesian approach to inference in a nonparanormal graphical model in which we put priors on the unknown transformations through a random series based on B-splines. We use a regression formulation to construct the likelihood through the Cholesky decomposition on the underlying precision matrix of the transformed variables and put shrinkage priors on the regression coefficients. We apply a plug-in variational Bayesian algorithm for learning the sparse precision matrix and compare the performance to a posterior Gibbs sampling scheme in a simulation study. We finally apply the proposed methods to a microarray dataset. The proposed methods have better performance as the dimension increases, and in particular, the variational Bayesian approach has the potential to speed up the estimation in the Bayesian nonparanormal graphical model without the Gaussianity assumption while retaining the information to construct the graph.", "journal": "STATISTICAL ANALYSIS AND DATA MINING", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Statistics & Probability", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000819019200130", "keywords": "Synchronization; Protocols; Vehicle dynamics; Task analysis; Nonlinear dynamical systems; Multi-agent systems; Integrated circuits; Consensus; cooperative tracking control; mixed-order dynamics; multiagent systems (MASs)", "title": "Cooperative Tracking Control of Heterogeneous Mixed-Order Multiagent Systems With Higher-Order Nonlinear Dynamics", "abstract": "This article investigates a class of finite-time cooperative tracking problems of heterogeneous mixed-order multiagent systems (MASs) with higher-order dynamics. Different from the previous works of heterogeneous MASs, the agents in this study are considered to have different first-, second-, or even higher-order nonlinear dynamics. It means that, according to different tasks and situations, the following agents can have nonidentical orders or different numbers of states to be synchronized, which is more general for the practical cooperative applications. The leader is a higher-order nonautonomous system and contains full state information to be synchronized for all agents with mixed-order dynamics. Accordingly, the spanning tree is defined based on the specific state rather than on the agent to guarantee that each following agent can receive adequate state information. Distributed control protocols are designed for all agents to achieve the ultimate state synchronization to the leader in finite time. The Lyapunov approach is used for the stability analysis and a practical example of mixed-order mechanical MASs verifies the effectiveness and performance of the proposed distributed control protocols.", "journal": "IEEE TRANSACTIONS ON CYBERNETICS", "category": "Automation & Control Systems; Computer Science, Artificial Intelligence; Computer Science, Cybernetics", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000602709000010", "keywords": "Feature extraction; Image reconstruction; Spatial resolution; Convolutional neural networks; Training; Graphics; Convolutional neural network; multiscale learning; residual learning; single-image super-resolution (SISR)", "title": "Cascading and Enhanced Residual Networks for Accurate Single-Image Super-Resolution", "abstract": "Deep convolutional neural networks (CNNs) have contributed to the significant progress of the single-image super-resolution (SISR) field. However, the majority of existing CNN-based models maintain high performance with massive parameters and exceedingly deeper structures. Moreover, several algorithms essentially have underused the low-level features, thus causing relatively low performance. In this article, we address these problems by exploring two strategies based on novel local wider residual blocks (LWRBs) to effectively extract the image features for SISR. We propose a cascading residual network (CRN) that contains several locally sharing groups (LSGs), in which the cascading mechanism not only promotes the propagation of features and the gradient but also eases the model training. Besides, we present another enhanced residual network (ERN) for image resolution enhancement. ERN employs a dual global pathway structure that incorporates nonlocal operations to catch long-distance spatial features from the the original low-resolution (LR) input. To obtain the feature representation of the input at different scales, we further introduce a multiscale block (MSB) to directly detect low-level features from the LR image. The experimental results on four benchmark datasets have demonstrated that our models outperform most of the advanced methods while still retaining a reasonable number of parameters.", "journal": "IEEE TRANSACTIONS ON CYBERNETICS", "category": "Automation & Control Systems; Computer Science, Artificial Intelligence; Computer Science, Cybernetics", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000620293800004", "keywords": "Medical Concept Normalization; Clinical Natural Language Processing; BERT; Highway Network", "title": "BertMCN: Mapping colloquial phrases to standard medical concepts using BERT and highway network", "abstract": "In the last few years, people started to share lots of information related to health in the form of tweets, reviews and blog posts. All these user generated clinical texts can be mined to generate useful insights. However, automatic analysis of clinical text requires identification of standard medical concepts. Most of the existing deep learning based medical concept normalization systems are based on CNN or RNN. Performance of these models is limited as they have to be trained from scratch (except embeddings). In this work, we propose a medical concept normalization system based on BERT and highway layer. BERT, a pre-trained context sensitive deep language representation model advanced state-of-the-art performance in many NLP tasks and gating mechanism in highway layer helps the model to choose only important information. Experimental results show that our model outperformed all existing methods on two standard datasets. Further, we conduct a series of experiments to study the impact of different learning rates and batch sizes, noise and freezing encoder layers on our model.", "journal": "ARTIFICIAL INTELLIGENCE IN MEDICINE", "category": "Computer Science, Artificial Intelligence; Engineering, Biomedical; Medical Informatics", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000836145700001", "keywords": "Gradient-based neural network; semidefinite programming; convex programming; convergent and stability", "title": "A smoothing gradient-based neural network strategy for solving semidefinite programming problems", "abstract": "Linear semidefinite programming problems have received a lot of attentions because of large variety of applications. This paper deals with a smooth gradient neural network scheme for solving semidefinite programming problems. According to some properties of convex analysis and using a merit function in matrix form, a neural network model is constructed. It is shown that the proposed neural network is asymptotically stable and converges to an exact optimal solution of the semidefinite programming problem. Numerical simulations are given to show that the numerical behaviours are in good agreement with the theoretical results.", "journal": "NETWORK-COMPUTATION IN NEURAL SYSTEMS", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Neurosciences", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000700339000001", "keywords": "additive manufacturing; embedded systems; multimaterial 3D printing; polymer composites; smart objects", "title": "Fused Deposition Modeling-Based 3D-Printed Electrical Interconnects and Circuits", "abstract": "Multimaterial 3D printing in electronics is expanding due to the ability to realize geometrically complex systems with simplified processes compared with conventional printed circuit board. Herein, the feasibility of using a copper-based filament to realize 3D circuits with planar and vertical interconnections is presented. The resistivity of the tracks (1-3 mm wide) is studied with reference to printing parameters and orientation. Using lateral infill for 1 mm tracks offers lower resistance compared with longitudinal infill (approximate to 75%). For wider tracks, the effect of infill orientation on resistance diminishes. The evaluation of tracks embedded in polylactic acid shows a drop in maximum current (to approximate to 11 mA) compared with exposed tracks (approximate to 16 mA). There is no observed correlation between electrical performance and number of embedding layers. However, a significant correlation is observed between the tracks' resistance and the amount of time the filament remains in the heated nozzle. This in-depth study leads to optimum resolution to realize conductive tracks of 0.67 mm thickness and the first integration of fused deposition modeling (FDM)-printed conductive traces with small-outline integrated circuits to open a pathway for higher-density 3D printed circuits. Finally, the transmission of digital data by a 3D printed circuit is demonstrated.", "journal": "ADVANCED INTELLIGENT SYSTEMS", "category": "Automation & Control Systems; Computer Science, Artificial Intelligence; Robotics", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000645032600010", "keywords": "Microarray data; sparse regularization; feature selection; logistic regression; and lasso", "title": "An Additive Sparse Logistic Regularization Method for Cancer Classification in Microarray Data", "abstract": "Now a day's cancer has become a deathly disease due to the abnormal growth of the cell. Many researchers are working in this area for the early prediction of cancer. For the proper classification of cancer data, demands for the identification of proper set of genes by analyzing the genomic data. Most of the researchers used microarrays to identify the cancerous genomes. However, such kind of data is high dimensional where number of genes are more compared to samples. Also the data consists of many irrelevant features and noisy data. The classification technique deal with such kind of data influences the performance of algorithm. A popular classification algorithm (i.e., Logistic Regression) is considered in this work for gene classification. Regularization techniques like Lasso with L-1 penalty, Ridge with L-2 penalty, and hybrid Lasso with L-1/2+2 penalty used to minimize irrelevant features and avoid overfitting. However, these methods are of sparse parametric and limits to linear data. Also methods have not produced promising performance when applied to high dimensional genome data. For solving these problems, this paper presents an Additive Sparse Logistic Regression with Additive Regularization (ASLR) method to discriminate linear and non-linear variables in gene classification. The results depicted that the proposed method proved to be the best-regularized method for classifying microarray data compared to standard methods.", "journal": "INTERNATIONAL ARAB JOURNAL OF INFORMATION TECHNOLOGY", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000710540200016", "keywords": "Rail to rail inputs; Feature extraction; Heart rate variability; Accidents; Intelligent vehicles; Games; Heart rate variability analysis; drowsiness detection; wakefulness-keeping support", "title": "Development of Game-Like System Using Active Behavior Input for Wakefulness-Keeping Support in Driving", "abstract": "Various drowsiness detection systems have been developed to prevent traffic accidents. Drivers feel such systems are annoying or useless when the systems provide many false drowsiness alarms. Drowsy driving prevention systems should satisfy the following three features to be accepted by many drivers: 1) Precise drowsiness detection, 2) Drowsy driving warning whereby drivers are less annoyed by false alarms, and 3) Mechanisms that promote drivers to adopt active behavior to maintain his/her wakefulness. The present work proposes a new drowsy driving prevention system that has these features. The proposed system, referred to as a wakefulness-keeping support system (WKSS), consists of a drowsiness detection system (DDS), which corresponds to the first feature, and an active game system (AGS), which corresponds to the second and third features. The AGS is a simple game, which encourages drivers to adopt specific active behavior to play the game. Drivers can keep their wakefulness by playing the AGS, while enjoying it, even though false drowsiness alarms may occur. The usefulness of the proposed WKSS was evaluated through experiments using a driving simulator, which suggest that WKSS could be accepted by many drivers and contributing to realizing a zero-traffic-accident society.", "journal": "IEEE TRANSACTIONS ON INTELLIGENT VEHICLES", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Transportation Science & Technology", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000711638200027", "keywords": "Discrete-time; exponential mean-square stability; state estimation; vector optimization", "title": "Exponential H-infinity State Estimation for Memristive Neural Networks: Vector Optimization Approach", "abstract": "This article presents the theoretical results on the H-infinity state estimation problem for a class of discrete-time memristive neural networks. By utilizing a Lyapunov-Krasovskii functional, sufficient conditions are derived to guarantee that the error system is exponentially mean-square stable; subsequently, the prespecified H-infinity disturbance rejection attenuation level is also guaranteed. It should be noted that the vector optimization method is employed to find the maximum bound of function and the minimum disturbance turning simultaneously. Finally, the corresponding simulation results are included to show the effectiveness of the proposed methodology.", "journal": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Hardware & Architecture; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000805817800001", "keywords": "Synchronization; Symmetric matrices; Delays; Delay effects; Biological neural networks; Uncertainty; Time measurement; Delayed impulses; lag synchronization; Lyapunov methods; neural networks; partial unmeasurable states", "title": "Delayed Impulsive Control for Lag Synchronization of Delayed Neural Networks Involving Partial Unmeasurable States", "abstract": "In the framework of impulsive control, this article deals with the lag synchronization problem of neural networks involving partially unmeasurable states, where the time delay in impulses is fully addressed. Since the complexity of external environment and uncertainty of networks, which may lead to a result that the information of partial states is unmeasurable, the key problem for lag synchronization control is how to utilize the information of measurable states to design suitable impulsive control. By using linear matrix inequality (LMI) and transition matrix method coupled with dimension expansion technique, some sufficient conditions are derived to guarantee lag synchronization, where the requirement for information of all states is needless. Moreover, our proposed conditions not only allow the existence of unmeasurable states but also reduce the restrictions on the number of measurable states, which shows the generality of our results and wide-application in practice. Finally, two illustrative examples and their numerical simulations are presented to demonstrate the effectiveness of main results.", "journal": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Hardware & Architecture; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000670541500008", "keywords": "Neural networks; Stability analysis; Mathematical model; Measurement; Optimization; Sun; Numerical stability; Metric projector; neural network; second-order cone (SOC); second-order sufficient condition; stability; variational inequality (VI)", "title": "A Neural Network Based on the Metric Projector for Solving SOCCVI Problem", "abstract": "We propose an efficient neural network for solving the second-order cone constrained variational inequality (SOCCVI). The network is constructed using the Karush-Kuhn-Tucker (KKT) conditions of the variational inequality (VI), which is used to recast the SOCCVI as a system of equations by using a smoothing function for the metric projection mapping to deal with the complementarity condition. Aside from standard stability results, we explore second-order sufficient conditions to obtain exponential stability. Especially, we prove the nonsingularity of the Jacobian of the KKT system based on the second-order sufficient condition and constraint nondegeneracy. Finally, we present some numerical experiments, illustrating the efficiency of the neural network in solving SOCCVI problems. Our numerical simulations reveal that, in general, the new neural network is more dominant than all other neural networks in the SOCCVI literature in terms of stability and convergence rates of trajectories to SOCCVI solution.", "journal": "IEEE TRANSACTIONS ON NEURAL NETWORKS AND LEARNING SYSTEMS", "category": "Computer Science, Artificial Intelligence; Computer Science, Hardware & Architecture; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000703905700005", "keywords": "Building condition assessment; Artificial neural network; Structural equation modeling; Maintenance management; Facilities management", "title": "Building condition assessment using artificial neural network and structural equations", "abstract": "Building facilities condition assessment is considered a fundamental aspect of an effective decision-making maintenance management plan to fulfill service requirements. A noticeable dearth of studies is believed to have delivered condition assessment approaches for existing buildings; however, these approaches are still deemed premature, with some limitations in demand enhancement. This paper presents a novel physical condition assessment framework for existing educational buildings that contribute to the body of knowledge by offering a state-of-the-art approach incorporating an Artificial Neural Network (ANN) predictive model and a Structural Equation Model (SEM). The ANN predictive model aims to forecast the future condition-rating states for each facility component in various building spaces. Simultaneously, the SEM determines the proportionate weights of building facilities components. The primary objectives of this paper are to prioritize building components for maintenance purposes and record the potential effects of several parameters influencing the condition state of building components. These objectives can be achieved via four sequential modules: 1) scan to BIM module; 2) condition assessment prediction module; 3) proportionate weight determination module; and 4) entire space rating value module. Condition-monitoring data on six different buildings' internal components are analyzed to anticipate their future condition. The components carried out are: 1) wooden flooring tiles; 2) gypsum board ceiling tiles; 3) wooden doors; 4) wooden windows, 5) split air conditioner units; and 6) desktop computers. The overall coefficient of determination (R2) of the developed ANN models for the predicted six components conditions are 0.99, 0.99, 0.927, 0.88, 0.97, and 0.972, respectively.", "journal": "EXPERT SYSTEMS WITH APPLICATIONS", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000851504400001", "keywords": "Ensemble of deep neural networks; Ticket classification; Interpretable machine learning; Prediction explanation", "title": "I-CenterNet: Road infrared target detection based on improved CenterNet", "abstract": "Infrared target detection has strong anti-interference ability, long working distance and can work day and night. So it is widely used in military security and transportation fields, and infrared road object detection is critical in traffic checkpoints and autonomous driving. However, the target scale in infrared images changes greatly, small targets are difficult to detect, the poor image quality and low signal-to-noise ratio are still huge challenges in infrared target detection. This paper proposes an improved infrared target detection model I-CenterNet based on the anchor-free model CenterNet. The EfficientNetV2 with the channel attention mechanism is used instead of the traditional structure as the backbone network to enhance feature extraction. In order to reduce the noise of the input infrared image, Dilated-Residual U-net (DRUNet) is used. Meanwhile, feature pyramid and Sub-Pixel are combined for multi-scale feature fusion. Data enhancement is implemented to improve model performance. The experimental results show that the average detection accuracy of this model on the Flir infrared data set is 87.9%, and the average detection speed reaches 14.2 frames/s.", "journal": "IET IMAGE PROCESSING", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000516308200001", "keywords": "Quasi-flat zone hierarchy; Component-trees; Multi-valued component-trees; Image segmentation; Object tracking", "title": "Real-time maximally stable homogeneous regions", "abstract": "In this paper, we present the concept of maximally stable homogeneous regions (MSHR). MSHR are conceptually very similar to maximally stable extremal regions but can be segmented in images with an arbitrary number of channels. The computation of the presented MSHR relies on the construction of a quasi-flat zone hierarchy. We present a fast algorithm for computing the hierarchy that overcomes the runtime restrictions of existing approaches. The proposed algorithm can construct the quasi-flat zone hierarchy efficiently in real time, scales linearly in the number of pixels and, in practice, sub-linearly in the number of channels. In the experiments, we display how MSHR can be used to improve the results of optical character recognition systems and to perform 3D object segmentation. We further demonstrate the universality and speed of the proposed algorithm for three example applications: image segmentation, object tracking, and image filtering.", "journal": "JOURNAL OF REAL-TIME IMAGE PROCESSING", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000748722800002", "keywords": "Nighttime traffic scenes; Scene layout; Fixation; Vanishing point; Saliency detection", "title": "A new representation of scene layout improves saliency detection in traffic scenes", "abstract": "It has been well established that scene context plays an important role in directing visual attention. A robust representation of scene layout is expected to facilitate further analysis of traffic scenes, especially under challenging visual conditions like nighttime and/or on a small-scale dataset. In this work, a new layout representation for traffic scenes is proposed and applied to the popular visual task of saliency detection. First, a general layout representation for traffic scenes is defined as a combination of an original point (Vanishing Point, VP) and two axes along roadsides. Then, a simple algorithm is proposed to build a robust layout representation for traffic scenes, along with an improved VP detection method. Finally, to verify the contribution of the proposed layout representation, a layout-guided saliency detection framework is proposed to improve existing methods by integrating layout-guided prior learned from human fixations collected with an eye-tracking recorder. Experimental results show that the proposed layout representation can significantly improve the performance of various saliency detection methods including classical bottom-up methods and deep-learning-based methods. Moreover, compared with the deep-learning methods, the layout-guided method has an obvious advantage in terms of robustness when only a small-scale dataset is available and under varying visual scenes.", "journal": "EXPERT SYSTEMS WITH APPLICATIONS", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000631197400001", "keywords": "Medical image fusion; Convolutional neural network; Non-subsampled contourlet transform", "title": "Medical image fusion based on convolutional neural networks and non-subsampled contourlet transform", "abstract": "Although many powerful convolutional neural networks (CNN) have been applied to various image processing fields, due to the lack of datasets for network training and the significant different intensities of diverse multi modal source images at the same location, CNN cannot be directly used for the field of medical image fusion (MIF), which is a major problem and limits the development of this field. In this article, a novel multimodal medical image fusion method based on non-subsampled contourlet transform (NSCT) and CNN is presented. The proposed algorithm not only solves this problem, but also exploits the advantages of both NSCT and CNN to obtain better fusion results. In the proposed algorithm, source multi-modality images are decomposed into low and high frequency subbands. For high frequency subbands, a new perceptual high frequency CNN (PHF-CNN), which is trained in the frequency domain, is designed as an adaptive fusion rule. In the matter of the low frequency subband, two result maps are adopted to generate the decision map. Finally, fused frequency subbands are integrated by the inverse NSCT. To verify the effectiveness of the proposed algorithm, ten state-of-the-art MIF algorithms are selected as comparative algorithms. Subjective evaluations by five doctors as well as objective evaluations by seven image quality metrics, demonstrate that the proposed algorithm is superior to the other comparative algorithms in terms of fusing multimodal medical images.", "journal": "EXPERT SYSTEMS WITH APPLICATIONS", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000830886800006", "keywords": "Two-sided disassembly line; Line balancing; Sequence -dependent setup time; Constraint programming; Metaheuristic algorithms; Mixed -integer linear programming", "title": "Two-sided disassembly line balancing problem with sequence-dependent setup time: A constraint programming model and artificial bee colony algorithm", "abstract": "The large-size products can allow workers to perform tasks on both sides of the line. Hence, a two-sided disassembly line is preferred to ensure several advantages, such as a shorter line. The two-sided disassembly line balancing problem (TDLBP) is relatively new in the literature. This study considers the two-sided disas-sembly line balancing problem with sequence-dependent setup time (TDLBP-SDST) to reflect the real practice better, as sequence-dependent setup times may exist between tasks in many real-life applications. To the authors' best knowledge, sequence-dependent setup time has not been considered for the TDLBP in the current literature. The proposed problem creates a more complicated problem. Therefore, proposing effective solution techniques is more critical for obtaining better results. This study proposes two new mixed-integer linear programming models and a novel constraint programming (CP) model to define and solve the TDLBP-SDST. A genetic algorithm, an artificial bee colony algorithm, and the improved versions of these two algorithms are also developed to solve the large-size problems due to the NP-hardness of the TDLBP-SDST. Furthermore, a novel CP model is proposed for the standard TDLBP without considering sequence-dependent setup times. Initially, we compare the performance of the proposed CP model to those of the previous state-of-the-art methods in the literature for the TDLBP without sequence-dependent setup time. The computational results show that the proposed CP model out-performs all the other solution methods and reports the best-known results for all existing benchmark instances for the TDLBP. Then, we present the computational results of the proposed models and algorithms for the TDLBP-SDST. Computational study on a comprehensive set of generated instances indicates that the proposed solution methods effectively solve the TDLBP-SDST.", "journal": "EXPERT SYSTEMS WITH APPLICATIONS", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000736286700003", "keywords": "Phishing; Rule-based tree; Frauds prevent; Expert system; Prediction model", "title": "Piracema.io: A rules-based tree model for phishing prediction", "abstract": "Phishing has been consolidating itself as a chronic problem due to its approach to exploiting the end-user, seen as the weakest factor. Through social engineering, the attacker seeks a carelessness of the human being to intercept sensitive data. Concomitantly, the richness in details makes it more difficult to mitigate the attack by most anti-phishing mechanisms since they are sustained in classifying a malicious page that lacks visual and textual details. This study aims to present a rule-based model approach, called piracema.io, for phishing prediction. Compared with other solutions proposed in the literature, the study believes that it has a different model that increases its efficiency in prediction as phishing presents greater richness based on page reputationdriven. In the light of the results obtained in logistic regression, the study detected static and dynamic features, considering relevance, relationship, and similarity between them. As a proof of concept, the study uses a statistical approach to evaluate the prediction modeling over the gradual depth and adherent acting strategies adopted to the proposal. As a result, the study discusses the quantitative and qualitative data obtained by the proposal, presenting contributions, threats, and limitations, as well as perspectives for future work for the continuity and improvement of the model in its current state.", "journal": "EXPERT SYSTEMS WITH APPLICATIONS", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000806605800007", "keywords": "Forensic science; Forensic evidence; Criminal investigation; Fingerprint identification; Facial image identification; Deep neural network", "title": "Efficient bi-traits identification using CEDRNN classifier for forensic applications", "abstract": "The evidence, traits, in addition to clues that are amassed as of the Crime Scenes (CS) are analyzed by a process termed Forensic application. Nevertheless, the Forensic Analysis (FA) faces the challenges of irrelevant Feature Extraction (FE), which is because of the over impression of Finger-Print (FP), facial expressions, pose-invariant, together with poor lightings that bring about the mis-identification of the offender. To trounce such challenges, this paper proposes a bi-traits-based offender identification using Cross Entropy-based Deep Remainder Neural Network (CEDRNN) for forensic application. Pre-processing, FE, Feature Selection (FS), together with identification are '4' phases that the proposed model encompasses. In the pre-processing phase, the input data is processed via modifying the image size, augmenting the Image Contrast (IC), and neglecting the unwanted data. Then, as of the pre-processed data, the more apt and helpful data are extracted in FE. Then, the Gaussian Sailfish Optimization (GSO) technique selects the important data in the FS phase. Finally, the CEDRNN identifies the offender centered on the selected features. In this research, publically available datasets are utilized for comparing the outcomes of the CEDRNN with the previous top-notch algorithms. The experimental outcomes exhibited that the CEDRNN attains the highest accuracy and also effectively identifies the offender without mis-prediction.", "journal": "EXPERT SYSTEMS WITH APPLICATIONS", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Operations Research & Management Science", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000528836400004", "keywords": "image denoising; Gaussian noise; image matching; image filtering; BM3D-GT&amp; AD; improved BM3D denoising algorithm; Gaussian threshold; three-dimensional filtering; image denoising; hard thresholding; transform domain; frequency domain; image detail information; low amplitude; adaptable threshold; Gaussian function; high-frequency information; normalised angular distance; higher peak signal-to-noise ratio; Gaussian noise; denoised images; block-matching and three-dimensional filtering", "title": "BM3D-GT&AD: an improved BM3D denoising algorithm based on Gaussian threshold and angular distance", "abstract": "Block-matching and three-dimensional filtering (BM3D) is generally considered as a milestone for its outstanding performance in the area of image denoising. However, it still suffers from the loss of image detail due to the utilisation of hard thresholding on transform domain during the phase of the basic estimate. In the frequency domain, a large amount of image detail information is in high frequency, which tends to be mixed with noise. Since its low amplitude is below the threshold, some image detail is filtered out with the noise. To retain more details, this study proposes an improved BM3D. It adopts an adaptable threshold with the core of Gaussian function during hard thresholding, which can filter out more noise while retaining more high-frequency information. When grouping, the normalised angular distance is taken as a measure of similarity to relieve the interference of noise further and achieve a higher peak signal-to-noise ratio (PSNR). The experimental results show that under the background of Gaussian noise with standard deviation of 20-60, the PSNR of denoised images (with a large amount of detail), applied with the authors' improved algorithm, can be improved by $0.1 - 0.4 \\, {\\rm dB}$0.1-0.4dB compared with original BM3D.", "journal": "IET IMAGE PROCESSING", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000712269500002", "keywords": "Vertebra labeling and segmentation; Transformers; Arbitrary field-of-view spine CT; One-to-one set prediction; Joint regression and segmentation", "title": "Spine-transformers: Vertebra labeling and segmentation in arbitrary field-of-view spine CTs via 3D transformers", "abstract": "In this paper, we address the problem of fully automatic labeling and segmentation of 3D vertebrae in arbitrary Field-Of-View (FOV) CT images. We propose a deep learning-based two-stage solution to tackle these two problems. More specifically, in the first stage, the challenging vertebra labeling problem is solved via a novel transformers-based 3D object detector that views automatic detection of vertebrae in arbitrary FOV CT scans as a one-to-one set prediction problem. The main components of the new method, called Spine-Transformers, are a one-to-one set based global loss that forces unique predictions and a light-weighted 3D transformer architecture equipped with a skip connection and learnable positional embeddings for encoder and decoder, respectively. We additionally propose an inscribed sphere based object detector to replace the regular box-based object detector for a better handling of volume orientation variations. Our method reasons about the relationships of different levels of vertebrae and the global volume context to directly infer all vertebrae in parallel. In the second stage, the segmentation of the identified vertebrae and the refinement of the detected centers are then done by training one single multi-task encoder-decoder network for all vertebrae as the network does not need to identify which vertebra it is working on. The two tasks share a common encoder path but with different decoder paths. Comprehensive experiments are conducted on two public datasets and one in-house dataset. The experimental results demonstrate the efficacy of the present approach. (c) 2021 Elsevier B.V. All rights reserved.", "journal": "MEDICAL IMAGE ANALYSIS", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Engineering, Biomedical; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000712269500010", "keywords": "Spatial similarity-aware learning; Obsessive-compulsive disorder; Fused deep polynomial networks; Brain functional connectivity network", "title": "Diagnosis of obsessive-compulsive disorder via spatial similarity-aware learning and fused deep polynomial network", "abstract": "Obsessive-compulsive disorder (OCD) is a type of hereditary mental illness, which seriously affect the normal life of the patients. Sparse learning has been widely used in detecting brain diseases objectively by removing redundant information and retaining monitor valuable biological characteristics from the brain functional connectivity network (BFCN). However, most existing methods ignore the relationship between brain regions in each subject. To solve this problem, this paper proposes a spatial similarity aware learning (SSL) model to build BFCNs. Specifically, we embrace the spatial relationship between adjacent or bilaterally symmetric brain regions via a smoothing regularization term in the model. We develop a novel fused deep polynomial network (FDPN) model to further learn the powerful information and attempt to solve the problem of curse of dimensionality using BFCN features. In the FDPN model, we stack a multi-layer deep polynomial network (DPN) and integrate the features from multiple output layers via the weighting mechanism. In this way, the FDPN method not only can identify the high-level informative features of BFCN but also can solve the problem of curse of dimensionality. A novel framework is proposed to detect OCD and unaffected first-degree relatives (UFDRs), which combines deep learning and traditional machine learning methods. We validate our algorithm in the resting-state functional magnetic resonance imaging (rs-fMRI) dataset collected by the local hospital and achieve promising performance. (c) 2021 Elsevier B.V. All rights reserved.", "journal": "MEDICAL IMAGE ANALYSIS", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Engineering, Biomedical; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": ["machine learning", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000586783400001", "keywords": "area of feasible solutions; multivariate curve resolution; polygon inflation; rank&#8208; deficiency; self&#8208; modeling curve resolution", "title": "On the area of feasible solutions for rank-deficient problems: I. Introduction of a generalized concept", "abstract": "Rank deficiency of a spectral data matrix means that its rank is smaller than the number of the anticipated chemical components. A rank deficiency can hide the true chemical structure of the underlying pure components and complicates the application of multivariate curve resolution and self-modeling curve resolution techniques. A new approach for the analysis of the factor ambiguities is introduced, and the area of feasible solutions (AFS) is generalized to rank-deficient spectral data. The extended tools are tested for the Michaelis-Menten kinetics and abstract model data.", "journal": "JOURNAL OF CHEMOMETRICS", "category": "Automation & Control Systems; Chemistry, Analytical; Computer Science, Artificial Intelligence; Instruments & Instrumentation; Mathematics, Interdisciplinary Applications; Statistics & Probability", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000746302100001", "keywords": "Process visualisation; Legal process; Legislation; Flowcharts; Lawmaps", "title": "Lawmaps: enabling legal AI development through visualisation of the implicit structure of legislation and lawyerly process", "abstract": "Modelling that exploits visual elements and information visualisation are important areas that have contributed immensely to understanding and the computerisation advancements in many domains and yet remain unexplored for the benefit of the law and legal practice. This paper investigates the challenge of modelling and expressing structures and processes in legislation and the law by using visual modelling and information visualisation (InfoVis) to assist accessibility of legal knowledge, practice and knowledge formalisation as a basis for legal AI. The paper uses a subset of the well-defined Unified Modelling Language (UML) to visually express the structure and process of the legislation and the law to create visual flow diagrams called lawmaps, which form the basis of further formalisation. A lawmap development methodology is presented and evaluated by creating a set of lawmaps for the practice of conveyancing and the Landlords and Tenants Act 1954 of the United Kingdom. This paper is the first of a new breed of preliminary solutions capable of application across all aspects, from legislation to practice; and capable of accelerating development of legal AI.", "journal": "ARTIFICIAL INTELLIGENCE AND LAW", "category": "Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Law", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000636182000003", "keywords": "Energy not supplied; Shunt capacitors; Distributed generation units; Uncertainty source; Modified shuffled frog leaping algorithm; Demand response program", "title": "Optimal sizing of distributed generation units and shunt capacitors in the distribution system considering uncertainty resources by the modified evolutionary algorithm", "abstract": "Distributed generation units (DGUs) as auxiliary sources of power generation can play an effective role in meeting the load consumption of the distribution network, also have positive effects such as reducing loss and improving voltage. Moreover, capacitors by reactive power compensation produce positive effects similar to DGUs in the distribution networks. The idea of joint operation of DGUs and shunt capacitors (SCs) in the presence of demand response program (DRP) to derive maximum benefits from their installation is proposed in this paper. The time of use (TOU) mechanism is used as one of the demand response programs (DRPs) to alter the consumption pattern of subscribers and improve the performance of the distribution system. Objective functions include minimization of energy loss, operational cost, and energy not supplied (ENS). In general, the problem of determining the optimal capacity of DGUs and SCs is complex due to the demand variation. Also, considering the effect of uncertainty sources complicate the optimization problem. Hence, a modified shuffled frog leaping algorithm (MSFLA) is proposed to overcome the complexities of this problem. The proposed approach is tested on two 95, and 136-node test networks, and the results are compared with other evolutionary algorithms. According to the obtained results, after using the proposed approach in determining the optimal capacity of DGUs and SCs in the first system, the amount of energy loss, operational cost and ENS dropped by 11, 25.5 and 5% compared to baseline values. After applying the TOU mechanism in allocation of DGUs and SCs simultaneously in the second system by proposed method, the values obtained for the mentioned objectives reduced by 29, 65 and 7% compared to initial values.", "journal": "JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems; Telecommunications", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000566045300005", "keywords": "Deep learning modified neural network (DLMNN); Hadoop distributed file system (HDFS); Naive bayes (NB); K-means clustering (KMC); Improved K-means clustering (IKMC) algorithm", "title": "A predictive risk level classification of diabetic patients using deep learning modified neural network", "abstract": "In health care firm, data mining (DM) has an effectual role in predicting the diseases. Today, diabetes is the chief global health issue. Several algorithms are introduced for predicting the diabetes disease and its accuracy estimation. Yet, there is no effectual algorithm for providing the severity of diabetes in respect of ratio which interprets the impact of diabetes on different organs of the human body. To overcome such drawbacks, predictive and risk level classification of diabetes patients using DLMNN and Naive Bayes (NB) classification methods is system model. This system model system comprises 2 phases namely, phase-1: diabetic disease prediction model, and phase-2: risk analysis. In phase-1, the patient data are taken as of the dataset. Then, from this patient dataset repeated data are removed using HDFS Map Reduce (). Next, as the preprocessing stage, the missing attributes are replaced by averaging the considered data. After that, from the preprocessed data the disease is predicted using DLMNN classification method which results in obtaining the diabetic patient data. Then, the diabetic patient data are sent to phase-2. In phase 2, the missing attributes are replaced using the same average method. Next, the patient data is sorted centered on age utilizing recursive K-means clustering algorithm. Finally, the clustered patient data is classified using the NB classifier algorithm. Experiential results contrasted the system model modified deep learning algorithm with the existing IKMC algorithm in rapports of precision, accuracy, F-measure, and recall. The outcomes confirmed that the system model diabetes prediction and analysis model shows better results on considering the existent methods.", "journal": "JOURNAL OF AMBIENT INTELLIGENCE AND HUMANIZED COMPUTING", "category": "Computer Science, Artificial Intelligence; Computer Science, Information Systems; Telecommunications", "annotated_keywords": ["neural net", "deep learning", "deep learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000805820500014", "keywords": "Semantics; Biological system modeling; Cognition; Task analysis; Legged locomotion; Computational modeling; Message passing; Human parsing; hierarchical model; relation reasoning; graph neural network", "title": "Hierarchical Human Semantic Parsing With Comprehensive Part-Relation Modeling", "abstract": "Modeling the human structure is central for human parsing that extracts pixel-wise semantic information from images. We start with analyzing three types of inference processes over the hierarchical structure of human bodies: direct inference (directly predicting human semantic parts using image information), bottom-up inference (assembling knowledge from constituent parts), and top-down inference (leveraging context from parent nodes). We then formulate the problem as a compositional neural information fusion (CNIF) framework, which assembles the information from the three inference processes in a conditional manner, i.e., considering the confidence of the sources. Based on CNIF, we further present a part-relation-aware human parser (PRHP), which precisely describes three kinds of human part relations, i.e., decomposition, composition, and dependency, by three distinct relation networks. Expressive relation information can be captured by imposing the parameters in the relation networks to satisfy specific geometric characteristics of different relations. By assimilating generic message-passing networks with their edge-typed, convolutional counterparts, PRHP performs iterative reasoning over the human body hierarchy. With these efforts, PRHP provides a more general and powerful form of CNIF, and lays the foundation for more sophisticated and flexible human relation patterns of reasoning. Experiments on five datasets demonstrate that our two human parsers outperform the state-of-the-arts in all cases.", "journal": "IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE", "category": "Computer Science, Artificial Intelligence; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000842066600044", "keywords": "Actuator saturation; describing function; generalized Nyquist criterion; global consensus; multiagent system", "title": "On Consensus of Second-Order Multiagent Systems With Actuator Saturations: A Generalized-Nyquist-Criterion-Based Approach", "abstract": "In this article, a frequency-domain approach is developed to deal with the global consensus problem for a class of general second-order multiagent systems (MASs) subject to actuator saturations. By employing the describing function and the generalized Nyquist criterion, the global consensus problem is thoroughly investigated for both undirected and directed topologies. First, the describing function is introduced to characterize the actuator saturations in the s-plane, and the inherent representation error is quantitatively analyzed from a frequency-domain perspective. Then, by means of the Kronecker product, the addressed consensus problem of the MAS is transformed into a corresponding stability analysis problem for a certain multi-input-multi-output (MIMO) system and, consequently, the generalized Nyquist criterion for MIMO systems is exploited to derive the condition for the global consensus of the MAS where the impact from the actuator saturation is explicitly reflected. Finally, numerical simulations are provided to illustrate the validity of the proposed theoretical result.", "journal": "IEEE TRANSACTIONS ON CYBERNETICS", "category": "Automation & Control Systems; Computer Science, Artificial Intelligence; Computer Science, Cybernetics", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000395866800054", "keywords": "", "title": "Prediction of lung tumor motion using a generalized neural network optimized from the average prediction outcome of a group of patients", "abstract": "", "journal": "MEDICAL PHYSICS", "category": "Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000412160600001", "keywords": "finite-time anti-synchronization; intermittent adjustment feedback control; Lyapunov functional", "title": "Stability Analysis of Impulsive Stochastic Reaction-Diffusion Cellular Neural Network with Distributed Delay via Fixed Point Theory", "abstract": "This paper investigates the stochastically exponential stability of reaction-diffusion impulsive stochastic cellular neural networks (CNN). The reaction-diffusion pulse stochastic system model characterizes the complexity of practical engineering and brings about mathematical difficulties, too. However, the difficulties have been overcome by constructing a new contraction mapping and an appropriate distance on a product space which is guaranteed to be a complete space. This is the first time to employ the fixed point theorem to derive the stability criterion of reaction-diffusion impulsive stochastic CNN with distributed time delays. Finally, an example is provided to illustrate the effectiveness of the proposed methods.", "journal": "COMPLEXITY", "category": "Mathematics, Interdisciplinary Applications; Multidisciplinary Sciences", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000414039900012", "keywords": "Water level; Turbidity; Optical sensors; Suspended sediment concentration; Artificial neural networks", "title": "Estimate of Suspended Sediment Concentration from Monitored Data of Turbidity and Water Level Using Artificial Neural Networks", "abstract": "Artificial neural networks (ANNs) are promising alternatives for the estimation of suspended sediment concentration (SSC), but they are dependent on the availability data. This study investigates the use of an ANN model for forecasting SSC using turbidity and water level. It is used an original method, idealized to investigate the minimum complexity of the ANN that does not present, in relation to more complex networks, loss of efficiency when applied to other samples, and to perform its training avoiding the overfitting even when data availability is insufficient to use the cross-validation technique. The use of a validation procedure by resampling, the control of overfitting through a previously researched condition of training completion, as well as training repetitions to provide robustness are important aspects of the method. Turbidity and water level data, related to 59 SSC values, collected between June 2013 and October 2015, were used. The development of the proposed ANN was preceded by the training of an ANN, without the use of the new resources, which clearly showed the overfitting occurrence when resources were not used to avoid it, with Nash-Sutcliffe efficiency (NS) equals to 0.995 in the training and NS = 0.788 in the verification. The proposed method generated efficient models (NS = 0.953 for verification), with well distributed errors and with great capacity of generalization for future applications. The final obtained model enabled the SSC calculation, from water level and turbidity data, even when few samples were available for the training and verification procedures.", "journal": "WATER RESOURCES MANAGEMENT", "category": "Engineering, Civil; Water Resources", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000404054700014", "keywords": "Customer identification; Feature selection; Class imbalance; Absolute rarity; Transfer learning; Group method of data handling", "title": "A multi-scale convolutional neural network for phenotyping high-content cellular images", "abstract": "Motivation: Identifying phenotypes based on high-content cellular images is challenging. Conventional image analysis pipelines for phenotype identification comprise multiple independent steps, with each step requiring method customization and adjustment of multiple parameters. Results: Here, we present an approach based on a multi-scale convolutional neural network (M-CNN) that classifies, in a single cohesive step, cellular images into phenotypes by using directly and solely the images' pixel intensity values. The only parameters in the approach are the weights of the neural network, which are automatically optimized based on training images. The approach requires no a priori knowledge or manual customization, and is applicable to single- or multichannel images displaying single or multiple cells. We evaluated the classification performance of the approach on eight diverse benchmark datasets. The approach yielded overall a higher classification accuracy compared with state-of-the-art results, including those of other deep CNN architectures. In addition to using the network to simply obtain a yes-or-no prediction for a given phenotype, we use the probability outputs calculated by the network to quantitatively describe the phenotypes. This study shows that these probability values correlate with chemical treatment concentrations. This finding validates further our approach and enables chemical treatment potency estimation via CNNs.", "journal": "BIOINFORMATICS", "category": "Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000386808000001", "keywords": "Ultrasonic evaluation; Welded joints; Feature extraction; Neural networks; PCA", "title": "Efficient feature selection for neural network based detection of flaws in steel welded joints using ultrasound testing", "abstract": "This work studies methods for efficient extraction and selection of features in the context of a decision support system based on neural networks. The data comes from ultrasonic testing of steel welded joints, in which are found three types of flaws. The discrete Fourier, wavelet and cosine transforms are applied for feature extraction. Statistical techniques such as principal component analysis and the Wilcoxon-Mann-Whitney test are used for optimal feature selection. Two different artificial neural network architectures are used for automatic classification. Through the proposed approach, it is achieved a high discrimination efficiency by using only 20 features to feed the classifier, instead of the original 2500 A-scan sample points. (C) 2016 Elsevier B.V. All rights reserved.", "journal": "ULTRASONICS", "category": "Acoustics; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000392748700010", "keywords": "neurocontrollers; observers; nonlinear control systems; feedback; MIMO systems; adaptive control; nonlinear functions; function approximation; control system synthesis; neural networks; uncertain nonlinear function approximation; nonlinear disturbance observer; state observer; output-feedback neural controller design; coordinate transform; state trajectories; input disturbances; multiple-input multiple-output pure-feedback systems; adaptive neural backstepping control; input saturation; MIMO pure-feedback nonlinear systems; observer-based neural control", "title": "Drug drug interaction extraction from biomedical literature using syntax convolutional neural network", "abstract": "Motivation: Detecting drug-drug interaction (DDI) has become a vital part of public health safety. Therefore, using text mining techniques to extract DDIs from biomedical literature has received great attentions. However, this research is still at an early stage and its performance has much room to improve. Results: In this article, we present a syntax convolutional neural network (SCNN) based DDI extraction method. In this method, a novel word embedding, syntax word embedding, is proposed to employ the syntactic information of a sentence. Then the position and part of speech features are introduced to extend the embedding of each word. Later, auto-encoder is introduced to encode the traditional bag-of-words feature (sparse 0-1 vector) as the dense real value vector. Finally, a combination of embedding-based convolutional features and traditional features are fed to the softmax classifier to extract DDIs from biomedical literature. Experimental results on the DDIExtraction 2013 corpus show that SCNN obtains a better performance (an F-score of 0.686) than other state-of-the-art methods.", "journal": "BIOINFORMATICS", "category": "Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000401202300012", "keywords": "Price forecasting; Generalized neuron model; Environment adaptation method; Wavelet transform; IEAM", "title": "Short term electricity price forecast based on environmentally adapted generalized neuron", "abstract": "The liberalization of the power markets gained a remarkable momentum in the context of trading electricity as a commodity. With the upsurge in restructuring of the power markets, electricity price plays a dominant role in the current deregulated market scenario which is majorly influenced by the economics being governed. In the deregulated environment price forecasting is an important aspect for the power system planning. The problem of price forecasting can be entirely viewed as a signal processing problem with proper estimation of model parameters, modeling of uncertainties, etc. Among the different existing models the artificial neural network based models have gained wide popularity due their black box structure but it too has its own limitations. In the proposed work in order to overcome the limitations of the classical artificial neural network model, generalized neuron model is used for forecasting the short term electricity price of Australian electricity market. The pre-processing of the input parameters is accomplished using wavelet transform for better representation of the low and high frequency components. The free parameters of the generalized neuron model are tuned using environment adaptation method algorithm for increasing the generalization ability and efficacy of the model. (C) 2017 Elsevier Ltd. All rights reserved.", "journal": "ENERGY", "category": "Thermodynamics; Energy & Fuels", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000371555000012", "keywords": "Multiscale convolutional neural network (MCNN); Deep learning; Feature extraction; Remote sensing image classification", "title": "Learning multiscale and deep representations for classifying remotely sensed imagery", "abstract": "It is widely agreed that spatial features can be combined with spectral properties for improving interpretation performances on very-high-resolution (VHR) images in urban areas. However, many existing methods for extracting spatial features can only generate low-level features and consider limited scales, leading to unpleasant classification results. In this study, multiscale convolutional neural network (MCNN) algorithm was presented to learn spatial-related deep features for hyperspectral remote imagery classification. Unlike traditional methods for extracting spatial features, the MCNN first transforms the original data sets into a pyramid structure containing spatial information at multiple scales, and then automatically extracts high-level spatial features using multiscale training data sets. Specifically, the MCNN has two merits: (1) high-level spatial features can be effectively learned by using the hierarchical learning structure and (2) multiscale learning scheme can capture contextual information at different scales. To evaluate the effectiveness of the proposed approach, the MCNN was applied to classify the well-known hyperspectral data sets and compared with traditional methods. The experimental results shown a significant increase in classification accuracies especially for urban areas. (C) 2016 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V. All rights reserved.", "journal": "ISPRS JOURNAL OF PHOTOGRAMMETRY AND REMOTE SENSING", "category": "Geography, Physical; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000371553600006", "keywords": "Proton exchange membrane fuel cell; Modeling; Simulation; Operation optimization; Artificial neural network", "title": "Modeling and operation optimization of a proton exchange membrane fuel cell system for maximum efficiency", "abstract": "This paper presents an operation optimization method and demonstrates its application to a proton exchange membrane fuel cell system. A constrained optimization problem was formulated to maximize the efficiency of a fuel cell system by incorporating practical models derived from actual operations of the system. Empirical and semi-empirical models for most of the system components were developed based on artificial neural networks and semi-empirical equations. Prior to system optimizations, the developed models were validated by comparing simulation results with the measured ones. Moreover, sensitivity analyses were performed to elucidate the effects of major operating variables on the system efficiency under practical operating constraints. Then, the optimal operating conditions were sought at various system power loads. The optimization results revealed that the efficiency gaps between the worst and best operation conditions of the system could reach 1.2-5.5% depending on the power output range. To verify the optimization results, the optimal operating conditions were applied to the fuel cell system, and the measured results were compared with the expected optimal values. The discrepancies between the measured and expected values were found to be trivial, indicating that the proposed operation optimization method was quite successful for a substantial increase in the efficiency of the fuel cell system. (C) 2016 Elsevier Ltd. All rights reserved.", "journal": "ENERGY CONVERSION AND MANAGEMENT", "category": "Thermodynamics; Energy & Fuels; Mechanics", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000411244200002", "keywords": "Convolutional neural networks (CNNs); deep global descriptors; handcrafted descriptors; hybrid nested invariance pooling; MPEG CDVA; MPEG CDVS", "title": "HNIP: Compact Deep Invariant Representations for Video Matching, Localization, and Retrieval", "abstract": "With emerging demand for large-scale video analysis, MPEG initiated the compact descriptor for video analysis (CDVA) standardization in 2014. Beyond handcrafted descriptors adopted by the current MPEG-CDVA reference model, we study the problem of deep learned global descriptors for video matching, localization, and retrieval. First, inspired by a recent invariance theory, we propose a nested invariance pooling (NIP) method to derive compact deep global descriptors from convolutional neural networks (CNNs), by progressively encoding translation, scale, and rotation invariances into the pooled descriptors. Second, our empirical studies have shown that a sequence of well designed pooling moments (e.g.,max or average) may drastically impact video matching performance, which motivates us to design hybrid pooling operations via NIP (HNIP). HNIP has further improved the discriminability of deep global descriptors. Third, the technical merits and performance improvements by combining deep and handcrafted descriptors are provided to better investigate the complementary effects. We evaluate the effectiveness of HNIP within the well-established MPEG-CDVA evaluation framework. The extensive experiments have demonstrated that HNIP outperforms the state-of-the-art deep and canonical handcrafted descriptors with significant mAP gains of 5.5% and 4.7%, respectively. In particular the combination of HNIP incorporated CNN descriptors and handcrafted global descriptors has significantly boosted the performance of CDVA core techniques with comparable descriptor size.", "journal": "IEEE TRANSACTIONS ON MULTIMEDIA", "category": "Computer Science, Information Systems; Computer Science, Software Engineering; Telecommunications", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000338390200040", "keywords": "Metaheuristics; Operations research; Scheduling; Hybrid flowshop", "title": "An improved migrating birds optimisation for a hybrid flowshop scheduling with total flowtime minimisation", "abstract": "Migrating birds optimisation (MBO) is a new nature-inspired metaheuristic for combinatorial optimisation problems. This paper proposes an improved MBO to minimise the total flowtime for a hybrid flowshop scheduling problem, which has important practical applications in modern industry. A diversified method is presented to form an initial population spread out widely in solution space. A mixed neighbourhood is constructed for the leader and the following birds to easily find promising neighbouring solutions. A leaping mechanism is developed to help MBO escape from suboptimal solutions. Problem-specific heuristics and local search procedures are added to enhance the MBO's intensification capability. Extensive comparative evaluations are conducted with seven recently published algorithms in the literature. The results indicate that the proposed MBO is effective in comparison after comprehensive computational and statistical analyses. (C) 2014 Elsevier Inc. All rights reserved.", "journal": "INFORMATION SCIENCES", "category": "Computer Science, Information Systems", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000342773800012", "keywords": "Average dwell time; Markovian jump linear systems; stochastic finite-time boundness; stochastic finite-time stability", "title": "Finite-time H-infinity control for discrete-time Markovian jump systems subject to average dwell time", "abstract": "This paper investigates the finite-time H control problem for discrete-time Markovian jump linear systems with time delay and norm-bounded exogneous disturbance. The stochastic finite-time boundness is achieved by employing an average dwell time approach, which has more design freedom by allowing the stochastic Lyapunov-like function (SLLF) to increase more slowly during the running time of each operation mode. Note that the SLLF increases at every jumping instant rather than at every sampling time instant. Sufficient conditions for the solvability of the controller, which are dependent or independent on the time delay, are given in terms of linear matrix inequalities. Numerical examples are given to verify the efficiency of the proposed method.", "journal": "TRANSACTIONS OF THE INSTITUTE OF MEASUREMENT AND CONTROL", "category": "Automation & Control Systems; Instruments & Instrumentation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000309242500034", "keywords": "Designer; Welding process selection; Fuzzy data envelopment analysis; Supplier; TOPSIS", "title": "Welding process selection for repairing nodular cast iron engine block by integrated fuzzy data envelopment analysis and TOPSIS approaches", "abstract": "The selection of welding process is one of the most significant decision making problems and it requires a wide range of information in accordance with the type of product. Hence, the automation of knowledge through a knowledge-based system will greatly enhance the decision-making process. A combined fuzzy data envelopment analysis (DEA) and TOPSIS is proposed to investigate the relative welding process selection factors and it can compare and evaluate different welding processes. The proposed approach compares each decision making unit (DMU) with the worst and the ideal virtual DMU and it ranks them via the relative closeness index. The proposed approach is used for ranking eleven welding processes which are commonly used for repairing nodular cast iron engine block in four cases and it is shown that the approach is sensitive to changes in dataset. (c) 2012 Elsevier Ltd. All rights reserved.", "journal": "MATERIALS & DESIGN", "category": "Materials Science, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000328663200003", "keywords": "Fault tolerance; Globally asynchronous locally synchronous; Low power system; Massively-parallel architecture; Spiking neural networks; System-on-chip", "title": "SpiNNaker: Fault tolerance in a power- and area- constrained large-scale neuromimetic architecture", "abstract": "SpiNNaker is a biologically-inspired massively-parallel computer designed to model up to a billion spiking neurons in real-time. A full-fledged implementation of a SpiNNaker system will comprise more than 10(5) integrated circuits (half of which are SDRAMs and half multicore systems-on-chip). Given this scale, it is unavoidable that some components fail and, in consequence, fault-tolerance is a foundation of the system design. Although the target application can tolerate a certain, low level of failures, important efforts have been devoted to incorporate different techniques for fault tolerance. This paper is devoted to discussing how hardware and software mechanisms collaborate to make SpiNNaker operate properly even in the very likely scenario of component failures and how it can tolerate system-degradation levels well above those expected. (C) 2013 The Authors. Published by Elsevier B.V. All rights reserved.", "journal": "PARALLEL COMPUTING", "category": "Computer Science, Theory & Methods", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000340155000008", "keywords": "fabrication; spinning; fiber; yarn; fabric; formation", "title": "Recognition of fault process conditions based on spinline tension in melt spinning", "abstract": "Proper setting of process conditions in the melt spinning setup is one way to yield uniform quality of spinline tension. The optimum setting to give uniform spinline tension is determined using experiment plans in the Taguchi method and significant process parameters are also identified. When the setting shifts from the optimum, the spinline tension becomes non-uniform and downgrades product quality. This study aims to diagnose single or double fault conditions of those significant process parameters based on the spinline tension signal. The critical procedures of fault diagnosis are feature extraction and classification. The tension signal is decomposed into a wavelet packet tree of four resolution levels. Four entropies from the best-basis wavelet packet tree and the lowest entropy at level four are selected as features. The back-propagation neural network acts as a classifier. The experimental results demonstrate that the features and classifier actually work well to identify the single and double fault conditions with high accuracy in melt spinning.", "journal": "TEXTILE RESEARCH JOURNAL", "category": "Materials Science, Textiles", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000403326400006", "keywords": "neuromorphic computing; photonic integrated circuits; ultrafast information processing; excitable semiconductor lasers", "title": "Progress in neuromorphic photonics", "abstract": "As society's appetite for information continues to grow, so does our need to process this information with increasing speed and versatility. Many believe that the one-size-fits-all solution of digital electronics is becoming a limiting factor in certain areas such as data links, cognitive radio, and ultrafast control. Analog photonic devices have found relatively simple signal processing niches where electronics can no longer provide sufficient speed and reconfigurability. Recently, the landscape for commercially manufacturable photonic chips has been changing rapidly and now promises to achieve economies of scale previously enjoyed solely by microelectronics. By bridging the mathematical prowess of artificial neural networks to the underlying physics of optoelectronic devices, neuromorphic photonics could breach new domains of information processing demanding significant complexity, low cost, and unmatched speed. In this article, we review the progress in neuromorphic photonics, focusing on -photonic integrated devices. The challenges and design rules for optoelectronic instantiation of artificial neurons are presented. The proposed photonic architecture revolves around the processing network node composed of two parts: a nonlinear element and a network interface. We then survey excitable lasers in the recent literature as candidates for the nonlinear node and microring-resonator weight banks as the network interface. Finally, we compare metrics between neuromorphic electronics and neuromorphic photonics and discuss potential applications.", "journal": "NANOPHOTONICS", "category": "Nanoscience & Nanotechnology; Materials Science, Multidisciplinary; Optics; Physics, Applied", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000333520600016", "keywords": "Genetic algorithm; Stationary and batch-fed culture; Laccase; Daedalea flavida", "title": "Improved production of laccase by Daedalea flavida: consideration of evolutionary process optimization and batch-fed culture", "abstract": "The genetic algorithm was used effectively to find the optimal values of eight process variables for the maximum laccase production by Daedalea flavida in a stationary culture. The algorithm was modified suitably to improve laccase production with 18 parallel experiments in 4 generations. A high enzyme titer of 65 % was achieved after the optimization and compared to the titer obtained before optimization. To study the effect of the surface immobilized growth on the enzyme production, the fungus was grown on three solid carriers. When cultured on polymer composite fibers, polyurethane foam, or steel wool, at least 2.5 times more biomass was produced, compared to the biomass produced in support-free growth. On the contrary, the mycelia grown on solid support produced much less laccase than non-adhering mycelia. Four parallel runs of batch-fed cultures were done, using the cell mass of D. flavida to evaluate the influence of four different volumes of medium exchanged on laccase production. For sustainable production of the enzyme, complete exchange of medium was favorable, where the laccase activity increased continuously in six consecutive cycles, though, 50 % exchange of medium produced the maximum laccase in terms of mean enzyme activity obtained in six cycles.", "journal": "BIOPROCESS AND BIOSYSTEMS ENGINEERING", "category": "Biotechnology & Applied Microbiology; Engineering, Chemical", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000353754700007", "keywords": "Cu2+; Pb2+; Simultaneous determination; CWT; PLS", "title": "Chemometrics-assisted spectrophotometric method for simultaneous determination of Pb2+ and Cu2+ ions in different foodstuffs, soil and water samples using 2-benzylspiro [isoindoline-1,5 '-oxazolidine]-2 ',3,4 '-trione using continuous wavelet transformation and partial least squares - Calculation of pK(f) of complexes with rank annihilation factor analysis", "abstract": "A new multi-component analysis method based on zero-crossing point-continuous wavelet transformation (CWT) was developed for simultaneous spectrophotometric determination of Cu2+ and Pb2+ ions based on the complex formation with 2-benzyl espiro[isoindoline-1,5oxasolidine]-2,3,4 trione (BSIIOT). The absorption spectra were evaluated with respect to synthetic ligand concentration, time of complexation and pH. Therefore according the absorbance values, 0.015 mmol L-1 BSIIOT, 10 min after mixing and pH 8.0 were used as optimum values. The complex formation between BSIIOT ligand and the cations Cu2+ and Pb2+ by application of rank annihilation factor analysis (RAFA) were investigated. Daubechies-4 (db4), discrete Meyer (dmey), Monet (morl) and Symlet-8 (sym8) continuous wavelet transforms for signal treatments were found to be suitable among the wavelet families. The applicability of new synthetic ligand and selected mother wavelets were used for the simultaneous determination of strongly overlapped spectra of species without using any pre-chemical treatment. Therefore, CWT signals together with zero crossing technique were directly applied to the overlapping absorption spectra of Cu2+ and Pb2+. The calibration graphs for estimation of Pb2+ and Cu2+ were obtained by measuring the CWT amplitudes at zero crossing points for Cu2+ and Pb2+ at the wavelet domain, respectively. The proposed method was validated by simultaneous determination of Cu2+ and Pb2+ ions in red beans, walnut, rice, tea and soil samples. The obtained results of samples with proposed method have been compared with those predicted by partial least squares (PLS) and flame atomic absorption spectrophotometiy (FAAS). (C) 2015 Elsevier B.V. All rights reserved.", "journal": "SPECTROCHIMICA ACTA PART A-MOLECULAR AND BIOMOLECULAR SPECTROSCOPY", "category": "Spectroscopy", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000350935300010", "keywords": "Fisheries ecology; VMS; Pair trawl; Mediterranean; Sustainability", "title": "Modelling the strategy of mid-water trawlers targeting small pelagic fish in the Adriatic Sea and its drivers", "abstract": "Mid-water pair trawling (PTM) targeting small pelagic resources represents a key fishing activity in the Adriatic Sea. This fishery is experiencing a long period of crisis due to resource depletion and the lack of appropriate market strategies, and vessels spend most of the time searching for fishing schools. The searching strategy largely depends on the interaction between vessels: the captains of the PTM units take their decision also checking the position and the fishing status of other vessels. Understanding this strategy represents a key step towards a more effective resource management, since strategies directly determine the pattern of fishing effort. A Conditional Logit model has been devised to analyze fishermen's strategy as a non-cooperative game. This category of games is characterized by the existence of (at least) one equilibrium point - a Nash Equilibrium - in which each player plays his strategy, that is a Best Response to the strategies of the other players. This equilibrium point was estimated for the different scenarios defined by environmental (sea surface temperature and atmospheric pressure) and economic (fuel and fish prices at market) variables. Vessel Monitoring System data were used to capture fleet activity, while different datasets were collected to reconstruct environmental and economic drivers. Results indicate a good predictive power of the model, and suggest that the equilibrium strategy that guides units' behaviour is invariant with respect to environmental conditions, whereas it is largely influenced by economic factors. These latter, via strategies, may determine important consequences on the resources in terms of exploited areas and the impact of fishing activity. In particular, a low fuel price when fish price is high leads to higher values of CPUE, and then to a more efficient but also impacting fishing activity. (C) 2014 Elsevier B.V. All rights reserved.", "journal": "ECOLOGICAL MODELLING", "category": "Ecology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000388922100011", "keywords": "Cooperative systems; learning systems; software engineering", "title": "Interactive Teachable Cognitive Agents: Smart Building Blocks for Multiagent Systems", "abstract": "Developing a complex intelligent system by abstracting their behaviors, functionalities, and reasoning mechanisms can be tedious and time consuming. In this paper, we present a framework for developing an application or software system based on smart autonomous components that collaborate with the developer or user to realize the entire system. Inspired by teachable approaches and programming-by-demonstration methods in robotics and end-user development, we treat intelligent agents as teachable components that make up the system to be built. Each agent serves different functionalities and may have prebuilt operations to accomplish its own design objectives. However, each agent may also be equipped with in-built social-cognitive traits to interact with the user or other agents in order to adapt its own operations, objectives, and relationships with others. The results of adaptation can be in the form of groups or multiagent systems as new aggregated components. This approach is made to tackle the difficulties in completely programming the entire system by allowing the user to teach the components toward the desired behaviors in the situated context of the application. We exemplify this novel method with cases in the domains of human-like agents in virtual environment and agents for in-house caregiving.", "journal": "IEEE TRANSACTIONS ON SYSTEMS MAN CYBERNETICS-SYSTEMS", "category": "Automation & Control Systems; Computer Science, Cybernetics", "annotated_keywords": ["intelligent agent"], "label": "1", "title_label": "1"}
{"id": "WOS:000390722100011", "keywords": "Building clustering; Electricity losses; Data mining; Semi-supervised learning; Deep-learning framework", "title": "Building energy modeling (BEM) using clustering algorithms and semi-supervised machine learning approaches", "abstract": "Energy efficiency is a critical element of building energy conservation. Energy Information Administration (EIA) and International Electrotechnical Commission (IEC) estimated that over 6% of electrical energy was lost during transmission and distribution. Sensing and tracking technologies, and data-mining offer new windows to better understanding these losses in real-time. Recent developments in energy optimization computational methods also allow engineers to better characterize energy consumption load profiles. The paper focuses on developing new and robust data-mining techniques to explore large and complex data generated by sensing and tracking technologies. These techniques would potentially offer new avenues to understand and prevent energy losses during transmission. The paper presents two new concepts: First, a set of clustering algorithms that model the supply-demand characterization of four different substations clusters, and second, a semi-supervised machine learning and clustering technique are developed to optimize the losses and automate the process of identifying loss factors contributing to the total loss. This three-step process uses real-time data from buildings and the substations that supply electricity to the buildings to develop the proposed technique. The preliminary findings of this paper help the utility service providers to understand the energy supply-demand requirements. (C) 2016 Elsevier B.V. All rights reserved.", "journal": "AUTOMATION IN CONSTRUCTION", "category": "Construction & Building Technology; Engineering, Civil", "annotated_keywords": ["machine learning", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000393636700027", "keywords": "Fuzzy logic; Adaptation; Sensors; Robotics; Artificial intelligence; Sensing structure; Conductive silicone rubber; Contact detection; Adaptive neuro fuzzy system", "title": "Machine learning for quantum physics", "abstract": "Purpose - Tactile sensing is the process of determining physical properties and events through contact with objects in the world. The purpose of this paper is to establish a novel design of an adaptive neuro-fuzzy inference system (ANFIS) for estimation of contact position of a new tactile sensing structure. Design/methodology/approach - The major task is to investigate implementations of carbon-black-filled silicone rubber for tactile sensation; the silicone rubber is electrically conductive and its resistance changes by loading or unloading strains. Findings - The sensor-elements for the tactile sensing structure were made by press-curing from carbon-black-filled silicone rubber. The experimental results can be used as training and checking data for the ANFIS network. Originality/value - This system is capable to find any change of contact positions and thus indicates state of the current contact location of the tactile sensing structure. The behavior of the use silicone rubber shows strong non-linearity, therefore, the sensor cannot be used for high accurate measurements. The greatest advantage of this sensing material lies in its high elasticity.", "journal": "SCIENCE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000317337000014", "keywords": "Bayesian inference; change detection; machine learning; multiple sclerosis; new lesion segmentation; subtraction imaging", "title": "Refining Diagnostic MicroRNA Signatures by Whole-miRNome Kinetic Analysis in Acute Myocardial Infarction", "abstract": "BACKGROUND: Alterations in microRNA (miRNA) expression patterns in whole blood may be useful biomarkers of diverse cardiovascular disorders. We previously reported that miRNAs are significantly dysregulated in acute myocardial infarction (AMI) and applied machine-learning techniques to define miRNA subsets with high diagnostic power for AMI diagnosis. However, the kinetics of the time-dependent sensitivity of these novel miRNA biomarkers remained unknown. METHODS: To characterize temporal changes in the expressed human miRNAs (miRNome), we performed here the first whole-genome miRNA kinetic study in AMI patients. We measured miRNA expression levels at multiple time points (0, 2, 4, 12, 24 h after initial presentation) in patients with acute ST-elevation myocardial infarction by using microfluidic primer extension arrays and quantitative real-time PCR. As a prerequisite, all patients enrolled had to have cardiac troponin T concentrations <50 ng/L on admission as measured with a high-sensitivity assay. RESULTS: We found a subset of miRNAs to be significantly dysregulated both at initial presentation and during the course of AMI. Additionally, we identified novel miRNAs that are dysregulated early during myocardial infarction, such as miR-1915 and miR-181c*. CONCLUSIONS: The present proof-of-concept study provides novel insights into the dynamic changes of the human miRNome during AMI. (C) 2012 American Association for Clinical Chemistry", "journal": "CLINICAL CHEMISTRY", "category": "Medical Laboratory Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000317373600005", "keywords": "Cellular neural network (CNN); modular cellular neural network (MCNN); wave computing; diffusion; trigger wave; edge detection", "title": "Modular Cellular Neural Network Structure for Wave-Computing-Based Image Processing", "abstract": "This paper introduces the modular cellular neural network (CNN), which is a new CNN structure constructed from nine one-layer modules with intercellular interactions between different modules. The new network is suitable for implementing many image processing operations. Inputting an image into the modules results in nine outputs. The topographic characteristic of the cell interactions allows the outputs to introduce new properties for image processing tasks. The stability of the system is proven and the performance is evaluated in several image processing applications. Experiment results on texture segmentation show the power of the proposed structure. The performance of the structure in a real edge detection application using the Berkeley dataset BSDS300 is also evaluated.", "journal": "ETRI JOURNAL", "category": "Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000395995200002", "keywords": "Multi-view learning; Statistical learning theory; Co-training; Co-regularization; Margin consistency", "title": "Machine learning based interatomic potential for amorphous carbon", "abstract": "We introduce a Gaussian approximation potential (GAP) for atomistic simulations of liquid and amorphous elemental carbon. Based on a machine learning representation of the density-functional theory (DFT) potentialenergy surface, such interatomic potentials enable materials simulations with close- to DFT accuracy but at much lower computational cost. We first determine the maximum accuracy that any finite-range potential can achieve in carbon structures; then, using a hierarchical set of two-, three-, and many-body structural descriptors, we construct a GAP model that can indeed reach the target accuracy. The potential yields accurate energetic and structural properties over a wide range of densities; it also correctly captures the structure of the liquid phases, at variance with a state-of-the-art empirical potential. Exemplary applications of the GAP model to surfaces of \"diamondlike\" tetrahedral amorphous carbon (ta-C) are presented, including an estimate of the amorphous material's surface energy and simulations of high-temperature surface reconstructions (\"graphitization\"). The presented interatomic potential appears to be promising for realistic and accurate simulations of nanoscale amorphous carbon structures.", "journal": "PHYSICAL REVIEW B", "category": "Materials Science, Multidisciplinary; Physics, Applied; Physics, Condensed Matter", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000340625100013", "keywords": "Blurred objects segmentation; Isolabel-contour map; Support vector machine; Supervised machine learning; Brain lesion segmentation", "title": "Functionally diverse dendritic mRNAs rapidly associate with ribosomes following a novel experience", "abstract": "The subcellular localization and translation of messenger RNA (mRNA) supports functional differentiation between cellular compartments. In neuronal dendrites, local translation of mRNA provides a rapid and specific mechanism for synaptic plasticity and memory formation, and might be involved in the pathophysiology of certain brain disorders. Despite the importance of dendritic mRNA translation, little is known about which mRNAs can be translated in dendrites in vivo and when their translation occurs. Here we collect ribosome-bound mRNA from the dendrites of CA1 pyramidal neurons in the adult mouse hippocampus. We find that dendritic mRNA rapidly associates with ribosomes following a novel experience consisting of a contextual fear conditioning trial. High throughput RNA sequencing followed by machine learning classification reveals an unexpected breadth of ribosome-bound dendritic mRNAs, including mRNAs expected to be entirely somatic. Our findings are in agreement with a mechanism of synaptic plasticity that engages the acute local translation of functionally diverse dendritic mRNAs.", "journal": "NATURE COMMUNICATIONS", "category": "Multidisciplinary Sciences", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000343730600017", "keywords": "Biodiversity; macroecological modelling; method; non-stationarity; prediction; spatial scale; species distribution modelling", "title": "Generalizing the use of geographical weights in biodiversity modelling", "abstract": "AimDetermining how ecological processes vary across space is a major focus in ecology. Current methods that investigate such effects remain constrained by important limiting assumptions. Here we provide an extension to geographically weighted regression in which local regression and spatial weighting are used in combination. This method can be used to investigate non-stationarity and spatial-scale effects using any regression technique that can accommodate uneven weighting of observations, including machine learning. InnovationWe extend the use of spatial weights to generalized linear models and boosted regression trees by using simulated data for which the results are known, and compare these local approaches with existing alternatives such as geographically weighted regression (GWR). The spatial weighting procedure (1) explained up to 80% deviance in simulated species richness, (2) optimized the normal distribution of model residuals when applied to generalized linear models versus GWR, and (3) detected nonlinear relationships and interactions between response variables and their predictors when applied to boosted regression trees. Predictor ranking changed with spatial scale, highlighting the scales at which different species-environment relationships need to be considered. Main conclusionsGWR is useful for investigating spatially varying species-environment relationships. However, the use of local weights implemented in alternative modelling techniques can help detect nonlinear relationships and high-order interactions that were previously unassessed. Therefore, this method not only informs us how location and scale influence our perception of patterns and processes, it also offers a way to deal with different ecological interpretations that can emerge as different areas of spatial influence are considered during model fitting.", "journal": "GLOBAL ECOLOGY AND BIOGEOGRAPHY", "category": "Ecology; Geography, Physical", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000410836000007", "keywords": "Amiodarone; beta blocker; big data; calcium channel blocker; critical care; machine learning", "title": "MANAGEMENT OF ATRIAL FIBRILLATION WITH RAPID VENTRICULAR RESPONSE IN THE INTENSIVE CARE UNIT: A SECONDARY ANALYSIS OF ELECTRONIC HEALTH RECORD DATA", "abstract": "Purpose: Atrial fibrillation with rapid ventricular response (RVR) is common during critical illness. In this study, we explore the comparative effectiveness of three commonly used drugs (metoprolol, diltiazem, and amiodarone) in the management of atrial fibrillation with RVR in the intensive care unit (ICU). Methods: Data pertaining to the first ICU admission were extracted from the Medical Information Mart for Intensive Care III database. Patients who received one of the above pharmacologic agents while their heart rate was> 110 bpm and had atrial fibrillation documented in the clinical chart were included. Propensity score weighting using a generalized boosted model was used to compare medication failure rates (second agent prior to termination of RVR). Secondary outcomes included time to control, control within 4 h, and mortality. Results: One thousand six hundred forty-six patients were included: 736 received metoprolol, 292 received diltiazem, and 618 received amiodarone. Compared with those who received metoprolol, failure rates were higher amongst those who received amiodarone (OR 1.39, 95% CI 1.03-1.87, P = 0.03) and there was a trend towards increased failure rates in patients who received diltiazem (OR 1.35, CI 0.89-2.07, P = 0.16). Amongst patients who received a single agent, patients who received diltiazem were less likely to be controlled at 4-h than those who received metoprolol (OR 0.64, CI 0.43-097, P = 0.03). Initial agent was not associated with in-hospital mortality. Conclusions: In this study, metoprolol was the most commonly used agent for atrial fibrillation with RVR. Metoprolol had a lower failure rate than amiodarone and was superior to diltiazem in achieving rate control at 4 h.", "journal": "SHOCK", "category": "Critical Care Medicine; Hematology; Surgery; Peripheral Vascular Disease", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000401240800001", "keywords": "Food allergy; epigenetics; DNA methylation profiling; Infinium; methylation profiling; epigenetic epidemiology; allergy epigenetics; biomarkers; shrunken centroids", "title": "Inductive Supervised Quantum Learning", "abstract": "In supervised learning, an inductive learning algorithm extracts general rules from observed training instances, then the rules are applied to test instances. We show that this splitting of training and application arises naturally, in the classical setting, from a simple independence requirement with a physical interpretation of being nonsignaling. Thus, two seemingly different definitions of inductive learning happen to coincide. This follows from the properties of classical information that break down in the quantum setup. We prove a quantum de Finetti theorem for quantum channels, which shows that in the quantum case, the equivalence holds in the asymptotic setting, that is, for large numbers of test instances. This reveals a natural analogy between classical learning protocols and their quantum counterparts, justifying a similar treatment, and allowing us to inquire about standard elements in computational learning theory, such as structural risk minimization and sample complexity.", "journal": "PHYSICAL REVIEW LETTERS", "category": "Physics, Multidisciplinary", "annotated_keywords": ["learning algorithm", "supervised learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000432440802024", "keywords": "Depression; Memory; Reward; Dopamine; Reinforcement learning", "title": "The Neuroscience of Positive Memory Deficits in Unipolar Depression", "abstract": "Background: Testing for venous thromboembolism (VTE) is associated with cost and risk to patients (e.g. radiation). To assess the appropriateness of imaging utilization at the provider level, it is important to know that provider's diagnostic yield (percentage of tests positive for the diagnostic entity of interest). However, determining diagnostic yield typically requires either time-consuming, manual review of radiology reports or the use of complex and/or proprietary natural language processing software. Objectives: The objectives of this study were twofold: 1) to develop and implement a simple, user configurable, and open-source natural language processing tool to classify radiology reports with high accuracy and 2) to use the results of the tool to design a provider-specific VTE imaging dashboard, consisting of both utilization rate and diagnostic yield. Methods: Two physicians reviewed a training set of 400 lower extremity ultrasound (UTZ) and computed tomography pulmonary angiogram (CTPA) reports to understand the language used in VTE-positive and VTE-negative reports. The insights from this review informed the arguments to the five modifiable parameters of the NLP tool. A validation set of 2,000 studies was then independently classified by the reviewers and by the tool; the classifications were compared and the performance of the tool was calculated. Results: The tool was highly accurate in classifying the presence and absence of VTE for both the UTZ (sensitivity 95.7%; 95% CI 91.5-99.8, specificity 100%; 95% CI 100-100) and CTPA reports (sensitivity 97.1%; 95% CI 94.3-99.9, specificity 98.6%; 95% CI 97.8-99.4). The diagnostic yield was then calculated at the individual provider level and the imaging dashboard was created. Conclusions: We have created a novel NLP tool designed for users without a background in computer programming, which has been used to classify venous thromboembolism reports with a high degree of accuracy. The tool is open-source and available for download at http://iturrate.comisimpleNLP. Results obtained using this tool can be applied to enhance quality by presenting information about utilization and yield to providers via an imaging dashboard. (c) 2017 Elsevier B.V. All rights reserved.", "journal": "BIOLOGICAL PSYCHIATRY", "category": "Neurosciences; Psychiatry", "annotated_keywords": ["natural language processing"], "label": "1", "title_label": "0"}
{"id": "WOS:000362429900011", "keywords": "Expert system; Diagnosis; Sugarcane; CaneDES; Rule; Classified knowledge", "title": "CaneDES: A Web-Based Expert System for Disorder Diagnosis in Sugarcane", "abstract": "Sugarcane being a long duration crop of 10-18 months faces many biotic and abiotic stresses which causes heavy losses in cane production and productivity. Timely and accurate diagnosis, of disorders that appear in crop due to these stresses, is required to reduce the losses. Paper illustrates CaneDES, an expert system to diagnose disorders in sugarcane caused by various biotic and abiotic stresses. It is a web based system available in both Hindi and English languages with remote accessibility to sugarcane farmers. A new classified knowledge representation technique has been adopted for the development of expert system, to manage the diagnostic knowledge efficiently. System was evaluated with the help of domain experts and extension personnel engaged in the services of sugarcane farmers. Evaluation was made in terms of time taken by extension personnel in diagnosis of disorder and confidence factor received from the system and results indicated high usability and efficiency of the system. System is of generic nature and can be used in other crop environment with changes in knowledge base.", "journal": "SUGAR TECH", "category": "Agronomy", "annotated_keywords": ["expert system"], "label": "1", "title_label": "1"}
{"id": "WOS:000320497800010", "keywords": "Macaque monkey; Visual cortex; Neurophysiology; Reinforcement learning", "title": "Neuronal mechanisms of visual perceptual learning", "abstract": "Numerous psychophysical studies have described perceptual learning as long-lasting improvements in perceptual discrimination and detection capabilities following practice. Where and how long-term plastic changes occur in the brain is central to understanding the neural basis of perceptual learning. Here, neurophysiological research using non-human primates is reviewed to address the neural mechanisms underlying visual perceptual learning. Previous studies have shown that training either has no effect on or only weakly alters the sensitivity of neurons in early visual areas, but more recent evidence indicates that training can cause long-term changes in how sensory signals are read out in the later stages of decision making. These results are discussed in the context of learning specificity, which has been crucial in interpreting the mechanisms underlying perceptual learning. The possible mechanisms that support learning-related plasticity are also discussed. (C) 2013 The Authors. Published by Elsevier B.V. All rights reserved.", "journal": "BEHAVIOURAL BRAIN RESEARCH", "category": "Behavioral Sciences; Neurosciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000314471000014", "keywords": "Audio classification; Sparse coefficients; Dictionary learning; Support vector machines", "title": "Scoring relevancy of features based on combinatorial analysis of Lasso with application to lymphoma diagnosis", "abstract": "One challenge in applying bioinformatic tools to clinical or biological data is high number of features that might be provided to the learning algorithm without any prior knowledge on which ones should be used. In such applications, the number of features can drastically exceed the number of training instances which is often limited by the number of available samples for the study. The Lasso is one of many regularization methods that have been developed to prevent overfitting and improve prediction performance in high-dimensional settings. In this paper, we propose a novel algorithm for feature selection based on the Lasso and our hypothesis is that defining a scoring scheme that measures the \"quality\" of each feature can provide a more robust feature selection method. Our approach is to generate several samples from the training data by bootstrapping, determine the best relevance-ordering of the features for each sample, and finally combine these relevance-orderings to select highly relevant features. In addition to the theoretical analysis of our feature scoring scheme, we provided empirical evaluations on six real datasets from different fields to confirm the superiority of our method in exploratory data analysis and prediction performance. For example, we applied FeaLect, our feature scoring algorithm, to a lymphoma dataset, and according to a human expert, our method led to selecting more meaningful features than those commonly used in the clinics. This case study built a basis for discovering interesting new criteria for lymphoma diagnosis. Furthermore, to facilitate the use of our algorithm in other applications, the source code that implements our algorithm was released as FeaLect, a documented R package in CRAN.", "journal": "BMC GENOMICS", "category": "Biotechnology & Applied Microbiology; Genetics & Heredity", "annotated_keywords": ["learning algorithm"], "label": "1", "title_label": "0"}
{"id": "WOS:000390643000012", "keywords": "Pollution control flow; Correlation and regression analyses; Artificial neural networks; Drought management; Multi-reservoir system", "title": "Determination of the pollution control flow for drought management in a multi-reservoir system", "abstract": "The pollution, changing according to various standards, is directly proportional to water quality in rivers. In this study, data and restrictions prescribed for standards of water quality such as Turkish Standards Institute (TSI), European Commission (EC), and the World Health Organization (WHO) were used to determine water pollution. For this purpose, correlation analysis was made to identify strong relationships between data. Regression analysis and Arti fi cial Neural Networks (ANN) models are developed based on these standards by data obtained from correlation analysis. The Lower Sakarya River is selected as application area, and measurement of the water quality values of this river is used in these models. Pollution control flows in the river are obtained by the ANN models and regression analysis. The obtained results are compared with regard to these standards. (C) 2016 Sharif University of Technology. All rights reserved.", "journal": "SCIENTIA IRANICA", "category": "Engineering, Multidisciplinary", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000406938300005", "keywords": "HPA/HPG axis; nerve growth factors (NGFs); neurocognition; pathogenesis of psychiatric disorders; stress", "title": "Regulatory role of NGFs in neurocognitive functions", "abstract": "Nerve growth factors (NGFs), especially the prototype NGF and brain-derived neurotrophic factor (BDNF), have a diverse array of functions in the central nervous system through their peculiar set of receptors and intricate signaling. They are implicated not only in the development of the nervous system but also in regulation of neurocognitive functions like learning, memory, synaptic transmission, and plasticity. Evidence even suggests their role in continued neurogenesis and experience-dependent neural network remodeling in adult brain. They have also been associated extensively with brain disorders characterized by neurocognitive dysfunction. In the present article, we aimed to make an exhaustive review of literature to get a comprehensive view on the role of NGFs in neurocognitive functions in health and disease. Starting with historical perspective, distribution in adult brain, implied molecular mechanisms, and developmental basis, this article further provides a detailed account of NGFs' role in specified neurocognitive functions. Furthermore, it discusses plausible NGF-based homeostatic and adaptation mechanisms operating in the pathogenesis of neurocognitive disorders and has presents a survey of such disorders. Finally, it elaborates on current evidence and future possibilities in therapeutic applications of NGFs with an emphasis on recent research updates in drug delivery mechanisms. Conclusive remarks of the article make a strong case for plausible role of NGFs in comprehensive regulation of the neurocognitive functions and pathogenesis of related disorders and advocate that future research should be directed to explore use of NGF-based mechanisms in the prevention of implicated diseases as well as to target these molecules pharmacologically.", "journal": "REVIEWS IN THE NEUROSCIENCES", "category": "Neurosciences", "annotated_keywords": ["neural net"], "label": "0", "title_label": "0"}
{"id": "WOS:000467977400006", "keywords": "Locomotion mode recognition; strain gauge; convolutional neural network; robotic transtibial prosthesis", "title": "A strain gauge based locomotion mode recognition method using convolutional neural network", "abstract": "Locomotion mode recognition can contribute to precise control of active lower-limb prostheses in different environments. In this paper, we propose a novel locomotion mode recognition method based on convolutional neural network and strain gauge signals. The strain gauge only provides one-dimensional signals and is also used in the control strategy of the robotic prosthesis. The convolutional neural network takes the raw noisy signals as inputs. Three transtibial amputee subjects were recruited in the experiments, and three locomotion modes were recognized. The overall three-class locomotion mode recognition accuracy is in the hold-out test and in the 5-fold cross-validation. The results show that the strain gauge contains information of locomotion modes, and the convolutional neural network has the capacity of extracting features from raw signals.", "journal": "ADVANCED ROBOTICS", "category": "Robotics", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000471016700050", "keywords": "magnetic refrigeration; magnetocaloric effect; LaFe13-x - yCoxSiy; gadolinium; artificial neural network; modelling", "title": "Evaluating Magnetocaloric Effect in Magnetocaloric Materials: A Novel Approach Based on Indirect Measurements Using Artificial Neural Networks", "abstract": "The thermodynamic characterisation of magnetocaloric materials is an essential task when evaluating the performance of a cooling process based on the magnetocaloric effect and its application in a magnetic refrigeration cycle. Several methods for the characterisation of magnetocaloric materials and their thermodynamic properties are available in the literature. These can be generally divided into theoretical and experimental methods. The experimental methods can be further divided into direct and indirect methods. In this paper, a new procedure based on an artificial neural network to predict the thermodynamic properties of magnetocaloric materials is reported. The results show that the procedure provides highly accurate predictions of both the isothermal entropy and the adiabatic temperature change for two different groups of magnetocaloric materials that were used to validate the procedure. In comparison with the commonly used techniques, such as the mean field theory or the interpolation of experimental data, this procedure provides highly accurate, time-effective predictions with the input of a small amount of experimental data. Furthermore, this procedure opens up the possibility to speed up the characterisation of new magnetocaloric materials by reducing the time required for experiments.", "journal": "ENERGIES", "category": "Energy & Fuels", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000459341002493", "keywords": "dissipativity; stochastic fuzzy neural network; time-varying delay", "title": "Generation of Realistic (in silico) Histopathologic Images Using Generative Models Based on Deep Neural Networks", "abstract": "This paper presents an enrollment management model by applying artificial neural network (ANN). The aim of the research, which has been presented in this paper, is to show that ANNs are more successful in predicting than the classical statistical method - regression analysis (logistic regression). Both predictive models, no matter whether they are based on ANNs or logistic regression, offer satisfactory predictive results, and they can offer support in the decision-making process. However, the model based on neural networks shows certain advantages. ANNs demand understanding of functional connection between independent and dependent variables in order to evaluate the model. Also, they adapt easily to related independent variables, without the appearance of the problem of multicollinearity. In contrast to logistic regression, neural networks can recognize the appearance of nonlinearity and interactions in input data, and they can react on time.", "journal": "LABORATORY INVESTIGATION", "category": "Medicine, Research & Experimental; Pathology", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000492000700006", "keywords": "Urban metagenomics; Multi-label classification; Neural network", "title": "Prediction of microbial communities for urban metagenomics using neural network approach", "abstract": "Background Microbes are greatly associated with human health and disease, especially in densely populated cities. It is essential to understand the microbial ecosystem in an urban environment for cities to monitor the transmission of infectious diseases and detect potentially urgent threats. To achieve this goal, the DNA sample collection and analysis have been conducted at subway stations in major cities. However, city-scale sampling with the fine-grained geo-spatial resolution is expensive and laborious. In this paper, we introduce MetaMLAnn, a neural network based approach to infer microbial communities at unsampled locations given information reflecting different factors, including subway line networks, sampling material types, and microbial composition patterns. Results We evaluate the effectiveness of MetaMLAnn based on the public metagenomics dataset collected from multiple locations in the New York and Boston subway systems. The experimental results suggest that MetaMLAnn consistently performs better than other five conventional classifiers under different taxonomic ranks. At genus level, MetaMLAnn can achieve F1 scores of 0.63 and 0.72 on the New York and the Boston datasets, respectively. Conclusions By exploiting heterogeneous features, MetaMLAnn captures the hidden interactions between microbial compositions and the urban environment, which enables precise predictions of microbial communities at unmeasured locations.", "journal": "HUMAN GENOMICS", "category": "Genetics & Heredity", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000478790400007", "keywords": "Total dissolved solids; Artificial neural network; Principal component regression; Multivariate statistical analysis; Machine learning methods; Bias and precision", "title": "Training artificial neural network for optimization of nanostructured VO2-based smart window performance", "abstract": "In this work, we apply for the first time a machine learning approach to design and optimize VO2 based nanostructured smart window performance. An artificial neural network was trained to find the relationship between VO2 smart window structural parameters and performance metrics-luminous transmittance (T-lum) and solar modulation (Delta T-sol), calculated by first-principle electromagnetic simulations (FDTD method). Once training was accomplished, the combination of optimal and T(lum )and Delta T-sol was found by applying classical trust region algorithm on the trained network. The proposed method allows flexibility in definition of the optimization problem and provides clear uncertainty limits for future experimental realizations. (C) 2019 Optical Society of America under the terms of the OSA Open Access Publishing Agreement", "journal": "OPTICS EXPRESS", "category": "Optics", "annotated_keywords": ["neural net", "neural net", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000472635900092", "keywords": "static young's modulus; artificial neural networks; self-adaptive differential evolution algorithm; sandstone reservoirs", "title": "Estimation of Static Young's Modulus for Sandstone Formation Using Artificial Neural Networks", "abstract": "In this study, we used artificial neural networks (ANN) to estimate static Young's modulus (E-static) for sandstone formation from conventional well logs. ANN design parameters were optimized using the self-adaptive differential evolution optimization algorithm. The ANN model was trained to predict E-static from conventional well logs of the bulk density, compressional time, and shear time. The ANN model was trained on 409 data points from one well. The extracted weights and biases of the optimized ANN model was used to develop an empirical relationship for E-static estimation based on well logs. This empirical correlation was tested on 183 unseen data points from the same training well and validated using data from three different wells. The optimized ANN model estimated E-static for the training dataset with a very low average absolute percentage error (AAPE) of 0.98%, a very high correlation coefficient (R) of 0.999 and a coefficient of determination (R-2) of 0.9978. The developed ANN-based correlation estimated E-static for the testing dataset with a very high accuracy as indicated by the low AAPE of 1.46% and a very high R and R-2 of 0.998 and 0.9951, respectively. In addition, the visual comparison of the core-tested and predicted E-static of the validation dataset confirmed the high accuracy of the developed ANN-based empirical correlation. The ANN-based correlation overperformed four of the previously developed E-static correlations in estimating E-static for the validation data, E-static for the validation data was predicted with an AAPE of 3.8% by using the ANN-based correlation compared to AAPE's of more than 36.0% for the previously developed correlations.", "journal": "ENERGIES", "category": "Energy & Fuels", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000470094903045", "keywords": "Complex-valued neural networks; Attracting sets; Invariant sets; Infinite distributed delays; Integro-differential inequality", "title": "INCORPORATION OF TEMPORAL INFORMATION IN A DEEP NEURAL NETWORK IMPROVES PERFORMANCE LEVEL FOR AUTOMATED POLYP DETECTION AND DELINEATION", "abstract": "In this paper, we investigate the asymptotic property of non-autonomous complex-valued neural networks with time-varying delays and infinite distributed delays. By using the property of M-matrix, an integro-differential inequality is established. Based on the inequality and some sufficient conditions, we obtain the attracting and invariant sets of non-autonomous complex-valued neural networks with time-varying delays and infinite distributed delays. An example with numerical simulation is given out to illustrate the effectiveness of our results.", "journal": "GASTROINTESTINAL ENDOSCOPY", "category": "Gastroenterology & Hepatology", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000439328900007", "keywords": "Tungsten inert gas; regression analysis; heat-affected zone; under-bead depression; artificial neural network", "title": "Achieving optimized tungsten inert gas butt welding conditions of thin cold rolled steel sheets by response surface methodology and artificial neural networks", "abstract": "This paper describes the multiresponse optimization of tungsten inert gas welding for an optimal parametric combination to predict the weld characteristics of thin cold rolled steel sheets. The interaction effects of tungsten inert gas welding process parameters such as welding current, arc length, and traverse speed have been observed on the weld characteristic responses such as weld width, heat-affected zone width, and under-bead depression. Full factorial design of experiment was followed for determining the combinations of the experimental runs. Regression analysis was carried out to develop the mathematical models for the welding control factors and responses. Analysis of variance was used to check the adequacy of the developed mathematical model. The confirmatory tests were conducted to validate the accuracy of mathematical model. Sensitivity analysis was also done to analyze the effect of each individual process parameter on the weld responses. The full factorial experimental data was further utilized for multi-response optimization of the tungsten inert gas process parameters. It was observed that weld responses like weld width and heat-affected zone width could be optimized by regression modeling technique while the under-bead depression showed uncertain behavior. The under-bead depression ranged from 0.0 to 0.15mm and was observed only when the arc traverse speed was at the lowest level (4cm/min) for all values of the welding current and arc length. The experimental data were also modeled using the artificial neural network technique for the prediction of weld responses and the results were compared with that from the regression analysis.", "journal": "PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART E-JOURNAL OF PROCESS MECHANICAL ENGINEERING", "category": "Engineering, Mechanical", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000479284500012", "keywords": "GRA; BPNNGA; SKD 61; Wire-EDM", "title": "Multi-objective optimization in wire-EDM process using grey relational analysis method (GRA) and backpropagation neural network-genetic algorithm (BPNN-GA) methods", "abstract": "Purpose The purpose of this paper is to investigate prediction and optimization of multiple performance characteristics in the wire electrical discharge machining (wire-EDM) process of SKD 61 (AISI H13) tool steel. Design/methodology/approach The experimental studies were conducted under varying wire-EDM process parameters, which were arc on time, on time, open voltage, off time and servo voltage. The optimized responses were recast layer thickness (RLT), surface roughness (SR) and surface crack density (SCD). Arc on time was set at two different levels, whereas the other four parameters were set at three different levels. Based on Taguchi method, an L18 mixed-orthogonal array was selected for the experiments. Further, three methods, namely grey relational analysis (GRA), backpropagation neural network (BPNN) and genetic algorithm (GA), were applied separately. GRA was performed to obtain a rough estimation of optimum drilling parameters. The influences of drilling parameters on multiple performance characteristics were determined by using percentage contributions. BPNN architecture was determined to predict the multiple performance characteristics. GA method was then applied to determine the optimum wire-EDM parameters. Findings The minimum RLT, SR and SCD could be obtained by setting arc on time, on time, open voltage, off time and servo voltage at 2 ms, 3 ms, 90 volt, 10 ms and 38 volt, respectively. The experimental confirmation results showed that BPNN-based GA optimization method could accurately predict and significantly improve all of the responses. Originality/value There were no publications regarding multi-response optimization using a combination of GRA and BPNN-based GA methods during wire-EDM process available.", "journal": "MULTIDISCIPLINE MODELING IN MATERIALS AND STRUCTURES", "category": "Materials Science, Multidisciplinary; Mechanics", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000493892800013", "keywords": "Electronic expansion valve; Artificial neuron network; Principal component analysis; Variable refrigerant flow", "title": "An electronic expansion valve modeling framework development using artificial neural network: A case study on VRF systems", "abstract": "Electronic expansion valves (EEV) are widely used in variable refrigerant flow systems (VRF) to control the mass flow rate of each indoor unit. EEV model is used to predict the mass flow rate through an EEV. While the power-law correlation method has been used to build the EEV model so far, Artificial Neural Network (ANN) methods have been adapted to model the EEV with a fixed speed compressor thanks to its higher accuracy. However, the EEV is typically working with the variable speed compressor in a VRF system. In addition, the parameters used in ANN modeling could be further optimized. The objective of this study is to develop an EEV modeling framework and optimize the input parameter number and hidden neuron number. We presented the framework through an EEV model development for a VRF system. For these, we used the field test data, applied a principal components analysis approach in optimizing the ANN input parameter number, and investigated the proper number of hidden neurons and appropriate transfer function pairs. We found the performance of the ANN model would not improve much as the number of input parameters and the number of hidden neurons reached a threshold. Only three transfer function pairs are suitable for this case in total nine groups we studied. Our ANN model has the average absolute deviation of 2.2%. The framework can be easily applied to EEV modeling in other VRF and air condition systems with necessary data support. (C) 2019 Elsevier Ltd and IIR. All rights reserved.", "journal": "INTERNATIONAL JOURNAL OF REFRIGERATION-REVUE INTERNATIONALE DU FROID", "category": "Thermodynamics; Engineering, Mechanical", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000486107000001", "keywords": "consensus; DC-ADMM; distributed optimization; IDC-ADMM; network resource allocation", "title": "Distributed inexact dual consensus ADMM for network resource allocation", "abstract": "This paper investigates two novel distributed algorithms based on alternating direction method of multipliers (ADMM) for network resource allocation of N agents. The main objective is to derive an optimal allocation that minimizes a global objective expressed as a sum of locally known separable convex objective functions. Based on a communication matrix, the dual resource allocation problem is changed into a consensus optimization problem, in which each agent broadcasts the outcome of its local processing to all his neighbors. In this paper, we first propose a new distributed dual consensus ADMM (DC-ADMM) algorithm to address this consensus problem. Moreover, by applying an inexact step for each ADMM update, a distributed inexact DC-ADMM (IDC-ADMM) is also developed, which enables agents to perform cheap computation at each iteration. Finally, numerical simulations are delivered to illustrate and validate the proposed algorithm.", "journal": "OPTIMAL CONTROL APPLICATIONS & METHODS", "category": "Automation & Control Systems; Operations Research & Management Science; Mathematics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000469812500016", "keywords": "Airship; artificial neural network; dynamic modelling; lighter than air vehicle; nonlinear auto-regressive model with exogenous inputs model; system identification", "title": "Nonlinear auto-regressive neural network for mathematical modelling of an airship using experimental data", "abstract": "Autonomous flight of an aerial vehicle requires a sufficiently accurate mathematical model, which can capture system dynamics in the presence of external disturbances. Artificial neural network is known for ideal in capturing systems behaviour, where little knowledge about vehicle dynamics is available. In this paper, we explored this potential of artificial neural network for characterizing nonlinear dynamics of an unmanned airship. The flight experimentation data for an outdoor experimental airship are acquired through a series of pre-determined flight tests. The experimental data are subjected to a class of dynamic recurrent neural network model dubbed as nonlinear auto-regressive model with exogenous inputs for training. Sufficiently trained neural network model captured and demonstrated the longitudinal dynamics of the airship satisfactorily. We also demonstrated the usefulness of proposed technique for Lotte airship, wherein the performance of proposed model is validated and analysed for the Lotte airship flight test data.", "journal": "PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART G-JOURNAL OF AEROSPACE ENGINEERING", "category": "Engineering, Aerospace; Engineering, Mechanical", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000467564700107", "keywords": "Deep learning; smart building; smart city; thermal comfort", "title": "Thermal Comfort Modeling for Smart Buildings: A Fine-Grained Deep Learning Approach", "abstract": "The emerging Internet of Things (IoT) technology enables smart building management and operation to improve building energy efficiency and occupant thermal comfort. In this paper, we perform data analysis using the IoT generated building data to derive accurate thermal comfort model for smart building control. Deep neural network (DNN) is used to model the relationship between the controllable building operations and thermal comfort. As thermal comfort is determined by multiple comfort factors, a fine-grained architecture is proposed, where an exclusive model is trained for each factor and accordingly the corresponding thermal comfort can be evaluated. The experimental results show that the proposed fine-grained DNN outperforms its coarse-grained counterpart by 3.5x and is 1.7x, 2.5x, 2.4x, and 1.9x more accurate compared to four popular machine learning algorithms. Besides, DNN's performance promotes with deeper network topology and more neurons, and a simple topology with the same number of neurons per network hidden layer is sufficient to achieve high modeling accuracy. Finally, the derived thermal comfort model reveals a linear relationship between comfort and air conditioning setpoint. The linear property helps quickly and accurately search for the optimal controllable setpoint with the desired comfort.", "journal": "IEEE INTERNET OF THINGS JOURNAL", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": ["neural net", "machine learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000449241700006", "keywords": "Useful energy demand; Self-Organizing Maps; Building energy efficiency; Energy parameters; Energy performance; Procedural modelling; Random parameters; Unsupervised data mining", "title": "Revealing the relationships between the energy parameters of single-family buildings with the use of Self-Organizing Maps", "abstract": "With a large number of factors affecting the energy efficiency of buildings, the importance of analyzing the growing amount of data becomes important. The aim of this research is to check whether the use of Self-Organizing Maps will allow the indication of the relationships between building features which are considered important from the point of view of their energy performance. The research was carried out on a sample of 5040 randomly generated variants of single-family buildings with a fixed volume and location. These models were next subject to clustering, based on selected features, with the use of Self-Organizing Maps. The results prove the suitability of the method used. Grouping analysis shows the dependencies between particular parameters, confirming the importance of the U-value of partitions, the thermal mass of the building and its air-tightness for energy efficiency, while discovering unexpected relationships such as the irrelevancy of a building's orientation. In addition to expanding knowledge about the relationships between building features affecting their energy performance, it may allow optimal parameters for the given initial conditions to be found. (C) 2018 Elsevier B.V. All rights reserved.", "journal": "ENERGY AND BUILDINGS", "category": "Construction & Building Technology; Energy & Fuels; Engineering, Civil", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000427013400006", "keywords": "intermittent control; mode-dependent dwell time; stability; switched stochastic systems; time-varying discretized Lyapunov function", "title": "Unified dwell time-based stability and stabilization criteria for switched linear stochastic systems and their application to intermittent control", "abstract": "This paper considers the stability and stabilization problems for the switched linear stochastic systems under dwell time constraints, where the considered systems can be composed of an arbitrary combination of stable and unstable subsystems. First, a time-varying discretized Lyapunov function is constructed based on the projection of a linear Lagrange interpolant and a switching-time-dependent \"weighted\" function. The \"weighted\" function not only enforces the Lyapunov function to decrease at switching instants but also coordinates the dynamical behavior of the subsystems. As a result, some unified criteria for mean square stability and almost sure stability of the switched stochastic systems are established in terms of linear matrix inequalities. Based on the obtained stochastic stability criteria, 2 types of state feedback controllers for the systems are designed. Moreover, the novel results are applied to solve the intermittent control or the controller failure problems. Finally, conservatism analysis and numerical examples are provided to illustrate the effectiveness of the established theoretical results.", "journal": "INTERNATIONAL JOURNAL OF ROBUST AND NONLINEAR CONTROL", "category": "Automation & Control Systems; Engineering, Electrical & Electronic; Mathematics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000424898000003", "keywords": "Neural network; Kalman filter; Microelectrode; Lower urinary tract; Bladder; Dorsal root ganglia; DRG", "title": "Evaluation of Decoding Algorithms for Estimating Bladder Pressure from Dorsal Root Ganglia Neural Recordings", "abstract": "A closed-loop device for bladder control may offer greater clinical benefit compared to current open-loop stimulation devices. Previous studies have demonstrated the feasibility of using single-unit recordings from sacral-level dorsal root ganglia (DRG) for decoding bladder pressure. Automatic online sorting, to differentiate single units, can be computationally heavy and unreliable, in contrast to simple multi-unit thresholded activity. In this study, the feasibility of using DRG multi-unit recordings to decode bladder pressure was examined. A broad range of feature selection methods and three algorithms (multivariate linear regression, basic Kalman filter, and a nonlinear autoregressive moving average model) were used to create training models and provide validation fits to bladder pressure for data collected in seven anesthetized feline experiments. A non-linear autoregressive moving average (NARMA) model with regularization provided the most accurate bladder pressure estimate, based on normalized root-mean-squared error, NRMSE, (17 +/- 7%). A basic Kalman filter yielded the highest similarity to the bladder pressure with an average correlation coefficient, CC, of 0.81 +/- 0.13. The best algorithm set (based on NRMSE) was further evaluated on data obtained from a chronic feline experiment. Testing results yielded a NRMSE and CC of 10.7% and 0.61, respectively from a model that was trained on data recorded 2 weeks prior. From offline analysis, implementation of NARMA in a closed-loop scheme for detecting bladder contractions would provide a robust control signal. Ultimate integration of closed-loop algorithms in bladder neuroprostheses will require evaluations of parameter and signal stability over time.", "journal": "ANNALS OF BIOMEDICAL ENGINEERING", "category": "Engineering, Biomedical", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000454605100022", "keywords": "ephaptic coupling; electric field; hippocampus; propagation; slow periodic activity; sleep wave", "title": "Slow periodic activity in the longitudinal hippocampal slice can self-propagate non-synaptically by a mechanism consistent with ephaptic coupling", "abstract": "Slow oscillations are a standard feature observed in the cortex and the hippocampus during slow wave sleep. Slow oscillations are characterized by low-frequency periodic activity (<1 Hz) and are thought to be related to memory consolidation. These waves are assumed to be a reflection of the underlying neural activity, but it is not known if they can, by themselves, be self-sustained and propagate. Previous studies have shown that slow periodic activity can be reproduced in the in vitro preparation to mimic in vivo slow oscillations. Slow periodic activity can propagate with speeds around 0.1 m s(-1) and be modulated by weak electric fields. In the present study, we show that slow periodic activity in the longitudinal hippocampal slice is a self-regenerating wave which can propagate with and without chemical or electrical synaptic transmission at the same speeds. We also show that applying local extracellular electric fields can modulate or even block the propagation of this wave in both in silico and in vitro models. Our results support the notion that ephaptic coupling plays a significant role in the propagation of the slow hippocampal periodic activity. Moreover, these results indicate that a neural network can give rise to sustained self-propagating waves by ephaptic coupling, suggesting a novel propagation mechanism for neural activity under normal physiological conditions.", "journal": "JOURNAL OF PHYSIOLOGY-LONDON", "category": "Neurosciences; Physiology", "annotated_keywords": ["neural net"], "label": "0", "title_label": "0"}
{"id": "WOS:000472133300154", "keywords": "shape memory alloys; artificial neural networks; control; manipulators", "title": "Neural Network Direct Control with Online Learning for Shape Memory Alloy Manipulators", "abstract": "New actuators and materials are constantly incorporated into industrial processes, and additional challenges are posed by their complex behavior. Nonlinear hysteresis is commonly found in shape memory alloys, and the inclusion of a suitable hysteresis model in the control system allows the controller to achieve a better performance, although a major drawback is that each system responds in a unique way. In this work, a neural network direct control, with online learning, is developed for position control of shape memory alloy manipulators. Neural network weight coefficients are updated online by using the actuator position data while the controller is applied to the system, without previous training of the neural network weights, nor the inclusion of a hysteresis model. A real-time, low computational cost control system was implemented; experimental evaluation was performed on a 1-DOF manipulator system actuated by a shape memory alloy wire. Test results verified the effectiveness of the proposed control scheme to control the system angular position, compensating for the hysteretic behavior of the shape memory alloy actuator. Using a learning algorithm with a sine wave as reference signal, a maximum static error of 0.83 degrees was achieved when validated against several set-points within the possible range.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": ["neural net", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000441412000001", "keywords": "Three-dimensional ionospheric tomography; GNSS; Side rays; Inversion", "title": "A modified three-dimensional ionospheric tomography algorithm with side rays", "abstract": "The three-dimensional ionospheric tomography (3DCIT) algorithm based on Global Navigation Satellite System (GNSS) observations have been developed into an effective tool for ionospheric monitoring in recent years. However, because the rays that come into or come out from the side of the inversion region cannot be used, the distribution of the rays in the edge and bottom part of the inversion region is scarce and the electron density cannot be effectively improved in the inversion process. We present a three-dimensional tomography algorithm with side rays (3DCIT-SR) applying the side rays to the inversion. The partial slant total electron content (STEC) of side rays in the inversion region is obtained based on the NeQuick2 model and GNSS-STEC. The simulation experiment results show that the algorithm can effectively improve the distribution of GNSS rays in the inversion region. Meanwhile, the iteration accuracy has also been significantly improved. After the same number of iterations, the iterative results of 3DCIT-SR are closer to the truth than 3DCIT, in particular, the inversion of the edge regions is improved noticeably. The GNSS data of the International GNSS Service (IGS) stations in Europe are used to perform real data experiments, and the inversion results show that the electron density profiles of 3DCIT-SR are closer to the ionosonde measurements. The accuracy improvement of 3DCIT-SR is up to 56.3% while the improvement is more obvious during the magnetic storm compared to the case of a calm ionospheric state .", "journal": "GPS SOLUTIONS", "category": "Remote Sensing", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000437380300004", "keywords": "Soil erosion; RUSLE; Tibet; Factor downscale; Digital soil mapping", "title": "Integrating multi-source data to improve water erosion mapping in Tibet, China", "abstract": "Quantitative estimation for soil erosion is necessary for protection of the environment, and to improve agricultural productivity. However, due to the large area, sparse and limited data in Tibet, soil erosion there is still poorly quantified. Here, we improved the factors of the Revised Universal Soil Loss Equation (RUSLE) and calculated water erosion in Tibet. Rainfall erosivity (R) was calculated with the 0.25 CPC Morphing technique (CMORPH) data and subsequently downscaled to 1-km spatial resolution using artificial neural network (ANN) based on environmental covariates; slope length and steepness (LS) was estimated using the 3 arc sec Shuttle Rader Topography Mission (SRTM) digital elevation model (DEM); cover management (C) and control practice (P) were assigned based on land cover and protection measurements; and soil erodibility (K) was calculated using the Environmental Policy Integrated Climate model (EPIC) with inputs of the contents of sand, silt, clay and organic carbon in soil samples from Tibet. We used the data-mining algorithm to model the K factor and the spatially referenced variables to generate a K factor map. The obtained factors were then used to calculate soil loss in Tibet atl-km resolution. Our study estimated the annual water erosion at 5.43 t ha(-1) y(-1) in Tibet, about 5.44 x 10(8) t of soil lost yearly. The erosion rate increased from northwest to southeast, with most serious erosion occurring in the humid rain forest area in the southeast of Tibet. Our estimates of erosion area were generally consistent with previous national estimates. The largest differences were in the humid zone, Hengduan Mountain, and Yarlung Zangbo River basin, which are characterized by complex terrain and climate. Because of the applications of the best available data, we supply better, quantitative, finer spatial resolution estimates than previous studies. Our study is valuable for assessment of soil erosion in other data-scarce area suffering from soil loss by water erosion.", "journal": "CATENA", "category": "Geosciences, Multidisciplinary; Soil Science; Water Resources", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000487306300605", "keywords": "Artificial intelligence (AI); Internet of Things (IoT); multiple access; smart city", "title": "A Supervised Machine Learning Approach for Predicting Acute Kidney Injury Following Percutaneous Coronary Intervention", "abstract": "Purpose: A noninvasive diagnostic method to predict the degree of malignancy accurately would be of great help in glioma management. This study aimed to create a highly accurate machine learning model to perform glioma grading. Methods and Materials: Preoperative magnetic resonance imaging acquired for cases of glioma operated on at our institution from October 2014 through January 2018 were obtained retrospectively. Six types of magnetic resonance imaging sequences (T-2-weighted image, diffusion-weighted image, apparent diffusion coefficient [ADC], fractional anisotropy, and mean kurtosis [MK]) were chosen for analysis; 476 features were extracted semiautomatically for each sequence (2856 features in total). Recursive feature elimination was used to select significant features for a machine learning model that distinguishes glioblastoma from lower-grade glioma (grades 2 and 3). Results: Fifty-five data sets from 54 cases were obtained (14 grade 2 gliomas, 12 grade 3 gliomas, and 29 glioblastomas), of which 44 and 11 data sets were used for machine learning and independent testing, respectively. We detected 504 features with significant differences (false discovery rate <0.05) between glioblastoma and lower-grade glioma. The most accurate machine learning model was created using 6 features extracted from the ADC and MK images. In the logistic regression, the area under the curve was 0.90 +/- 0.05, and the accuracy of the test data set was 0.91 (10 out of 11); using a support vector machine, they were 0.93 +/- 0.03 and 0.91 (10 out of 11), respectively (kernel, radial basis function; c = 1.0). Conclusions: Our machine learning model accurately predicted glioma tumor grade. The ADC and MK sequences produced particularly useful features. (C) 2019 Elsevier Inc. All rights reserved.", "journal": "JOURNAL OF THE AMERICAN COLLEGE OF CARDIOLOGY", "category": "Cardiac & Cardiovascular Systems", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000478733403008", "keywords": "Stroke volume; Perfusion imaging; CT angiography; Infarct size; Computed tomography", "title": "Automated Accurate Determinations of Acute Infarct Core Volumes From CT Angiography Using Machine Learning.", "abstract": "Distributed machine learning algorithms enable learning of models from datasets that are distributed over a network without gathering the data at a centralized location. While efficient distributed algorithms have been developed under the assumption of faultless networks, failures that can render these algorithms nonfunctional occur frequently in the real world. This paper focuses on the problem of Byzantine failures, which are the hardest to safeguard against in distributed algorithms. While Byzantine fault tolerance has a rich history, existing work does not translate into efficient and practical algorithms for high-dimensional learning in fully distributed (also known as decentralized) settings. In this paper, an algorithm termed Byzantine-resilient distributed coordinate descent is developed and analyzed that enables distributed learning in the presence of Byzantine failures. Theoretical analysis (convex settings) and numerical experiments (convex and nonconvex settings) highlight its usefulness for high-dimensional distributed learning in the presence of Byzantine failures.", "journal": "STROKE", "category": "Clinical Neurology; Peripheral Vascular Disease", "annotated_keywords": ["machine learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000458194900013", "keywords": "Logistic regression; homomorphic en'cryption; secure cloud computing; SGX; machine learning", "title": "SecureLR: Secure Logistic Regression Model via a Hybrid Cryptographic Protocol", "abstract": "Machine learning applications are intensively utilized in various science fields, and increasingly the biomedical and healthcare sector. Applying predictive modeling to biomedical data introduces privacy and security concerns requiring additional protection to prevent accidental disclosure or leakage of sensitive patient information. Significant advancements in secure computing methods have emerged in recent years, however, many of which require substantial computational and/or communication overheads, which might hinder their adoption in biomedical applications. In this work, we propose SecureLR, a novel framework allowing researchers to leverage both the computational and storage capacity of Public Cloud Servers to conduct learning and predictions on biomedical data without compromising data security or efficiency. Our model builds upon homomorphic encryption methodologies with hardware-based security reinforcement through Software Guard Extensions (SGX), and our implementation demonstrates a practical hybrid cryptographic solution to address important concerns in conducting machine learning with public clouds.", "journal": "IEEE-ACM TRANSACTIONS ON COMPUTATIONAL BIOLOGY AND BIOINFORMATICS", "category": "Biochemical Research Methods; Computer Science, Interdisciplinary Applications; Mathematics, Interdisciplinary Applications; Statistics & Probability", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000456745900004", "keywords": "Inclusive wealth; Energy consumption; Sustainability; Machine learning", "title": "New evidence of energy-growth nexus from inclusive wealth", "abstract": "Gross domestic product (GDP) has been inappropriately used as the main indicator for assessing the sustainability of economic development for a long time. Inclusive wealth (IW) offers a new approach to assess sustainability by comprehensively measuring the productive base of the economy that involves three types of capital assets of nations (produced, human and natural capital), and aggregates them into a single measure of wealth. This study proposes an alternative to the literature on the conventional energy - growth nexus that widely uses GDP as a proxy of the growth. This study aims to investigate the impact of energy consumption on wealth in the IW framework and forecast the growth of IW over the next three decades. For this purpose, this study uses both parametric and non-parametric analyses on 104 countries for 1993-2014. Our results indicate that there is a negative and significant impact of energy consumption on IW growth, suggesting an unsustainable pattern of world energy consumption. Using a machine learning technique, it is forecasted that increasing the efficiency of energy consumption leads to a higher growth in average per capita IW. This study also suggests that a shift to renewables is a precondition for sustainable development.", "journal": "RENEWABLE & SUSTAINABLE ENERGY REVIEWS", "category": "Green & Sustainable Science & Technology; Energy & Fuels", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000474672200040", "keywords": "BCI cinematics; Machine learning, Movie analysis; Pre-release analysis; Deep learning, H2O framework", "title": "BCI cinematics - A pre-release analyser for movies using H2O deep learning platform", "abstract": "Entertainment industry has seen a phenomenal growth throughout the globe in recent times and movie industry enjoys a crucial role in the above emergence. A movie can capture the attention of a viewer and can trigger cognitive and emotional processes in the brain. In this article we assess the emotional outcome of the viewer while they watch the movie before its actual release that is, during its preview. Traditionally FMRI was used to assess the activity of brain but proved to be non-feasible and costly so we used EEG Sensors to monitor and record the functioning of the brain of movie viewer for further analysis. The collected data through EEG sensor were analysed using deep learning framework. H2O package of deep learning was employed to find high and low of different brain waves mapping to the emotions depicted in the every scene of the movie. Our proposed system named BCI cinematics obtained 85% accuracy and results were validated by obtaining the feedback from the stake holders. The outcome of this work will assist the creators to understand the emotional impact of movie over a normal viewer impartially thus enable them to modify certain scenes or change sequence of scenes and so on. When deployed in real time our system prove to be a cost saver for movie makers. (C) 2018 Elsevier Ltd. All rights reserved.", "journal": "COMPUTERS & ELECTRICAL ENGINEERING", "category": "Computer Science, Hardware & Architecture; Computer Science, Interdisciplinary Applications; Engineering, Electrical & Electronic", "annotated_keywords": ["deep learning", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000485291200002", "keywords": "psychosis; schizophrenia; computational methods; language dysfunction; culture", "title": "A Comprehensive Review of Computational Methods for Automatic Prediction of Schizophrenia With Insight Into Indigenous Populations", "abstract": "Psychiatrists rely on language and speech behavior as one of the main clues in psychiatric diagnosis. Descriptive psychopathology and phenomenology form the basis of a common language used by psychiatrists to describe abnormal mental states. This conventional technique of clinical observation informed early studies on disturbances of thought form, speech, and language observed in psychosis and schizophrenia. These findings resulted in language models that were used as tools in psychosis research that concerned itself with the links between formal thought disorder and language disturbances observed in schizophrenia. The end result was the development of clinical rating scales measuring severity of disturbances in speech, language, and thought form. However, these linguistic measures do not fully capture the richness of human discourse and are time-consuming and subjective when measured against psychometric rating scales. These linguistic measures have not considered the influence of culture on psychopathology. With recent advances in computational sciences, we have seen a re-emergence of novel research using computing methods to analyze free speech for improving prediction and diagnosis of psychosis. Current studies on automated speech analysis examining for semantic incoherence are carried out based on natural language processing and acoustic analysis, which, in some studies, have been combined with machine learning approaches for classification and prediction purposes.", "journal": "FRONTIERS IN PSYCHIATRY", "category": "Psychiatry", "annotated_keywords": ["machine learning", "natural language processing"], "label": "1", "title_label": "0"}
{"id": "WOS:000479569600001", "keywords": "Consensus statements; guidelines; risk; participation processes; science; faith in expert systems", "title": "What is the role of consensus statements in a risk society?", "abstract": "This paper explores the role of consensus statements in a risk society. It uses Beck's theory of risk to show that scientists have employed consensus statements in order to re-establish faith in science. Through analysing the goals of participants in consensus fora and comparing them to the fora processes, this paper considers how consensus statements and guidelines in public health can be viewed as remedies for the decline in expert trust experienced in the current risk society. To collect data, 25 interviews were undertaken with consensus panel participants from the USA, UK and Australia. Interviewees were from peak national agencies/commissioning agencies and were categorised as policymaker, practitioner and consumer stakeholders. Participants made recommendations for improving consensus processes in order to mitigate perceptions of risk. These were: (1) clearly stated goals; (2) robust, evidence-based and transparent processes of methodological development and participation/deliberation/decision-making; (3) diverse stakeholder representation, including increased consumer participation; (4) transparency about conflicts of interest; and, (5) robust, carefully worded recommendations. Poor-quality consensus statements can further entrench scepticism about the scientific enterprise. While consensus statements can be seen as a tool for moderating perceptions of risk, policymakers and scientists must ensure the integrity, strength and transparency of their research methods. This has the potential to facilitate policy, improve scientific accountability to the public and legitimise processes. While fostering greater trust is not a primary objective for scientists, an increase in legitimacy of process can be an important unintended consequence of improved quality consensus statements and an important antidote to the risk society.", "journal": "JOURNAL OF RISK RESEARCH", "category": "Social Sciences, Interdisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000419381000001", "keywords": "lung adenocarcinoma; lncRNA; prognostic markers; overall survival", "title": "LncRNA Expression Signature in Prediction of the Prognosis of Lung Adenocarcinoma", "abstract": "Objective: To determine if lncRNA expression can be used for the prognoses of patients diagnosed with lung adenocarcinoma (LUAD) patients. Methods: The Cancer Genome Atlas database was used to identify 409 LUAD patients for whom there was both, gene expression data and relevant clinical information available. LncRNAs were then selected from the expression data through record linkage between the National Center for Biotechnology Information (NCBI) and Ensemble databases. lncRNAs with significantly different expression levels between normal and tumor tissues were screened, and those whose levels correlated with a positive LUAD prognosis were identified. Based on the expression of the selected lncRNAs, an unsupervised learning method was used to cluster these patients into two groups, and survival analyses were performed to assess the overall survival (OS) between the two groups. Receiver operating characteristic curves were used to calculate the specificity and sensitivity of the models based on the presence of these lncRNAs, and the model was tested with another dataset from the Gene Expression Omnibus. Results: A total of 151 lncRNAs were found to be differentially expressed between tumor and normal tissues (permutation p-values <0.05) based on the Cancer Genome Atlas dataset, and 20 lncRNAs were associated with OS. Two lncRNAs (DKFZP434 L187 and LOC285548) were correlated with LUAD. All patients with high expression of these two lncRNAs from the two datasets exhibited poor OS compared with those with low expression (p<0.05). Conclusions: The expression of DKFZP434 L187 and LOC285548 may have prognostic value for the OS of LUAD patients.", "journal": "GENETIC TESTING AND MOLECULAR BIOMARKERS", "category": "Biochemistry & Molecular Biology; Genetics & Heredity", "annotated_keywords": ["supervised learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000489027300003", "keywords": "brain vasculature; polarization-sensitive optical coherence tomography; contrast enhancement; deep learning", "title": "Contrast-enhanced serial optical coherence scanner with deep learning network reveals vasculature and white matter organization of mouse brain", "abstract": "Optical coherence tomography provides volumetric reconstruction of brain structure with micrometer resolution. Gray matter and white matter can be highlighted using conventional and polarization-based contrasts; however, vasculature in ex-vivo fixed brain has not been investigated at large scale due to lack of intrinsic contrast. We present contrast enhancement to visualize the vasculature by perfusing titanium dioxide particles transcardially into the mouse vascular system. The brain, after dissection and fixation, is imaged by a serial optical coherence scanner. Accumulation of particles in blood vessels generates distinguishable optical signals. Among these, the cross-polarization images reveal the vasculature organization remarkably well. The conventional and polarization-based contrasts are still available for probing the gray matter and white matter structures. The segmentation and reconstruction of the vasculature are presented by using a deep learning algorithm. Axonal fiber pathways in the mouse brain are delineated by utilizing the retardance and optic axis orientation contrasts. This is a low-cost method that can be further developed to study neurovascular diseases and brain injury in animal models. (C) The Authors. Published by SPIE under a Creative Commons Attribution 4.0 Unported License.", "journal": "NEUROPHOTONICS", "category": "Neurosciences; Optics", "annotated_keywords": ["deep learning", "deep learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000449980300292", "keywords": "Deep learning; Smart refrigerator; Fruit recognition; Multi-source data fusion", "title": "Deep Learning Based Risk Stratification of Patients with Suspicious Nodules", "abstract": "Food recognition is one of the core functions for a smart refrigerator. But there are many challenges for accurate food recognition due to reasons of too many kinds of food inside the refrigerator which tends to obscure each other, and they may look very similar. This paper proposes a fruit recognition approach that fuses weight information and multi deep learning models. The proposed approach can remarkably improve recognition accuracy. We have extensively evaluated the proposed approach for its performance and accuracy, which demonstrate the effectiveness of the proposed approach. (C) 2017 Elsevier B.V. All rights reserved.", "journal": "AMERICAN JOURNAL OF RESPIRATORY AND CRITICAL CARE MEDICINE", "category": "Critical Care Medicine; Respiratory System", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000475468600003", "keywords": "Object detection; Single shot detector; Feature refinement; Detail preservation", "title": "Automated cell boundary and 3D nuclear segmentation of cells in suspension", "abstract": "To characterize cell types, cellular functions and intracellular processes, an understanding of the differences between individual cells is required. Although microscopy approaches have made tremendous progress in imaging cells in different contexts, the analysis of these imaging data sets is a long-standing, unsolved problem. The few robust cell segmentation approaches that exist often rely on multiple cellular markers and complex time-consuming image analysis. Recently developed deep learning approaches can address some of these challenges, but they require tremendous amounts of data and well-curated reference data sets for algorithm training. We propose an alternative experimental and computational approach, called CellDissect, in which we first optimize specimen preparation and data acquisition prior to image processing to generate high quality images that are easier to analyze computationally. By focusing on fixed suspension and dissociated adherent cells, CellDissect relies only on widefield images to identify cell boundaries and nuclear staining to automatically segment cells in two dimensions and nuclei in three dimensions. This segmentation can be performed on a desktop computer or a computing cluster for higher throughput. We compare and evaluate the accuracy of different nuclear segmentation approaches against manual expert cell segmentation for different cell lines acquired with different imaging modalities.", "journal": "SCIENTIFIC REPORTS", "category": "Multidisciplinary Sciences", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000517101700012", "keywords": "Competitive neural networks; proportional delays; exponential stability", "title": "New Results on Exponential Stability of Competitive Neural Networks with Multi-Proportional Delays", "abstract": "In this paper, we are concerned with a class of competitive neural networks with multi-proportional delays. By applying the Banach fixed point theorem and constructing suitable Lyapunov functions, we obtain new sufficient conditions for the global exponential stability to this class of neural networks, which are easily verifiable. Finally, two examples are given to illustrate the effectiveness of the obtained results.", "journal": "ASIAN JOURNAL OF CONTROL", "category": "Automation & Control Systems", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000549464100011", "keywords": "Fuzzy Neural Network; Heat Shock Protein; Tumor Cells; Prognostic Impact", "title": "Predictive Fuzzy Neural Network Based Effect of Heat Shock Protein on Prognosis of Tumor Cells", "abstract": "In this study, a mouse model of pancreatic carcinoma in situ and a subcutaneous model were used to simulate the actual situation of pancreatic carcinoma after operation, Mice inoculated with Mycobacterium tuberculosis heat shock protein 65 (mHSP65) pancreatic cancer tissue-derived lysate (TDL) and pancreatic cancer cell-derived lysate (TCL) respectively to prepare pancreatic cancer vaccine, and combined with low dose cyclophosphamide (CCY) to observe its anti-pancreatic cancer effect in tumor-bearing mice. The experiments showed that survival time of tumor-bearing mice can be prolong by the combination of mHSP65-TCL and CY, decrease the tumor-producing rate, inhibit tumor growth and induce long-term immune memory, but the combination of mHSP65-TCL and CY could not show this effect. We further validated the antitumor effect and mechanism of mHSP65-TTL combined with CY in vivo.", "journal": "JOURNAL OF MEDICAL IMAGING AND HEALTH INFORMATICS", "category": "Mathematical & Computational Biology; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000542340000027", "keywords": "Cambered otter board; Hydrodynamic characteristics; Lift-to-drag ratio; Neural network; Multi-objective genetic algorithm; PIV", "title": "Shape optimization approach for cambered otter board using neural network and multi-objective genetic algorithm", "abstract": "The shape optimization approach of the cambered otter board has been performed by the integration of the neural network model and the multi-objective genetic algorithm (MOGA). Because the excellent performance of an otter board is expressed by great lift and less drag force, in this study the lift and drag coefficients were chosen as objective functions to obtain the optimal otter board. The Bezier curve represented the cambered otter board as a simple structure with five control points resulting in the six coordinates, which were adopted as the design variables. The hydrodynamic characteristics of twenty-five otter board models were calculated in a two-dimension computational fluid dynamics (CFD) analysis at an attack angle of 20 degrees. The implicit fitness function in the MOGA algorithm was then obtained by the backpropagation neural network model based on the estimated results of CFD calculation. A set of thirty optimal otter board models were extracted in the optimal solutions of the MOGA, and two optimal models were selected to verify the feasibility of the approach by hydrodynamic and visualization experiments with a comparative hyper-lift trawl door (HLTD). The model 1 showed greatest lift-todrag ratio before the attack angle of 30 degrees as a high lift-to-drag ratio otter board, and the model 2 showed a large lift coefficient and lift-to-drag ratio than the HLTD before the attack angle of 25 degrees as a large lift force otter board. Through the flow distribution around the model 2, it is observed that the flow separation on the suction side is prevented as a result of less drag owing to the modified shape. In summary, the shape optimization approach is efficient in designing optimal otter board to satisfy supposed needs in otter trawling.", "journal": "APPLIED OCEAN RESEARCH", "category": "Engineering, Ocean; Oceanography", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000542927200001", "keywords": "Unsupervised domain adaptation; EEG-based classification; cognitive load; deep learning", "title": "Custom Domain Adaptation: A New Method for Cross-Subject, EEG-Based Cognitive Load Recognition", "abstract": "Electroencephalograms (EEG) have shown to be a useful approach to measure the cognitive load in tasks where mental effort is involved. However, EEG signals present a high variability among subjects as well as a non-stationary behavior, so that distributions among samples of different subjects are mismatched. Methods based on Unsupervised Domain Adaptation (UDA) have been used as an effective solution to reduce such discrepancy, while the ones leveraged by deep learning (D-UDA) have improved the classification results over shallow approaches. However, most D-UDA methods assume that even though there are differences in marginal distributions between source and target domains, their conditional distributions remain fixed, which does not hold in many EEG databases. To address this problem, we propose a new D-UDA method, named Custom Domain Adaptation (CDA), which integrates Adaptive Batch Normalization (AdaBN) and Maximum Mean Discrepancy (MMD) into two independent deep neural networks in order to reduce the marginal and conditional distribution differences. CDA was compared with six popular D-UDA methods using a free-available dataset of cognitive loads and obtained an accuracy of , which outperformed these state-of-the-art methods.", "journal": "IEEE SIGNAL PROCESSING LETTERS", "category": "Engineering, Electrical & Electronic", "annotated_keywords": ["neural net", "deep learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000528845200032", "keywords": "Artificial neural network; arterial blood pressure; blood pressure waveform; electrocardiography; nonlinear autoregressive with exogenous input; photoplethysmography; time series modeling", "title": "Nonlinear Dynamic Modeling of Blood Pressure Waveform: Towards an Accurate Cuffless Monitoring System", "abstract": "The objective is to develop a cuffless modelling approach to accurately estimate the blood pressure (BP) waveform and extract important BP features, such as the systolic BP (SBP), diastolic BP (DBP), and mean BP (MAP). Access to the full waveform has significant advantages over previous cuffless BP estimation tools in terms of accuracy and access to additional cardiovascular health markers (e.g., cardiac output), as well as potentially providing arterial stiffness and identifying different cardiovascular diseases. Nonlinear autoregressive models with exogenous input (NARX) are implemented using an artificial neural network to predict the BP waveforms using electrocardiography (ECG), and/or photoplethysmography (PPG) signals as inputs. The efficacy of the model is compared with a pulse arrival time (PAT) model using 15 subjects from the MIMIC II database. Two training modes are considered: training on the first eight minutes of data for each subject (Predictive training) and testing on the rest (up to 5.2 hours); and training on the first and the last eight minutes (Interval training) and testing the model in between. Predictive training and Interval training exhibited similar results initially, while Interval training resulted in higher accuracy over longer periods. The proposed method models the BP as a dynamical system leading to better accuracy in the estimation of SBP, DBP and MAP when compared to the PAT model. Moreover, the NARX model, with its ability to provide the BP waveform, yields more insight into patient health.", "journal": "IEEE SENSORS JOURNAL", "category": "Engineering, Electrical & Electronic; Instruments & Instrumentation; Physics, Applied", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000588263500001", "keywords": "Goniometric measurements; Upper extremity; Reachable workspace; Dynamical form calibration; Measurement error; Compensation method; Neural network model", "title": "The compensation of biomechanical errors in electrogoniometric measurements of the upper extremity kinematics", "abstract": "The reliability of electrogoniometers as wearable sensors depends on the biomechanical characteristics of the human body. The major causes of electrogoniometric measurement errors are the complexity of movements and the anatomical features of joints. Accordingly, the main objective of this study is to enhance goniometric measurements of body kinematics as a dynamical form of calibration method. In this case study, an experiment was designed in the reachable workspace for gathering joint angles and modeling the related information among them, so a nonlinear neural network was established to compensate the goniometric measurement errors with mapping angular variations simultaneously. The results can be confirmed that the proposed method compensated measurement errors. According to the mean absolute error (MAE) criterion, the results indicated up to 4.5 cm difference between the desired and estimated values of target positions in a reachable workspace and elucidated the significant effect between the desired and estimated 3D target positions before and after using the proposed method. Consequently, since the electrogoniometer has been limited to measuring variations of joint angles separately, the most striking result to emerge from the present study is that the proposed method copes with the finding relationship among measurements of upper extremity kinematics to compensate the goniometric measurement errors. (C) 2020 Elsevier B.V. All rights reserved.", "journal": "SENSORS AND ACTUATORS A-PHYSICAL", "category": "Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000517842100016", "keywords": "Real time; Super-resolution; Pixel shuffling", "title": "A Real-Time Super-Resolution Method Based on Convolutional Neural Networks", "abstract": "The aim of single-image super-resolution is to recover a high-resolution image based on a low-resolution image. Deep convolutional neural networks have largely enhanced the reconstruction performance of image super-resolution. Since the input image is always bicubic-interpolated, the main weakness of deep convolutional neural networks is that they are time-consuming. Moreover, fast convolutional neural networks can perform real-time image super-resolution but are unable to achieve reliable performance. To address those drawbacks, we propose a real-time image super-resolution method with good reconstruction performance. We replace the default upsampling method (bicubic interpolation) with a pixel shuffling layer. Local and global residual connections are taken to guarantee better performance. As shown in Fig. 1, our proposed method is not only fast but also accurate.", "journal": "CIRCUITS SYSTEMS AND SIGNAL PROCESSING", "category": "Engineering, Electrical & Electronic", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000595589100029", "keywords": "Character control; reinforcement learning; physics simulation; deformable materials", "title": "Learning to Manipulate Amorphous Materials", "abstract": "We present a method of training character manipulation of amorphous materials such as those often used in cooking. Common examples of amorphous materials include granular materials (salt, uncooked rice), fluids (honey), and visco-plastic materials (sticky rice, softened butter). A typical task is to spread a given material out across a flat surface using a tool such as a scraper or knife. We use reinforcement learning to train our controllers to manipulate materials in various ways. The training is performed in a physics simulator that uses position-based dynamics of particles to simulate the materials to be manipulated. The neural network control policy is given observations of the material (e.g. a low-resolution density map), and the policy outputs actions such as rotating and translating the knife. We demonstrate policies that have been successfully trained to carry out the following tasks: spreading, gathering, and flipping. We produce a final animation by using inverse kinematics to guide a character's arm and hand to match the motion of the manipulation tool such as a knife or a frying pan.", "journal": "ACM TRANSACTIONS ON GRAPHICS", "category": "Computer Science, Software Engineering", "annotated_keywords": ["neural net", "reinforcement learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000544712000003", "keywords": "cloud; CPU usage prediction; DBN; PSO; resource provisioning", "title": "CPUusage prediction for cloud resource provisioning based on deep belief network and particle swarm optimization", "abstract": "Resource usage prediction is increasingly important in cloud computing environments, and CPU usage prediction is especially helpful for improving the efficiency of resource provisioning and reducing energy consumption of cloud datacenters. However, accurate CPU usage prediction remains a challenge and few works have been done on predicting CPU usage of physical machines in cloud datacenters. In this article, we present a deep belief network (DBN) and particle swarm optimization (PSO) based CPU usage prediction algorithm, which is named DP-CUPA and aimed to provide more accurate prediction results. The DP-CUPA consists of three main steps. First, the historic data on CPU usage are preprocessed and normalized. Then, the autoregressive model and grey model are adopted as base prediction models and trained to provide extra input information for training DBN. Finally, the PSO is used to estimate DBN parameters and the DBN neural network is trained to predict CPU usage. The effectiveness of the DP-CUPA is evaluated by extensive experiments with a real-world dataset of Google cluster usage trace.", "journal": "CONCURRENCY AND COMPUTATION-PRACTICE & EXPERIENCE", "category": "Computer Science, Software Engineering; Computer Science, Theory & Methods", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000539614800001", "keywords": "synthetic aperture radar; image despeckling; dilated convolution; residual skip connection", "title": "Learning synthetic aperture radar image despeckling without clean data", "abstract": "Speckle noise can reduce the image quality of synthetic aperture radar (SAR) and make interpretation more difficult. Existing SAR image despeckling convolutional neural networks require quantities of noisy-clean image pairs. However, obtaining clean SAR images is very difficult. Because continuous convolution and pooling operations result in losing many informational details while extracting the deep features of the SAR image, the quality of recovered clean images becomes worse. Therefore, we propose a despeckling network called multiscale dilated residual U-Net (MDRU-Net). The MDRU-Net can be trained directly using noisy-noisy image pairs without clean data. To protect more SAR image details, we design five multiscale dilated convolution modules that extract and fuse multiscale features. Considering that the deep and shallow features are very distinct in fusion, we design different dilation residual skip connections, which make features at the same level have the same convolution operations. Afterward, we present an effective L-hybrid loss function that can effectively improve the network stability and suppress artifacts in the predicted clean SAR image. Compared with the state-of-the-art despeckling algorithms, the proposed MDRU-Net achieves a significant improvement in several key metrics. (C) The Authors. Published by SPIE under a Creative Commons Attribution 4.0 Unported License.", "journal": "JOURNAL OF APPLIED REMOTE SENSING", "category": "Environmental Sciences; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000536911700001", "keywords": "intellectual disability; Down syndrome; learning and memory; functional connectivity; neural networks", "title": "A New Computer-Aided Diagnosis System with Modified Genetic Feature Selection for BI-RADS Classification of Breast Masses in Mammograms", "abstract": "Mammography remains the most prevalent imaging tool for early breast cancer screening. The language used to describe abnormalities in mammographic reports is based on the Breast Imaging Reporting and Data System (BI-RADS). Assigning a correct BI-RADS category to each examined mammogram is a strenuous and challenging task for even experts. This paper proposes a new and effective computer-aided diagnosis (CAD) system to classify mammographic masses into four assessment categories in BI-RADS. The mass regions are first enhanced by means of histogram equalization and then semiautomatically segmented based on the region growing technique. A total of 130 handcrafted BI-RADS features are then extracted from the shape, margin, and density of each mass, together with the mass size and the patient's age, as mentioned in BI-RADS mammography. Then, a modified feature selection method based on the genetic algorithm (GA) is proposed to select the most clinically significant BI-RADS features. Finally, a back-propagation neural network (BPN) is employed for classification, and its accuracy is used as the fitness in GA. A set of 500 mammogram images from the digital database for screening mammography (DDSM) is used for evaluation. Our system achieves classification accuracy, positive predictive value, negative predictive value, and Matthews correlation coefficient of 84.5%, 84.4%, 94.8%, and 79.3%, respectively. To our best knowledge, this is the best current result for BI-RADS classification of breast masses in mammography, which makes the proposed system promising to support radiologists for deciding proper patient management based on the automatically assigned BI-RADS categories.", "journal": "BIOMED RESEARCH INTERNATIONAL", "category": "Biotechnology & Applied Microbiology; Medicine, Research & Experimental", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000502785400011", "keywords": "compositionality; language; electroencephalography; neural oscillations; phase synchronization; naturalistic language processing", "title": "Phase synchronization varies systematically with linguistic structure composition", "abstract": "Computation in neuronal assemblies is putatively reflected in the excitatory and inhibitory cycles of activation distributed throughout the brain. In speech and language processing, coordination of these cycles resulting in phase synchronization has been argued to reflect the integration of information on different timescales (e.g. segmenting acoustics signals to phonemic and syllabic representations; (Giraud and Poeppel 2012 Nat. Neurosci. 15, 511 (doi:10.1038/nn.3036)). A natural extension of this claim is that phase synchronization functions similarly to support the inference of more abstract higher-level linguistic structures (Martin 2016 Front. Psychol. 7, 120; Martin and Doumas 2017 PLoS Biol. 15, e2000663 (doi:10.1371/journal.pbio.2000663); Martin and Doumas. 2019 Curr. Opin. Behav. Sci. 29, 77-83 (doi:10.1016/j.cobeha.2019.04.008)). Hale et al. (Hale et al. 2018 Finding syntax in human encephalography with beam search. arXiv 1806.04127 (http://arxiv.org/abs/1806.04127)) showed that syntactically driven parsing decisions predict electroencephalography (EEG) responses in the time domain; here we ask whether phase synchronization in the form of either inter-trial phrase coherence or cross-frequency coupling (CFC) between high-frequency (i.e. gamma) bursts and lower-frequency carrier signals (i.e. delta, theta), changes as the linguistic structures of compositional meaning (viz., bracket completions, as denoted by the onset of words that complete phrases) accrue. We use a naturalistic story-listening EEG dataset from Hale et al. to assess the relationship between linguistic structure and phase alignment. We observe increased phase synchronization as a function of phrase counts in the delta, theta, and gamma bands, especially for function words. A more complex pattern emerged for CFC as phrase count changed, possibly related to the lack of a one-to-one mapping between 'size' of linguistic structure and frequency band-an assumption that is tacit in recent frameworks. These results emphasize the important role that phase synchronization, desynchronization, and thus, inhibition, play in the construction of compositional meaning by distributed neural networks in the brain. This article is part of the theme issue 'Towards mechanistic models of meaning composition'.", "journal": "PHILOSOPHICAL TRANSACTIONS OF THE ROYAL SOCIETY B-BIOLOGICAL SCIENCES", "category": "Biology", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000566327900001", "keywords": "time series classification; multivariate time series; image concatenation; convolutional neural network", "title": "Musclesense: a Trained, Artificial Neural Network for the Anatomical Segmentation of Lower Limb Magnetic Resonance Images in Neuromuscular Diseases", "abstract": "This paper proposes a framework to perform the sensor classification by using multivariate time series sensors data as inputs. The framework encodes multivariate time series data into two-dimensional colored images, and concatenate the images into one bigger image for classification through a Convolutional Neural Network (ConvNet). This study applied three transformation methods to encode time series into images: Gramian Angular Summation Field (GASF), Gramian Angular Difference Field (GADF), and Markov Transition Field (MTF). Two open multivariate datasets were used to evaluate the impact of using different transformation methods, the sequences of concatenating images, and the complexity of ConvNet architectures on classification accuracy. The results show that the selection of transformation methods and the sequence of concatenation do not affect the prediction outcome significantly. Surprisingly, the simple structure of ConvNet is sufficient enough for classification as it performed equally well with the complex structure of VGGNet. The results were also compared with other classification methods and found that the proposed framework outperformed other methods in terms of classification accuracy.", "journal": "NEUROINFORMATICS", "category": "Computer Science, Interdisciplinary Applications; Neurosciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000513341300068", "keywords": "Senior citizens; Feature extraction; Biomedical monitoring; Unsupervised learning; Monitoring; Analytical models; Hardware; Daily behavior; elderly; homecare; implicit irregularity; unsupervised learning", "title": "COMPUTATIONAL METHODS COMPLEMENT IN VITRO SCREENS: USING MACHINE LEARNING AND REAL-WORLD DATA TO UNDERSTAND TRANSPORTER-MEDIATED DRUG-NUTRIENT INTERACTIONS.", "abstract": "Background Improved methods are needed to predict outcomes in biliary tract cancers (BTCs). We aimed to build an immune-related signature and establish holistic models using machine learning. Methods Samples were from 305 BTC patients treated with curative-intent resection, divided into derivation and validation cohorts in a two-to-one ratio. Spatial resolution of T cell infiltration and PD-1/PD-L1 expression was assessed by immunohistochemistry. An immune signature was constructed using classification and regression tree. Machine learning was applied to develop prediction models for disease-specific survival (DSS) and recurrence-free survival (RFS). Results The immune signature composed of CD3(+), CD8(+), and PD-1(+) cell densities and PD-L1 expression within tumor epithelium significantly stratified patients into three clusters, with median DSS varying from 11.7 to 80.8 months and median RFS varying from 6.2 to 62.0 months. Gradient boosting machines (GBM) outperformed rival machine-learning algorithms and selected the same 11 covariates for DSS and RFS prediction: immune signature, tumor site, age, bilirubin, albumin, carcinoembryonic antigen, cancer antigen 19-9, tumor size, tumor differentiation, resection margin, and nodal metastasis. The clinical-immune GBM models accurately predicted DSS and RFS, with respective concordance index of 0.776-0.816 and 0.741-0.781. GBM models showed significantly improved performance compared with tumor-node-metastasis staging system. Conclusions The immune signature promises to stratify prognosis and allocate treatment in resected BTC. The clinical-immune GBM models accurately predict recurrence and death from BTC following surgery.", "journal": "CLINICAL PHARMACOLOGY & THERAPEUTICS", "category": "Pharmacology & Pharmacy", "annotated_keywords": ["machine learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000595719800007", "keywords": "machine learning; climate biomes; wavelet; information theory; clustering; interpretable machine learning", "title": "Machine learning approaches reveal genomic regions associated with sugarcane brown rust resistance", "abstract": "Sugarcane is an economically important crop, but its genomic complexity has hindered advances in molecular approaches for genetic breeding. New cultivars are released based on the identification of interesting traits, and for sugarcane, brown rust resistance is a desirable characteristic due to the large economic impact of the disease. Although marker-assisted selection for rust resistance has been successful, the genes involved are still unknown, and the associated regions vary among cultivars, thus restricting methodological generalization. We used genotyping by sequencing of full-sib progeny to relate genomic regions with brown rust phenotypes. We established a pipeline to identify reliable SNPs in complex polyploid data, which were used for phenotypic prediction via machine learning. We identified 14,540 SNPs, which led to a mean prediction accuracy of 50% when using different models. We also tested feature selection algorithms to increase predictive accuracy, resulting in a reduced dataset with more explanatory power for rust phenotypes. As a result of this approach, we achieved an accuracy of up to 95% with a dataset of 131 SNPs related to brown rust QTL regions and auxiliary genes. Therefore, our novel strategy has the potential to assist studies of the genomic organization of brown rust resistance in sugarcane.", "journal": "SCIENTIFIC REPORTS", "category": "Multidisciplinary Sciences", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000608875100041", "keywords": "Artificial intelligence; Bias and discrimination; Rational concern; Right to refuse; Health care", "title": "MultiCon: A Semi-Supervised Approach for Predicting Drug Function from Chemical Structure Analysis", "abstract": "Semi-supervised learning has proved its efficacy in utilizing extensive unlabeled data to alleviate the use of a large amount of supervised data and improve model performance. Despite its tremendous potential, semi-supervised learning has yet to be implemented in the field of drug discovery. Empirical testing of drugs and their classification is costly and time-consuming. In contrast, predicting therapeutic applications of drugs from their structural formulas using semi-supervised learning would reduce costs and time significantly. Herein, we employ a new multicontrastive-based semi-supervised learning algorithm-MultiCon-for classifying drugs into 12 categories, according to therapeutic applications, on the basis of image analyses of their structural formulas. By rational use of data balancing, online augmentations of the drug image data during training, and the combined use of multicontrastive loss with consistency regularization, MultiCon achieves better class prediction accuracies when compared with the state-of-the-art machine learning methods across a variety of existing semi-supervised learning benchmarks. In particular, it performs exceptionally well with a limited number of labeled examples. For instance, with just 5000 labeled drugs in a PubChem (D-3) data set, MultiCon achieved a class prediction accuracy of 97.74%.", "journal": "JOURNAL OF CHEMICAL INFORMATION AND MODELING", "category": "Chemistry, Medicinal; Chemistry, Multidisciplinary; Computer Science, Information Systems; Computer Science, Interdisciplinary Applications", "annotated_keywords": ["machine learning", "learning algorithm", "supervised learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000520401300014", "keywords": "Telepresence robot; Machine learning; Online user adaptation; Unpleasant stimuli; Binaural microphone", "title": "A Robot for Test Bed Aimed at Improving Telepresence System and Evasion from Discomfort Stimuli by Online Learning", "abstract": "This study contributes to improving the comfort of telepresence communication by adaptations to people. We developed a handheld telepresence robot comprised of a binaural microphone and a head mounted display as a test bed and conducted surveys on unpleasantness caused by special devices in use. We found that numerous people experienced an increase in unpleasant sound when the binaural microphone was being used. It was also found that types of unpleasant stimuli differed from person to person. Furthermore, we propose an automatic unpleasant stimuli avoidance system using online machine learning architecture constructed from echo state networks (ESNs) and Accumulator Based Arbitration Models (ABAMs) that can also flexibly adapt to remote users. Because the handheld telepresence robot can avoid unpleasant stimuli before remote users experience these stimuli, it provides them with a more comfortable communication environment while notifying people around the robot that uncomfortable stimulation is being avoided.", "journal": "INTERNATIONAL JOURNAL OF SOCIAL ROBOTICS", "category": "Robotics", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000598482602221", "keywords": "Artificial intelligence; Collaborative filtering; Online learning; Recommender system", "title": "MLI-score: a machine learning tool generating an integrated score for pathogenic variant prediction", "abstract": "Collaborative Filtering (CF) is one of the most popular technologies used in online recommendation systems. Most of the existing CF studies focus on the offline algorithms, a major drawback of these algorithms is the lack of ability to use the latest user feedbacks to update the learned model in realtime, due to the high cost of the offline training procedure. In this work, we propose Logo, an online CF algorithm. Our proposed method is based on a hierarchical generative model, with which, we derive a set of local and global consistency constraints for the prediction targets, and eventually obtain the design of the learning algorithm. We conduct comprehensive experiments to evaluate the proposed algorithm, the results show that: (1) Under the online setting, our algorithm achieves notably better prediction results than the benchmark algorithms; (2) Under the offline setting, our algorithm attains comparable accurate prediction results with the best performed competitors; (3) In all the experiments, our algorithm performs tens or even hundreds of times faster than the comparison algorithms. (C) 2019 Elsevier Inc. All rights reserved.", "journal": "EUROPEAN JOURNAL OF HUMAN GENETICS", "category": "Biochemistry & Molecular Biology; Genetics & Heredity", "annotated_keywords": ["machine learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000518729600009", "keywords": "Uterine cervical cancer; Radiomics; Machine learning; In-field recurrence; Prognosis", "title": "A multi-scanner study of MRI radiomics in uterine cervical cancer: prediction of in-field tumor control after definitive radiotherapy based on a machine learning method including peritumoral regions", "abstract": "Purpose This study aimed to identify the most appropriate volume of interest (VOI) setting in prognostic prediction using pretreatment magnetic resonance imaging (MRI) radiomic analysis for cervical cancer (CC) treated with definitive radiotherapy. Materials and methods The study participants were 87 patients who had undergone pretreatment MRI and definitive radiotherapy for CC. VOItumor was created with tumor alone and VOI+4 mm-VOI+20 mm mechanically expanded by 4-20 mm around each VOItumor in axial T2-weighted images (T2WI) and an apparent diffusion coefficient (ADC) map. A model was constructed to predict recurrence within the irradiation field within 2 years after treatment using imaging features from the VOI of each sequence. Sorting ability was evaluated by area under the receiver operator characteristic curve (AUC-ROC) analysis. Results VOI expansion improved AUC-ROCs compared with the predictive models of VOItumor (0.59 and 0.67 in T2WI and ADC, respectively). The AUC-ROCs of the models with imaging features from expanded VOI+4 mm in T2WI and VOI+4 mm and VOI+8 mm in ADC were 0.82, 0.82, and 0.86, respectively. Conclusion Recurrence could be predicted with high accuracy using expanded VOI for CC treated with definitive radiotherapy, suggesting that including the pathological characteristics of invasive margins in radiomics may improve predictive ability.", "journal": "JAPANESE JOURNAL OF RADIOLOGY", "category": "Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000595832300002", "keywords": "fracture; tracer; pressure; heterogeneous; machine learning; principal component analysis", "title": "Predictive Inverse Model for Advective Heat Transfer in a Short-Circuited Fracture: Dimensional Analysis, Machine Learning, and Field Demonstration", "abstract": "Identifying fluid flow maldistribution in planar geometries is a well-established problem in subsurface science/engineering. Of particular importance to the thermal performance of enhanced (or \"engineered\") geothermal systems is identifying the existence of nonuniform (i.e., heterogeneous) permeability and subsequently predicting advective heat transfer. Here, machine learning via a genetic algorithm (GA) identifies the spatial distribution of an unknown permeability field in a two-dimensional Hele-Shaw geometry (i.e., parallel plates). The inverse problem is solved by minimizing the L-2 norm between simulated residence time distribution (RTD) and measurements of an inert tracer breakthrough curve (BTC) (C-Dot nanoparticle). Principal component analysis (PCA) of spatially correlated permeability fields enabled reduction of the parameter space by more than a factor of 10 and restricted the inverse search to reservoir-scale permeability variations. Thermal experiments and tracer tests conducted at the mesoscale Altona Field Laboratory (AFL) demonstrate that the method accurately predicts the effects of extreme flow channeling on heat transfer in a single bedding-plane rock fracture. However, this is only true when the permeability distributions provide adequate matches to both tracer RTD and frictional pressure loss. Without good agreement to frictional pressure loss, it is still possible to match a simulated RTD to measurements, but subsequent predictions of heat transfer are grossly inaccurate. The results of this study suggest that it is possible to anticipate the thermal effects of flow maldistribution, but only if both simulated RTDs and frictional pressure loss between fluid inlets and outlets are in good agreement with measurements.", "journal": "WATER RESOURCES RESEARCH", "category": "Environmental Sciences; Limnology; Water Resources", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000553192800011", "keywords": "coronary artery disease; early coronary revascularization; explainable machine learning; SPECT myocardial perfusion imaging; new-generation cardiac camera", "title": "Machine learning predicts per-vessel early coronary revascularization after fast myocardial perfusion SPECT: results from multicentre REFINE SPECT registry", "abstract": "Aims To optimize per-vessel prediction of early coronary revascularization (ECR) within 90 days after fast single-photon emission computed tomography (SPECT) myocardial perfusion imaging (MPI) using machine learning (ML) and introduce a method for a patient-specific explanation of ML results in a clinical setting. Methods and results A total of 1980 patients with suspected coronary artery disease (CAD) underwent stress/rest Tc-99m-sestamibi/tetrofosmin MPI with new-generation SPECT scanners were included. All patients had invasive coronary angiography within 6 months after SPECT MPI. ML utilized 18 clinical, 9 stress test, and 28 imaging variables to predict per-vessel and per-patient ECR with 10-fold cross-validation. Area under the receiver operator characteristics curve (AUC) of ML was compared with standard quantitative analysis [total perfusion deficit (TPD)] and expert interpretation. ECR was performed in 958 patients (48%). Per-vessel, the AUC of ECR prediction by ML (AUC 0.79, 95% confidence interval (CI) [0.77, 0.80]) was higher than by regional stress TPD (0.71, [0.70, 0.73]), combined-view stress TPD (AUC 0.71, 95% CI [0.69, 0.72]), or ischaemic TPD (AUC 0.72, 95% CI [0.71, 0.74]), all P < 0.001. Per-patient, the AUC of ECR prediction by ML (AUC 0.81, 95% CI [0.79, 0.83]) was higher than that of stress TPD, combined-view TPD, and ischaemic TPD, all P < 0.001. ML also outperformed nuclear cardiologists' expert interpretation of MPI for the prediction of early revascularization performance. A method to explain ML prediction for an individual patient was also developed. Conclusions In patients with suspected CAD, the prediction of ECR by ML outperformed automatic MPI quantitation by TPDs ( per-vessel and per-patient) or nuclear cardiologists' expert interpretation (per-patient).", "journal": "EUROPEAN HEART JOURNAL-CARDIOVASCULAR IMAGING", "category": "Cardiac & Cardiovascular Systems; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000600303400001", "keywords": "Artificial neural networks; Nonlinear systems; Adaptive systems; Control systems; Uncertain systems; Licenses; Backstepping; Adaptive neural networks; non-strict feedback form; nonsymmetric dead-zone; uncertain nonlinear systems", "title": "Direct Neural Network Adaptive Tracking Control for Uncertain Non-Strict Feedback Systems With Nonsymmetric Dead-Zone", "abstract": "In this paper, combined with the approximation of neural network, a novel direct adaptive alleviating tracking control algorithm is presented for a class of non-strict feedback uncertain nonlinear systems. Here, both nonlinear uncertainties and nonsymmetric dead-zone inputs are considered. First, according to some coordinate transforms and variable separation methods, the non-strict feedback form is converted into the normal form. Second, the relationship of state vector and error functions are established, and the inputs of dead-zone are compensated with adaptive approaches. This novel direct scheme assumes that the approximation error and optimal approximation norms of NN are to be bounded by unknown constants and can alleviate the number of online adjusted parameters so as to improve the robust control performance of the systems. At last, under Lyapunov theorem analysis, the uniformly ultimately boundness of all the signals in the closed-loop systems can be guaranteed and the dead-zone inputs can be compensated, the effectiveness of this algorithm is well demonstrated by simulation results.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000573082500001", "keywords": "Welding; Numerical models; Microorganisms; Geometry; Shape; Memetics; Fuzzy systems; Bacterial memetic algorithm; fuzzy system; machine learning; TIG welding; weld bead geometry", "title": "Bacterial Memetic Algorithm Trained Fuzzy System-Based Model of Single Weld Bead Geometry", "abstract": "This article presents a fuzzy system-based modeling approach to estimate the weld bead geometry (WBG) from the welding process variables (WPVs) and to achieve a specific weld bead shape. The bacterial memetic algorithm (BMA) is applied to solve these problems in two different roles, as a supervised trainer, and as an optimizer. As a supervised trainer, the BMA is applied to tune two different WBG models. The bead geometry properties (BGP) model follows a traditional approach providing the WBG properties as outputs. The direct profile measurement (DPM) model describes the bead profiles points by a non-linear function realized in the form of fuzzy rules. As an optimizer, the BMA utilizes the developed fuzzy systems to find the solution sets of WPVs to acquire the desired WBG. The best performance is achieved by applying six rules in the BGP model and eleven rules in the DPM model. The results indicate that the normalized root means square error for the validation data set lies in the range of 0.40 - 1.56% for the BGP model and 4.49 - 7.52% for the DPM model. The comparative analysis suggests that the BGP model estimates the BWG in a superior manner when several WPVs are altered. The developed fuzzy systems provide a tool for interpreting the effects of the WPVs. The developed optimizer provides multiple valid set of WPVs to produce the desired WBG, thus supporting the selection of those process variables in applications.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000522979400003", "keywords": "Computation offloading; Deep learning; Mobile edge computing; Energy and performance optimization", "title": "Deep learning-based computation offloading with energy and performance optimization", "abstract": "With the benefit of partially or entirely offloading computations to a nearby server, mobile edge computing gives user equipment (UE) more powerful capability to run computationally intensive applications. However, a critical challenge emerged: how to select the optimal set of components to offload considering the UE performance as well as its battery usage constraints. In this paper, we propose a novel energy and performance efficient deep learning based offloading algorithm. The optimal offloading schemes of components based on remaining energy and its performance can be determined by our proposed algorithm. All of these considerations are modeled as a cost function; then, a deep learning network is trained to compute the solution by which the optimal offloading scheme can be determined. Experimental results show that the proposed method is superior to existing methods in terms of energy and performance constraints.", "journal": "EURASIP JOURNAL ON WIRELESS COMMUNICATIONS AND NETWORKING", "category": "Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000560695700103", "keywords": "Machine learning; deep learning; compressive sensing; millimeter-wave massive MIMO; channel estimation; intelligent reflecting surfaces", "title": "Deep Denoising Neural Network Assisted Compressive Channel Estimation for mmWave Intelligent Reflecting Surfaces", "abstract": "Integrating large intelligent reflecting surfaces (IRS) into millimeter-wave (mmWave) massive multi-input-multi-ouput (MIMO) has been a promising approach for improved coverage and throughput. Most existing work assumes the ideal channel estimation, which can be challenging due to the high-dimensional cascaded MIMO channels and passive reflecting elements. Therefore, this paper proposes a deep denoising neural network assisted compressive channel estimation for mmWave IRS systems to reduce the training overhead. Specifically, we first introduce a hybrid passive/active IRS architecture, where very few receive chains are employed to estimate the uplink user-to-IRS channels. At the channel training stage, only a small proportion of elements will be successively activated to sound the partial channels. Moreover, the complete channel matrix can be reconstructed from the limited measurements based on compressive sensing, whereby the common sparsity of angular domain mmWave MIMO channels among different subcarriers is leveraged for improved accuracy. Besides, a complex-valued denoising convolution neural network (CV-DnCNN) is further proposed for enhanced performance. Simulation results demonstrate the superiority of the proposed solution over state-of-the-art solutions.", "journal": "IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY", "category": "Engineering, Electrical & Electronic; Telecommunications; Transportation Science & Technology", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000575187600003", "keywords": "Action recognition; Deep learning; Kinematic feature; Attention mechanism", "title": "Structure-based enzyme engineering improves donor-substrate recognition of Arabidopsis thaliana glycosyltransferases", "abstract": "Glycosylation of secondary metabolites involves plant UDP-dependent glycosyltransferases (UGTs). UGTs have shown promise as catalysts in the synthesis of glycosides for medical treatment. However, limited understanding at the molecular level due to insufficient biochemical and structural information has hindered potential applications of most of these UGTs. In the absence of experimental crystal structures, we employed advanced molecular modeling and simulations in conjunction with biochemical characterization to design a workflow to study five Group H Arabidopsis thaliana (76E1, 76E2, 76E4, 76E5, 76D1) UGTs. Based on our rational structural manipulation and analysis, we identified key amino acids (P129 in 76D1; D374 in 76E2; K275 in 76E4), which when mutated improved donor substrate recognition than wildtype UGTs. Molecular dynamics simulations and deep learning analysis identified structural differences, which drive substrate preferences. The design of these UGTs with broader substrate specificity may play important role in biotechnological and industrial applications. These findings can also serve as basis to study other plant UGTs and thereby advancing UGT enzyme engineering.", "journal": "BIOCHEMICAL JOURNAL", "category": "Biochemistry & Molecular Biology", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000549805400001", "keywords": "Multi-BS association; pilot allocation; pursuit learning; pilot contamination; learning automata", "title": "Pursuit Learning-Based Joint Pilot Allocation and Multi-Base Station Association in a Distributed Massive MIMO Network", "abstract": "Pilot contamination (PC) interference causes inaccurate user equipment (UE) channel estimations and significant signal-to-interference ratio (SINR) degradations. Pilot allocation and multi-base-station (BS) association have been used to combat the PC effect and to maximize the network spectral efficiency. However, current approaches solve the pilot allocation and multi-BS association separately. This leads to a sub-optimal solution. In this paper, we propose a parallel pursuit-learning-based joint pilot allocation and multi-BS association. We first formulate the pilot allocation and multi-BS association problem as a joint optimization function. To solve the optimization function, we use a parallel optimization solver, based on a pursuit learning algorithm, that decomposes the optimization function into multiple subfunctions. Each subfunction collaborates with the other ones to obtain an optimal solution by learning from rewards obtained from probabilistically testing random solution samples. A mathematical proof to guarantee the solution convergence is provided. Simulation results show that our scheme outperforms the existing schemes by an average of 18% in terms of the network spectral efficiency.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": ["learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000578282900002", "keywords": "Nonlinear systems; Control systems; Adaptive systems; Convergence; Artificial neural networks; Lyapunov methods; Backstepping; Barrier Lyapunov functions (BLFs); dynamic surface control (DSC); finite-time control; neural networks (NNs); state constraints; strict-feedback nonlinear systems", "title": "Identification and Classification of Atmospheric Particles Based on SEM Images Using Convolutional Neural Network with Attention Mechanism", "abstract": "Accurate identification and classification of atmospheric particulates can provide the basis for their source apportionment. Most current research studies mainly focus on the classification of atmospheric particles based on the energy spectrum of particles, which has the problems of low accuracy and being time-consuming. It is necessary to study the classification method of atmospheric particles with higher accuracy. In this paper, a convolutional neural network (CNN) model with attention mechanism is proposed to identify and classify the scanning electron microscopy (SEM) images of atmospheric particles. First, this work established a database, Qingdao 2016-2018, for atmospheric particles classification research. This database consists of 3469 SEM images of single particulates. Secondly, by analyzing the morphological characteristics of single particle SEM images, it can be divided into four categories: fibrous particles, flocculent particles, spherical particles, and mineral particles. Thirdly, by introducing attention mechanism into convolutional neural network, an Attention-CNN model for the identification and classification of the four types of atmospheric particles based on the SEM images is established. Finally, the Attention-CNN model is trained and tested based on the SEM images database, and the results of identification and classification for four types of particles are obtained. Under the same SEM images database, the classification results from Attention-CNN are compared with those of CNN and SVM. It is found that Attention-CNN has higher classification accuracy and reduces significantly the misclassification number of particles, which shows the focusing effect of attention mechanism.", "journal": "COMPLEXITY", "category": "Mathematics, Interdisciplinary Applications; Multidisciplinary Sciences", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000599525100064", "keywords": "Circuit faults; Fault diagnosis; Time series analysis; Velocity control; Sensors; Vibrations; Feature extraction; Attention; fault diagnosis; interturn short-circuit fault (ISCF); permanent magnet synchronous machine (PMSM); recurrent neural network (RNN)", "title": "Attention Recurrent Neural Network-Based Severity Estimation Method for Interturn Short-Circuit Fault in Permanent Magnet Synchronous Machines", "abstract": "With the development of smart factories, deep learning, which automatically extracts features and diagnoses faults, has become an important approach for fault diagnosis. In this article, a novel interturn short-circuit fault (ISCF) diagnosis approach using an attention-based recurrent neural network is proposed. An encoder-decoder architecture using an attention mechanism diagnoses the ISCF by estimating a fault indicator that directly reflects the severity of the fault, using currents and rotational speed signals as inputs. The attention mechanism helps the decoding process in accurate diagnosis and solves the long-term dependence problem of the encoder-decoder structure. The proposed algorithm uses only three-phase current and rotational speed as the inputs to evaluate the severity of the ISCF and enable early stage diagnosis of ISCF. The diagnosis of ISCF is achieved in various operating points and fault conditions, and no additional sensors, such as voltage and vibration sensors, are required. Experimental results for various operating and fault conditions demonstrate that the proposed method effectively diagnoses ISCFs.", "journal": "IEEE TRANSACTIONS ON INDUSTRIAL ELECTRONICS", "category": "Automation & Control Systems; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": ["neural net", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000632907700001", "keywords": "spiking neural network; neuronal plasticity; synaptic plasticity; reward propagation; sparse connections", "title": "Neuronal-Plasticity and Reward-Propagation Improved Recurrent Spiking Neural Networks", "abstract": "Different types of dynamics and plasticity principles found through natural neural networks have been well-applied on Spiking neural networks (SNNs) because of their biologically-plausible efficient and robust computations compared to their counterpart deep neural networks (DNNs). Here, we further propose a special Neuronal-plasticity and Reward-propagation improved Recurrent SNN (NRR-SNN). The historically-related adaptive threshold with two channels is highlighted as important neuronal plasticity for increasing the neuronal dynamics, and then global labels instead of errors are used as a reward for the paralleling gradient propagation. Besides, a recurrent loop with proper sparseness is designed for robust computation. Higher accuracy and stronger robust computation are achieved on two sequential datasets (i.e., TIDigits and TIMIT datasets), which to some extent, shows the power of the proposed NRR-SNN with biologically-plausible improvements.", "journal": "FRONTIERS IN NEUROSCIENCE", "category": "Neurosciences", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000711207200005", "keywords": "T2DM; GPR40; artificial intelligence; oligopeptides; molecular fingerprint; site-directed mutagenesis", "title": "Rotational and reflectional equivariant convolutional neural network for data-limited applications: Multiphase flow demonstration", "abstract": "This article deals with approximating steady-state particle-resolved fluid flow around a fixed particle of interest under the influence of randomly distributed stationary particles in a dispersed multiphase setup using convolutional neural network (CNN). The considered problem involves rotational symmetry about the mean velocity (streamwise) direction. Thus, this work enforces this symmetry using SE(3)-equivariant, special Euclidean group of dimension 3, CNN architecture, which is translation and three-dimensional rotation equivariant. This study mainly explores the generalization capabilities and benefits of a SE(3)-equivariant network. Accurate synthetic flow fields for Reynolds number and particle volume fraction combinations spanning over a range of [86.22, 172.96] and [0.11, 0.45], respectively, are produced with careful application of symmetry-aware data-driven approach.</p>", "journal": "PHYSICS OF FLUIDS", "category": "Mechanics; Physics, Fluids & Plasmas", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000714225400001", "keywords": "soft robotics; soft hydraulic actuator; iterative learning control; feed-forward neural network; individual deformability compensation; trajectory tracking", "title": "Individual deformability compensation of soft hydraulic actuators through iterative learning-based neural network", "abstract": "Robotic devices with soft actuators have been developed to realize the effective rehabilitation of patients with motor paralysis by enabling soft and safe interaction. However, the control of such robots is challenging, especially owing to the difference in the individual deformability occurring in manual fabrication of soft actuators. Furthermore, soft actuators used in wearable rehabilitation devices involve a large response delay which hinders the application of such devices for at-home rehabilitation. In this paper, a feed-forward control method for soft actuators with a large response delay, comprising a simple feed-forward neural network (FNN) and an iterative learning controller (ILC), is proposed. The proposed method facilitates the effective learning and acquisition of the inverse model (i.e. the model that can generate control input to the soft actuator from a target trajectory) of soft actuators. First, the ILC controls a soft actuator and iteratively learns the actuator deformability. Subsequently, the FNN is trained to obtain the inverse model of the soft actuator. The control results of the ILC are used as training datasets for supervised learning of the FNN to ensure that it can efficiently acquire the inverse model of the soft actuator, including the deformability and the response delay. Experiments with fiber-reinforced soft bending hydraulic actuators are conducted to evaluate the proposed method. The results show that the ILC can learn and compensate for the actuator deformability. Moreover, the iterative learning-based FNN serves to achieve a precise tracking performance on various generalized trajectories. These facts suggest that the proposed method can contribute to the development of robotic rehabilitation devices with soft actuators and the field of soft robotics.", "journal": "BIOINSPIRATION & BIOMIMETICS", "category": "Engineering, Multidisciplinary; Materials Science, Biomaterials; Robotics", "annotated_keywords": ["neural net", "neural net", "supervised learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000677527600001", "keywords": "Natural language processing; Document representation; Document classification; Graph neural network; Quantum probability", "title": "Testing the reliability of interpretable neural networks in geoscience using the Madden-Julian oscillation", "abstract": "We test the reliability of two neural network interpretation techniques, backward optimization and layerwise relevance propagation, within geoscientific applications by applying them to a commonly studied geophysical phenomenon, the Madden-Julian oscillation. The Madden- Julian oscillation is a multi-scale pattern within the tropical atmosphere that has been extensively studied over the past decades, which makes it an ideal test case to ensure the interpretability methods can recover the current state of knowledge regarding its spatial structure. The neural networks can, indeed, reproduce the current state of knowledge and can also provide new insights into the seasonality of the Madden- Julian oscillation and its relationships with atmospheric state variables. The neural network identifies the phase of the Madden- Julian oscillation twice as accurately as a linear regression approach, which means that nonlinearities used by the neural network are important to the structure of the Madden-Julian oscillation. Interpretations of the neural network show that it accurately captures the spatial structures of the Madden-Julian oscillation, suggest that the nonlinearities of the Madden-Julian oscillation are manifested through the uniqueness of each event, and offer physically meaningful insights into its relationship with atmospheric state variables. We also use the interpretations to identify the seasonality of the Madden-Julian oscillation and find that the conventionally defined extended seasons should be shifted later by 1 month. More generally, this study suggests that neural networks can be reliably interpreted for geoscientific applications and may thereby serve as a dependable method for testing geoscientific hypotheses.", "journal": "GEOSCIENTIFIC MODEL DEVELOPMENT", "category": "Geosciences, Multidisciplinary", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000718186300004", "keywords": "artificial neural networks; nutrient management; phosphorus index; water quality; weighting factors", "title": "Using artificial neural networks to improve phosphorus indices", "abstract": "The phosphorus index (PI) was developed as a field-scale assessment tool used to identify critical source areas of phosphorus (P) loss, thus most US states have adopted the PI as their strategy for targeted management and conservation practices for effective mitigation of P loss front agricultural landscapes to surface waters. Recent studies have focused on evaluating and updating PI weighting factors (WFs) to ensure agreement between PI values and measured losses of P. Given that the WF of each site characteristic is usually determined individually without considering possible interactions, the goal of this study was to demonstrate how artificial neural networks (ANNs) that consider real-world interdependence can be used to determine WFs. Our specific objectives were to evaluate ANN performance for predicting soluble P (SP) concentrations in tile effluent using site characteristics as predictor variables, and to evaluate whether ANN-generated WFs can be used to improve PI performance. Garson's algorithm was used to determine the relative importance of each site characteristic to SP loss. Data from a monitored in-field laboratory were used to evaluate the ability of a PI with no WFs (PINO), a PI with WFs as proposed in the original Lemunyon and Gilbert PI (PILG), and a PI with ANN-generated WFs (MANN), to estimate SP loss potential in tile discharge. Simulation results showed that the ANN model provided reliable estimates of SP in tile effluent (R-2 = 0.99; RMSE = 0.0024).The relative importance analysis underscored the value of routine soil P testing for agronomic sufficiency for environmental stewardship, and highlighted the necessity of prioritizing both contemporary and legacy P sources during P loss risk assessments. Unlike the other PIs, PIANN was able to provide reasonable estimates of SP loss potential as illustrated with significant exponential relationships (R-2 = 0.60; p < 0.001) between PIANN values and measured mean annual SP concentrations in tile effluent. These findings demonstrate that ANNs can be used to develop PIs with a strong correlation to measured SP.", "journal": "JOURNAL OF SOIL AND WATER CONSERVATION", "category": "Ecology; Soil Science; Water Resources", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000606475800004", "keywords": "Deep neural networks; deep learning; channel detection; image processing; two-dimensional seismic data", "title": "Automatic Channel Detection Using DNN on 2D Seismic Data", "abstract": "Geologists interpret seismic data to understand subsurface properties and subsequently to locate underground hydrocarbon resources. Channels are among the most important geological features interpreters analyze to locate petroleum reservoirs. However, manual channel picking is both time consuming and tedious. Moreover, similar to any other process dependent on human intervention, manual channel picking is error prone and inconsistent. To address these issues, automatic channel detection is both necessary and important for efficient and accurate seismic interpretation. Modern systems make use of real-time image processing techniques for different tasks. Automatic channel detection is a combination of different mathematical methods in digital image processing that can identify streaks within the images called channels that are important to the oil companies. In this paper, we propose an innovative automatic channel detection algorithm based on machine learning techniques. The new algorithm can identify channels in seismic data/images fully automatically and tremendously increases the efficiency and accuracy of the interpretation process. The algorithm uses deep neural network to train the classifier with both the channel and non-channel patches. We provide a field data example to demonstrate the performance of the new algorithm. The training phase gave a maximum accuracy of 84.6% for the classifier and it performed even better in the testing phase, giving a maximum accuracy of 90%.", "journal": "COMPUTER SYSTEMS SCIENCE AND ENGINEERING", "category": "Computer Science, Hardware & Architecture; Computer Science, Theory & Methods", "annotated_keywords": ["neural net", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000610344700001", "keywords": "jacking force; vertical tunneling method; artificial neural network; genetic algorithm", "title": "Prediction of Jacking Force in Vertical Tunneling Projects Based on Neuro-Genetic Models", "abstract": "The vertical tunneling method is an emerging technique to build sewage inlets or outlets in constructed horizontal tunnels. The jacking force used to drive the standpipes upward is an essential factor during the construction process. This study aims to predict the jacking forces during the vertical tunneling construction process through two intelligence systems, namely, artificial neural networks (ANNs) and hybrid genetic algorithm optimized ANNs (GA-ANNs). In this paper, the Beihai hydraulic tunnel constructed by the vertical tunneling method in China is introduced, and the direct shear tests have been conducted. A database composed of 546 datasets with ten inputs and one output was prepared. The effective parameters are classified into three categories, including tunnel geometry factors, the geological factor, and jacking operation factors. These factors are considered as input parameters. The tunnel geometry factors include the jacking distance, the thickness of overlaying soil, and the height of overlaying water; the geological factor refers to the geological conditions; and the jacking operation factors consist of the dead weight of standpipes, effective overburden soil pressure, effective lateral soil pressure, average jacking speed, construction hours, and soil weakening measure. The output parameter, on the other hand, refers to the jacking force. Performance indices, including the coefficient of determination (R-2), root mean square error (RMSE), and the absolute value of relative error (RE), are computed to compare the performance of the ANN models and the GA-ANN models. Comparison results show that the GA-ANN models perform better than the ANN model, especially on the RMSE values. Finally, parametric sensitivity analysis between the input parameters and output parameter is conducted, reaching the result that the height of overlaying water, the average jacking speed, and the geological condition are the most effective input parameters on the jacking force in this study.", "journal": "JOURNAL OF MARINE SCIENCE AND ENGINEERING", "category": "Engineering, Marine; Engineering, Ocean; Oceanography", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000712375400005", "keywords": "Regulatory mechanisms; Profile prediction; Prediction tasks; Motif; detection; Neural networks; Interpretation", "title": "Machine learning for profile prediction in genomics", "abstract": "fueled the development of machine learning methods aimed at investigating important questions in genomics. Although the motivations for these methods vary, a task that is commonly adopted is that of profile prediction, where predictions are made for one or more forms of biochemical activity along the genome, for example, histone modification, chromatin accessibility, or protein binding. In this review, we give an overview of the research works performing profile prediction, define two broad categories of profile prediction tasks, and discuss the types of scientific questions that can be answered in each.", "journal": "CURRENT OPINION IN CHEMICAL BIOLOGY", "category": "Biochemistry & Molecular Biology; Biophysics", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000597384600004", "keywords": "Magnetic resonance imaging; simulation; machine learning; supervised techniques; segmentation", "title": "Simulator-generated training datasets as an alternative to using patient data for machine learning: An example in myocardial segmentation with MRI", "abstract": "Background and Objective: Supervised Machine Learning techniques have shown significant potential in medical image analysis. However, the training data that need to be collected for these techniques in the field of MRI 1) may not be available, 2) may be available but the size is small, 3) may be available but not representative and 4) may be available but with weak labels. The aim of this study was to overcome these limitations through advanced MR simulations on a realistic computer model of human anatomy without using a real MRI scanner, without scanning patients and without having personnel and the associated expenses. Methods: The 4D-XCAT model was used with the coreMRI simulation platform for generating artificial short-axis MR-images for training a neural-network to automatic delineate the LV endocardium and epicardium. Its performance was assessed on real MR-images acquired from eight healthy volunteers. The neural-network was also trained on real MR-images from a publicly available dataset and its performance was assessed on the same volunteers' data. Results: The proposed solution demonstrated a performance of 94% (endocardium) and 90% DICE (epicardium) in real mid-ventricular slices, whereas a 10% addition of real MR-images in the artificial training dataset increased the performance to 97% DICE. The use of artificial MR-images that cover the entire LV yielded 85% (endocardium) and 88% DICE (epicardium) when combined with real MR data with an 80%-20% mix respectively. Conclusions: This study suggests a low-cost solution for constructing artificial training datasets for supervised learning techniques in the field of MR by using advanced MR simulations without the use of a real MRI scanner, without scanning patients and without having to use specialized personnel, such as technologists and radiologists. (C) 2020 The Authors. Published by Elsevier B.V.", "journal": "COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE", "category": "Computer Science, Interdisciplinary Applications; Computer Science, Theory & Methods; Engineering, Biomedical; Medical Informatics", "annotated_keywords": ["machine learning", "supervised learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000707046300006", "keywords": "Force and tactile sensing; grasping", "title": "Targeted Free Energy Perturbation Revisited: Accurate Free Energies from Mapped Reference Potentials", "abstract": "We present an approach that extends the theory of targeted free energy perturbation (TFEP) to calculate free energy differences and free energy surfaces at an accurate quantum mechanical level of theory from a cheaper reference potential. The convergence is accelerated by a mapping function that increases the overlap between the target and the reference distributions. Building on recent work, we show that this map can be learned with a normalizing flow neural network, without requiring simulations with the expensive target potential but only a small number of single-point calculations, and, crucially, avoiding the systematic error that was found previously. We validate the method by numerically evaluating the free energy difference in a system with a double-well potential and by describing the free energy landscape of a simple chemical reaction in the gas phase.", "journal": "JOURNAL OF PHYSICAL CHEMISTRY LETTERS", "category": "Chemistry, Physical; Nanoscience & Nanotechnology; Materials Science, Multidisciplinary; Physics, Atomic, Molecular & Chemical", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000648429800007", "keywords": "Image classification; Medical image segmentation; Neural network architecture; Image processing techniques; Wavelet transform", "title": "A Wavelet Transform and Neural Network Based Segmentation & Classification System For Bone Fracture Detection", "abstract": "This paper deals with fracture detection of a human bone from X-ray images applying image processing techniques. It describes the wavelet transform based segmentation and neural network-based classification method for medical images, especially X-ray images, to locate or detect any fracture in the bones. Among different image segmentation techniques, using Wavelet Transform based segmentation manifests particular significance compared to other conventional segmentation techniques for the images considered in this study. Also in terms of statistical image identifier, namely Signal to Noise Ratio (SNR), Peak Signal to Noise Ratio (PSNR), Structural similarity (SSIM), and Entropy etc. The proposed methods have observable advantages, particularly in medical image segmentation. The Haar Wavelet showed maximum correlation with the considered images. The detailed vertical, horizontal and diagonal components can decompose Xray images based on the Wavelet Transform algorithm. The vertical detail component of third level decomposition of the Wavelet Transform has preferred because it shows the minimum Entropy. The Error Backpropagation Neural Network (EBP-NN) fed with the obtained medical images from wavelet-based segmentation technique. This neural network has trained with fractured and non-fractured images then tested on various other X-ray images. An EBP-NN classification system with the architecture of 1024-22-2 gives the maximum accuracy. The developed system can detect fractured bone images accurately.", "journal": "OPTIK", "category": "Optics", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000612027700001", "keywords": "computational spectroscopy; deep learning; hyperspectral imaging; optical inverse design", "title": "Deep-Learned Broadband Encoding Stochastic Filters for Computational Spectroscopic Instruments", "abstract": "Computational spectroscopic instruments with broadband encoding stochastic (BEST) filters allow the reconstruction of the spectrum at high precision with only a few filters. However, conventional design manners of BEST filters are often heuristic and may fail to fully explore the encoding potential of BEST filters. The parameter constrained spectral encoder and decoder (PCSED)-a neural network-based framework-is presented for the design of BEST filters in spectroscopic instruments. By incorporating the target spectral response definition and the optical design procedures comprehensively, PCSED links the mathematical optimum and practical limits confined by available fabrication techniques. Benefiting from this, a BEST-filter-based spectral camera presents a higher reconstruction accuracy with up to 30 times enhancement and a better tolerance to fabrication errors. The generalizability of PCSED is validated in designing metasurface- and interference-thin-film-based BEST filters.", "journal": "ADVANCED THEORY AND SIMULATIONS", "category": "Multidisciplinary Sciences", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000605797100001", "keywords": "machine learning; tactile sensing; perception for grasping", "title": "Gaining a Sense of Touch Object Stiffness Estimation Using a Soft Gripper and Neural Networks", "abstract": "Soft grippers are gaining significant attention in the manipulation of elastic objects, where it is required to handle soft and unstructured objects, which are vulnerable to deformations. The crucial problem is to estimate the physical parameters of a squeezed object to adjust the manipulation procedure, which poses a significant challenge. The research on physical parameters estimation using deep learning algorithms on measurements from direct interaction with objects using robotic grippers is scarce. In our work, we proposed a trainable system which performs the regression of an object stiffness coefficient from the signals registered during the interaction of the gripper with the object. First, using the physics simulation environment, we performed extensive experiments to validate our approach. Afterwards, we prepared a system that works in a real-world scenario with real data. Our learned system can reliably estimate the stiffness of an object, using the Yale OpenHand soft gripper, based on readings from Inertial Measurement Units (IMUs) attached to the fingers of the gripper. Additionally, during the experiments, we prepared three datasets of IMU readings gathered while squeezing the objects-two created in the simulation environment and one composed of real data. The dataset is the contribution to the community providing the way for developing and validating new approaches in the growing field of soft manipulation.", "journal": "ELECTRONICS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Physics, Applied", "annotated_keywords": ["deep learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000642445900002", "keywords": "Tumor; Antigen; Prediction; Machine learning; T-cell; Peptide", "title": "TAP 1.0: A robust immunoinformatic tool for the prediction of tumor T-cell antigens based on AAindex properties", "abstract": "Immunotherapy is a research area with great potential in drug discovery for cancer treatment. Because of the capacity of tumor antigens to activate the immune response and promote the destruction of tumor cells, they are considered excellent immunotherapeutic drugs. In this work, we evaluated fifteen machine learning algorithms for the classification of tumor antigens. For this purpose, we build robust datasets, carefully selected from the TANTIGEN and IEDB databases. The feature computation of all antigens in this study was performed by developing a script written in Python 3.8, which allowed the calculation of 544 physicochemical and biochemical properties extracted from the AAindex database. All classifiers were subjected to the training, 10-fold cross-validation, and testing on an independent dataset. The results of this study showed that the quadratic discrim-inant classifier presented the best performance measures over the independent dataset, accuracy = 0.7384, AUC = 0.817, recall = 0.676, precision = 0.7857, F1 = 0.713, kappa = 0.4764, and Matthews correlation coefficient = 0.4834, outperforming common machine learning classifiers used in the bioinformatics area. We believe that our prediction model could be of great importance in the field of cancer immunotherapy for the search of po-tential tumor antigens. Taking all aspects mentioned before, we developed an immunoinformatic tool called TAP 1.0 with a friendly interface for tumor antigens prediction, available at https://tapredictor.herokuapp.com/.", "journal": "COMPUTATIONAL BIOLOGY AND CHEMISTRY", "category": "Biology; Computer Science, Interdisciplinary Applications", "annotated_keywords": ["machine learning", "learning algorithm"], "label": "1", "title_label": "0"}
{"id": "WOS:000680687600002", "keywords": "Systematic literature review; Artificial intelligence; Financial services; Bank marketing", "title": "Towards smart optical focusing: deep learning-empowered dynamic wavefront shaping through nonstationary scattering media", "abstract": "Optical focusing through scattering media is of great significance yet challenging in lots of scenarios, including biomedical imaging, optical communication, cybersecurity, three-dimensional displays, etc. Wavefront shaping is a promising approach to solve this problem, but most implementations thus far have only dealt with static media, which, however, deviates from realistic applications. Herein, we put forward a deep learning-empowered adaptive framework, which is specifically implemented by a proposed Timely-Focusing-Optical-Transformation-Net (TFOTNet), and it effectively tackles the grand challenge of real-time light focusing and refocusing through time-variant media without complicated computation. The introduction of recursive fine-tuning allows timely focusing recovery, and the adaptive adjustment of hyperparameters of TFOTNet on the basis of medium changing speed efficiently handles the spatiotemporal non-stationarity of the medium. Simulation and experimental results demonstrate that the adaptive recursive algorithm with the proposed network significantly improves light focusing and tracking performance over traditional methods, permitting rapid recovery of an optical focus from degradation. It is believed that the proposed deep learning-empowered framework delivers a promising platform towards smart optical focusing implementations requiring dynamic wavefront control. (C) 2021 Chinese Laser Press", "journal": "PHOTONICS RESEARCH", "category": "Optics", "annotated_keywords": ["deep learning", "deep learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000667100600001", "keywords": "Recurrent neural networks; Recursive neural networks; Image classification; Convolutional neural network", "title": "College English Teaching Quality Evaluation System Based on Information Fusion and Optimized RBF Neural Network Decision Algorithm", "abstract": "In the process of deepening and developing the current higher education reform, people pay more and more attention to the research of college English education. The key to improve the college English education is to improve the quality of education, and learning evaluation is the key measure to improve the quality of education and training. This paper mainly studies the college English teaching quality evaluation system based on information fusion and optimized RBF neural network decision algorithm. This paper analyzes the main problems and complexity of creating an ideal learning quality evaluation system. On the basis of analyzing the advantages and disadvantages of the previous learning quality evaluation methods, this paper summarizes the existing learning quality evaluation methods and puts forward some suggestions according to the existing evaluation methods. A learning quality evaluation model based on RBF algorithm of neural network is proposed. RBF regularization network method, RBF neural network decision algorithm, and experimental investigation method are used to study the college English teaching quality evaluation system based on information fusion and optimization of RBF neural network decision algorithm. By innovating teaching methods and enriching teaching means, college students' thirst for English knowledge can be aroused, and teachers' teaching level can be improved. The results show that 50% of college students think that the level of college English teaching is average and needs to be improved. In the performance evaluation system of college English teaching quality based on information fusion and optimized RBF neural network decision algorithm, it is necessary to establish a learning evaluation system, monitor the learning quality in real time, find problems and improve them in time, and recognize the current situation of education.", "journal": "JOURNAL OF SENSORS", "category": "Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000681288300012", "keywords": "Seam tracking; Industrial automation; Orbital welding; Adaptive GMAW; Vision sensor; Pipeline", "title": "Using distance on the Riemannian manifold to compare representations in brain and in models", "abstract": "Representational similarity analysis (RSA) summarizes activity patterns for a set of experimental conditions into a matrix composed of pairwise comparisons between activity patterns. Two examples of such matrices are the condition-by-condition inner product and correlation matrix. These representational matrices reside on the manifold of positive semidefinite matrices, called the Riemannian manifold. We hypothesize that representational similarities would be more accurately quantified by considering the underlying manifold of the representational matrices. Thus, we introduce the distance on the Riemannian manifold as a metric for comparing representations. Analyzing simulated and real fMRI data and considering a wide range of metrics, we show that the Riemannian distance is least susceptible to sampling bias, results in larger intra-subject reliability, and affords searchlight mapping with high sensitivity and specificity. Furthermore, we show that the Riemannian distance can be used for measuring multi-dimensional connectivity. This measure captures both univariate and multivariate connectivity and is also more sensitive to nonlinear regional interactions compared to the state-of-the-art measures. Applying our proposed metric to neural network representations of natural images, we demonstrate that it also possesses outstanding performance in quantifying similarity in models. Taken together, our results lend credence to the proposition that RSA should consider the manifold of the representational matrices to summarize response patterns in the brain and in models.", "journal": "NEUROIMAGE", "category": "Neurosciences; Neuroimaging; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000684293900006", "keywords": "Structural neuroimaging; Diffusion weighted imaging (DWI); Neuroplasticity; Learning and memory consolidation; Sleep", "title": "Post-learning micro- and macro-structural neuroplasticity changes with time and sleep", "abstract": "Neuroplasticity refers to the fact that our brain can partially modify both structure and function to adequately respond to novel environmental stimulations. Neuroplasticity mechanisms are not only operating during the acquisition of novel information (i.e., online) but also during the offline periods that take place after the end of the actual learning episode. Structural brain changes as a consequence of learning have been consistently demonstrated on the long term using non-invasive neuroimaging methods, but short-term changes remained more elusive. Fortunately, the swift development of advanced MR methods over the last decade now allows tracking fine-grained cerebral changes on short timescales beyond gross volumetric modifications stretching over several days or weeks. Besides a mere effect of time, post-learning sleep mechanisms have been shown to play an important role in memory consolidation and promote long-lasting changes in neural networks. Sleep was shown to contribute to structural modifications over weeks of prolonged training, but studies evidencing more rapid post-training sleep structural effects linked to memory consolidation are still scarce in human. On the other hand, animal studies convincingly show how sleep might modulate synaptic microstructure. We aim here at reviewing the literature establishing a link between different types of training/learning and the resulting structural changes, with an emphasis on the role of post-training sleep and time in tuning these modifications. Open questions are raised such as the role of post-learning sleep in macrostructural changes, the links between different MR structural measurement-related modifications and the underlying microstructural brain processes, and bidirectional influences between structural and functional brain changes.", "journal": "BIOCHEMICAL PHARMACOLOGY", "category": "Pharmacology & Pharmacy", "annotated_keywords": ["neural net"], "label": "0", "title_label": "0"}
{"id": "WOS:000634174700001", "keywords": "Chemical-induced hematotoxicity; Machine learning; Deep learning; Consensus model; Structural alert", "title": "Special Issue \"Advances in Machine Learning and Deep Learning Based Machine Fault Diagnosis and Prognosis\"", "abstract": "Chemical-induced hematotoxicity is an important concern in the drug discovery, since it can often be fatal when it happens. It is quite useful for us to give special attention to chemicals which can cause hematotoxicity. In the present study, we focused on in silico prediction of chemical-induced hematotoxicity with machine learning (ML) and deep learning (DL) methods. We collected a large data set contained 632 hematotoxic chemicals and 1525 approved drugs without hematotoxicity. Computational models were built using several different machine learning and deep learning algorithms integrated on the Online Chemical Modeling Environment (OCHEM). Based on the three best individual models, a consensus model was developed. It yielded the prediction accuracy of 0.83 and balanced accuracy of 0.77 on external validation. The consensus model and the best individual model developed with random forest regression and classification algorithm (RFR) and QNPR descriptors were made available at https://ochem.eu/article/135149, respectively. The relevance of 8 commonly used molecular properties and chemical-induced hematotoxicity was also investigated. Several molecular properties have an obvious differentiating effect on chemical-induced hematotoxicity. Besides, 12 structural alerts responsible for chemical hematotoxicity were identified using frequency analysis of substructures from Klekota-Roth fingerprint. These results should provide meaningful knowledge and useful tools for hematotoxicity evaluation in drug discovery and environmental risk assessment.", "journal": "PROCESSES", "category": "Engineering, Chemical", "annotated_keywords": ["machine learning", "deep learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000667248100032", "keywords": "Classification; doppler effect; hand gesture recognition (HGR); machine learning; radar architecture", "title": "Hand Gesture Recognition for Smart Devices by Classifying Deterministic Doppler Signals", "abstract": "Personal devices such as smartphones and tablets are rapidly becoming personal communication, information, and control centers. Apart from multitouch screens, human gestures are considered as a new interactive human-smart device interface. In this work, we propose a noncontact solution to implement hand gesture recognitions for smart devices. It is based on a continuous wave, time-division-multiplexing (TDM), single-input multiple-output (SIMO) Doppler radar sensor that can be realized by slightly modifying existing RF front ends of smart devices, and a machine-learning algorithm to recognize predefined gestures by classifying deterministic Doppler signals. An experimental setup emulating a smartphone-based radar sensor was implemented, and the experimental results verified the robustness and the accuracy of the proposed approach.", "journal": "IEEE TRANSACTIONS ON MICROWAVE THEORY AND TECHNIQUES", "category": "Engineering, Electrical & Electronic", "annotated_keywords": ["learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000619231300006", "keywords": "Wind turbine blade; Operational modal analysis; Modal parameters; Continuous wavelet transform; Machine learning", "title": "Continuous wavelet transform-based method for enhancing estimation of wind turbine blade natural frequencies and damping for machine learning purposes", "abstract": "In the current study, an operational modal analysis is performed on a wind turbine blade with pull-and-release excitation at the tip. The experiments were carried out in large-scale facility of Risoe campus in Technical University of Denmark. Two different blade configurations are compared - blade on the block and a configuration for fatigue testing comprising of blade on the block with mass resonance exciter and seismic masses. A continuous wavelet transform is employed for estimation of modal parameters of the fundamental flapwise bending mode of the blade from acceleration responses. Statistical analysis along with an outlier removal from the extracted modal parameters is carried out to provide clear estimates of the extracted values. These results are compared to the values extracted with other algorithms. The proposed method for modal parameter extraction aims at extraction of numerous observations allowing for statistical decision making and machine learning capabilities.", "journal": "MEASUREMENT", "category": "Engineering, Multidisciplinary; Instruments & Instrumentation", "annotated_keywords": ["machine learning", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000654211200001", "keywords": "data-driven method; machine learning; fault detection and diagnosis; applications and development; nuclear power plant", "title": "Data-Driven Machine Learning for Fault Detection and Diagnosis in Nuclear Power Plants: A Review", "abstract": "Data-driven machine learning (DDML) methods for the fault diagnosis and detection (FDD) in the nuclear power plant (NPP) are of emerging interest in the recent years. However, there still lacks research on comprehensive reviewing the state-of-the-art progress on the DDML for the FDD in the NPP. In this review, the classifications, principles, and characteristics of the DDML are firstly introduced, which include the supervised learning type, unsupervised learning type, and so on. Then, the latest applications of the DDML for the FDD, which consist of the reactor system, reactor component, and reactor condition monitoring are illustrated, which can better predict the NPP behaviors. Lastly, the future development of the DDML for the FDD in the NPP is concluded.", "journal": "FRONTIERS IN ENERGY RESEARCH", "category": "Energy & Fuels", "annotated_keywords": ["machine learning", "supervised learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000686909600027", "keywords": "Fault diagnosis; radial basis function; unscented Kalman filter; Tennessee Eastman process; dynamic modeling", "title": "Measuring landslide vulnerability status of Chukha, Bhutan using deep learning algorithms", "abstract": "Landslides are major natural hazards that have a wide impact on human life, property, and natural environment. This study is intended to provide an improved framework for the assessment of landslide vulnerability mapping (LVM) in Chukha Dzongkhags (district) of Bhutan. Both physical (22 nos.) and social (9 nos.) conditioning factors were considered to model vulnerability using deep learning neural network (DLNN), artificial neural network (ANN) and convolution neural network (CNN) approaches. Selection of the factors was conceded by the collinearity test and information gain ratio. Using Google Earth images, official data, and field inquiry a total of 350 (present and historical) landslides were recorded and training and validation sets were prepared following the 70:30 ratio. Nine LVMs were produced i.e. a landslide susceptibility (LS), one social vulnerability (SV) and a relative vulnerability (RLV) map for each model. The performance of the models was evaluated by area under curve (AUC) of receiver operating characteristics (ROC), relative landslide density index (R-index) and different statistical measures. The combined vulnerability map of social and physical factors using CNN (CNN-RLV) had the highest goodness-of-fit and excellent performance (AUC = 0.921, 0.928) followed by DLNN and ANN models. This approach of combined physical and social factors create an appropriate and more accurate LVM that may-support landslide prediction and management.", "journal": "SCIENTIFIC REPORTS", "category": "Multidisciplinary Sciences", "annotated_keywords": ["neural net", "deep learning", "deep learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000651056200001", "keywords": "history of science education; literature review; machine learning; methods; natural language processing", "title": "How has Science Education changed over the last 100 years? An analysis using natural language processing", "abstract": "For well over a century, the journal Science Education has been publishing articles about the teaching and learning of science. These articles represent more than just a repository of past work: they have the potential to offer insights into both the history of science education as well as well as the dynamics of field-specific change. It can be difficult, however, for educators, researchers, reformers, and policymakers to grasp the nuances of over 100 years of scholarship given the overwhelming amount of textual material. To address this problem, we have used latent Dirichlet allocation, an automated machine-learning algorithm from the field of natural language processing, to perform an automated literature review and classification of the corpus of work in Science Education. Using this technique, we have classified research in the journal into 21 distinct topics, falling into three thematic groups: science content topics, teaching-focused topics, and student-focused topics. We have also quantified the rise and fall of these topics and groups over time, and used them to begin to extract insight into the development of the field, including the effects of national policy changes on topics of interest to the research community, the interrelationships between different research topics, and the effects of intellectual cross-pollination. Based on this analysis, we argue that this technique shows great promise for even larger-scale analyses of educational literature and other textual data.", "journal": "SCIENCE EDUCATION", "category": "Education & Educational Research", "annotated_keywords": ["natural language processing", "natural language processing", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000620739500001", "keywords": "Earth observation; digital earth; copernicus; SDGs; (GE)OBIA; digitalisation; informisation; EARSeL", "title": "Digital vertical bar Earth vertical bar observation", "abstract": "The transforming world evokes changes in social, environmental, and economic dimensions, pushed by the digitalisation of many, if not all, aspects of our lives. Satellite Earth observation, while being \"digital\" from early on, has experienced a boost by digitalisation in recent years, with new trend s of cloud processing, data cube infrastructure, computer vision, machine learning, at unprecedented speeds. This Special Issue on \"Digital | Earth | Observation\" is dedicated to the fruitful interplay between the Digital Earth concept and Earth observation, embedded in the great technological trends in this field, and demonstrates how this potential can be used in various application contexts.", "journal": "EUROPEAN JOURNAL OF REMOTE SENSING", "category": "Remote Sensing", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000699274000001", "keywords": "open hardware; open source; low-cost; water quality; sensor network; LoRa; fish", "title": "A Custom Sensor Network for Autonomous Water Quality Assessment in Fish Farms", "abstract": "The control of water quality is crucial to ensure the survival of fish in aquaculture production facilities. Today, the combination of sensors with communication technologies permits to monitor these crucial parameters in real-time, allowing to take fast management decisions. However, out-of-the-box solutions are expensive, due to the small market and the industrial nature of sensors, besides being little customizable. To solve this, the present work describes a low-cost hardware and software architecture developed to achieve the autonomous water quality assessment and management on a remote facility for fish conservation aquaculture within the framework of the Smart Comunidad Rural Digital (smartCRD) project. The developed sensor network has been working uninterruptedly since its installation (20 April 2021). It is based on open source technology and includes a central gateway for on-site data monitoring of water quality nodes as well as an online management platform for data visualization and sensor network configuration. Likewise, the system can detect autonomously water quality parameters outside configurable thresholds and deliver management alarms. The described architecture, besides low-cost, is highly customizable, compatible with other sensor network projects, machine-learning applications, and is capable of edge computing. Thus, it contributes to making open sensorization more accessible to real-world applications.", "journal": "ELECTRONICS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Physics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000684377600015", "keywords": "machine learning; algorithmic induction; theory building", "title": "Algorithm Supported Induction for Building Theory: How Can We Use Prediction Models to Theorize?", "abstract": "Across many fields of social science, machine learning (ML) algorithms are rapidly advancing research as tools to support traditional hypothesis testing research (e.g., through data reduction and automation of data coding or for improving matching on observable features of a phenomenon or constructing instrumental variables). In this paper, we argue that researchers are yet to recognize the value of ML techniques for theory building from data. This may be in part because of scholars' inherent distaste for predictions without explanations that ML algorithms are known to produce. However, precisely because of this property, we argue that ML techniques can be very useful in theory construction during a key step of inductive theorizing-pattern detection. ML can facilitate algorithm supported induction, yielding conclusions about patterns in data that are likely to be robustly replicable by other analysts and in other samples from the same population. These patterns can then be used as inputs to abductive reasoning for building or developing theories that explain them. We propose that algorithm-supported induction is valuable for researchers interested in using quantitative data to both develop and test theories in a transparent and reproducible manner, and we illustrate our arguments using simulations.", "journal": "ORGANIZATION SCIENCE", "category": "Management", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000686127700010", "keywords": "Malware classification; Huffman encoding; Malware abstraction; Feature construction; Compression; Machine learning", "title": "Malware family classification via efficient Huffman features", "abstract": "As malware evolves and becomes more complex, researchers strive to develop detection and classification schemes that abstract away from the internal intricacies of binary code to represent malware without the need for architectural knowledge or invasive analysis procedures. Such approaches can reduce the complexities of feature generation and simplify the analysis process. In this paper, we present efficient Huffman features (eHf), a novel compression-based approach to feature construction, based on Huffman encoding, where malware features are represented in a compact format, without the need for intrusive reverse-engineering or dynamic analysis processes. We demonstrate the viability of eHf as a solution for classifying malware into their respective families on a large malware corpus of 15 k samples, indicative of the current threat landscape. We evaluate eHf against current compression-based alternatives and show that our method is comparable or superior for classification accuracy, while exhibiting considerably greater runtime efficiency. Finally we demonstrate that eHf is resilient against code reordering obfuscation. (C) 2021 The Authors. Published by Elsevier Ltd.", "journal": "FORENSIC SCIENCE INTERNATIONAL-DIGITAL INVESTIGATION", "category": "Computer Science, Information Systems; Computer Science, Interdisciplinary Applications", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000704110300025", "keywords": "Decoding; Deep learning; Training; Convolutional codes; Bandwidth; Uplink; Precoding; CSI feedback; massive MIMO; deep learning", "title": "Deep Learning Phase Compression for MIMO CSI Feedback by Exploiting FDD Channel Reciprocity", "abstract": "Large scale MIMO FDD systems are often hampered by bandwidth required to feedback downlink CSI. Previous works have made notable progresses in efficient CSI encoding and recovery by taking advantage of FDD uplink/downlink reciprocity between their CSI magnitudes. Such framework separately encodes CSI phase and magnitude. To further enhance feedback efficiency, we propose a new deep learning architecture for phase encoding based on limited CSI feedback and magnitude-aided information. Our contribution features a framework with a modified loss function to enable end-to-end joint optimization of CSI magnitude and phase recovery. Our test results show superior performance in indoor/outdoor scenarios.", "journal": "IEEE WIRELESS COMMUNICATIONS LETTERS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000642441000058", "keywords": "false discovery; knockoffs; deep learning; microbiome; single-cell", "title": "Deep Learning to Predict TBI Outcomes in the Low Resource Setting (vol 67, nyaa488, 2020)", "abstract": "We propose a deep learning-based knockoffs inference framework, DeepLINK, that guarantees the false discovery rate (FDR) control in high-dimensional settings. DeepLINK is applicable to a broad class of covariate distributions described by the possibly nonlinear latent factor models. It consists of two major parts: an autoencoder network for the knockoff variable construction and a multilayer perceptron network for feature selection with the FDR control. The empirical performance of DeepLINK is investigated through extensive simulation studies, where it is shown to achieve FDR control in feature selection with both high selection power and high prediction accuracy. We also apply DeepLINK to three real data applications to demonstrate its practical utility.", "journal": "NEUROSURGERY", "category": "Clinical Neurology; Surgery", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000741486400007", "keywords": "Internet of things; Blockchain; Deep learning; Pharmaceutical cold chain; Cloud storage", "title": "Pharmaceutical Cold Chain Management Based on Blockchain and Deep Learning", "abstract": "Pharmaceutical cold chain is a special branch of the logistics industry, which has strict requirements for warehousing and transportation in the supply chain. In order to improve the credibility of pharmaceutical products and ensure the life safety of the people, it is necessary to realize the credible traceability of the entire life cycle of pharmaceutical products. The cost of pharmaceutical cold chain is high. Warehousing and transportation cost per unit of cold chain products is much higher than those of ordinary supply chain products. Intelligent prediction of cold chain product demand is an important way to reduce warehousing and transportation cost and improve market competitiveness of cold chain enterprises. Firstly, this paper proposes a pharmaceutical cold chain supervision scheme based on blockchain, cloud storage and Internet of things to realize the trusted traceability of the whole life cycle of pharmaceutical products and ensure the product safety. Then, based on the high-quality and large-scale data generated in the proposed cold chain supervision system, a cold chain product demand forecasting scheme based on deep learning is constructed to assist the cold chain inventory management decision-making, so as to reduce the warehousing cost of cold chain products.", "journal": "JOURNAL OF INTERNET TECHNOLOGY", "category": "Computer Science, Information Systems; Telecommunications", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000702731000001", "keywords": "COVID-19; CT image; Segmentation; Transfer learning", "title": "Design and development of resistance heating apparatus-cum-solar drying system for enhancing fish drying rate", "abstract": "Resistance heating is unique to thermic food manufacturing applications, particularly getting popularity across the food sector. Here, a prototype ohmic heating device over a capability of 6.25 kg/hr has been developed to process fish. The developed device was used combined with PID-controlled solar dryer, and the parameters have been optimized by using central composite rotatable design and artificial neural network (ANN). During the processing of hybrid drying (ohmic heating with solar drying), the selected input variables were voltage (160-200 V), salt concentration (0-2%), solar drying temperature (40-72 degrees C), and loading density (109-190 g) to improve the drying rate (drying time, overall temperature, and final moisture content). The designed ohmic heater reduced the moisture in fish muscles from 348.5% (db) to 260.91% at the first stage of drying under optimum conditions (180 V ohmic voltage and 1% salt concentration) in the time interval of 165-300 s for all experiments, followed by final moisture content, which was reduced to 12.66% (db), and the overall drying rate was improved to 2.73 g/min at the second stage of solar drying during optimum conditions (72 degrees C temperature and 0.672 cm(2) load density). The R-2 (determination coefficient 0.96-0.94) values, SSE (sum of square error values 0.15-025) values, and MSE (mean square values 1-8.7) were more significant for RSM than ANN. This developed resistance heating apparatus for enhancing fish drying rate and also reduced post-harvest losses, and it is considered to be a good technology for drying the fish and making a fish powder that can be further used for value-added product. Practical Applications The fish comes readily digestible; on the other hand, extremely spoilable accordingly, and fish cannot be gathered for a longer time below atmosphere circumstance, leading to enormous reap as well as commercial disadvantage. The processing of fish may impede against spoiling before reaching the consumers. Dehydration becomes a more frequent also essential approach to post-processing conservation. Fish dehydration must be promoted by farmers' stage in addition to micro-industry scale by way of convenience to frozen food locker. Hybrid or combined drying is an attraction for its nutritive values retention, uniformity of drying, reducing the microbial load, and improving product storage properties. Therefore, the present study aimed to develop an ohmic device accompanied by solar drying for preservation of Rohu fish muscles under ambient condition.", "journal": "JOURNAL OF FOOD PROCESS ENGINEERING", "category": "Engineering, Chemical; Food Science & Technology", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000749804500001", "keywords": "fractional order; synchronization; competitive neural network; Lyapunov-Krasovskii functional", "title": "Synchronization of Fractional Order Uncertain BAM Competitive Neural Networks", "abstract": "This article examines the drive-response synchronization of a class of fractional order uncertain BAM (Bidirectional Associative Memory) competitive neural networks. By using the differential inclusions theory, and constructing a proper Lyapunov-Krasovskii functional, novel sufficient conditions are obtained to achieve global asymptotic stability of fractional order uncertain BAM competitive neural networks. This novel approach is based on the linear matrix inequality (LMI) technique and the derived conditions are easy to verify via the LMI toolbox. Moreover, numerical examples are presented to show the feasibility and effectiveness of the theoretical results.", "journal": "FRACTAL AND FRACTIONAL", "category": "Mathematics, Interdisciplinary Applications", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000757508700001", "keywords": "relative positioning; ICP; AMR; neural architecture search; artificial neural network", "title": "Neural architecture search for the estimation of relative positioning of the autonomous mobile robot", "abstract": "In the present work, an artificial neural network (ANN) will be developed to estimate the relative rotation and translation of the autonomous mobile robot (AMR). The ANN will work as an iterative closed point, which is commonly used with the singular value decomposition algorithm. This development will provide better resolution for a relative positioning technique that is essential for the AMR localization. The ANN requires a specific architecture, although in the current work a neural architecture search will be adapted to select the best ANN for estimating the relative motion. At the end, these ANNs will be compared with conventional algorithms to check the good performance of adopting an intelligent method for relative positioning estimation.", "journal": "LOGIC JOURNAL OF THE IGPL", "category": "Mathematics, Applied; Mathematics; Logic", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000798485300028", "keywords": "Computer architecture; Convolutional neural networks; Remote sensing; Field programmable gate arrays; Satellites; Earth; Training; Convolutional neural networks (CNNs); deep learning; field-programmable gate array (FPGA); machine learning; remote sensing; target detection", "title": "Hippocampal representations for deep learning on Alzheimer's disease", "abstract": "Deep learning offers a powerful approach for analyzing hippocampal changes in Alzheimer's disease (AD) without relying on handcrafted features. Nevertheless, an input format needs to be selected to pass the image information to the neural network, which has wide ramifications for the analysis, but has not been evaluated yet. We compare five hippocampal representations (and their respective tailored network architectures) that span from raw images to geometric representations like meshes and point clouds. We performed a thorough evaluation for the prediction of AD diagnosis and time-to-dementia prediction with experiments on an independent test dataset. In addition, we evaluated the ease of interpretability for each representation-network pair. Our results show that choosing an appropriate representation of the hippocampus for predicting Alzheimer's disease with deep learning is crucial, since it impacts performance and ease of interpretation.", "journal": "SCIENTIFIC REPORTS", "category": "Multidisciplinary Sciences", "annotated_keywords": ["neural net", "deep learning", "deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000860730700003", "keywords": "Track quality; railway track maintenance; railroad tracks; preventive maintenance; kernel mean weight fuzzy local information C means (KMWFLICM); clustering algorithm; fuzzy weight convolution neural network (FWCNN); metro rail", "title": "Macroeconomic Forecasting Based on Mixed Frequency Vector Autoregression and Neural Network Models", "abstract": "Macroeconomic indicators include gross domestic product (GDP), consumer price index (CPI), and retail price index (RPI). These indicators are important for understanding the macroeconomic situation and controlling the macroeconomic trend, as they provide a macroscopic view of a country or region's economic performance. If macroeconomic indicators can be predicted accurately in advance, the government and relevant macroeconomic control departments can propose more forward-looking and targeted macroeconomic control policies and deploy the necessary control measures. In addition, individuals can make more reasonable decisions on their investments and savings if they know the macroeconomic indexes in advance. From these two aspects, prediction of macroeconomic indexes is of great research significance. In this paper, we propose a prediction model based on chaotic vector autoregression and neural networks for macroeconomic forecasting, and we model and test the prediction with GDP and inflation as the main concerns, and we find that the improvement of GDP forecasting shows an increase of expected inflation rate, indicating the usefulness of using expected GDP data.", "journal": "WIRELESS COMMUNICATIONS & MOBILE COMPUTING", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000741083200001", "keywords": "Forecasting; Load forecasting; Uncertainty; Probabilistic logic; Global Positioning System; Artificial neural networks; Training; Anomalous events; deep Gaussian process (GP) regression; limited data; probabilistic load forecasting; uncertainty; quantification", "title": "MaxwellNet: Physics-driven deep neural network training based on Maxwell's equations", "abstract": "Maxwell's equations govern light propagation and its interaction with matter. Therefore, the solution of Maxwell's equations using computational electromagnetic simulations plays a critical role in understanding light-matter interaction and designing optical elements. Such simulations are often time-consuming, and recent activities have been described to replace or supplement them with trained deep neural networks (DNNs). Such DNNs typically require extensive, computationally demanding simulations using conventional electromagnetic solvers to compose the training dataset. In this paper, we present a novel scheme to train a DNN that solves Maxwell's equations speedily and accurately without relying on other computational electromagnetic solvers. Our approach is to train a DNN using the residual of Maxwell's equations as the physics-driven loss function for a network that finds the electric field given the spatial distribution of the material property. We demonstrate it by training a single network that simultaneously finds multiple solutions of various aspheric micro-lenses. Furthermore, we exploit the speed of this network in a novel inverse design scheme to design a micro-lens that maximizes a desired merit function. We believe that our approach opens up a novel way for light simulation and optical design of photonic devices.", "journal": "APL PHOTONICS", "category": "Optics; Physics, Applied", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000814945200001", "keywords": "Monthly runoff prediction; Nonstationary and skewed runoff time series; Variational mode decomposition; Box-Cox transformation; Elman neural network; Wei River Basin", "title": "A Hybrid Model Integrating Elman Neural Network with Variational Mode Decomposition and Box-Cox Transformation for Monthly Runoff Time Series Prediction", "abstract": "Precise and reliable monthly runoff prediction plays a vital role in the optimal management of water resources, but the nonstationarity and skewness of monthly runoff time series can pose major challenges for developing appropriate prediction models. To address these issues, this paper proposes a novel hybrid prediction model by introducing variational mode decomposition (VMD) and Box-Cox transformation (BC) into the Elman neural network (Elman), named the VMD-BC-Elman model. First, the observed runoff is decomposed into sub-time series using VMD for better frequency resolution. Second, the input datasets are transformed into a normal distribution using Box-Cox, and as a result, skewedness in the data is removed, and the correlation between the input and output variables is enhanced. The proposed VMD-BC preprocessing technology is expected to overcome the problems arising from nonstationary and skewed runoff data. Finally, Elman is used to simulate the respective sub-time series. The proposed model is evaluated using monthly runoff time series at Zhangjiashan, Zhuangtou and Huaxian hydrological stations in the Wei River Basin in China. The model performances are compared with those of single models (SVM, Elman), decomposition-based (VMD-SVM, VMD-Elman et al.) and BC-based models (BC-SVM and BC-Elman) by employing four metrics. The results show that the hybrid models outperform single models, and the VMD-BC-Elman model performs best in all considered hybrid models with an NSE greater than 0.95, R greater than 0.98, NMSE less than 4.7%, and PBIAS less than 0.4% in both the training and testing periods. The study indicates that the VMD-BC-Elman model is a satisfactory data-driven approach to predict nonstationary and skewed monthly runoff time series, representing an effective tool for predicting monthly runoff series.", "journal": "WATER RESOURCES MANAGEMENT", "category": "Engineering, Civil; Water Resources", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000844587300001", "keywords": "LSTM network; sequence to sequence (Seq2Seq) model; autoencoder; solar energy monitoring; sustainable renewable energy; deep learning", "title": "Extending machine learning beyond interatomic potentials for predicting molecular properties", "abstract": "Machine learning (ML) is becoming a method of choice for modelling complex chemical processes and materials. ML provides a surrogate model trained on a reference dataset that can be used to establish a relationship between a molecular structure and its chemical properties. This Review highlights developments in the use of ML to evaluate chemical properties such as partial atomic charges, dipole moments, spin and electron densities, and chemical bonding, as well as to obtain a reduced quantum-mechanical description. We overview several modern neural network architectures, their predictive capabilities, generality and transferability, and illustrate their applicability to various chemical properties. We emphasize that learned molecular representations resemble quantum-mechanical analogues, demonstrating the ability of the models to capture the underlying physics. We also discuss how ML models can describe non-local quantum effects. Finally, we conclude by compiling a list of available ML toolboxes, summarizing the unresolved challenges and presenting an outlook for future development. The observed trends demonstrate that this field is evolving towards physics-based models augmented by ML, which is accompanied by the development of new methods and the rapid growth of user-friendly ML frameworks for chemistry.", "journal": "NATURE REVIEWS CHEMISTRY", "category": "Chemistry, Multidisciplinary", "annotated_keywords": ["neural net", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000902411200030", "keywords": "Training; Object detection; Feature extraction; Remote sensing; Detectors; Proposals; Task analysis; Attention mechanism; few-shot object detection (FSOD); remote sensing images", "title": "MM-RCNN: Toward Few-Shot Object Detection in Remote Sensing Images With Meta Memory", "abstract": "In the area of optical remote sensing image processing, object detection is among the utmost essential and difficult tasks. By virtue of the excellent feature representation abilities of deep convolutional neural networks (DCNNs), the performance of remote sensing object detection has recently increased dramatically. However, DCNN-based methods necessitate sufficient labeled training samples and tend to experience a considerable performance fall when training examples are insufficient. Many of the recently developed few-shot object detection (FSOD) methods attempt to solve the issue using the idea of meta-learning, which intends to extract knowledge that can be generalized across various tasks. Despite its success, the negligence of the knowledge learned in the past and interclass correlations hinder the detection ability of novel classes. In this article, we propose a new meta-memory-based FSOD approach named MM-RCNN. Specifically, MM-RCNN adopts a memory module to store each category's knowledge learned in the training stage and the memory-based external attention (MEA) to aggregate all the categories' information simultaneously. Based on MEA, we design two feature enhancement modules for the region proposal network (RPN) and detection head to boost the performance. Experiments over two remote sensing benchmarks, i.e., DIOR and NWPU VHR10, verify the capability of our method (0.239 and 0.557 average mAP across all settings).", "journal": "IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING", "category": "Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000809496800003", "keywords": "Exosystem; Lyapunovfunction; Outputregulation; Wavelettransform", "title": "Mental practice modulates functional connectivity between the cerebellum and the primary motor cortex", "abstract": "Our brain has the extraordinary capacity to improve motor skills through mental practice. Conceptually, this ability is attributed to internal forward models, which are cerebellar neural networks that can predict the sensory consequences of motor commands. In our study, we employed single and dual-coil transcranial magnetic stimulations to probe the level of corticospinal excitability and cerebellar brain-inhibition, respectively, before and after a mental practice session or a control session. Motor skill (i.e., accuracy and speed) was measured using a sequential finger tapping-task. We found that mental practice enhanced both speed and accuracy. In parallel, the functional connectivity between the cerebellum and the primary motor cortex changed, with less inhibition from the first to the second. These findings reveal the existence of neuroplastic changes within the cerebellum, supporting the involvement of internal models after mental practice.", "journal": "ISCIENCE", "category": "Multidisciplinary Sciences", "annotated_keywords": ["neural net"], "label": "0", "title_label": "0"}
{"id": "WOS:000823249000001", "keywords": "Uncertainty theory; Finite-time stability in measure; Fractional order difference equations", "title": "Finite-time stability in measure for nabla uncertain discrete linear fractional order systems", "abstract": "With the development of mathematical theory, fractional order equation is becoming a potential tool in the context of neural networks. This paper primarily concerns with the stability for systems governed by the linear fractional order uncertain difference equations, which may properly portray neural networks. First, the solutions of these linear difference equations are provided. Secondly, the definition of finite-time stability in measure for the proposed systems is introduced. Furthermore, some sufficient conditions checking for it are achieved by the property of fractional order difference and uncertainty theory. Besides, the relationship between finite-time stability almost surely and in measure is discussed. Finally, some numerical examples are analysed by employing the proposed results.", "journal": "MATHEMATICAL SCIENCES", "category": "Mathematics, Applied; Mathematics", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000808096100029", "keywords": "Dependent task offloading; directed acyclic graphs (DAGs); graph convolutional neural network (GCN); multi-access edge computing (MEC)", "title": "Multitask Offloading Strategy Optimization Based on Directed Acyclic Graphs for Edge Computing", "abstract": "With the advancement of the user application service demands, the IoT system tends to offload the tasks to the edge server for execution. Most of the current studies on edge computation offloading ignore the dependencies between components of the application. The few pieces of research on edge computing offloading which focus on the topology of application are primarily applied in single-user scenarios. Unlike previous work, our work mainly solves dependent task offloading with edge computing in multiuser scenarios, which is more in line with reality. In this article, the dependent task offloading problem is modeled as a Markov decision process (MDP) first. Then, we propose an actor-critic mechanism with two embedding layers for directed acyclic graphs (DAGs)-based multiple dependent tasks computation offloading, namely, ACED, by jointly considering the topology of the application and the channel interference between several users. Finally, the results of simulations also show the priorities of the proposed ACED algorithm.", "journal": "IEEE INTERNET OF THINGS JOURNAL", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000781702900020", "keywords": "DC-AC power converters; electric vehicle; methodical overview; multi-level inverters; resonant DC-link; soft-switching", "title": "An Adaptive Task Migration Scheduling Approach for Edge-Cloud Collaborative Inference", "abstract": "Deep Neural Network (DNN) models have achieved excellent performance in many inference tasks and have been widely used in many intelligent applications. However, DNN models often require a lot of computational resources to complete the inference tasks, which hinders the deployment of such models to resource-constrained edge devices. In order to extend the application scenarios of DNN models, the edge-cloud collaborative inference methods, represented by model partition, have attracted much research attention in recent years. In scenarios that have multiple edge devices deployed, the edge-cloud collaborative inference method requires partial migration of tasks, but traditional scheduling methods only migrate tasks at the task level. In this paper, we propose two task scheduling methods, which can solve the problem of partial migration of tasks in multiedge scenarios. The first scheduling method is based on the optimal cutting of a single DNN. The cutting positions of all the models are the same, regardless of the influence of external factors. This method is suitable for chain and directed acyclic graph(DAG-) type DNNs. The second scheduling method takes external factors such as congestion and queuing delay at the cloud side into consideration, which dynamically selects the cutting position of each DNN to optimize the overall delay and thus is applicable to chain DNN models. The experimental results show that, compared with the baseline method, our proposed scheduling method can reduce the delay by up to 6.48x.", "journal": "WIRELESS COMMUNICATIONS & MOBILE COMPUTING", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000751023900001", "keywords": "bone mineral density; convolutional neural network; opportunistic imaging; osteoporosis; proton density fat fraction; texture analysis; vertebral fracture", "title": "Texture Analysis Using CT and Chemical Shift Encoding-Based Water-Fat MRI Can Improve Differentiation Between Patients With and Without Osteoporotic Vertebral Fractures", "abstract": "Purpose: Osteoporosis is a highly prevalent skeletal disease that frequently entails vertebral fractures. Areal bone mineral density (BMD) derived from dual-energy X-ray absorptiometry (DXA) is the reference standard, but has well-known limitations. Texture analysis can provide surrogate markers of tissue microstructure based on computed tomography (CT) or magnetic resonance imaging (MRI) data of the spine, thus potentially improving fracture risk estimation beyond areal BMD. However, it is largely unknown whether MRI-derived texture analysis can predict volumetric BMD (vBMD), or whether a model incorporating texture analysis based on CT and MRI may be capable of differentiating between patients with and without osteoporotic vertebral fractures.Materials and Methods: Twenty-six patients (15 females, median age: 73 years, 11 patients showing at least one osteoporotic vertebral fracture) who had CT and 3-Tesla chemical shift encoding-based water-fat MRI (CSE-MRI) available were analyzed. In total, 171 vertebral bodies of the thoracolumbar spine were segmented using an automatic convolutional neural network (CNN)-based framework, followed by extraction of integral and trabecular vBMD using CT data. For CSE-MRI, manual segmentation of vertebral bodies and consecutive extraction of the mean proton density fat fraction (PDFF) and T2* was performed. First-order, second-order, and higher-order texture features were derived from texture analysis using CT and CSE-MRI data. Stepwise multivariate linear regression models were computed using integral vBMD and fracture status as dependent variables.Results: Patients with osteoporotic vertebral fractures showed significantly lower integral and trabecular vBMD when compared to patients without fractures (p < 0.001). For the model with integral vBMD as the dependent variable, T2* combined with three PDFF-based texture features explained 40% of the variance (adjusted R-2 [R-a(2) ] = 0.40; p < 0.001). Furthermore, regarding the differentiation between patients with and without osteoporotic vertebral fractures, a model including texture features from CT and CSE-MRI data showed better performance than a model based on integral vBMD and PDFF only (R-a(2) = 0.47 vs. R-a(2) = 0.81; included texture features in the final model: integral vBMD, CT_Short-run_emphasis, CT_Varianceglobal, and PDFF_Variance).Conclusion: Using texture analysis for spine CT and CSE-MRI can facilitate the differentiation between patients with and without osteoporotic vertebral fractures, implicating that future fracture prediction in osteoporosis may be improved.", "journal": "FRONTIERS IN ENDOCRINOLOGY", "category": "Endocrinology & Metabolism", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000813512400001", "keywords": "FT-IR spectroscopy; Streptococcus pneumoniae; machine learning; pneumococcus; serotyping", "title": "Validation of Fourier Transform Infrared Spectroscopy for Serotyping of Streptococcus pneumoniae", "abstract": "Fourier transform infrared (FT-IR) spectroscopy (IR Biotyper; Bruker) allows highly discriminatory fingerprinting of closely related bacterial strains. In this study, FT-IR spectroscopy-based capsular typing of Streptococcus pneumoniae was validated as a rapid, cost-effective, and medium-throughput alternative to the classical phenotypic techniques. A training set of 233 strains was defined, comprising 34 different serotypes and including all 24 vaccine types (VTs) and 10 non-vaccine types (NVTs). The acquired spectra were used to (i) create a dendrogram where strains clustered together according to their serotypes and (ii) train an artificial neural network (ANN) model to predict unknown pneumococcal serotypes. During validation using 153 additional strains, we reached 98.0% accuracy for determining serotypes represented in the training set. Next, the performance of the IR Biotyper was assessed using 124 strains representing 59 non-training set serotypes. In this setting, 42 of 59 serotypes (71.1%) could be accurately categorized as being non-training set serotypes. Furthermore, it was observed that comparability of spectra was affected by the source of the Columbia medium used to grow the pneumococci and that this complicated the robustness and standardization potential of FT-IR spectroscopy. A rigorous laboratory workflow in combination with specific ANN models that account for environmental noise parameters can be applied to overcome this issue in the near future. The IR Biotyper has the potential to be used as a fast, cost-effective, and accurate phenotypic serotyping tool for S. pneumoniae.", "journal": "JOURNAL OF CLINICAL MICROBIOLOGY", "category": "Microbiology", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000848352400001", "keywords": "indicator diagram; deep learning; convolutional neural network; AlexNet; batch normalization", "title": "Indicator diagram analysis based on deep learning", "abstract": "At present, more than 90% of China's oil production equipment comprises rod pump production systems. Indicator diagram analysis of the pumping unit is not only an effective method for monitoring the current working condition of a rod pump production system but also the main way to prevent, detect, and rectify various faults in the oil production process. However, the identification of the pumping unit indicator diagram mainly involves manual effort, and the identification accuracy depends on the experience of the monitoring personnel. Automatic and accurate identification and classification of the pumping unit indicator diagram using new computer technology has long been the research focus of studies for monitoring the pumping unit working condition. In this paper, the indicator diagram is briefly introduced, and the AlexNet model is presented to distinguish the indicator diagram of abnormal wells. The influence of the step size, convolution kernel size, and batch normalization (BN) layer on the accuracy of the model is analyzed. Finally, the AlexNet model is improved. The improved model reduces the calculation cost and parameters, accelerates the convergence, and improves the accuracy and speed of the calculation. In the experimental analysis of abnormal well diagnosis, the data are preprocessed via data deduplication, binary filling, random line distortion, random scaling and stretching, and random vertical horizontal displacement. In addition, the image is expanded by transforming several well indicator diagrams. Finally, data sets of 10 types of indicator diagrams are created for better adaptability and application in the analysis and classification of indicator diagrams, and the ideal application effect is achieved in actual working conditions. In summary, this technology not only improves the recognition accuracy but also saves manpower. Thus, it has good application prospects in the field of oil production.", "journal": "FRONTIERS IN EARTH SCIENCE", "category": "Geosciences, Multidisciplinary", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000729816600014", "keywords": "HAADF-STEM; segmentation; symmetry descriptors; unsupervised learning", "title": "Segmentation of Static and Dynamic Atomic-Resolution Microscopy Data Sets with Unsupervised Machine Learning Using Local Symmetry Descriptors", "abstract": "We present an unsupervised machine learning approach for segmentation of static and dynamic atomic-resolution microscopy data sets in the form of images and video sequences. In our approach, we first extract local features via symmetry operations. Subsequent dimension reduction and clustering analysis are performed in feature space to assign pattern labels to each pixel. Furthermore, we propose the stride and upsampling scheme as well as separability analysis to speed up the segmentation process of image sequences. We apply our approach to static atomic-resolution scanning transmission electron microscopy images and video sequences. Our code is released as a python module that can be used as a standalone program or as a plugin to other microscopy packages.", "journal": "MICROSCOPY AND MICROANALYSIS", "category": "Materials Science, Multidisciplinary; Microscopy", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000850279004330", "keywords": "ginseng origin; laser-induced breakdown spectroscopy; machine learning algorithm; random forest", "title": "DEEP IMMUNE PHENOTYPING OF SLE AND RA BY MACHINE LEARNING", "abstract": "In this paper, the ginsengs from five ginseng origins are discriminated by using laser- induced breakdown spectrum (LIBS) combined with random forest- support vector machine (RF- SVM) and random forest- multilayer perception (RF- MLP) machine learning algorithms. The raw LIBS of ginseng is pretreated by using the wavelet threshold method, denoise the background information and normalazation to improve the signal- to- background ratio and the experimental reliability. The RF algorithm is used to select 10 characteristic spectral lines as the input vectors of the MLP and the SVM models to identify the ginseng orgin. The experimental results show that the discrimination accuracy rates of RF- MLP and RF- SVM models are 99.75% and 99.5%, respectively. The disrimination accuracy of ginseng origin used in the RF-MLP machine algorithm model is slightly higher than that of the RF- SVM model, and then calculated the speed of the RF- MLP model is faster than the RF- SVM model. The results show that LIBS combined with machine algorithms are both promising rapid discrimination methods for ginseng origin.", "journal": "ANNALS OF THE RHEUMATIC DISEASES", "category": "Rheumatology", "annotated_keywords": ["machine learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000746405400004", "keywords": "Spontaneous miscarriage; In vitro fertilization and embryo transfer; Biomarker; Lipidomics; Machine learning", "title": "Candidate Circulating Biomarkers of Spontaneous Miscarriage After IVF-ET Identified via Coupling Machine Learning and Serum Lipidomics Profiling", "abstract": "Spontaneous miscarriage is a common pregnancy complication. Multiple etiologies have been proposed such as genetic aberrations, endocrinology disorder, and immunologic derangement; however, the relevance of circulating lipidomes to the specific condition remains unclear. In the present study, lipidomics profiling was examined on serum of women with spontaneous miscarriage after in vitro fertilization and embryo transfer (IVF-ET). Screening and analysis of differential lipid levels were conducted using a machine learning approach to verify the stability and validity of potential serum biomarkers. Seven lipid species presented significant differences between the abortion and term birth patients, including three types of sphingomyelins (SMs), two types of diglycerides (DGs), one phosphatidylcholine (PC), and one lysophosphatidylethanolamine (LPE). All the SMs presented with a fold change of > 1, while both the PC and LPE had a fold change of < 1. The DG containing two saturated fatty acyl chains was decreased, but that containing two unsaturated fatty acyl chains was increased in the miscarriage group compared to the control group. This study reveals the relevance of lipid profiles to spontaneous abortion after IVF-ET, providing potential biomarkers and therapeutic targets for the specific clinical scenario.", "journal": "REPRODUCTIVE SCIENCES", "category": "Obstetrics & Gynecology; Reproductive Biology", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000767079800001", "keywords": "statistical moments; cloudiness; Baltic Sea", "title": "Application of Shape Moments for Cloudiness Assessment in Marine Environmental Research", "abstract": "The search for clouds in satellite images is a challenging subject which still attracts a lot of attention due to the amount and quality of data, which is growing at a tremendous pace, the development of satellite techniques and methods, inexpensive equipment, and automation of satellite imaging processes. This paper presents a new approach to the assessment of cloudiness based on the use of the theory of moments with invariants. The values of moments with invariants, determined on the basis of the available cloudiness maps, create a new, valuable set of data, which are the geometrical parameters of the scene representing the cloud cover. In further research, the obtained data sets will be used in machine learning methods, deep machine learning methods, etc. The method is used for different conditions, including different angular positions of the Sun and time periods. The effectiveness of the method is checked on the basis of comparing the entropy results of the input maps after subtracting clouds masked by various methods. The obtained results additionally indicate the potential of the moments method as a support for the existing methods of estimating cloudiness over the sea surface.", "journal": "REMOTE SENSING", "category": "Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000767321100003", "keywords": "Intrusion detection; Fuzzy optimum-path forest; IoT; Machine learning", "title": "In vivo simultaneous nonlinear absorption Raman and fluorescence (SNARF) imaging of mouse brain cortical structures", "abstract": "Label-free multiphoton microscopy is a powerful platform for biomedical imaging. Recent advancements have demonstrated the capabilities of transient absorption microscopy (TAM) for label-free quantification of hemoglobin and stimulated Raman scattering (SRS) microscopy for pathological assessment of label-free virtual histochemical staining. We propose the combination of TAM and SRS with two-photon excited fluorescence (TPEF) to characterize, quantify, and compare hemodynamics, vessel structure, cell density, and cell identity in vivo between age groups. In this study, we construct a simultaneous nonlinear absorption, Raman, and fluorescence (SNARF) microscope with the highest reported in vivo imaging depth for SRS and TAM at 250-280 mu m to enable these multimodal measurements. Using machine learning, we predict capillary-lining cell identities with 90% accuracy based on nuclear morphology and capillary relationship. The microscope and methodology outlined herein provides an exciting route to study several research topics, including neurovascular coupling, blood-brain barrier, and neurodegenerative diseases. In this study a microscope is constructed that carries out simultaneous nonlinear absorption, Raman, and fluorescence (SNARF). Machine learning is then used to predict capillary-lining cell identities with 90% accuracy based on nuclear morphology and capillary relationship, which in combination with the developed microscope, can provide a means to study several fields such as neurovascular coupling.", "journal": "COMMUNICATIONS BIOLOGY", "category": "Biology; Multidisciplinary Sciences", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000788033000002", "keywords": "Deep learning; Computer architecture; Neurons; Time complexity; Intelligent systems; Feature extraction; Computational modeling; Artificial intelligence; Goldbach&#x0027; s function; Deep learning; Out-ofscope inference", "title": "Modeling antiphase boundary energies of Ni3Al-based alloys using automated density functional theory and machine learning", "abstract": "Antiphase boundaries (APBs) are planar defects that play a critical role in strengthening Ni-based superalloys, and their sensitivity to alloy composition offers a flexible tuning parameter for alloy design. Here, we report a computational workflow to enable the development of sufficient data to train machine-learning (ML) models to automate the study of the effect of composition on the (111) APB energy in Ni3Al-based alloys. We employ ML to leverage this wealth of data and identify several physical properties that are used to build predictive models for the APB energy that achieve a cross-validation error of 0.033 J m(-2). We demonstrate the transferability of these models by predicting APB energies in commercial superalloys. Moreover, our use of physically motivated features such as the ordering energy and stoichiometry-based features opens the way to using existing materials properties databases to guide superalloy design strategies to maximize the APB energy.", "journal": "NPJ COMPUTATIONAL MATERIALS", "category": "Chemistry, Physical; Materials Science, Multidisciplinary", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000795398500001", "keywords": "Engines; Calibration; Stochastic processes; Optimization; Noise measurement; Combustion; Uncertainty; Constrained multiobjective optimization; engine dyno test; stochastic optimization; surrogate model", "title": "Experimental Case Study of Stochastic Surrogate-Assisted Engine Calibration", "abstract": "Reducing experimental calibration cost to achieve optimal system performance is challenging, especially for highly nonlinear engine systems. With the advancement and widespread adoption of machine learning methods for control applications, it is now possible to use an intelligent black-box model to efficiently optimize nonlinear control systems without the detailed knowledge of system dynamics. The surrogate-assisted optimization approach has been recently proven to work effectively to reduce the overall experimental budget (cost). However, its application to calibrate and optimize a practical control system experimentally is a challenge due to stochastic system and measurement noises. This article experimentally calibrates and optimizes the control parameters of a 6.7L Ford diesel engine using the stochastic surrogate-assisted optimization automatically based on a stochastic Kriging model, which is the first step toward automating the engine calibration process. Two engine control variables, namely, exhaust gas recirculation valve and variable geometry turbocharger vane positions, are calibrated to obtain a tradeoff between engine efficiency, in terms of brake-specific fuel consumption, and $\\text{NO}_{x}$ emissions, utilizing both the deterministic and stochastic surrogate-assisted learning algorithms. Experimental results show a much better representation of the output response surfaces with the stochastic surrogate-assisted optimization than that from the deterministic approach. Note that the stochastic approach helps direct the search toward the global optimal region without wasting the evaluation budget in exploring the local optima, and the deterministic approach cannot effectively handle stochastic system and measurement noises, resulting in overfitting the data. Therefore, to solve a practical optimization problem with noises, stochastic surrogate-assisted optimization is recommended.", "journal": "IEEE-ASME TRANSACTIONS ON MECHATRONICS", "category": "Automation & Control Systems; Engineering, Manufacturing; Engineering, Electrical & Electronic; Engineering, Mechanical", "annotated_keywords": ["machine learning", "learning algorithm"], "label": "1", "title_label": "0"}
{"id": "WOS:000749864000001", "keywords": "data federation; structural biology; data repositories; reproducible research; nuclear magnetic resonance", "title": "Merging NMR Data and Computation Facilitates Data-Centered Research", "abstract": "The Biological Magnetic Resonance Data Bank (BMRB) has served the NMR structural biology community for 40 years, and has been instrumental in the development of many widely-used tools. It fosters the reuse of data resources in structural biology by embodying the FAIR data principles (Findable, Accessible, Inter-operable, and Re-usable). NMRbox is less than a decade old, but complements BMRB by providing NMR software and high-performance computing resources, facilitating the reuse of software resources. BMRB and NMRbox both facilitate reproducible research. NMRbox also fosters the development and deployment of complex meta-software. Combining BMRB and NMRbox helps speed and simplify workflows that utilize BMRB, and enables facile federation of BMRB with other data repositories. Utilization of BMRB and NMRbox in tandem will enable additional advances, such as machine learning, that are poised to become increasingly powerful.", "journal": "FRONTIERS IN MOLECULAR BIOSCIENCES", "category": "Biochemistry & Molecular Biology", "annotated_keywords": ["machine learning"], "label": "0", "title_label": "0"}
{"id": "WOS:000753401000001", "keywords": "Asperger's disorder; pervasive developmental disorder; fractal free; neurodevelopmental; Pearson correlation; machine learning", "title": "Non-Oscillatory Connectivity Approach for Classification of Autism Spectrum Disorder Subtypes Using Resting-State fMRI", "abstract": "Resting-state functional magnetic resonance imaging (rs-fMRI) is an efficient tool to measure brain connectivity and it can reveal patterns that distinguish autism spectrum disorder (ASD) from normal controls (NC). It is established that the fractal nature of neuroimaging signals will affect the estimation of brain's functional connectivity. Therefore, the ordinary correlation of rs-fMRI may not provide the original neuronal activity of the brain. In this work, the non-oscillatory brain connectivity method is proposed to distinguish subtypes of ASD from NC. The three subtypes of ASD namely autistic disorder (ATD), Asperger's disorder (APD), and Pervasive developmental disorder-not other specified (PDD) are classified from NC by extracting the non-oscillatory connectivity from the BOLD rs-fMRI signal. A number of significant connections are extracted by utilizing the p-value analysis and these significant connections are fed to machine learning (ML) classifiers for classification of ASD subtypes against normal control. The performance for binary classification is recorded at accuracy of 98.6%, 97.2%, 97.2%, respectively, for ATD vs. NC, APD vs. NC and PDD vs. NC. Whereas, for multiclass (ATD, APD, PDD and NC), the best accuracy is 88.9%. Both binary and multiclass classification outperformed the conventional Pearson correlation-based connectivity and benchmark approaches in terms of accuracy, sensitivity, specificity. This work demonstrates the great potential of non-oscillatory connectivity approaches, not only for autism diagnosis but also for other neurological disorders.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000761216100001", "keywords": "Resource management; Autonomous aerial vehicles; Communication networks; Reinforcement learning; Information processing; Machine learning algorithms; Heuristic algorithms; Federated learning; location deployment; reinforcement learning; resource allocation; unmanned aerial vehicle networks", "title": "A Multi-Agent Collaborative Environment Learning Method for UAV Deployment and Resource Allocation", "abstract": "The dynamic position deployment and resource allocation of the unmanned aerial vehicle (UAV) communication networks has great significance in terms of interference management, coverage enhancement, and capacity improvement. Since the transmission power and energy resources of the UAVs are limited and the actual communication environment is complex and time-varying, it is challenging for the multiple UAVs to dynamically make decisions to ensure the communication performance of the system. Meanwhile, the centralized architecture may generate a certain degree of communication delay and affect communication efficiency. Facing this challenge, a resource allocation algorithm for the UAV networks based on multi-agent collaborative environment learning is proposed. This method is based on a distributed architecture. Each UAV is modeled as an independent agent, which improves the utility of the UAV networks through the dynamic selection decisions of its deployment position, transmission power, and occupied sub-channels. Each UAV learns the mapping of the network information to the position deployment and resource selection decisions based on the reinforcement learning algorithm according to partial of the state information it can observe. For the overall network, a multi-agent reinforcement learning method based on federated learning is designed on the purpose of realizing information interaction and combined dispatching of the UAVs. In the multi-agent system, the framework of federated learning is introduced to realize the sharing of non-privacy data among the UAVs. Simulation results indicate that the proposed method can effectively improve the network utility compared with the multi-agent deep reinforcement learning algorithm without information interaction.", "journal": "IEEE TRANSACTIONS ON SIGNAL AND INFORMATION PROCESSING OVER NETWORKS", "category": "Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": ["reinforcement learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000752036800001", "keywords": "Counterfactual analysis; debiased machine learning; doubly; locally robust score; C14; C21", "title": "Discovery of 16 New Members of the Solar Neighborhood Using Proper Motions from CatWISE2020", "abstract": "In an effort to identify nearby and unusual cold objects in the solar neighborhood, we searched for previously unidentified moving objects using CatWISE2020 proper motion data combined with machine learning methods. We paired the motion candidates with their counterparts in 2MASS, UHS, and VHS. Then we searched for white dwarf, brown dwarf, and subdwarf outliers on the resulting color-color diagrams. This resulted in the discovery of 16 new dwarfs, including 2 nearby M dwarfs (<30 pc), a possible young L dwarf, a high-motion early-T dwarf, and 3 later-T dwarfs. This research represents a step forward in completing the census of the Sun's neighbors.", "journal": "ASTRONOMICAL JOURNAL", "category": "Astronomy & Astrophysics", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000806722000013", "keywords": "Semi-supervised learning; SAR target recognition; threshold filtering; out-of-class data", "title": "Threshold Filtering Semi-Supervised Learning Method for SAR Target Recognition", "abstract": "The semi-supervised deep learning technology driven by a small part of labeled data and a large amount of unlabeled data has achieved excellent performance in the field of image processing. However, the existing semisupervised learning techniques are all carried out under the assumption that the labeled data and the unlabeled data are in the same distribution, and its performance is mainly due to the two being in the same distribution state. When there is out-of-class data in unlabeled data, its performance will be affected. In practical applications, it is difficult to ensure that unlabeled data does not contain out-of-category data, especially in the field of Synthetic Aperture Radar (SAR) image recognition. In order to solve the problem that the unlabeled data contains out-of-class data which affects the performance of the model, this paper proposes a semi-supervised learning method of threshold filtering. In the training process, through the two selections of data by the model, unlabeled data outside the category is filtered out to optimize the performance of the model. Experiments were conducted on the Moving and Stationary Target Acquisition and Recognition (MSTAR) dataset, and compared with existing several state-of-the-art semi-supervised classification approaches, the superiority of our method was confirmed, especially when the unlabeled data contained a large amount of out-of-category data.", "journal": "CMC-COMPUTERS MATERIALS & CONTINUA", "category": "Computer Science, Information Systems; Materials Science, Multidisciplinary", "annotated_keywords": ["deep learning", "supervised learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000831815600001", "keywords": "lithium-ion battery; efficiency degradation; reusable battery; degradation diagnosis; battery aggregation; Kalman filter", "title": "Kalman-Filter-Based Learning of Characteristic Profiles of Lithium-Ion Batteries", "abstract": "The main analyzed aspect of lithium-ion battery (LIB) degradation so far has been capacity fading. On the other hand, interest in efficiency degradation has also increased in recent years. Battery aggregation, which is expected to absorb the surplus of variable renewable energies such as photovoltaic energy, is affected by efficiency degradation in terms of the decreases in the economic gain and renewable energy use. Reusable LIBs could be used as aggregation components in the future; naturally, the variety of charge-discharge efficiencies might be more complex. To improve the operation efficiency of aggregation, including that obtained using reusable LIBs, we propose the Kalman-filter-based quasi-unsupervised learning of the characteristic profiles of LIBs. This method shows good accuracy in the estimation of charge-discharge energy. It should be emphasized that there are no reports of charge-discharge energy estimation using the Kalman filter. In addition, this study shows that the incorrect open-circuit voltage function for the state of charge, which is assumed in the case of a reused battery, could be applied as the reference for the Kalman filter for LIB state estimation. In summary, it is expected that this diagnosis method could contribute to the economic and renewable energy usage improvement of battery aggregation.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": ["supervised learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000777556900012", "keywords": "Historical network research; Natural language processing; Portuguese empire; Digital humanities; Colonial studies; Correspondence networks; Early modern letters", "title": "Networks from archives: Reconstructing networks of official correspondence in the early modern Portuguese empire", "abstract": "Historical archives provide invaluable insights into societies of the past, including social networks. However, the required amount of traditional archival work makes historical network studies usually small-scaled. We consider the problem of processing a large corpus of unstructured textual information to extract network data. The corpus consists of almost 170,000 documents of administrative correspondence of the Portuguese Empire, from 1610 to 1833, catalogued in the Portuguese Overseas Archives of Lisbon. Our contribution is twofold: the method and the result. Firstly, grounded in the review of manual, semi-manual and automatic methods of network data extraction from natural language corpora, we propose and demonstrate an approach using modern natural language processing algorithms. This approach tries to mimic traditional archivist's coding practices and is applicable to large corpora of texts, for which manual coding is infeasible because of scale. We believe our approach is generic and adaptable to other substantive contexts, languages, and types of historical archives. Secondly, the dataset created is rich in additional information such as occupation, administrative affiliation, and geographical location of senders and recipients. We provide a preliminary network analysis suggesting that the dataset is an attractive material for historians and social network researchers for addressing research questions about the political and social evolution of the early modern Portuguese Empire, spanning the reign of seven Portuguese monarchs.", "journal": "SOCIAL NETWORKS", "category": "Anthropology; Sociology", "annotated_keywords": ["natural language processing"], "label": "1", "title_label": "1"}
{"id": "WOS:000721587100203", "keywords": "Human-robot coexisting; Part-behavior assembly and; or graph; Behavior prediction; Self-attention; Adaptive decision making; Reinforcement learning", "title": "Associations between deep learning segmented macular optical coherence tomography cell layer thicknesses and primary open-angle glaucoma outcomes in the PROGRESSA study", "abstract": "In today's prevailing manufacturing paradigm of mass personalization, neither human operators nor robots alone can perform all assembly tasks efficiently. To overcome it, human-robot collaborative assembly shows its great potentials to ensure the flexibility of human operations with high reliability of robot assistance. However, it is often challenging to achieve harmonious coexistence between humans and robots to complete the tasks safely and efficiently. In this regard, this research provides a detailed description of the human-robot coexisting environment and further introduces key issues in collaborative assembly. A part-behavior assembly and/or graph based on process requirements is proposed to represent the assembly task of complex products. Moreover, the human behavior prediction network based on self-attention can achieve higher accuracy. Combined with the robustness of Soft Actor-Critic (SAC), the collaborative system improves the self-decision ability of the robot in the dynamic scene. Finally, the effectiveness of the method is verified through experimental analysis. The results indicate that the accuracy of the proposed behavior recognition based on self-attention method is 91%. At the same time, it is proved that the reinforcement learning method is theoretically feasible to provide adaptive decision-making for robots in human-machine collaboration. In addition, the convergence speed of the reward function proves the feasibility of SAC for adaptive decision-making in a human-robot collaborative environment.", "journal": "CLINICAL AND EXPERIMENTAL OPHTHALMOLOGY", "category": "Ophthalmology", "annotated_keywords": ["deep learning", "reinforcement learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000848058900004", "keywords": "Multi-flexible beam coupling system; Active vibration control; RL algorithm; Model identification; MATD3", "title": "Optimizing Multitask Assignment of Internet of Things Devices by Reinforcement Learning in Mobile Crowdsensing Scenes", "abstract": "The objective is to optimize the multitask assignment (MTA) in mobile crowdsensing (MCS) scenarios. From the perspective of reinforcement learning (RL), an Internet of Things (IoT) devices-oriented MTA model is established using MCS, IoT technology, and other related theories. Then, the data collected by the University of Cambridge and the University of St. Andrews are chosen to verify the three MTA algorithms on IoT devices. They are multistage online task assignment (MOTA), average makespan-sensitive online task assignment (AOTA), and water filling (WF). Experiments are designed by considering different algorithms' MTA time consumption and accuracy in simple and complex task scenarios. The research results manifest that with a constant load or task quantity, the MOTA algorithm takes the shortest time to assign tasks. In simple task scenarios, MOTA is compared with the WF. The MOTA algorithm's total moving distance is relatively short, and the task completion degree is the highest. AOTA algorithm lends best to complex tasks, with the highest MTA accuracy and the shortest time consumption. Therefore, the research on IoT devices' MTA optimization based on RL in the MCS scenario provides a certain theoretical basis for subsequent MTA studies.", "journal": "SECURITY AND COMMUNICATION NETWORKS", "category": "Computer Science, Information Systems; Telecommunications", "annotated_keywords": ["reinforcement learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000878165100001", "keywords": "Feature extraction; Target recognition; Radar polarimetry; Synthetic aperture radar; Training; Adaptation models; Image recognition; Heterogeneous Synthetic aperture radar (SAR) image; SAR; target recognition; unsupervised domain adaptation", "title": "Pixel-Level and Feature-Level Domain Adaptation for Heterogeneous SAR Target Recognition", "abstract": "The performance of synthetic aperture radar (SAR) target recognition has been substantially enhanced by deep learning technology. It is still difficult to get strong recognition performance due to the distribution differences across heterogeneous SAR images. To enhance target recognition performance in heterogeneous SAR situations, a pixel-level and feature-level domain adaptation (PFDA) approach is proposed in this letter to deal with this problem. The pixel-level translation module in the first step generates images with high visual similarity with other distributed images from one distributed image. The second step involves introducing feature alignment into the network to lower the probability distribution divergence of heterologous SAR images in the feature space and enhance recognition performance. We evaluated our method on the synthetic and measured paired labeled experiment (SAMPLE) dataset and a self-built airplane dataset to ensure its effectiveness. Compared to existing domain adaption approaches, experimental results reveal that our strategy greatly improves recognition performance and model stability for heterogeneous SAR target recognition.", "journal": "IEEE GEOSCIENCE AND REMOTE SENSING LETTERS", "category": "Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000814828000012", "keywords": "Training; Servers; Data models; Arithmetic; Costs; Convergence; Computational modeling; Distributed deep learning; federated learning; parallel optimization; robust aggregation; outlier pruning", "title": "DESSO-DB: A web database for sequence and shape motif analyses and identification", "abstract": "Cis-regulatory motif (motif for short) identification and analyses are essential steps in detecting gene regulatory mechanisms. Deep learning (DL) models have shown substantial advances in motif prediction. In parallel, intuitive and integrative web databases are needed to make effective use of DL models and ensure easy access to the identified motifs. Here, we present DESSO-DB, a web database developed to allow efficient access to the identified motifs and diverse motif analyses. DESSO-DB provides motif prediction results and visualizations of 690 ENCODE human Chromatin Immunoprecipitation sequencing (ChIP-seq) data (including 161 transcription factors (TFs) in 91 cell lines) and 1,677 human ChIP-seq data (including 547 TFs in 359 cell lines) from Cistrome DB using DESSO, which is an in-house developed DL tool for motif prediction. It also provides online motif finding and scanning functions for new ChIP-seq/ ATAC-seq datasets and downloadable motif results of the above 690 DECODE datasets, 126 cancer ChIPseq, 55 RNA Crosslinking-Immunoprecipitation and high-throughput sequencing (CLIP-seq) data. DESSODB is deployed on the Google Cloud Platform, providing stabilized and efficient resources freely to the public. (C) 2022 The Authors. Published by Elsevier B.V. on behalf of Research Network of Computational and Structural Biotechnology.", "journal": "COMPUTATIONAL AND STRUCTURAL BIOTECHNOLOGY JOURNAL", "category": "Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000861684500001", "keywords": "phase change memories; resistive memories; neural networks; SPICE simulation; memristor", "title": "Editorial: Emerging non-volatile memories and beyond: From fundamental physics to applications", "abstract": "Physics-informed neural network (PINN) models are developed in this work for solving highly anisotropic diffusion equations. Compared to traditional numerical discretization schemes such as the finite volume method and finite element method, PINN models are meshless and, therefore, have the advantage of imposing no constraint on the orientations of the diffusion tensors or the grid orthogonality conditions. To impose solution positivity, we tested PINN models with positivity-preserving activation functions for the last layer and found that the accuracy of the corresponding PINN solutions is quite poor compared to the vanilla PINN model. Therefore, to improve the monotonicity properties of PINN models, we propose a new loss function that incorporates additional terms which penalize negative solutions, in addition to the usual partial differential equation (PDE) residuals and boundary mismatch. Various numerical experiments show that the PINN models can accurately capture the tensorial effect of the diffusion tensor, and the PINN model utilizing the new loss function can reduce the degree of violations of monotonicity and improve the accuracy of solutions compared to the vanilla PINN model, while the computational expenses remain comparable. Moreover, we further developed PINN models that are composed of multiple neural networks to deal with discontinuous diffusion tensors. Pressure and flux continuity conditions on the discontinuity line are used to stitch the multiple networks into a single model by adding another loss term in the loss function. The resulting PINN models were shown to successfully solve the diffusion equation when the principal directions of the diffusion tensor change abruptly across the discontinuity line. The results demonstrate that the PINN models represent an attractive option for solving difficult anisotropic diffusion problems compared to traditional numerical discretization methods.", "journal": "FRONTIERS IN PHYSICS", "category": "Physics, Multidisciplinary", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000857305400001", "keywords": "Automated machine learning; miRNA target interactions; AI classifier model; GPCR; miRTarBase; paralogs; post transcriptional regulation; protein family", "title": "Inferring microRNA regulation: A proteome perspective", "abstract": "Post-transcriptional regulation in multicellular organisms is mediated by microRNAs. However, the principles that determine if a gene is regulated by miRNAs are poorly understood. Previous works focused mostly on miRNA seed matches and other features of the 3 & PRIME;-UTR of transcripts. These common approaches rely on knowledge of the miRNA families, and computational approaches still yield poor, inconsistent results, with many false positives. In this work, we present a different paradigm for predicting miRNA-regulated genes based on the encoded proteins. In a novel, automated machine learning framework, we use sequence as well as diverse functional annotations to train models on multiple organisms using experimentally validated data. We present insights from tens of millions of features extracted and ranked from different modalities. We show high predictive performance per organism and in generalization across species. We provide a list of novel predictions including Danio rerio (zebrafish) and Arabidopsis thaliana (mouse-ear cress). We compare genomic models and observe that our protein model outperforms, whereas a unified model improves on both. While most membranous and disease related proteins are regulated by miRNAs, the G-protein coupled receptor (GPCR) family is an exception, being mostly unregulated by miRNAs. We further show that the evolutionary conservation among paralogs does not imply any coherence in miRNA regulation. We conclude that duplicated paralogous genes that often changed their function, also diverse in their tendency to be miRNA regulated. We conclude that protein function is informative across species in predicting post-transcriptional miRNA regulation in living cells.", "journal": "FRONTIERS IN MOLECULAR BIOSCIENCES", "category": "Biochemistry & Molecular Biology", "annotated_keywords": ["machine learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000852941900006", "keywords": "MFCC; LPC; Pitch; Indian speech; Emotion recognition; Emotion classification; Catboost", "title": "ERIL: An Algorithm for Emotion Recognition from Indian Languages Using Machine Learning", "abstract": "It is critical for a computer to understand the speaker's mood during a human-machine conversation. Until now, we've only used neutral phrases or utterances to train robots. A person's mood affects their performance. Machines have a hard time deciphering human mood from voice because humans can make fourteen distinct sounds in a second. For a machine to comprehend human behavior, it must first comprehend the human ear's acoustic skills. Linear Prediction Coefficients (LPC) and Mel Frequency Cepstral Coefficients (MFCC) can simulate the human auditory system. Emotion Recognition from Indian Languages (ERIL) extracts emotions like fear, anger, surprise, sadness, happiness, and neutral. ERIL first pre-processes the voice signal, extracts selective MFCC, LPC, pitch, and voice quality features, then classifies the speech using Catboost. We tested ERIL on different benchmark classifiers to choose Catboost. ERIL is a multilingual emotion classifier, it is independent of any language. We checked it on Hindi, Gujarati, Marathi, Punjabi, Bangla, Tamil, Oriya, Kannada, Assamese, and Telugu. We recorded a speech dataset of various emotions in these languages. The accuracy of distinct emotions is 95.05 percent on average. The languages have a combined average of 95.05082 percent.", "journal": "WIRELESS PERSONAL COMMUNICATIONS", "category": "Telecommunications", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000854622600001", "keywords": "flash flooding; remote sensing data; machine learning algorithms; Tarim city; Yemen", "title": "High-throughput parallelized testing of membrane electrode assemblies for CO2 reduction", "abstract": "High-throughput characterization of electrochemical reactions can accelerate discovery and optimization cycles, and provide the data required for further acceleration via machine-learning guided experiment planning. There are a range of high-throughput methods available for catalyst discovery. However, the development and testing of electrochemical systems - integrated electrocatalysts, membranes, and electrodes - currently relies on serial, labor-intensive lab processes. Membrane electrode assembly (MEA) cells have shown particular promise in carbon dioxide (CO2) reduction, providing commercially viable reaction rates. Experimental testing of MEAs is slow, requiring a serial assembly process that can result in electrode compression levels that are non-uniform over the cell area and challenging to reproduce. Here we demonstrate a new MEA testing system that offers an accelerated, parallelized assembly process and enables high-throughput electrochemical system testing. The approach accelerates electrochemical system testing, controls compression and improves repeatability and reliability. We benchmark our system with CO2 reduction to ethylene, running 10 MEA experiments in parallel, demonstrating an acceleration factor up to 12x over conventional approaches, and achieving a cell-to-cell gas selectivity deviation of +/- 2.5%.", "journal": "CATALYSIS SCIENCE & TECHNOLOGY", "category": "Chemistry, Physical", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000869785600004", "keywords": "Pin fin heat sink; Lattice Boltzmann method; Phase change materials; Thermal management; Charging time; Heat dissipation power", "title": "Research on thermal performance and optimization design of phase change pin fin heat sink based on Lattice Boltzmann method", "abstract": "This paper reports a numerical research of the phase change materials (PCMs) based pin fin heat sinks for thermal management of battery and electronic device. The 58 semi-refined paraffin and several paraffin/CNTs com-posites (CNT mass fraction of 5 %, 8 %, 10 %) were adopted as PCMs filled in the heat sink. Lattice Boltzmann method (LBM) was used to numerically study the influence of fin geometry (square fin, circular fin, cross fin), fin number (one to five), fin arrangement (array arrangement, cross arrangement) and PCMs on the working per-formance (charging time and heat dissipation power) of the heat sinks. The LBM code was solved in Visual Basic. The results report that the four/five square fins (arrangement A) heat sink with 10 % CNT compound paraffin has the shortest charging time and the heat sink with four square fins (arrangement A) and 8 % CNT compound paraffin has the largest heat dissipation power. It is worth noted that compared with square fin heat sinks, the cross fin heat sinks still present 83 % of the heat dissipation power with only 36 % of the fin volume. Besides, aiming at acquiring the largest heat dissipation power, a back propagation neural network-genetic algorithm was developed to optimize the structure of the pin fin heat sink. The corresponding fin number, fin side length and fin distance for the optimized square fin heat sink (arrangement A) are 4, 2.13 mm and 5.88 mm, respectively.", "journal": "JOURNAL OF ENERGY STORAGE", "category": "Energy & Fuels", "annotated_keywords": ["neural net"], "label": "1", "title_label": "0"}
{"id": "WOS:000869544700001", "keywords": "aquaculture; disease outbreak prediction; machine learning; rainbow trout; sustainability", "title": "Using machine learning technique for disease outbreak prediction in rainbow trout (Oncorhynchus mykiss) farms", "abstract": "Water quality parameters such as temperature, dissolved oxygen, pH and total dissolved solids are important environmental factors affecting fish welfare. The deterioration of these parameters beyond the tolerance limits causes environmental stress and suppression of the immune system. Moreover, it allows opportunistic pathogens that are always present in the environment to infect immune-suppressed fish and cause serious disease outbreaks. In this study, water quality parameters and pathogenic bacteria profiles were monitored for 1 year in rainbow trout farms operating in the same river basin. Then, a data set was created considering the pathogenic bacteria in the diseased fish and the water quality parameters in the farm environment. Each of the water quality parameters in the data set was first used as an attribute and their order of importance in terms of disease outbreak was determined. Then, using multinomial logistic regression (MLR) analysis, which is one of the machine learning (ML) techniques, the possibility of water quality parameters revealing a disease outbreak was evaluated. Furthermore, very effective models that can be used to predict the probability of disease occurrence in trout farms with an accuracy of 95.65% have been created.", "journal": "AQUACULTURE RESEARCH", "category": "Fisheries", "annotated_keywords": ["machine learning", "machine learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000890728500001", "keywords": "atmospheric boundary layer; machine learning; unsupervised classification", "title": "Toward instrument combination for boundary layer classification", "abstract": "To handle the complexity of the atmospheric boundary layer (ABL) and make accurate feature detection (top height, low-level jets, inversions, etc.), a prior necessary step is to identify the type of boundary layer. This study proposes a new method to identify the boundary layer type through unsupervised classification and the synergistic use of ground-based remote sensing. Unsupervised classification is used to lighten the human supervision. The new classification was applied to a 1-day case study collected during wintertime in the Arve River valley near Chamonix-Mont-Blanc during the Passy-2015 field experiment. The ABL classification obtained from microwave radiometer and ceilometer observations (ground-based remote sensors [GBReS]) combination is compared with high-frequency radiosoundings (RS) data and the French convective scale AROME model outputs. Classifications from RS and GBReS broadly agree, demonstrating the good behavior of the method, AROME leading to different results at night. The difference of AROME is likely due to the different nature of the data (model fields are smoother and include forecasting errors). The results show the ability of unsupervised classification to segment relevant objects in the boundary layer and the benefit to use a combination of GBReS.", "journal": "ATMOSPHERIC SCIENCE LETTERS", "category": "Geochemistry & Geophysics; Meteorology & Atmospheric Sciences", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000867709800001", "keywords": "optical coherence tomography; deep neural network; image segmentation; laser-induced skin injury", "title": "Automatic Segmentation of Laser-Induced Injury OCT Images Based on a Deep Neural Network Model", "abstract": "Optical coherence tomography (OCT) has considerable application potential in noninvasive diagnosis and disease monitoring. Skin diseases, such as basal cell carcinoma (BCC), are destructive; hence, quantitative segmentation of the skin is very important for early diagnosis and treatment. Deep neural networks have been widely used in the boundary recognition and segmentation of diseased areas in medical images. Research on OCT skin segmentation and laser-induced skin damage segmentation based on deep neural networks is still in its infancy. Here, a segmentation and quantitative analysis pipeline of laser skin injury and skin stratification based on a deep neural network model is proposed. Based on the stratification of mouse skins, a laser injury model of mouse skins induced by lasers was constructed, and the multilayer structure and injury areas were accurately segmented by using a deep neural network method. First, the intact area of mouse skin and the damaged areas of different laser radiation doses are collected by the OCT system, and then the labels are manually labeled by experienced histologists. A variety of deep neural network models are used to realize the segmentation of skin layers and damaged areas on the skin dataset. In particular, the U-Net model based on a dual attention mechanism is used to realize the segmentation of the laser-damage structure, and the results are compared and analyzed. The segmentation results showed that the Dice coefficient of the mouse dermis layer and injury area reached more than 0.90, and the Dice coefficient of the fat layer and muscle layer reached more than 0.80. In the evaluation results, the average surface distance (ASSD) and Hausdorff distance (HD) indicated that the segmentation results are excellent, with a high overlap rate with the manually labeled area and a short edge distance. The results of this study have important application value for the quantitative analysis of laser-induced skin injury and the exploration of laser biological effects and have potential application value for the early noninvasive detection of diseases and the monitoring of postoperative recovery in the future.", "journal": "INTERNATIONAL JOURNAL OF MOLECULAR SCIENCES", "category": "Biochemistry & Molecular Biology; Chemistry, Multidisciplinary", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000869735400002", "keywords": "Proteomics; Bayesian mixture models; semi-supervised learning", "title": "SEMI-SUPERVISED NONPARAMETRIC BAYESIAN MODELLING OF SPATIAL PROTEOMICS", "abstract": "Understanding subcellular protein localisation is an essential component in the analysis of context specific protein function. Recent advances in quantitative mass-spectrometry (MS) have led to high-resolution mapping of thousands of proteins to subcellular locations within the cell. Novel modelling considerations to capture the complex nature of these data are thus necessary. We approach analysis of spatial proteomics data in a nonparametric Bayesian framework, using K-component mixtures of Gaussian process regression models. The Gaussian process regression model accounts for correlation structure within a subcellular niche, with each mixture component capturing the distinct correlation structure observed within each niche. The availability of marker proteins (i.e., proteins with a priori known labelled locations) motivates a semi-supervised learning approach to inform the Gaussian process hyperparameters. We moreover provide an efficient Hamiltonianwithin-Gibbs sampler for our model. Furthermore, we reduce the computational burden associated with inversion of covariance matrices by exploiting the structure in the covariance matrix. A tensor decomposition of our covariance matrices allows extended Trench and Durbin algorithms to be applied to reduce the computational complexity of inversion and hence accelerate computation. We provide detailed case-studies on Drosophila embryos and mouse pluripotent embryonic stem cells to illustrate the benefit of semi-supervised functional Bayesian modelling of the data.", "journal": "ANNALS OF APPLIED STATISTICS", "category": "Statistics & Probability", "annotated_keywords": ["supervised learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000890021000001", "keywords": "Consensus reaching process; decision quality; group decision making; three-way decisions", "title": "A Two-Stage Method for Improving the Decision Quality of Consensus-Driven Three-Way Group Decision-Making", "abstract": "In three-way group decision-making based on the minimum risk Bayesian decision theory, the consensus of basic loss function evaluation becomes its main core issue. However, if we only consider evaluation information consensus, it does not ensure the classification quality of three-way decisions. Thus, to balance the consensus and decision quality, we design a three-way group decision-making joint learning process via constructing a two-stage group consensus method. Inspired by supervised learning, Stage 1 establishes the minimum decision error optimization model (MDEOM) to learn the optimal parameters of three-way decisions and calculate decision loss reference values. Then, we design an algorithm to solve MDEOM based on particle swarm optimization (PSO) algorithm. In Stage 2, we calculate adjusted decision losses with the minimum decision loss difference consensus model (MDLDCM), which can guide the consensus adjustment of loss functions and improve the decision quality of three-way group decision making. Finally, some implications of three-way group decision making with the two-stage group consensus method are discussed by a series of experiments which prove the improvement effectiveness of our proposed method.", "journal": "IEEE TRANSACTIONS ON SYSTEMS MAN CYBERNETICS-SYSTEMS", "category": "Automation & Control Systems; Computer Science, Cybernetics", "annotated_keywords": ["supervised learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000891249100002", "keywords": "Deep reinforcement learning; Distribution network; Wind energy; Optimal scheduling", "title": "Optimal scheduling of a wind energy dominated distribution network via a deep reinforcement learning approach", "abstract": "With the development of clean energy systems, large-scale renewable energy is being connected to the traditional distribution network, which also brings new challenges to the reliable and economic scheduling of the power grid. To address these challenges, this paper proposes an intelligent scheduling strategy for a wind energy dominated distribution network, which aims to reduce the fluctuation caused by the wind energy. First, the energy scheduling model and objective function of the distribution network system are established and the constraints of various types of components are considered. Then, deep reinforcement learning is introduced to realize real-time decision in distribution network to solve the problem of fluctuation caused by the uncertain wind power output. The energy scheduling method is developed into a Markov decision process based on deep deterministic policy gradient (DDPG) algorithm. Finally, the simulation is verified on the IEEE14 node system. The results verify that the proposed approach can effectively reduce power fluctuations in the distribution network. The superiority of the adopted DDPG algorithm is demonstrated by comparing with the deep Q network algorithm.", "journal": "RENEWABLE ENERGY", "category": "Green & Sustainable Science & Technology; Energy & Fuels", "annotated_keywords": ["reinforcement learning", "reinforcement learning"], "label": "1", "title_label": "1"}
{"id": "WOS:000908789800001", "keywords": "Multiple metal detoxification; Ureolytic microbes; Functional genes (ureC); Three combined pathways; Soil nitrogen transformation", "title": "Detoxification pathways of multiple metals from intensive copper production by indigenous ureolytic microbes coupled with soil nitrogen transformation", "abstract": "The ubiquitous ureolytic microbes were potential to clean up toxic heavy metals from environment. Their re-sponses and detoxification mechanisms are largely unknown for multiple metals in field soils. In this study, high -throughput sequencing of the urease-encoding genes and deep learning from bioinformatic analysis were con-ducted for identifying ureolytic prokaryote-related processes throughout China. The passive adsorption of heavy metals onto cell walls or cell surfaces was mainly performed by G+ ureolytic bacteria in soils located in the large-scale smelting areas with slightly high Hg and low Cu/Zn/Ni contents. For the large-scale smelting areas with the highest Cr and lowest Cu/As contents, the Cr(VI) was reduced to Cr(III) by G- ureolytic bacteria and then accumulated in their periplasmic spaces or cell surfaces. The G- ureolytic bacteria detoxified high contents of multiple metals by intracellular antioxidant defense mechanisms and extracellular urease-induced (co)precipi-tation in the medium-scale smelting areas, consistent with high urease activities and ureolytic microbial di-versity. In addition, this large-scale field work verified that toxic metals stimulated the bacterial hydrolysis of urea as soil nitrogen fertilizers and depressed the bacterial fixation of nitrogen from air. Simultaneously, the heavy metals used nitrate-N and nitrite-N as alternative electron acceptors, driving the soil denitrification processes.", "journal": "JOURNAL OF CLEANER PRODUCTION", "category": "Green & Sustainable Science & Technology; Engineering, Environmental; Environmental Sciences", "annotated_keywords": ["deep learning"], "label": "1", "title_label": "0"}
{"id": "WOS:000887125700001", "keywords": "hyperparameter; optimization; grid search; random search; adaptive hyperparameter tuning; convolutional neural network", "title": "Improving the Robustness and Quality of Biomedical CNN Models through Adaptive Hyperparameter Tuning", "abstract": "Deep learning is an obvious method for the detection of disease, analyzing medical images and many researchers have looked into it. However, the performance of deep learning algorithms is frequently influenced by hyperparameter selection, the question of which combination of hyperparameters are best emerges. To address this challenge, we proposed a novel algorithm for Adaptive Hyperparameter Tuning (AHT) that automates the selection of optimal hyperparameters for Convolutional Neural Network (CNN) training. All of the optimal hyperparameters for the CNN models were instantaneously selected and allocated using a novel proposed algorithm Adaptive Hyperparameter Tuning (AHT). Using AHT, enables CNN models to be highly autonomous to choose optimal hyperparameters for classifying medical images into various classifications. The CNN model (Deep-Hist) categorizes medical images into basic classes: malignant and benign, with an accuracy of 95.71%. The most dominant CNN models such as ResNet, DenseNet, and MobileNetV2 are all compared to the already proposed CNN model (Deep-Hist). Plausible classification results were obtained using large, publicly available clinical datasets such as BreakHis, BraTS, NIH-Xray and COVID-19 X-ray. Medical practitioners and clinicians can utilize the CNN model to corroborate their first malignant and benign classification assessment. The recommended Adaptive high F1 score and precision, as well as its excellent generalization and accuracy, imply that it might be used to build a pathologist's aid tool.", "journal": "APPLIED SCIENCES-BASEL", "category": "Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials Science, Multidisciplinary; Physics, Applied", "annotated_keywords": ["deep learning", "learning algorithm"], "label": "1", "title_label": "1"}
{"id": "WOS:000873067200001", "keywords": "landslide susceptibility mapping; multi-criteria decision analysis; fuzzy-analytical hierarchy process; artificial neural network; Darjeeling Himalayas", "title": "Development and Assessment of GIS-Based Landslide Susceptibility Mapping Models Using ANN, Fuzzy-AHP, and MCDA in Darjeeling Himalayas, West Bengal, India", "abstract": "Landslides, a natural hazard, can endanger human lives and gravely affect the environment. A landslide susceptibility map is required for managing, planning, and mitigating landslides to reduce damage. Various approaches are used to map landslide susceptibility, with varying degrees of efficacy depending on the methodology utilized in the research. An analytical hierarchy process (AHP), a fuzzy-AHP, and an artificial neural network (ANN) are utilized in the current study to construct maps of landslide susceptibility for a part of Darjeeling and Kurseong in West Bengal, India. On a landslide inventory map, 114 landslide sites were randomly split into training and testing with a 70:30 ratio. Slope, aspect, profile curvature, drainage density, lineament density, geomorphology, soil texture, land use and land cover, lithology, and rainfall were used as model inputs. The area under the curve (AUC) was used to examine the models. When tested for validation, the ANN prediction model performed best, with an AUC of 88.1%. AUC values for fuzzy-AHP and AHP are 86.1% and 85.4%, respectively. According to the statistics, the northeast and eastern portions of the study area are the most vulnerable. This map might help development in the area by preventing human and economic losses.", "journal": "LAND", "category": "Environmental Studies", "annotated_keywords": ["neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000893056100002", "keywords": "Deep neural network; Dropout; Genetic algorithm; Dropout rate", "title": "Supervision dropout: guidance learning in deep neural network", "abstract": "In deep neural networks, the generalization is a vital evaluation metric. As it contributes to avoid over-fitting, Dropout plays an important role in improving the generalization of deep neural networks. Without fully utilizing the training data and the real-time performance of the networks, traditional Dropout and its variants lack of specificity in the selection of inactivated neurons and the planning of dropout rates, resulting in a weaker performance in enhancing the generalization. Therefore, this paper offers an improved Dropout method. As both the training data and the real-time performance of networks can be quantified by the loss, the method uses the loss of the network prediction to guide the selection of inactivated neurons and the determination of dropout rates. The selection is performed by the genetic algorithm, while the results of the selection are used to plan the dropout rate. In essence, this approach encourages the subset of neurons with the higher loss to be trained so as to increase the robustness of neurons and thus improves the generalization of networks. The experimental results demonstrate that the proposed method achieves better generalization on MiniImageNet and Caltech-256 datasets. Compared with the backbone network, the accuracy improves from 66.56% to 72.95%.", "journal": "MULTIMEDIA TOOLS AND APPLICATIONS", "category": "Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": ["neural net", "neural net"], "label": "1", "title_label": "1"}
{"id": "WOS:000638870200001", "keywords": "jumping robot; intelligent optimization algorithm; pitch control", "title": "Study of Obstacle-Crossing and Pitch Control Characteristic of a Novel Jumping Robot", "abstract": "In this study, we demonstrated a novel jumping robot that has the ability of accurate obstacle-crossing jumping and aerial pitch control. The novel robot can quickly leap high into the air with a powerful water jet thruster. The robot was designed to overcome multiple general obstacles via accurate jumping. Then a modified whale optimization algorithm (MWOA) was proposed to determine an optimized jumping trajectory according to the form of obstacles. By comparing with classical intelligent optimization algorithms, the MWOA revealed superiority in convergence rate and precision. Besides, the dynamics model of aerial pitch control was built and its effect was verified by the pitch control experiment. Lastly, the robot's obstacle-crossing experiments were performed and the results validated the robot's good ability of obstacle-crossing and aerial body righting. We believe the optimization of trajectory and the pitch control are of great help for the jumping robot's complex jumping and obstacle-crossing performance.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000647071400001", "keywords": "Domestic water; consumption; fsQCA; household; urban districts; water price", "title": "Assessment of domestic water consumption in Valencia city through fuzzy-set qualitative comparative analysis", "abstract": "The influence of quantitative and qualitative population conditions needs to be jointly considered in the assessment of urban water demand. Through fuzzy-set qualitative comparative analysis (fsQCA), quantitative and qualitative conditions can be jointly analysed to obtain the patterns or recipes that explain the hypothesis tested. This work assesses the influence of water price, income level, ageing index, number of members in household, household antiquity and household surface on the urban water consumption from a district-level approach. The approach proposed is a novelty since it is focused on urban water demand and the related socioeconomic conditions in Valencia city. Results obtained show the influence of water price and members in household as main conditions that explain the water consumption of Valencia districts. This work highlights the relationship between the conditions analysed and water consumption, allowing the identification of the water consumption patterns in a developed urban area from a district-level approach.", "journal": "URBAN WATER JOURNAL", "category": "Water Resources", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000666037400001", "keywords": "forest windthrow mapping; sentinel-2; satellite multispectral data; continues change detection algorithm; BEAST; CCDC; change detection", "title": "Estimating VAIA Windstorm Damaged Forest Area in Italy Using Time Series Sentinel-2 Imagery and Continuous Change Detection Algorithms", "abstract": "Mapping forest disturbances is an essential component of forest monitoring systems both to support local decisions and for international reporting. Between the 28 and 29 October 2018, the VAIA storm hit the Northeast regions of Italy with wind gusts exceeding 200 km h(-1). The forests in these regions have been seriously damaged. Over 490 Municipalities in six administrative Regions in Northern Italy registered forest damages caused by VAIA, that destroyed or intensely damaged forest stands spread over an area of 67,000 km(2). The present work tested the use of two continuous change detection algorithms, i.e., the Bayesian estimator of abrupt change, seasonal change, and trend (BEAST) and the continuous change detection and classification (CCDC) to map and estimate forest windstorm damage area using a normalized burned ration (NBR) time series calculated on three years Sentinel-2 (S2) images collection (i.e., January 2017-October 2019). We analyzed the accuracy of the maps and the damaged forest area using a probability-based stratified estimation within 12 months after the storm with an independent validation dataset. The results showed that close to the storm (i.e., 1 to 6 months November 2018-March 2019) it is not possible to obtain accurate results independently of the algorithm used, while accurate results were observed between 7 and 12 months from the storm (i.e., May 2019-October 2019) in terms of Standard Error (SE), percentage SE (SE%), overall accuracy (OA), producer accuracy (PA), user accuracy (UA), and g(mean) for both BEAST and CCDC (SE < 3725.3 ha, SE% < 9.69, OA > 89.7, PA and UA > 0.87, g(mean) > 0.83).", "journal": "FORESTS", "category": "Forestry", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000684140300001", "keywords": "global and local optimization; hybrid algorithm; nonlinear optimization; real world problem", "title": "An advanced hybrid algorithm for nonlinear function optimization with real world applications", "abstract": "This article presents an advanced hybrid algorithm (haDEPSO) for nonlinear constrained function optimization problem. It consists of the advised advanced differential evolution, that is, aDE (wherein a novel mutation, crossover, and selection strategy is introduced, to avoid premature convergence) and particle swarm optimization, namely, aPSO (in which novel gradually varying parameters is utilized, to escape stagnation). The proposed haDEPSO achieve better results as aDE and aPSO provides diverse convergence characteristic to the solution space. Moreover, in haDEPSO distinct population is merged with other in a predefined way, to balance between exploration and exploitation. Efficiency of the proposed hybrid haDEPSO along with its integrating component aDE and aPSO are inspected on IEEE CEC'2006 constrained benchmark functions and IEEE CEC'2011 real world problems. According to the comparison results, the proposed methods achieved better results than traditional DE and PSO with their hybrids as well as over many state-of-the-art algorithms.", "journal": "CONCURRENCY AND COMPUTATION-PRACTICE & EXPERIENCE", "category": "Computer Science, Software Engineering; Computer Science, Theory & Methods", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000661542700043", "keywords": "Evolutionary game theory; Federated fog; Fog computing; Stability; IoT", "title": "Sequence-Engineering Polyethylene-Polypropylene Copolymers with High Thermal Conductivity Using a Molecular-Dynamics-Based Genetic Algorithm", "abstract": "Polymer sequence engineering is emerging as a potential tool to modulate material properties. Here, we employ a combination of a genetic algorithm (GA) and atomistic molecular dynamics (MD) simulation to design polyethylene-polypropylene (PE-PP) copolymers with the aim of identifying a specific sequence with high thermal conductivity. PE-PP copolymers with various sequences at the same monomer ratio are found to have a broad distribution of thermal conductivities. This indicates that the monomer sequence has a crucial effect on thermal energy transport of the copolymers. A non-periodic and non-intuitive optimal sequence is indeed identified by the GA, which gives the highest thermal conductivity compared with any regular block copolymers, for example, diblock, triblock, and hexablock. In comparison to the bulk density, chain conformations, and vibrational density of states, the monomer sequence has the strongest impact on the efficiency of thermal energy transport via inter- and intra-molecular interactions. Our work highlights polymer sequence engineering as a promising approach for tuning the thermal conductivity of copolymers, and it provides an example application of integrating atomistic MD modeling with the GA for computational material design.", "journal": "JOURNAL OF CHEMICAL THEORY AND COMPUTATION", "category": "Chemistry, Physical; Physics, Atomic, Molecular & Chemical", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000638930600001", "keywords": "tourist behaviour; COVID-19; Twitter; Andalusia; text mining; KNIME; RStudio", "title": "Using Social Media in Tourist Sentiment Analysis: A Case Study of Andalusia during the Covid-19 Pandemic", "abstract": "This paper explores the role of social media in tourist sentiment analysis. To do this, it describes previous studies that have carried out tourist sentiment analysis using social media data, before analyzing changes in tourists' sentiments and behaviors during the COVID-19 pandemic. In the case study, which focuses on Andalusia, the changes experienced by the tourism sector in the southern Spanish region as a result of the COVID-19 pandemic are assessed using the Andalusian Tourism Situation Survey (ECTA). This information is then compared with data obtained from a sentiment analysis based on the social network Twitter. On the basis of this comparative analysis, the paper concludes that it is possible to identify and classify tourists' perceptions using sentiment analysis on a mass scale with the help of statistical software (RStudio and Knime). The sentiment analysis using Twitter data correlates with and is supplemented by information from the ECTA survey, with both analyses showing that tourists placed greater value on safety and preferred to travel individually to nearby, less crowded destinations since the pandemic began. Of the two analytical tools, sentiment analysis can be carried out on social media on a continuous basis and offers cost savings.", "journal": "SUSTAINABILITY", "category": "Green & Sustainable Science & Technology; Environmental Sciences; Environmental Studies", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000685891800016", "keywords": "MU-MIMO; hybird beamforming; user selection; power azimuth spectrum; channel state information", "title": "Enabling Practical Large-Scale MIMO in WLANs With Hybrid Beamforming", "abstract": "In theory, the capacity of a wireless network grows linearly with the number of users and antennas equipped at the communication devices, and hence large-scale MU-MIMO can scale up the network throughput. However, three main challenges are impeding the implementation of this promising technology in the state-of-the-art WLANs. Firstly, the current large-scale MU-MIMO technology demands a large number of high-priced RF chains. Secondly, the wireless access points (APs) are overwhelmed by channel state information (CSI) feedback for nulling multi-user and -antenna interference. Thirdly, the lack of scalable user selection scheme limits the capability of APs to serve a large user population. To address these problems, we design BUSH, a large-scale MU-MIMO prototype that performs scalable beam user selection with hybrid beamforming for phased-array antennas in legacy WLANs. We design a low complexity algorithm that assigns each pair of RF chain and analog beam to the users to effectively reduce channel correlation and cross-talk interference without instantaneous CSI feedbacks. As a prerequisite of user selection, BUSH presents a low-overhead probing scheme in multi-carrier WLANs and designs a highly accurate blind Power Azimuth Spectrum (PAS) estimation algorithm using a single RF chain. For reducing the number of RF-chains used, the phased-array antennas use analog beamforming to steer beams toward each selected downlink user, and multiple RF chains use beamforming to further mitigate the interference among users. We implement BUSH on a software-defined radio platform and evaluate its performance in more than 30 indoor scenarios. The experimental results reveal that for throughput, BUSH outperforms the legacy 802.11ac by 2.08x, and an alternative benchmark system by 1.22x on average.", "journal": "IEEE-ACM TRANSACTIONS ON NETWORKING", "category": "Computer Science, Hardware & Architecture; Computer Science, Theory & Methods; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000315738200017", "keywords": "Hyperstructure; semihypergroup; Gamma - semihypergroup; intuitionistic fuzzy set; Gamma - hyperideal; intuitionistic fuzzy Gamma - hyperideal", "title": "Enhanced Detection of Open-angle Glaucoma with an Anatomically Accurate Optical Coherence Tomography-Derived Neuroretinal Rim Parameter", "abstract": "Objective: Neuroretinal rim assessment based on the clinical optic disc margin (DM) lacks a sound anatomic basis for 2 reasons: (1) The DM is not reliable as the outer border of rim tissue because of clinically and photographically invisible extensions of Bruch's membrane (BM) inside the DM and (2) nonaccountability of rim tissue orientation in the optic nerve head (ONH). The BM opening-minimum rim width (BMO-MRW) is a parameter that quantifies the rim from its true anatomic outer border, BMO, and accounts for its variable orientation. We report the diagnostic capability of BMO-MRW. Design: Case control. Participants: Patients with open-angle glaucoma (n = 107) and healthy controls (n = 48). Methods: Spectral-domain optical coherence tomography (SD-OCT) with 24 radial and 1 circumpapillary B-scans, centered on the ONH, and confocal scanning laser tomography (CSLT) were performed. The internal limiting membrane (ILM) and BMO were manually segmented in each radial B-scan. Three SD-OCT parameters were computed globally and sectorally: (1) circumpapillary retinal nerve fiber layer thickness (RNFLT); (2) BMO-horizontal rim width (BMO-HRW), the distance between BMO and ILM in the BMO reference plane; and (3) BMO-MRW, the minimum distance between BMO and ILM. Moorfields Regression Analysis (MRA) with CLST was performed globally and sectorally to yield MRA1 and MRA2, where \"borderline\" was classified as normal and abnormal, respectively. Main Outcome Measures: Sensitivity, specificity, and likelihood ratios (LRs) for positive and negative test results (LR+/LR-). Results: The median (interquartile range) age and mean deviation of patients and controls were 69.9 (64.3-76.9) and 65.0 (58.1-74.3) years and -3.92 (-7.87 to -1.62) and 0.33 (-0.32 to 0.98) dB, respectively. Globally, BMO-MRW yielded better diagnostic performance than the other parameters. At 95% specificity, the sensitivity of RNFLT, BMO-HRW, and BMO-MRW was 70%, 51%, and 81%, respectively. The corresponding LR+/LR- was 14.0/0.3, 10.2/0.5, and 16.2/0.2. Sectorally, at 95% specificity, the sensitivity of RNFLT ranged from 31% to 59%, of BMO-HRW ranged from 35% to 64%, and of BMO-MRW ranged from 54% to 79%. Globally and in all sectors, BMO-MRW performed better than MRA1 or MRA2. Conclusions: The higher sensitivity at 95% specificity in early glaucoma of BMO-MRW compared with current BMO methods is significant, indicating a new structural marker for the detection and risk profiling of glaucoma. Financial Disclosure(s): Proprietary or commercial disclosure may be found after the references. Ophthalmology 2013;120:535-543 (C) 2013 by the American Academy of Ophthalmology.", "journal": "OPHTHALMOLOGY", "category": "Ophthalmology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000324263500011", "keywords": "Lamb wave; fractal dimension; damage imaging; composite structures; multi-damage detection", "title": "Fractal dimension-based damage imaging for composites", "abstract": "In this paper, a damage imaging algorithm based on fractal dimension is developed for quantitative damage detection of composite structures. Box-counting dimension, a typical fractal dimension, is employed to analyze the difference of Lamb wave signals, extract damage feature and define damage index. An enhanced reconstruction algorithm for probabilistic inspection of damage is developed for damage imaging. Experimental investigation in a composite laminate and a stiffened composite panel shows that the developed algorithm could quantitatively predict the location and size of not only single but also multiple damages. The influence of parameters in the developed algorithm on the imaging quality and accuracy is studied, and reference values for parameters are presented.", "journal": "SHOCK AND VIBRATION", "category": "Acoustics; Engineering, Mechanical; Mechanics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000325485600060", "keywords": "Distributed generation; harmony search algorithm; multi-objective optimization", "title": "An Improved Multi-Objective Harmony Search for Optimal Placement of DGs in Distribution Systems", "abstract": "In this paper a new approach using Harmony Search (HS) algorithm is presented for placing Distributed Generators (DGs) in radial distribution systems. The approach is making use of a multiple objective planning framework, named an Improved Multi-objective HS (IMOHS), to evaluate the impact of DG placement and sizing for an optimal development of the distribution system. In this study, the optimum sizes and locations of DG units are found by considering the power losses and voltage profile as objective functions. The feasibility of the proposed technique is demonstrated in two distribution networks, where the qualitative comparisons are made against a well-known technique, known as Non-dominated Sorting Genetic Algorithm II (NSGA-II). Furthermore, the results obtained are compared with those available in the literature.", "journal": "IEEE TRANSACTIONS ON SMART GRID", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000313029300006", "keywords": "assessment tasks; elicitation; generalized linear model; piecewise-linear function; prior distribution; subjective probability", "title": "Prior distribution elicitation for generalized linear and piecewise-linear models", "abstract": "An elicitation method is proposed for quantifying subjective opinion about the regression coefficients of a generalized linear model. Opinion between a continuous predictor variable and the dependent variable is modelled by a piecewise-linear function, giving a flexible model that can represent a wide variety of opinion. To quantify his or her opinions, the expert uses an interactive computer program, performing assessment tasks that involve drawing graphs and bar-charts to specify medians and other quantiles. Opinion about the regression coefficients is represented by a multivariate normal distribution whose parameters are determined from the assessments. It is practical to use the procedure with models containing a large number of parameters. This is illustrated through practical examples and the benefit from using prior knowledge is examined through cross-validation.", "journal": "JOURNAL OF APPLIED STATISTICS", "category": "Statistics & Probability", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000321222200007", "keywords": "sparse deconvolution; constraint criterion; modified Cauchy criterion; resolution", "title": "Experimental analysis and application of sparsity constrained deconvolution", "abstract": "Sparsity constrained deconvolution can improve the resolution of band-limited seismic data compared to conventional deconvolution. However, such deconvolution methods result in nonunique solutions and suppress weak reflections. The Cauchy function, modified Cauchy function, and Huber function are commonly used constraint criteria in sparse deconvolution. We used numerical experiments to analyze the ability of sparsity constrained deconvolution to restore reflectivity sequences and protect weak reflections under different constraint criteria. The experimental results demonstrate that the performance of sparsity constrained deconvolution depends on the agreement between the constraint criteria and the probability distribution of the reflectivity sequences; furthermore, the modified Cauchyconstrained criterion protects the weak reflections better than the other criteria. Based on the model experiments, the probability distribution of the reflectivity sequences of carbonate and clastic formations is statistically analyzed by using well-logging data and then the modified Cauchy-constrained deconvolution is applied to real seismic data much improving the resolution.", "journal": "APPLIED GEOPHYSICS", "category": "Geochemistry & Geophysics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000316655500010", "keywords": "Internet of Things; ontology; semantic web; ubiquitous learning", "title": "Designing the Internet of Things for learning environmentally responsible behaviour", "abstract": "We present two designs in the area of the Internet of Things, utilizing the ontology-driven Smart Objects For Intelligent Applications (SOFIA) Interoperability Platform (IOP). The IOP connects domestic objects in the physical world to the information world, allowing for coaching the behaviour of, or raising awareness in, domestic energy consumption. The concept and architecture of the SOFIA IOP is introduced, in which the domestic objects are knowledge processors connected to a semantic information broker. This broker, using a blackboard design pattern, ontologies, and semantic web technologies, enables interoperability among both digital and physical entities. The two designs based on the SOFIA IOP are presented as examples for coaching with and learning from the Internet of Things. Although both designs are in the area of domestic energy consumption, they can be seen as good starting points towards broader areas of ubiquitous learning enabled by the Internet of Things.", "journal": "INTERACTIVE LEARNING ENVIRONMENTS", "category": "Education & Educational Research", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000314167700008", "keywords": "Liu-type estimator; Logistic regression; Multicollinearity; Ridge estimator", "title": "Liu-Type Logistic Estimator", "abstract": "It is known that multicollinearity inflates the variance of the maximum likelihood estimator in logistic regression. Especially, if the primary interest is in the coefficients, the impact of collinearity can be very serious. To deal with collinearity, a ridge estimator was proposed by Schaefer et al. The primary interest of this article is to introduce a Liu-type estimator that had a smaller total mean squared error (MSE) than the Schaefer's ridge estimator under certain conditions. Simulation studies were conducted that evaluated the performance of this estimator. Furthermore, the proposed estimator was applied to a real-life dataset.", "journal": "COMMUNICATIONS IN STATISTICS-SIMULATION AND COMPUTATION", "category": "Statistics & Probability", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000315611100024", "keywords": "Heart rate sensor; Photoelectric plethysmography; Heart rate variability; Blood volume pulse; Biomedical monitoring; U-health care", "title": "Real-Time Heart Rate Monitoring System based on Ring-Type Pulse Oximeter Sensor", "abstract": "With the continuous aging of the populations in developed countries, the medical requirements of the aged are expected to increase. In this paper, a ring-type pulse oximeter finger sensor and a 24-hour ambulatory heart rate monitoring system for the aged are presented. We also demonstrate the feasibility of extracting accurate heart rate variability measurements from photoelectric plethysmography signals gathered using a ring-type pulse oximeter sensor attached to the finger. We designed the heart rate sensor using a CPU with built-in ZigBee stack for simplicity and low power consumption. We also analyzed the various distorted signals caused by motion artifacts using a FFT, and designed an algorithm using a least squares estimator to calibrate the signals for better accuracy.", "journal": "JOURNAL OF ELECTRICAL ENGINEERING & TECHNOLOGY", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000313764300024", "keywords": "tissue inhibitor of metalloproteinase 4; OPCRIT; schizophrenia; autism spectrum disorders; single nucleotide polymorphism; Korean", "title": "Assessment of the correlation between TIMP4 SNPs and schizophrenia and autism spectrum disorders", "abstract": "Tissue inhibitors of metalloproteinases (TIMPs) are involved in synaptic plasticity, neuronal cell differentiation and neuroprotection in the central nervous system. To investigate whether TIMP4 polymorphisms are associated with schizophrenia and autism spectrum disorders (ASDs), 480 patients (schizophrenia, n=287; ASDs, n=193) and 296 controls were enrolled. Clinical symptoms of schizophrenia and ASDs were assessed using the operation criteria checklist for psychotic illness (OPCRIT) and Childhood Autism Rating Scale (CARS), respectively. One promoter single nucleotide polymorphism (SNP; rs3755724, -55C/T) and one exonic SNP (rs17035945, 3'-untranslated region) were selected. SNPStats and SNPAnalyzer Pro programs were used to calculate odds ratios. Multiple logistic regression models were performed to analyze the genetic data. Based on the results, these two SNPs were not associated with schizophrenia and ASD. In the analysis of clinical features of schizophrenia, rs3755724 was nominally associated with schizophrenia with poor concentration (P=0.044 in the codominant2 model, P=0.041 in the log-additive model and P=0.043 in allele frequency). These results suggest that TIMP4 is not associated with the development of schizophrenia and ASD in the population studied.", "journal": "MOLECULAR MEDICINE REPORTS", "category": "Oncology; Medicine, Research & Experimental", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000318571200001", "keywords": "Energy-efficient management; Vulcanization process; Multivariate regression; Anomaly detection; Energy flow ratio", "title": "Statistical Modeling for Energy Consumption and Anomaly Detection in Rubber Vulcanization Process", "abstract": "Factories are forced to pay for higher energy costs in the next decades because of increasing demand for energy and limited fuel resources. Efficient management of energy is among the greatest of challenges, especially for those energy-intensive manufacturing businesses. Research efforts have been continuously made to improve the efficiency in energy consumption, and methods and tools for energy efficiency management have been developed with respect to economics/cost and environment. This paper proposes a new energy-efficient management model by investigating energy consumption at tire manufacturing workstations. The model is then used to support efficacy and safety for manufacturing operations from the perspective of energy consumption. The proposed model is based on the statistical analysis of energy consumption in the rubber vulcanization process. The efficient energy usage is characterized by the ratio of energy flow into product manufacturing process. The energy flow ratio provides a quantitative measure for detecting system anomaly. A study case for tire vulcanization is presented to validate the proposed approach. (C) 2013 American Society of Civil Engineers.", "journal": "JOURNAL OF ENERGY ENGINEERING", "category": "Energy & Fuels; Engineering, Civil", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000317304900001", "keywords": "IL-28B; mother-to-child transmission; pegylated interferon; predictive factor; ribavirin; rs8099917", "title": "Association between an IL-28B genetic polymorphism and the efficacy of the response-guided pegylated interferon therapy in children with chronic hepatic C infection", "abstract": "Aim The relation between interleukin-28B (IL-28B) genotypes and treatment-induced hepatitis C virus (HCV) clearance in children is unknown. This was a retrospective study to evaluate the association between an IL-28B genotype (rs8099917) and pegylated (PEG) interferon (IFN) response. Methods Sixty-three children (median age, 7years; range, 317years; 22 with HCV genotype 1 and 41 with genotype non-1) with chronic HCV infection who were treated with response-guided PEG IFN on the basis of viral load were evaluated. Results The duration of treatment with PEG IFN was 24weeks in one child (2%), 36weeks in eight children (13%), 48weeks in 36 children (57%), 60weeks in 11 children (17%) and 72weeks in seven children (11%). Of the total 63 children, 54 (86%) were initially treated with PEG IFN--2a monotherapy. The remaining nine (14%) received PEG IFN plus ribavirin as the initial therapy. Of the 54 children initially treated with monotherapy, 35 (65%) continued receiving monotherapy until the end of treatment. In the remaining 19 (35%), monotherapy was changed to PEG IFN plus ribavirin at 12 or 24weeks of treatment. Of the total 63 children, 54 (86%) achieved a sustained virological response (SVR). In univariate analysis, rs8099917 genotype TT (P=0.075) showed a weak association with SVR. However, the multivariate analysis revealed no predictive factors which had a significant association with SVR. Conclusion The IL-28B genotype was not a strong pretreatment predictor for SVR in a mixed genotype cohort of children treated with response-guided PEG IFN therapy.", "journal": "HEPATOLOGY RESEARCH", "category": "Gastroenterology & Hepatology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000330743200008", "keywords": "Cumulative sum; hypertemporal and sequential change detection; Moderate Resolution Imaging Spectroradiometer (MODIS)", "title": "Genetic risk variants in African Americans with multiple sclerosis", "abstract": "Objectives: To assess the association of established multiple sclerosis (MS) risk variants in 3,254 African Americans (1,162 cases and 2,092 controls). Methods: Human leukocyte antigen (HLA)-DRB1, HLA-DQB1, and HLA-A alleles were typed by molecular techniques. Single nucleotide polymorphism (SNP) genotyping was conducted for 76 MS-associated SNPs and 52 ancestry informative marker SNPs selected throughout the genome. Self-declared ancestry was refined by principal component analysis of the ancestry informative marker SNPs. An ancestry-adjusted multivariate model was applied to assess genetic associations. Results: The following major histocompatibility complex risk alleles were replicated: HLA-DRB1(star) 15:01 (odds ratio [OR] = 2.02 [95% confidence interval: 1.54-2.63], p = 2.50e-07), HLA-DRB1(star) 03: 01 (OR = 1.58 [1.29-1.94], p = 1.11e-05), as well as HLA-DRB1(star) 04:05 (OR = 2.35 [1.26-4.37], p = 0.007) and the African-specific risk allele of HLA-DRB1(star) 15:03 (OR = 1.26 [1.05-1.51], p = 0.012). The protective association of HLA-A(star) 02: 01 was confirmed (OR = 0.72 [0.55-0.93], p = 0.013). None of the HLA-DQB1 alleles were associated with MS. Using a significance threshold of p < 0.01, outside the major histocompatibility complex region, 8MS SNPs were also found to be associated with MS in African Americans. Conclusion: MS genetic risk in African Americans only partially overlaps with that of Europeans and could explain the difference of MS prevalence between populations.", "journal": "NEUROLOGY", "category": "Clinical Neurology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000315539000045", "keywords": "Acorn kernel oil; Transesterification; Biodiesel properties; Taguchi method", "title": "Biodiesel production from crude acorn (Quercus frainetto L.) kernel oil: An optimisation process using the Taguchi method", "abstract": "In this study, acorn kernel oil with high free fatty acid content is used as feedstock to produce biodiesel. Two stages are used to produce biodiesel after obtaining the acorn kernel oil. The 3.38% free fatty acid content is decreased to 0.14% in the first stage, whereas the acid ester biodiesel is produced using alkaline transesterification reaction in the second stage. The biodiesel production process parameters are the alcohol:oil molar ratio, catalyst concentration, reaction temperature and reaction time. Taguchi experimental design is used for acorn kernel oil methyl ester production via process parameter optimisation. The optimal process parameters are determined to be a catalyst concentration of 0.7 wt%, an 8:1 alcohol:oil molar ratio, a 50 degrees C reaction temperature and 40 min of reaction time using a KOH catalyst in experimental studies. According to the Taguchi method, the most efficient process parameter in acorn kernel oil methyl ester production. Finally, the acorn kernel oil methyl ester yield is 90% under the optimal process parameters obtained by the Taguchi method. (C) 2012 Elsevier Ltd. All rights reserved.", "journal": "RENEWABLE ENERGY", "category": "Green & Sustainable Science & Technology; Energy & Fuels", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000318553900038", "keywords": "liver surgery planning; sparse shape composition; segmentation; shape prior", "title": "A new segmentation framework based on sparse shape composition in liver surgery planning system", "abstract": "Purpose: To improve the accuracy and the robustness of the segmentation in living donor liver transplantation (LDLT) surgery planning system, the authors present a new segmentation framework that addresses challenges induced by the complex shape variations of patients' livers with cancer. It is designed to achieve the accurate and robust segmentation of hepatic parenchyma, portal veins, hepatic veins, and tumors in the LDLT surgery planning system. Methods: The segmentation framework proposed in this paper includes two important modules: (1) The robust shape prior modeling for liver, in which the sparse shape composition (SSC) model is employed to deal with the complex variations of liver shapes and obtain patient-specific liver shape priors. (2) The integration of the liver shape prior with a minimally supervised segmentation algorithm to achieve the accurate segmentation of hepatic parenchyma, portal veins, hepatic veins, and tumors simultaneously. The authors apply this segmentation framework to our previously developed LDLT surgery planning system to enhance its accuracy and robustness when dealing with complex cases of patients with liver cancer. Results: Compared with the principal component analysis, the SSC model shows a great advantage in handling the complex variations of liver shapes. It also effectively excludes gross errors and outliers that appear in the input shape and preserves local details for specific patients. The proposed segmentation framework was evaluated on the clinical image data of liver cancer patients, and the average symmetric surface distance for hepatic parenchyma, portal veins, hepatic veins, and tumors was 1.07 +/- 0.76, 1.09 +/- 0.28, 0.92 +/- 0.35 and 1.13 +/- 0.37 mm, respectively. The Hausdorff distance for these four tissues was 7.68, 4.67, 4.09, and 5.36 mm, respectively. Conclusions: The proposed segmentation framework improves the robustness of the LDLT surgery planning system remarkably when dealing with complex clinical liver shapes. The SSC model is able to handle non-Gaussian errors and preserve local detail information of the input liver shape. As a result, the proposed framework effectively addresses the problems caused by the complex shape variations of livers with cancer. Our framework not only obtains accurate segmentation results for healthy persons and common patients, but also shows high robustness when dealing with specific patients with large variations of liver shapes in complex clinical environments. (c) 2013 American Association of Physicists in Medicine.", "journal": "MEDICAL PHYSICS", "category": "Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000317418500045", "keywords": "Diffusion tensor imaging; Diffusivity; High-grade glioma; Meningioma; Vasogenic edema; Tumor-infiltrated edema", "title": "Diagnosing and Mapping Pulmonary Emphysema on X-Ray Projection Images: Incremental Value of Grating-Based X-Ray Dark-Field Imaging", "abstract": "Purpose: To assess whether grating-based X-ray dark-field imaging can increase the sensitivity of X-ray projection images in the diagnosis of pulmonary emphysema and allow for a more accurate assessment of emphysema distribution. Materials and Methods: Lungs from three mice with pulmonary emphysema and three healthy mice were imaged ex vivo using a laser-driven compact synchrotron X-ray source. Median signal intensities of transmission (T), dark-field (V) and a combined parameter (normalized scatter) were compared between emphysema and control group. To determine the diagnostic value of each parameter in differentiating between healthy and emphysematous lung tissue, a receiver-operating-characteristic (ROC) curve analysis was performed both on a per-pixel and a per-individual basis. Parametric maps of emphysema distribution were generated using transmission, dark-field and normalized scatter signal and correlated with histopathology. Results: Transmission values relative to water were higher for emphysematous lungs than for control lungs (1.11 vs. 1.06, p<0.001). There was no difference in median dark-field signal intensities between both groups (0.66 vs. 0.66). Median normalized scatter was significantly lower in the emphysematous lungs compared to controls (4.9 vs. 10.8, p<0.001), and was the best parameter for differentiation of healthy vs. emphysematous lung tissue. In a per-pixel analysis, the area under the ROC curve (AUC) for the normalized scatter value was significantly higher than for transmission (0.86 vs. 0.78, p<0.001) and dark-field value (0.86 vs. 0.52, p<0.001) alone. Normalized scatter showed very high sensitivity for a wide range of specificity values (94% sensitivity at 75% specificity). Using the normalized scatter signal to display the regional distribution of emphysema provides color-coded parametric maps, which show the best correlation with histopathology. Conclusion: In a murine model, the complementary information provided by X-ray transmission and dark-field images adds incremental diagnostic value in detecting pulmonary emphysema and visualizing its regional distribution as compared to conventional X-ray projections.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000329381100003", "keywords": "Automotive industry; Automotive components industry; Assembly; Suppliers; Automobile industry; Social networks; China; Institutional inertia; Interfirm networks; Social capital", "title": "Automobile industry, guanxi, and social networks in China Empirical study of 32 automakers and 477 parts suppliers", "abstract": "Purpose - The purpose of this paper is to examine the factors that affect the stability of interfirm trading ties in China's transitional economy. In particular, the paper explores whether the propensity to engage in repeat transactions with past partners is attributable to rational choice based on expectations for the benefits of social capital, or an outcome of institutional pressure that binds firms sharing similar positions within the institutional structure bequeathed from China's socialist past. Design/methodology/approach - This study utilizes data on the actual trading ties between 32 final vehicle assemblers and 477 parts and components suppliers in the auto industry during the period from 1998 to 2005. Using logistic regression analysis, the study highlights the factors that lead to the greater likelihood of repeat transactions between a particular pair of assemblers and suppliers. Findings - The result of the analysis suggests that while rational motives, such as transaction cost economization, do account for the propensity to engage in repeat transactions with past partners, it also confirms the persistence of a strong tendency to continue transacting with firms sharing similar institutional lineage, regardless of the benefits that could be accrued from such durable networks. Originality/value - This study adds to the existing literature on social networks in China by highlighting the path-dependency and institutional legacy in the formation of business networks during China's transition towards a market economy.", "journal": "CHINESE MANAGEMENT STUDIES", "category": "Management", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000326025000004", "keywords": "Bahamian Creole English; Bahamian Creole; computer-mediated communication; CMC; corpus linguistics; standardisation; lexicography; orthography", "title": "Bey or bouy Orthographic patterns in Bahamian Creole English on the web", "abstract": "This paper is the first study of the orthographic patterns of speakers of Bahamian Creole English (BCE) when attempting to write their language in online environments. For the study, a corpus of 2.5 million words was retrieved from the online forum site <www.bahamasissues.com>. Corpus-linguistic software packages were used to determine keywords, concordances, and token frequencies. The study finds that there exists evidence of a non-codified common core of spellings in BCE, a pattern that has not up to now been described in an academic publication. The piece has implications for future lexicographic and orthographic studies of BCE.", "journal": "ENGLISH WORLD-WIDE", "category": "Linguistics; Language & Linguistics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000315566000009", "keywords": "maintenance hemodialysis; nail disorders; onychomycosis", "title": "Polyethylene glycol bowel preparation does not eliminate the risk of acute renal failure: a population-based case-crossover study", "abstract": "Background and study aims: Polyethylene glycol (PEG) bowel preparations are regarded as effective and safe for colonoscopy; however, recent reports have indicated a risk of acute renal failure (ARF). This population-based case-crossover study evaluated the association between PEG and ARF in screening colonoscopy patients aged >= 50 years. Patients and methods: Korean Health Insurance Review and Assessment Service (HIRA) claims data from 1 January 2005 to 31 December 2009 were used in the study. The study population consisted of patients aged >= 50 years who were first hospitalized for ARF following colonoscopy involving PEG bowel preparation. For each patient, PEG use in a 1-, 2-, or 4-week period prior to the first hospital admission date for ARF (hazard period) was compared with PEG use in four earlier 1-, 2-, or 4-week control periods. Conditional logistic regression analysis was used to estimate odds ratios (ORs) and 95% confidence intervals (CIs), adjusting for concomitant medications that could induce ARF. Results: The total number of study patients was 1064 (59% were male). A greater proportion of patients used PEG during the hazard period than during the control periods (for 4-week time window: 8.8% vs. 3.2%). The adjusted ORs for ARF incidence when applying the 1-, 2-, and 4-week periods were 3.1 (95%CI 2.06-4.73), 2.5 (95%CI 1.76-3.53), and 2.1 (95%CI 1.61-4.85), respectively. Conclusions: The use of PEG was associated with the risk of ARF. Adequate hydration and renal function monitoring should be assured before and after colonoscopy, regardless of the bowel preparation regimen used.", "journal": "ENDOSCOPY", "category": "Gastroenterology & Hepatology; Surgery", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000322838900080", "keywords": "Mango; Microsatellite markers; Genetic diversity; Population structure", "title": "Genome-Wide Association Analysis of Aluminum Tolerance in Cultivated and Tibetan Wild Barley", "abstract": "Tibetan wild barley (Hordeum vulgare L. ssp. spontaneum), originated and grown in harsh enviroment in Tibet, is well-known for its rich germpalsm with high tolerance to abiotic stresses. However, the genetic variation and genes involved in Al tolerance are not totally known for the wild barley. In this study, a genome-wide association analysis (GWAS) was performed by using four root parameters related with Al tolerance and 469 DArT markers on 7 chromosomes within or across 110 Tibetan wild accessions and 56 cultivated cultivars. Population structure and cluster analysis revealed that a wide genetic diversity was present in Tibetan wild barley. Linkage disequilibrium (LD) decayed more rapidly in Tibetan wild barley (9.30 cM) than cultivated barley (11.52 cM), indicating that GWAS may provide higher resolution in the Tibetan group. Two novel Tibetan group-specific loci, bpb-9458 and bpb-8524 were identified, which were associated with relative longest root growth (RLRG), located at 2H and 7H on barely genome, and could explain 12.9% and 9.7% of the phenotypic variation, respectively. Moreover, a common locus bpb-6949, localized 0.8 cM away from a candidate gene HvMATE, was detected in both wild and cultivated barleys, and showed significant association with total root growth (TRG). The present study highlights that Tibetan wild barley could provide elite germplasm novel genes for barley Al-tolerant improvement.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000317290900002", "keywords": "Nonverbal learning disabilities; Visuospatial working memory; Intuitive geometry", "title": "Intuitive geometry and visuospatial working memory in children showing symptoms of nonverbal learning disabilities", "abstract": "Visuospatial working memory (VSWM) and intuitive geometry were examined in two groups aged 1113, one with children displaying symptoms of nonverbal learning disability (NLD; n=16), and the other, a control group without learning disabilities (n=16). The two groups were matched for general verbal abilities, age, gender, and socioeconomic level. The children were presented with simple storage and complex-span tasks involving VSWM and with the intuitive geometry task devised by Dehaene, Izard, Pica, and Spelke (2006). Results revealed that the two groups differed in the intuitive geometry task. Differences were particularly evident in Euclidean geometry and in geometrical transformations. Moreover, the performance of NLD children was worse than controls to a larger extent in complex-span than in simple storage tasks, and VSWM differences were able to account for group differences in geometry. Finally, a discriminant function analysis confirmed the crucial role of complex-span tasks involving VSWM in distinguishing between the two groups. Results are discussed with reference to the relationship between VSWM and mathematics difficulties in nonverbal learning disabilities.", "journal": "CHILD NEUROPSYCHOLOGY", "category": "Clinical Neurology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000316857500028", "keywords": "condition number; information matrix; K-optimal design; polynomial regression model; semidefinite programming; symmetric design", "title": "MINIMIZING THE CONDITION NUMBER TO CONSTRUCT DESIGN POINTS FOR POLYNOMIAL REGRESSION MODELS", "abstract": "In this paper we study a new optimality criterion, the K-optimality criterion, for constructing optimal experimental designs for polynomial regression models. We focus on the pth order polynomial regression model with symmetric design space [-1, 1]. For this model, we show that there is always a symmetric K-optimal design with exactly p + 1 support points including the boundary points -1 and 1. It is well known that the condition number for a positive definite matrix as the ratio of the maximum eigenvalue to the minimum eigenvalue is usually nonsmooth. We show that for our model, the condition number of the information matrix is continuously differentiable. Theoretical K-optimal designs are derived for p = 1 and 2. Numerical results are presented for 3 <= p <= 10.", "journal": "SIAM JOURNAL ON OPTIMIZATION", "category": "Mathematics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000314700600009", "keywords": "One-repetition maximum test; isokinetic dynamometer; correlation; regression analysis", "title": "One-repetition maximum test and isokinetic leg extension and flexion: Correlations and predicted values", "abstract": "The most typical maximum tests for measuring leg muscle performance are the one-repetition maximum leg press test (1RMleg) and the isokinetic knee extension/flexion (IKEF) test. Nevertheless, their inter-correlations have not been well documented, mainly the predicted values of these evaluations. This correlational and regression analysis study involved 30 healthy young males aged 18-24y, who have performed both tests. Pearson's product moment correlation between 1RMleg and IKEF varied from 0.20 to 0.69 and the more exact predicted test was to 1RMleg (R-2 = 0.71). The study showed correlations between 1RMleg and IKEF although these tests are different (isotonic vs. isokinetic) and provided further support for cross determination of 1RMleg and IKEF by linear and multiple linear regression analysis.", "journal": "ISOKINETICS AND EXERCISE SCIENCE", "category": "Engineering, Biomedical; Orthopedics; Sport Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000314488300004", "keywords": "Variable reduction; Partial least squares; Predictor-variable properties; Final complexity adapted models", "title": "Predictive-property-ranked variable reduction in partial least squares modelling with final complexity adapted models: Comparison of properties for ranking", "abstract": "The calibration performance of partial least squares regression for one response (PLS1) can be improved by eliminating uninformative variables. Many variable-reduction methods are based on so-called predictor-variable properties or predictive properties, which are functions of various PLS-model parameters, and which may change during the steps of the variable-reduction process. Recently, a new predictive-property-ranked variable reduction method with final complexity adapted models, denoted as PPRVR-FCAM or simply FCAM, was introduced. It is a backward variable elimination method applied on the predictive-property-ranked variables. The variable number is first reduced, with constant PLS1 model complexity A, until A variables remain, followed by a further decrease in PLS complexity, allowing the final selection of small numbers of variables. In this study for three data sets the utility and effectiveness of six individual and nine combined predictor-variable properties are investigated, when used in the FCAM method. The individual properties include the absolute value of the PLS1 regression coefficient (REG), the significance of the PLS1 regression coefficient (SIG), the norm of the loading weight (NLW) vector, the variable importance in the projection (VIP), the selectivity ratio (SR), and the squared correlation coefficient of a predictor variable with the response y (COR). The selective and predictive performances of the models resulting from the use of these properties are statistically compared using the one-tailed Wilcoxon signed rank test. The results indicate that the models, resulting from variable reduction with the FCAM method, using individual or combined properties, have similar or better predictive abilities than the full spectrum models. After mean-centring of the data, REG and SIG, provide low numbers of informative variables, with a meaning relevant to the response, and lower than the other individual properties, while the predictive abilities are similar or better. SIG has the best selective ability of all individual and combined properties, while the predictive ability is similar. REG is faster than SIG. This means that variable reduction with the FCAM method is preferably conducted with properties REG or SIG. The selective ability of REG can be improved by combining it with NLW or VIP. (C) 2012 Elsevier B.V. All rights reserved.", "journal": "ANALYTICA CHIMICA ACTA", "category": "Chemistry, Analytical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000313992800014", "keywords": "Dopamine; MRI; magnetic transfer contrast; nonhuman primate; substantia nigra; T2", "title": "Magnetic Transfer Contrast Accurately Localizes Substantia Nigra Confirmed by Histology", "abstract": "Background: Magnetic resonance imaging (MRI) has multiple contrast mechanisms. Like various staining techniques in histology, each contrast type reveals different information about the structure of the brain. However, it is not always clear how structures visible in MRI correspond to structures previously identified by histology. The purpose of this study was to determine if magnetic transfer contrast (MTC) or T-2 contrast MRI was better at delineating the substantia nigra (SN). Methods: MRI scans were acquired in vivo from two nonhuman primates (NHPs). The NHPs were subsequently euthanized, perfused, and their brains sectioned for histologic analyses. Each slice was photographed before sectioning. Each brain was sectioned into approximately 500 sections, 40 mu m each, encompassing most of the cortex, midbrain, and dorsal parts of the hindbrain. Levels corresponding to anatomic MRI images were selected. From these, adjacent sections were stained using Kluver-Barrera (myelin and cell bodies) or tyrosine hydroxylase (dopaminergic neurons) immunohistochemistry. The resulting images were coregistered to the block-face images using a moving least squares algorithm with similarity transformations. MR images were similarly coregistered to the block-face images, allowing the structures on MRI to be identified with structures on the histologic images. Results: We found that hyperintense (light) areas in MTC images were coextensive with the SN as delineated histologically. The hypointense (dark) areas in T-2-weighted images were not coextensive with the SN but extended partially into the SN and partially into the cerebral peduncles. Conclusions: MTC is more accurate than T-2-weighting for localizing the SN in vivo.", "journal": "BIOLOGICAL PSYCHIATRY", "category": "Neurosciences; Psychiatry", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000313461900008", "keywords": "cancer survivorship; psycho-oncology; return to work; RTW", "title": "Predictors of employment among cancer survivors after medical rehabilitation - a prospective study", "abstract": "Mehnert A, Koch U. Predictors of employment among cancer survivors after medical rehabilitation a prospective study. Scand J Work Environ Health. 2013;39(1):76-87. doi:10.5271/sjweh.3291 Objectives This study aimed to (i) investigate cancer survivor's employment status one year after the completion of a medical rehabilitation program and (ii) identify demographic, cancer, and psychosocial, treatment-, and work-related predictors of return to work (RTW) and time until RTW. Methods A total of 1520 eligible patients were consecutively recruited on average 11 months post diagnosis and assessed at the beginning (t(0)) (N=1148) and end of rehabilitation (t(1)) (N=1060) and 12 months after rehabilitation (t(2)) (N=750). Participants completed validated measures assessing functional impairments, pain, anxiety, depression, quality of life, social support, and work-related characteristics including work ability, sick leave absence, job requirements, work satisfaction, self-perceived employer accommodation, and perceived job loss. Physicians estimated the degree of cancer-entity-specific functional impairment. Results In a mean time of six weeks after rehabilitation, 568 patients (76%) had returned to work. The multivariate hierarchical logistic regression analysis indicated that baseline RTW intention [odds ratio (OR) 6.22, 95% confidence interval (95% CI) 1.98-19.51], perceived employer accommodation (OR 1.93, 95% CI 0.33-0.99), high job requirements (OR=1.84, 95% CI 1.02-3.30), cancer recurrence or progression (OR=0.27, 95% CI 0.12 0.63), baseline sick leave absence (OR=0.26, 95% CI 0.09-0.77), and problematic social interactions (OR=0.58, 95% CI 0.33-0.99) emerged as significant predictors for RTW. The explained variance of the total model was Nagelkerke's R-2=0.59 (P<0.001). Conclusion Our findings emphasize the high relevance of motivational factors. Occupational motivation and skepticism towards returning to work should be carefully assessed at the planning of the rehabilitation program.", "journal": "SCANDINAVIAN JOURNAL OF WORK ENVIRONMENT & HEALTH", "category": "Public, Environmental & Occupational Health", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000326586000008", "keywords": "Bergman projection; irregularity; Forelli-Rudin formula", "title": "L-p REGULARITY OF WEIGHTED BERGMAN PROJECTIONS", "abstract": "We investigate L-p regularity of weighted Bergman projections on the unit disc and L-p regularity of ordinary Bergman projections in higher dimensions.", "journal": "TRANSACTIONS OF THE AMERICAN MATHEMATICAL SOCIETY", "category": "Mathematics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000320958500001", "keywords": "Smoking; Roll-your-own cigarettes; Factory-made cigarettes; South Africa; Self-efficacy; Quitting", "title": "'Roll-your-own' cigarette smoking in South Africa between 2007 and 2010", "abstract": "Background: The prevalence of smoking and consumption of cigarettes have decreased in South Africa over the last 20 years. This decrease is a result of comprehensive tobacco control legislation, particularly large cigarette tax increases. However, little attention has been given to the potential use of 'roll-your-own' cigarettes as cheaper alternatives, especially among the socio-economically disadvantaged population. This study therefore sought to determine socio-demographic correlates of 'roll-your-own' cigarette use among South African adults (2007-2010). Methods: This secondary data analysis used a merged dataset from two nationally representative samples of 2 907 and 3 112 South African adults (aged >= 16 years) who participated in the 2007 and 2010 annual South African Social Attitude Surveys respectively. The surveys used a face-to-face interviewer-administered questionnaire. The overall response rates were 83.1% for 2007 and 88.9% for 2010. Data elicited included socio-demographic data, current smoking status, type of tobacco products used, past quit attempts and self-efficacy in quitting. Data analysis included chi-square statistics and multi-variable adjusted logistic regression analysis. Results: Of the 1 296 current smokers in this study, 24.1% (n = 306) reported using roll-your-own cigarettes. Some of whom also smoked factory-made cigarettes. Roll-your-own cigarette smoking was most common among black Africans and was more common among male smokers than among female smokers (27% vs 15.8%; p < 0.01). Compared to smokers who exclusively used factory-made cigarettes, roll-your-own cigarette smokers were less confident that they could quit, more likely to be less educated, and more likely to reside in rural areas. The odds of use of roll-your-own cigarette were significantly higher in 2010 than in 2007 (OR = 1.24; 95% CI: 1.07-1.44). Conclusions: Despite an aggregate decline in smoking prevalence, roll-your-own cigarette smoking has increased and is particularly common among smokers in the lower socio-economic group. The findings also suggest the need for a more intensive treatment intervention to increase self-efficacy to quit among roll-your-own cigarette smokers.", "journal": "BMC PUBLIC HEALTH", "category": "Public, Environmental & Occupational Health", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000323623700004", "keywords": "Dini series expansion; high-order Hankel transform; beam propagation", "title": "High order Hankel transform based on Dini expansion and its applications in beam propagation", "abstract": "Based on Dini series expansion, p(>0) order quasi discrete Hankel transform (pDQDHT) algorithm is deduced. The application of this algorithm in beam propagation is presented. pDQDHT algorithm is tested with various input functions and used in beam propagation through lens. Experimental results show that the pDQDHT algorithm possesses a high accuracy compared with existing Hankel transformation algorithms. The pDQDHT algorithm can be transformed forwardly and inversely and widely used in beam distribution of transmission. The implementing speed is comparable to that of general fast Hankel transform algorithm.", "journal": "ACTA PHYSICA SINICA", "category": "Physics, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000317794000002", "keywords": "Shrimp; Crustacean; Lectin; Innate immunity; Pattern recognition", "title": "Pole assignment for control of flexible link mechanisms", "abstract": "Although the dynamics of flexible link mechanisms and manipulators is nonlinear, motion and vibration control often relies on linear or piecewise-linear controllers based on linearized models in order to ensure real-time implementability. Keeping such an objective in mind, this paper proposes a general receptance-based method for pole assignment in flexible link mechanisms with a single rigid-body degree of freedom (dof) using a single control force (i.e. rank-one control). A chief advantage of the approach proposed is that it makes use of the second-order system model representation through the receptance matrix of the symmetric part of the asymmetric model. The asymmetric terms in the stiffness and damping matrices arise from the coupling between rigid-body motion and elastic motion. The proposed receptance-based formulation ensures numerical reliability and efficiency also for large dimensional and ill-conditioned system models originating from the simultaneous presence of high-frequency and weakly controllable oscillating modes, and of rigid-body motion low-frequency dynamics, which may also be unstable. The validation of the proposed technique is carried out by performing pole assignment through position and velocity feedback or acceleration and velocity feedback on a mechanism. Integral control is also introduced to improve the steady state system response. Numerical results indicate that the proposed method is more accurate and robust than two popular established methods. (c) 2013 Elsevier Ltd. All rights reserved.", "journal": "JOURNAL OF SOUND AND VIBRATION", "category": "Acoustics; Engineering, Mechanical; Mechanics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000320604100004", "keywords": "Mitochondria; Spectroscopy; Electron transport chain; Ischemia/reperfusion", "title": "Application of delta recycling to electron automated diffraction tomography data from inorganic crystalline nanovolumes", "abstract": "delta Recycling is a simple procedure for directly extracting phase information from Patterson-type functions [Rius (2012). Acta Cryst. A68, 399-400]. This new phasing method has a clear theoretical basis and was developed with ideal single-crystal X-ray diffraction data. On the other hand, introduction of the automated diffraction tomography (ADT) technique has represented a significant advance in electron diffraction data collection [Kolb et al. (2007). Ultramicroscopy, 107, 507-513]. When combined with precession electron diffraction, it delivers quasi-kinematical intensity data even for complex inorganic compounds, so that single-crystal diffraction data of nanometric volumes are now available for structure determination by direct methods. To check the tolerance of delta recycling to missing data-collection corrections and to deviations from kinematical behaviour of ADT intensities, delta recycling has been applied to differently shaped nanocrystals of various inorganic materials. The results confirm that it can phase ADT data very efficiently. In some cases even more complete structure models than those derived from conventional direct methods and least-squares refinement have been found. During this study it has been demonstrated that the Wilson-plot scaling procedure is largely insensitive to sample thickness variations and missing absorption corrections affecting electron ADT intensities.", "journal": "ACTA CRYSTALLOGRAPHICA SECTION A", "category": "Chemistry, Multidisciplinary; Crystallography", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000314595600006", "keywords": "Distributed control; Gradient methods; Predictive control", "title": "Visual Data Mining of Biological Networks: One Size Does Not Fit All", "abstract": "High-throughput technologies produce massive amounts of data. However, individual methods yield data specific to the technique used and biological setup. The integration of such diverse data is necessary for the qualitative analysis of information relevant to hypotheses or discoveries. It is often useful to integrate these datasets using pathways and protein interaction networks to get a broader view of the experiment. The resulting network needs to be able to focus on either the large-scale picture or on the more detailed small-scale subsets, depending on the research question and goals. In this tutorial, we illustrate a workflow useful to integrate, analyze, and visualize data from different sources, and highlight important features of tools to support such analyses.", "journal": "PLOS COMPUTATIONAL BIOLOGY", "category": "Biochemical Research Methods; Mathematical & Computational Biology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000320138700006", "keywords": "Arisaema franchetianum; Arisaema lobatum; Essential oil; Haemonchus contortus; Anthelmintic activity", "title": "Leaky least mean fourth adaptive algorithm", "abstract": "In this work, a leakage-based variant of the least mean fourth (LMF) algorithm, the leaky least mean fourth (LLMF) algorithm, is proposed. This algorithm will help mitigate the weight drift problem experienced in the conventional LMF algorithm. The main aim of this work is to derive the LLMF adaptive algorithm, analyse its convergence behaviour, and examine its performance in different noise environments. Furthermore, the tracking and transient analysis of the proposed LLMF algorithm are carried out using the energy-conservation relation framework. Finally, a number of simulation results are carried out to corroborate the theoretical findings, and show improved performance obtained through the use of LLMF over the conventional LMF algorithm in a weight drift scenario.", "journal": "IET SIGNAL PROCESSING", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000319872300026", "keywords": "base-composition; Markov invariants; phylogenetic invariants; quartets; rate-variation; supertrees", "title": "Mortality Attributable to Seasonal and Pandemic Influenza, Australia, 2003 to 2009, Using a Novel Time Series Smoothing Approach", "abstract": "Background: Official statistics under-estimate influenza deaths. Time series methods allow the estimation of influenza-attributable mortality. The methods often model background, non-influenza mortality using a cyclic, harmonic regression model based on the Serfling approach. This approach assumes that the seasonal pattern of non-influenza mortality is the same each year, which may not always be accurate. Aim: To estimate Australian seasonal and pandemic influenza-attributable mortality from 2003 to 2009, and to assess a more flexible influenza mortality estimation approach. Methods: We used a semi-parametric generalized additive model (GAM) to replace the conventional seasonal harmonic terms with a smoothing spline of time ('spline model') to estimate influenza-attributable respiratory, respiratory and circulatory, and all-cause mortality in persons aged <65 and >= 65 years. Influenza A(H1N1)pdm09, seasonal influenza A and B virus laboratory detection time series were used as independent variables. Model fit and estimates were compared with those of a harmonic model. Results: Compared with the harmonic model, the spline model improved model fit by up to 20%. In <65 year-olds, the estimated respiratory mortality attributable to pandemic influenza A(H1N1)pdm09 was 0.5 (95% confidence interval (CI), 0.3, 0.7) per 100,000; similar to that of the years with the highest seasonal influenza A mortality, 2003 and 2007 (A/H3N2 years). In >= 65 year-olds, the highest annual seasonal influenza A mortality estimate was 25.8 (95% CI 22.2, 29.5) per 100,000 in 2003, five-fold higher than the non-statistically significant 2009 pandemic influenza estimate in that age group. Seasonal influenza B mortality estimates were negligible. Conclusions: The spline model achieved a better model fit. The study provides additional evidence that seasonal influenza, particularly A/H3N2, remains an important cause of mortality in Australia and that the epidemic of pandemic influenza A (H1N1)pdm09 virus in 2009 did not result in mortality greater than seasonal A/H3N2 influenza mortality, even in younger age groups.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000320495800018", "keywords": "Thermal control; energy minimization; multicore; model predictive controller; system identification", "title": "Applications of high-resolution MS in bioanalysis", "abstract": "High-resolution MS (HRMS) in conjunction with LC (LC-HRMS) has become available to many laboratories in the pharmaceutical industry. Due to its enhanced, though sometime perceived, specificity using the high-resolution power and its capability of simultaneous quantitation and structural elucidation using the post-acquisition data mining feature, utilization of LC-HRMS for bioanalysis could lead to potential rapid and reliable method development as well as sample analysis, thus generating both cost and resource savings. Here, we would like to share our perspectives about several current and future applications of LC-HRMS in bioanalysis. We will also discuss the factors influencing the quality of method establishment and potential pitfalls that need to be considered for the utilization of LC-HRMS in the field of regulated bioanalysis. We believe when utilized appropriately, LC-HRMS will play a significant role in the future landscape of quantitative bioanalysis.", "journal": "BIOANALYSIS", "category": "Biochemical Research Methods; Chemistry, Analytical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000318032300017", "keywords": "petrochemical industry; industrial noise; industrial air pollution; annoyance; worry", "title": "Annoyance and Worry in a Petrochemical Industrial Area-Prevalence, Time Trends and Risk Indicators", "abstract": "In 1992, 1998, and 2006, questionnaires were sent to stratified samples of residents aged 18-75 years living near petrochemical industries (n = 600-800 people on each occasion) and in a control area (n = 200-1,000). The aims were to estimate the long-term prevalence and change over time of annoyance caused by industrial odour, industrial noise, and worries about possible health effects, and to identify risk indicators. In 2006, 20% were annoyed by industrial odour, 27% by industrial noise (1-4% in the control area), and 40-50% were worried about health effects or industrial accidents (10-20% in the control area). Multiple logistic regression analyses revealed significantly lower prevalence of odour annoyance in 1998 and 2006 than in 1992, while industrial noise annoyance increased significantly over time. The prevalence of worry remained constant. Risk of odour annoyance increased with female sex, worry of health effects, annoyance by motor vehicle exhausts and industrial noise. Industrial noise annoyance was associated with traffic noise annoyance and worry of health effects of traffic. Health-risk worry due to industrial air pollution was associated with female sex, having children, annoyance due to dust/soot in the air, and worry of traffic air pollution.", "journal": "INTERNATIONAL JOURNAL OF ENVIRONMENTAL RESEARCH AND PUBLIC HEALTH", "category": "Environmental Sciences; Public, Environmental & Occupational Health", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000315976100008", "keywords": "Self-pruning; Fault tolerance; Broadcasting storm; Minimum connected dominated set; Ad hoc network", "title": "A Reachable and Fault Tolerant Scheme for Broadcast in Ad-Hoc Wireless Networks", "abstract": "In ad hoc wireless network, nodes can communicate with each other through broadcasting. Nevertheless, such an ad hoc wireless network has many unstable factors causing the transmission being unreliable. Thus, the broadcasting algorithm needs fault tolerant capability to solve the possible failure of broadcasting process. In this paper, we propose self-pruning-with-fault-tolerant (SPFT) algorithm to achieve fault tolerance and reachable capability. By using the self-pruning approach, SPFT algorithm can reduce the redundant transmissions. In addition, SPFT algorithm selects the stable links in transmission to provide the better success ratio of transmission as well as uses the comparison mechanism to deal with the failure transmissions. By this way, fault tolerance degree can be increased. According to the experimental results, our algorithm has the fault-tolerant capability and offers the benefit of efficiency and reliability for broadcasting in ad hoc wireless network.", "journal": "JOURNAL OF INTERNET TECHNOLOGY", "category": "Computer Science, Information Systems; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000322339300006", "keywords": "breast cancer; neoadjuvant; HER2; surgery; breast conservation; trastuzumab", "title": "Factors associated with surgical management following neoadjuvant therapy in patients with primary HER2-positive breast cancer: results from the NeoALTTO phase III trial", "abstract": "The NeoALTTO trial showed that dual HER2 blockade nearly doubles the rate of pathologic complete response (pCR) in patients with primary HER2-positive breast cancer. However, this did not translate into a higher rate of breast-conserving surgery (BCS). In NeoALTTO, patients with HER2-positive breast cancer were randomly assigned to either trastuzumab, lapatinib or their combination with paclitaxel before surgery with pCR as the primary end point. We investigated the association between the surgery type and clinicopathological factors and response to treatment, adjusting for the treatment arm. Four hundred and twenty-nine patients were subjected to breast surgery. Two hundred and forty-two (56%) and 187 (44%) patients underwent mastectomy and BCS, respectively. In a logistic regression model, negative estrogen receptor (ER), multicentricity and the presence of a palpable mass before surgery were significantly associated with a low chance of BCS. Conversely, patients with small tumors and those eligible for BCS at diagnosis were managed more with BCS, independent of the treatment arm. Radiological response was not associated with the surgical decision. Tumor characteristics before neoadjuvant therapy play a main role in deciding the type of surgery calling for a clear consensus on the role of BCS in patients responding to neoadjuvant therapy.", "journal": "ANNALS OF ONCOLOGY", "category": "Oncology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000318760600001", "keywords": "Transport equation; Numerical methods; Integral operators", "title": "A Computational Method for Two-Point Boundary Value Problems of Fourth-Order Mixed Integrodifferential Equations", "abstract": "In this paper, reproducing kernel Hilbert space method is applied to approximate the solution of two-point boundary value problems for fourth-order Fredholm-Volterra integrodifferential equations. The analytical solution was calculated in the form of convergent series in the space W-2(5)[a, b] with easily computable components. In the proposed method, the n-term approximation is obtained and is proved to converge to the analytical solution. Meanwhile, the error of the approximate solution is monotone decreasing in the sense of the norm of W-2(5)[a, b]. The proposed technique is applied to several examples to illustrate the accuracy, efficiency, and applicability of the method.", "journal": "MATHEMATICAL PROBLEMS IN ENGINEERING", "category": "Engineering, Multidisciplinary; Mathematics, Interdisciplinary Applications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000316610700084", "keywords": "Biomarkers; early diagnosis; metabolites; metabolomics; obesity", "title": "Macrophage Scavenger Receptor A Promotes Tumor Progression in Murine Models of Ovarian and Pancreatic Cancer", "abstract": "Alternatively activated macrophages express the pattern recognition receptor scavenger receptor A (SR-A). We demonstrated previously that coculture of macrophages with tumor cells upregulates macrophage SR-A expression. We show in this study that macrophage SR-A deficiency inhibits tumor cell migration in a coculture assay. We further demonstrate that coculture of tumor-associated macrophages and tumor cells induces secretion of factors that are recognized by SR-A on tumor-associated macrophages. We tentatively identified several potential ligands for the SR-A receptor in tumor cell-macrophage cocultures by mass spectrometry. Competing with the coculture-induced ligand in our invasion assay recapitulates SR-A deficiency and leads to similar inhibition of tumor cell invasion. In line with our in vitro findings, tumor progression and metastasis are inhibited in SR-A(-/-) mice in two in vivo models of ovarian and pancreatic cancer. Finally, treatment of tumor-bearing mice with 4F, a small peptide SR-A ligand able to compete with physiological SR-A ligands in vitro, recapitulates the inhibition of tumor progression and metastasis observed in SR-A(-/-) mice. Our observations suggest that SR-A may be a potential drug target in the prevention of metastatic cancer progression. The Journal of Immunology, 2013, 190: 3798-3805.", "journal": "JOURNAL OF IMMUNOLOGY", "category": "Immunology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000316649300005", "keywords": "Speech perception; Speech production; Sensorimotor interactions; Phonetic features; Mirror system; fMRI; Sparse sampling", "title": "Shared and distinct neural correlates of vowel perception and production", "abstract": "Recent neurobiological models postulate that sensorimotor interactions play a key role in speech perception and speech motor control, especially under adverse listening conditions or in case of complex articulatory speech sequences. The present fMRI study aimed to investigate whether isolated vowel perception and production might also induce sensorimotor activity, independently of syllable sequencing and coarticulation mechanisms and using a sparse acquisition technique in order to limit influence of scanner noise. To this aim, participants first passively listened to French vowels previously recorded from their own voice. In a subsequent production task, done within the same imaging session and using the same acquisition parameters, participants were asked to overtly produce the same vowels. Our results demonstrate that a left postero-dorsal stream, linking auditory speech percepts with articulatory representations and including the posterior inferior frontal gyrus, the adjacent ventral premotor cortex and the temporoparietal junction, is an influential part of both vowel perception and production. Specific analyses on phonetic features further confirmed the involvement of the left postero-dorsal stream in vowel processing and motor control. Altogether, these results suggest that vowel representations are largely distributed over sensorimotor brain areas and provide further evidence for a functional coupling between speech perception and production systems. (C) 2013 Elsevier Ltd. All rights reserved.", "journal": "JOURNAL OF NEUROLINGUISTICS", "category": "Linguistics; Neurosciences; Psychology, Experimental", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000319055600088", "keywords": "vitamin K antagonists; cancer; population based; casecontrol study; pharmacoepidemiology", "title": "Secondhand Smoke Exposure and Health Services Use among Adolescent Current Smokers", "abstract": "Background: To investigate the associations of secondhand smoke (SHS) exposure with medical consultation and hospitalisation among adolescents in Hong Kong. Methods: A total of 35827 secondary 1 (US grade 7) to secondary 5 students from 85 randomly selected schools completed an anonymous questionnaire on smoking, SHS at home, SHS outside home, medical consultation in the past 14 days, hospitalisation in the past 12 months, and socio-demographic characteristics. Current smoking was defined as any smoking in the past 30 days. SHS exposure was classified as none (reference), 1-4 and 5-7 days/week. Logistic regression yielded adjusted odds ratios (AORs) for medical consultation and hospitalisation in relation to SHS exposure at home and outside home in current smokers. Analyses were also done among never-smokers for comparison. Results: Among all students, 15.9% had medical consultation and 5.2% had been hospitalised. Any SHS exposure at home was associated with AORs (95% CI) for medical consultation and hospitalisation of 1.69 (1.14-2.51) and 2.85 (1.47-5.52) in current smokers, and 1.03 (0.91-1.15) and 1.25 (1.02-1.54) in never-smokers, respectively, (P < 0.01 for interaction between smoking status and SHS exposure at home). SHS exposure outside home was generally not associated with medical consultation and hospitalisation in smokers and never-smokers. Conclusions: SHS exposure at home was associated with health services use among adolescent current smokers. Adolescent smokers should be aware of the harm of SHS in addition to that from their own smoking. The evidence helps health care professionals to advise adolescent smokers to avoid SHS exposure and stop smoking.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000328384800001", "keywords": "Array signal processing; audio quality assessment; audio watermarking; informed audio source separation", "title": "An Efficient Approximate Algorithm for Disjoint QoS Routing", "abstract": "Disjoint routing is used to find the disjoint paths between a source and a destination subject to QoS requirements. Disjoint QoS routing is an effective strategy to achieve robustness, load balancing, congestion reduction, and an increased throughput in computer networks. For multiple additive constraints, disjoint QoS routing is an NP-complete class that cannot be exactly solved in polynomial time. In the paper, the disjoint QoS routing problem was formulated as a 0-1 integer linear programming. The complicating constraints were included in the objective function using an adaptive penalty function. The special model with a totally unimodular constraint coefficient matrix was constructed and could be solved rapidly as a linear programming. An efficient algorithm using an adaptive penalty function and 0-1 integer linear programming for the disjoint QoS routing problems was designed. The proposed algorithm could obtain the optimal solution, considerably reducing the computational time consumption and improving the computational efficiency. Theoretical analysis and simulation experiments were performed to evaluate the proposed algorithm performance. Through the establishment of random network topologies using Matlab, the average running time, the optimal objective value, and the success rate were evaluated based on the optimal values obtained in Cplex. The simulation experiments validated the effectiveness of the proposed heuristic algorithm.", "journal": "MATHEMATICAL PROBLEMS IN ENGINEERING", "category": "Engineering, Multidisciplinary; Mathematics, Interdisciplinary Applications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000320562100017", "keywords": "Complex-valued singular value decomposition (SVD); COordinate Rotation DIgital Computer (CORDIC); geometric mean decomposition (GMD); joint transceiver; multiple-input multiple-output (MIMO); precoding", "title": "A Constant Throughput Geometric Mean Decomposition Scheme Design for Wireless MIMO Precoding", "abstract": "The geometric mean decomposition (GMD) algorithm is a popular approach in developing a precoding scheme for joint multiple-input-multiple-output (MIMO) transceiver designs. In this paper, the adverse effects of conventional GMD algorithms on hardware implementations are first reviewed. Then, a constant throughput modified GMD algorithm is presented. The proposed GMD scheme is constructed on a QR decomposition framework and requires no singular value decomposition (SVD) preprocessing. The new scheme is exempt from the convergence problem, which may seriously degrade throughput performance. It also features lower computational complexity and permutation-free operations and supports hardware sharing between precoding and signal detection modules. Quantitative analysis shows that, under similar symbol-error-rate (SER) performance, the proposed scheme possesses a computational complexity edge over conventional schemes by a margin of 30%. The complexity breakdown indicates that the SVD nullification sweep is the dominant factor of the SVD-based GMD schemes. Even when the sweep number is set to twice the matrix size (2N), the implementation loss is still 0.5 dB inferior to the proposed scheme. Finally, an architecture design of the proposed scheme is given to demonstrate a constant throughput implementation and the feasibility of hardware sharing between precoding and signal detection modules.", "journal": "IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY", "category": "Engineering, Electrical & Electronic; Telecommunications; Transportation Science & Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000315440400031", "keywords": "Asclepias syriaca (common milkweed); cardenolides; partial least-squares regression (PLSR); phytochemical induction; spectroscopy", "title": "Spectroscopic sensitivity of real-time, rapidly induced phytochemical change in response to damage", "abstract": "An ecological consequence of plantherbivore interactions is the phytochemical induction of defenses in response to insect damage. Here, we used reflectance spectroscopy to characterize the foliar induction profile of cardenolides in Asclepias syriaca in response to damage, tracked invivo changes and examined the influence of multiple plant traits on cardenolide concentrations. Foliar cardenolide concentrations were measured at specific time points following damage to capture their induction profile. Partial least-squares regression (PLSR) modeling was employed to calibrate cardenolide concentrations to reflectance spectroscopy. In addition, subsets of plants were either repeatedly sampled to track invivo changes or modified to reduce latex flow to damaged areas. Cardenolide concentrations and the induction profile of A.syriaca were well predicted using models derived from reflectance spectroscopy, and this held true for repeatedly sampled plants. Correlations between cardenolides and other foliar-related variables were weak or not significant. Plant modification for latex reduction inhibited an induced cardenolide response. Our findings show that reflectance spectroscopy can characterize rapid phytochemical changes invivo. We used reflectance spectroscopy to identify the mechanisms behind the production of plant secondary metabolites, simultaneously characterizing multiple foliar constituents. In this case, cardenolide induction appears to be largely driven by enhanced latex delivery to leaves following damage.", "journal": "NEW PHYTOLOGIST", "category": "Plant Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000323202000002", "keywords": "fault tolerance; connectivity; spatial relativity; topology control", "title": "A topology control algorithm based on D-region fault tolerance", "abstract": "In a wireless network, node failure due to either natural disasters or human intervention can cause network partitioning and other communication problems. For this reason, a wireless network should be fault tolerant. At present, most researchers use k-connectivity to measure fault tolerance, which requires the network to be connected after the failure of any up to k-1 nodes. However, wireless network node failures are usually spatially related, and particularly in military applications, nodes from the same limited area can fail together. As a metric of fault-tolerance, k-connectivity fails to capture the spatial relativity of faults and hardly satisfies the fault tolerance requirements of a wireless network design. In this paper, a new metric of fault-tolerance, termed D-region fault tolerance, is introduced to measure wireless network fault tolerance. A D-region fault tolerant network means that even after all the nodes have failed in a circular region with diameter D, it still remains connected. Based on D-region fault tolerance, we propose two fault-tolerant topology control algorithms-the global region fault tolerance algorithm (GRFT) and the localized region fault tolerance algorithm (LRFT). It is theoretically proven that both algorithms are able to generate a network with D-region fault tolerance. Simulation results indicate that with the same fault tolerance capabilities, networks based on both GRFT and LRFT algorithms have a lower transmission radius and lower logical degree.", "journal": "SCIENCE CHINA-INFORMATION SCIENCES", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000318235300010", "keywords": "Korean; learner attitudes; learner variables; task-based language learning; task-based language teaching", "title": "Attitudes Toward Task-Based Language Learning: A Study of College Korean Language Learners", "abstract": "This study explores second/foreign language (L2) learners' attitudes toward task-based language learning (TBLL) and how these attitudes relate to selected learner variables, namely anxiety, integrated motivation, instrumental motivation, and self-efficacy. Ninety-one college students of Korean as a foreign language, who received task-based language instruction, participated in this questionnaire study. Data were analyzed using both descriptive and inferential statistics. A correlation analysis between variables indicated that students' attitudes toward TBLL were positively associated with self-efficacy and integrative motivation while they were negatively associated with anxiety. A multiple regression analysis further revealed that only one variable, self-efficacy, was the significant predictor of learners' attitudes toward TBLL.", "journal": "FOREIGN LANGUAGE ANNALS", "category": "Education & Educational Research; Linguistics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000319238800006", "keywords": "Stopping algorithm; Directed graph; Secretary problem", "title": "An efficient algorithm for stopping on a sink in a directed graph", "abstract": "Vertices of an unknown directed graph of order n are revealed one by one in some random permutation. At each point, we know the subgraph induced by the revealed vertices. Our goal is to stop on a sink, a vertex with no out-neighbors. We show that if a sink exists this can be achieved with probability Theta(1/root n), which is best possible. (C) 2013 Elsevier B.V. All rights reserved.", "journal": "OPERATIONS RESEARCH LETTERS", "category": "Operations Research & Management Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000319295000012", "keywords": "Epidemiology; Aging; Fat-free mass; Skeletal muscle; Skeletal muscle index", "title": "Prevalence of sarcopenia and associated factors among Thai population", "abstract": "The purpose of this study was to determine the prevalence of sarcopenia using the skeletal muscle index (SMI) criteria in the Thai population. The secondary objective was to demonstrate factors influencing low SMI in this population. Femoral neck bone mass density (BMD) was measured by dual-energy X-ray absorptiometry (GE Lunar, Madison, WI, USA) in 435 urban and 397 rural subjects (334 men and 498 women) between 20 and 84 years of age. Body mass index (BMI) was calculated from weight and height. The respective prevalence of sarcopenia among men and women was 35.33 % (95 % CI, 29.91, 40.41) and 34.74 % (95 % CI, 30.56, 39.10). Factors associated with sarcopenia using multiple logistic regression analyses in both sexes were (a) living in the city, (b) higher BMI, and (c) older age. Living in an urban area was the strongest factor, with an odds ratio (OR) of 17.26 +/- A 7.12 (95 % CI, 7.68, 38.76) in men and 8.62 +/- A 2.74 (95 % CI, 4.62, 16.05) in women (p < 0.05). The prevalence rate ratio for persons living in urban compared to rural areas was 2.01 (95 % CI, 1.14, 3.53) in men and 1.69 (95 % CI, 1.31, 2.17) in women (p < 0.05). Sarcopenia, as based on SMI, occurs frequently in the Thai population and increases with age. The prevalence of sarcopenia is particularly high among pre-retirement women (50-59 years of age) whereas the number of men with sarcopenia gradually rises with age. An urban environment is the most predictive factor for sarcopenia, followed by high BMI and age. Given the aging population, early recognition of this condition can be beneficial for prevention of an epidemic of sarcopenia-related disability.", "journal": "JOURNAL OF BONE AND MINERAL METABOLISM", "category": "Endocrinology & Metabolism; Medicine, Research & Experimental", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000318036400006", "keywords": "CO2 laser machining; optical fiber sensor; refractive index sensing", "title": "Fabrication Quality Analysis of a Fiber Optic Refractive Index Sensor Created by CO2 Laser Machining", "abstract": "This study investigates the CO2 laser stripped partial cladding of silica based optic fibers with a core diameter of 400 mu m, which enables them to sense the refractive index of the surrounding environment. However, inappropriate treatments during the machining process can generate a number of defects in the optic fiber sensors. Therefore, the quality of optic fiber sensors fabricated using CO2 laser machining must be analyzed. The results show that analysis of the fiber core size after machining can provide preliminary defect detection, and qualitative analysis of the optical transmission defects can be used to identify imperfections that are difficult to observe through size analysis. To more precisely and quantitatively detect fabrication defects, we included a tensile test and numerical aperture measurements in this study. After a series of quality inspections, we proposed improvements to the existing CO2 laser machining parameters, namely, a vertical scanning pathway, 4 W of power, and a feed rate of 9.45 cm/s. Using these improved parameters, we created optical fiber sensors with a core diameter of approximately 400 mu m, no obvious optical transmission defects, a numerical aperture of 0.52 +/- 0.019, a 0.886 Weibull modulus, and a 1.186 Weibull-shaped parameter. Finally, we used the optical fiber sensor fabricated using the improved parameters to measure the refractive indices of various solutions. The results show that a refractive-index resolution of 1.8 x 10(-4) RIU (linear fitting R-2 = 0.954) was achieved for sucrose solutions with refractive indices ranging between 1.333 and 1.383. We also adopted the particle plasmon resonance sensing scheme using the fabricated optical fibers. The results provided additional information, specifically, a superior sensor resolution of 5.73 x 10(-5) RIU, and greater linearity at R-2 = 0.999.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000321411000005", "keywords": "regulated high-voltage power supply; multiple-secondary transformer; transformer protection system", "title": "FAULT PROTECTION AND OVERLOAD DIAGNOSIS IN A REGULATED HIGH-VOLTAGE POWER SUPPLY", "abstract": "Regulated high-voltage power supplies (RHVPSs) have been developed at Institute for Plasma Research and utilized for neutral beam and radio-frequency heating applications of the steady-state superconducting tokamak (SST-1) up to 80-kV, 130-A rating. They were developed in-house and are being delivered to different research institutes for various applications. The RHVPS delivers power to various loads at the megawatt level. These loads have very low fault energy tolerance; therefore, fault protection is mandatory. In addition to this, at each stage of the power transformation/conversion, a special diagnosis is necessary to protect the power supply components. Also, the output fault protection has to be done in such a manner that fault energy is not more than 10 J. In fault conditions, the output has to be turned off within 2 mu s. Having these requirements, an output fault-protection system has been developed with suitable sensors and to manage fast turn off choosing appropriate components. The multiple-secondary transformers (two of them, each at a 5.6 MV.A rating with 40 outputs) are used at the front end of the RHVPS. They may become damaged for overload at any one of their secondaries, while remaining secondaries carry much less current or no current. Such a localized overload is not sufficient for tripping the main circuit breaker, whose tripping level is set to an actual overload of the transformer. A special technique is applied to sense and diagnose this fault in addition to routine overload sensing. Differentiation of such a typical fault from a real overload condition is done by sensing and monitoring the primary current of the transformer with reference to different operating scenarios. Electronic means are used for fast detection and isolation of the RHVPS from the utility supply. The presented system effectively protects the transformer from fault at any one of its 40 secondaries and in an actual overload situation. This paper describes an overall RHVPS power scheme along with output fault protection and an internal fault diagnosis system and test results thereof", "journal": "FUSION SCIENCE AND TECHNOLOGY", "category": "Nuclear Science & Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000313253100014", "keywords": "Federally qualified health centers; community health centers; safety net; residential segregation; geographic; spatial factors", "title": "Community Residential Segregation and the Local Supply of Federally Qualified Health Centers", "abstract": "Objective To examine associations between community residential segregation by income and race/ethnicity, and the supply of federally qualified health centers (FQHCs) in urban areas. Data Sources and Study Setting Area Resource File (20002007) linked with 2000 U.S. Census on U.S. metropolitan counties (N = 1,786). Study Design We used logistic and negative binomial regression models with state-level fixed effects to examine how county-level characteristics in 2000 are associated with the presence of FQHCs in 2000, and with the increase in FQHCs from 2000 to 2007. Income and racial/ethnic residential segregation were measured by poverty and the non-white dissimilarity indices, respectively. Covariates included measures of federal criteria for medically underserved areas/populations. Principal Findings Counties with a high non-white dissimilarity index and a high percentage of minorities were more likely to have an FQHC in 2000. When we examined the addition of new FQHCs from 2000 to 2007, the effects of both poverty and non-white dissimilarity indices were positive and significant. Conclusions Residential segregation likely produces geographic segregation of health services, such that provider maldistribution may explain the association between residential segregation and FQHC supply. Metropolitan areas that fail to achieve greater integration of poor and minority communities may require FQHCs to compensate for provider shortages.", "journal": "HEALTH SERVICES RESEARCH", "category": "Health Care Sciences & Services; Health Policy & Services", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000316396100001", "keywords": "In silico drug research; Semantic technologies; Text mining; Biomedical ontologies; Discovery of novel relationships", "title": "Discovery of novel biomarkers and phenotypes by semantic technologies", "abstract": "Background: Biomarkers and target-specific phenotypes are important to targeted drug design and individualized medicine, thus constituting an important aspect of modern pharmaceutical research and development. More and more, the discovery of relevant biomarkers is aided by in silico techniques based on applying data mining and computational chemistry on large molecular databases. However, there is an even larger source of valuable information available that can potentially be tapped for such discoveries: repositories constituted by research documents. Results: This paper reports on a pilot experiment to discover potential novel biomarkers and phenotypes for diabetes and obesity by self-organized text mining of about 120,000 PubMed abstracts, public clinical trial summaries, and internal Merck research documents. These documents were directly analyzed by the InfoCodex semantic engine, without prior human manipulations such as parsing. Recall and precision against established, but different benchmarks lie in ranges up to 30% and 50% respectively. Retrieval of known entities missed by other traditional approaches could be demonstrated. Finally, the InfoCodex semantic engine was shown to discover new diabetes and obesity biomarkers and phenotypes. Amongst these were many interesting candidates with a high potential, although noticeable noise (uninteresting or obvious terms) was generated. Conclusions: The reported approach of employing autonomous self-organising semantic engines to aid biomarker discovery, supplemented by appropriate manual curation processes, shows promise and has potential to impact, conservatively, a faster alternative to vocabulary processes dependent on humans having to read and analyze all the texts. More optimistically, it could impact pharmaceutical research, for example to shorten time-to-market of novel drugs, or speed up early recognition of dead ends and adverse reactions.", "journal": "BMC BIOINFORMATICS", "category": "Biochemical Research Methods; Biotechnology & Applied Microbiology; Mathematical & Computational Biology", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000316483000010", "keywords": "Bidirectional mapping; Parameter adaptive; Congestion awareness; Cross-layer; Wireless video transmission", "title": "PABM-EDCF: parameter adaptive bi-directional mapping mechanism for video transmission over WSNs", "abstract": "To support and keep high quality of video transmission over wireless sensor networks, this paper proposes a parameter adaptive bi-directional cross-layer mapping algorithm on the basis of the operation mechanism of IEEE 802.11e Enhanced Distributed Coordination Function (EDCF) supporting video service differentiation, named PABM-EDCF. Instead of classifying video data to a specific access category in 802.11e network, our proposed adaptive cross-layer scheme makes use of the hierarchy characteristic of video stream, dynamically maps video data to the appropriate access categories according to both the significance of the different video frames and the network traffic load. The significance passes from the application layer to the media access layer through a cross-layer architecture. In order to prevent the network congestion and keep the high transmission quality, the proposed algorithm adopts bi-directional floating mapping algorithm and congestion awareness mechanism based on the queue length and frame types. The mapping parameters are updated according to the network condition in time. Our simulation results indicate: the proposed method (a) improves the video transmission quality; (b) optimizes the management and utilization of queue resources; and (c) yields superior performance (under different loads) over 802.11e, static mapping and adaptive mapping schemes.", "journal": "MULTIMEDIA TOOLS AND APPLICATIONS", "category": "Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000319681400011", "keywords": "diagnosis; haematuria; kidney cancer; primary health care", "title": "Association of Oral Glucocorticoid Use With an Increased Risk of Acute Pancreatitis A Population-Based Nested Case-Control Study", "abstract": "Importance: Oral glucocorticoid use has been suggested to cause acute pancreatitis in several case reports. However, no epidemiological study has investigated this association. Objective: To conduct a nationwide population-based case-control study to investigate the potential association between oral glucocorticoid use and acute pancreatitis. Design: In this population-based case-control study, all individuals aged 40 to 84 years who developed a first episode of acute pancreatitis between 2006 and 2008 in Sweden were identified. Setting: Population-based, nationwide, register-based study. Participants: A total of 6161 cases with a first episode of acute pancreatitis and 61 637 controls were included in the final analyses. Cases were all patients diagnosed as having a first episode of acute pancreatitis during the study period, defined by the diagnosis code K85 in the International Statistical Classification of Diseases, 10th Revision (ICD-10). Controls were randomly selected from the source population at risk of developing acute pancreatitis. For each case, 10 controls, matched for age, sex, and calendar period, were randomly selected from the general population. Oral glucocorticoid use was assessed from the Swedish Prescribed Drug Register. Current, recent, and former users were defined as patients who collected their glucocorticoid prescription within 30, 31 to 180, and after 180 days before the index date, respectively Main Outcome Measures: Unconditional logistic regression was performed to calculate the odds ratios (ORs) with 95% confidence intervals for the association between oral glucocorticoid use and acute pancreatitis. Multivariable adjustment was made for potential confounders including, among others, alcohol abuse, diabetes, and concomitant drug use. Results: The study included 6161 cases of acute pancreatitis and 61 637 controls. The risk of acute pancreatitis was increased among current users of oral glucocorticoids compared with nonusers (OR, 1.53; 95% CI, 1.27-1.84). This risk was highest 4 to 14 days after drug dispensation (OR, 1.73; 95% CI, 1.31-2.28) and attenuated thereafter. There was no association between oral glucocorticoid use and acute pancreatitis immediately after drug dispensation. There was no increased risk of acute pancreatitis among recent or former users of glucocorticoids compared with nonusers. Conclusions and Relevance: Current oral glucocorticoid use is associated with an increased risk of acute pancreatitis.", "journal": "JAMA INTERNAL MEDICINE", "category": "Medicine, General & Internal", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000311802900045", "keywords": "Automation; forestry; image registration; laser scanning; least squares; parameter estimation; skeletonization", "title": "Localized Registration of Point Clouds of Botanic Trees", "abstract": "A global registration is often insufficient for estimating dendrometric characteristics of trees because individual branches of the same tree may exhibit different positions between two scanning procedures. Therefore, we introduce a localized approach to register point clouds of botanic trees. Given two roughly registered point clouds PC1 and PC2 of a tree, we apply a skeletonization method to both point clouds. Based on these two skeletons, initial correspondences between branch segments of both point clouds are established to estimate local transformation parameters. The transformation estimation relies on minimizing the distance between the points in PC1 and the skeleton of PC2. The performance of the method is demonstrated on two example trees. It is shown that significant improvements can be achieved for the registration of fine branches. These improvements are quantified as the residual point-to-line distances before and after the localized fine registration. In our experiment, the residual error after the local registration is on an average of 5 mm over 90 skeleton segments, which is about three times smaller than the average residual error of the initial rough registration.", "journal": "IEEE GEOSCIENCE AND REMOTE SENSING LETTERS", "category": "Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000311620100019", "keywords": "meat; fish; pancreatic cancer; cohort; EPIC", "title": "Meat and fish consumption and risk of pancreatic cancer: Results from the European Prospective Investigation into Cancer and Nutrition", "abstract": "Pancreatic cancer is the fourth most common cause of cancer death worldwide with large geographical variation, which implies the contribution of diet and lifestyle in its etiology. We examined the association of meat and fish consumption with risk of pancreatic cancer in the European Prospective Investigation into Cancer and Nutrition (EPIC). A total of 477,202 EPIC participants from 10 European countries recruited between 1992 and 2000 were included in our analysis. Until 2008, 865 nonendocrine pancreatic cancer cases have been observed. Calibrated relative risks (RRs) and 95% confidence intervals (CIs) were computed using multivariable-adjusted Cox hazard regression models. The consumption of red meat (RR per 50 g increase per day = 1.03, 95% CI = 0.931.14) and processed meat (RR per 50 g increase per day = 0.93, 95% CI = 0.711.23) were not associated with an increased pancreatic cancer risk. Poultry consumption tended to be associated with an increased pancreatic cancer risk (RR per 50 g increase per day = 1.72, 95% CI = 1.042.84); however, there was no association with fish consumption (RR per 50 g increase per day = 1.22, 95% CI = 0.921.62). Our results do not support the conclusion of the World Cancer Research Fund that red or processed meat consumption may possibly increase the risk of pancreatic cancer. The positive association of poultry consumption with pancreatic cancer might be a chance finding as it contradicts most previous findings.", "journal": "INTERNATIONAL JOURNAL OF CANCER", "category": "Oncology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000315397900072", "keywords": "DOPC; DPPC; AFM; force spectroscopy; supported lipid bilayer; vesicle fusion; breakthrough forces; force volume", "title": "Preparation of DOPC and DPPC Supported Planar Lipid Bilayers for Atomic Force Microscopy and Atomic Force Spectroscopy", "abstract": "Cell membranes are typically very complex, consisting of a multitude of different lipids and proteins. Supported lipid bilayers are widely used as model systems to study biological membranes. Atomic force microscopy and force spectroscopy techniques are nanoscale methods that are successfully used to study supported lipid bilayers. These methods, especially force spectroscopy, require the reliable preparation of supported lipid bilayers with extended coverage. The unreliability and a lack of a complete understanding of the vesicle fusion process though have held back progress in this promising field. We document here robust protocols for the formation of fluid phase DOPC and gel phase DPPC bilayers on mica. Insights into the most crucial experimental parameters and a comparison between DOPC and DPPC preparation are presented. Finally, we demonstrate force spectroscopy measurements on DOPC surfaces and measure rupture forces and bilayer depths that agree well with X-ray diffraction data. We also believe our approach to decomposing the force-distance curves into depth sub-components provides a more reliable method for characterising the depth of fluid phase lipid bilayers, particularly in comparison with typical image analysis approaches.", "journal": "INTERNATIONAL JOURNAL OF MOLECULAR SCIENCES", "category": "Biochemistry & Molecular Biology; Chemistry, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000316822200007", "keywords": "implantable devices; pacemaker-bradyarrhythmias; electrocardiography", "title": "Parallel modular computation of Grobner and involutive bases", "abstract": "An overview of an algorithm and an efficient implementation of parallel computing of involutive and Grobner bases with the help of modular operations is presented. Difficulties arising in modulo calculations and in the reconstruction of a basis with coefficients in a\"currency sign by its modular images are considered; Some ways to overcome these difficulties are indicated.", "journal": "PROGRAMMING AND COMPUTER SOFTWARE", "category": "Computer Science, Software Engineering", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000312004800100", "keywords": "Surface imprinting; Nano-TiO2; Chlorogenic acid; Adsorption", "title": "Surface imprinting on nano-TiO2 as sacrificial material for the preparation of hollow chlorogenic acid imprinted polymer and its recognition behavior", "abstract": "Surface imprinting chlorogenic acid (CGA) on nano-TiO2 particles as sacrificial support material was successfully performed by using 4-vinylpyridine (4-VP) as functional monomer to obtain a hollow CGA-imprinted polymer (H-MIP1). Fourier transmission infrared spectrometry (FTIR) and scanning electron microscopy (SEM) were utilized for structurally characterizing the polymers obtained and adsorption dynamics and thermodynamic behavior investigated according to different models. Binding selectivity, adsorption capacity and the reusability for this H-MIP1 were also evaluated. This hollow CGA imprinted polymer shows rapid binding dynamics and higher binding capability toward the template molecules. The pseudo first-order kinetic model was shown best to describe the binding process of CGA on the H-MIP1 and Langmuir isotherm model best to fit the experimental adsorption isotherm data. Through adsorption isotherms at different temperatures, thermodynamic parameter values were obtained. Selectivity coefficients for the H-MIP1 toward the template were 2.209, 3.213, 1.746 and 2.353 relative to CA, VA, PCA and GA, respectively. This H-MIP1 was also indicated with a good imprint effect and a high capability to capture CGA from methanol extract of Eucommia ulmoides (E. ulmoides) leaves. Additionally, a good reusability for this imprinted polymer was exhibited during repeated adsorption-desorption use. (C) 2012 Elsevier B. V. All rights reserved.", "journal": "APPLIED SURFACE SCIENCE", "category": "Chemistry, Physical; Materials Science, Coatings & Films; Physics, Applied; Physics, Condensed Matter", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000315637700001", "keywords": "Plasma; Amino acid; Lung cancer; Early detection", "title": "The significance and robustness of a plasma free amino acid (PFAA) profile-based multiplex function for detecting lung cancer", "abstract": "Background: We have recently reported on the changes in plasma free amino acid (PFAA) profiles in lung cancer patients and the efficacy of a PFAA-based, multivariate discrimination index for the early detection of lung cancer. In this study, we aimed to verify the usefulness and robustness of PFAA profiling for detecting lung cancer using new test samples. Methods: Plasma samples were collected from 171 lung cancer patients and 3849 controls without apparent cancer. PFAA levels were measured by high-performance liquid chromatography (HPLC)-electrospray ionization (ESI)-mass spectrometry (MS). Results: High reproducibility was observed for both the change in the PFAA profiles in the lung cancer patients and the discriminating performance for lung cancer patients compared to previously reported results. Furthermore, multivariate discriminating functions obtained in previous studies clearly distinguished the lung cancer patients from the controls based on the area under the receiver-operator characteristics curve (AUC of ROC = 0.731 similar to 0.806), strongly suggesting the robustness of the methodology for clinical use. Moreover, the results suggested that the combinatorial use of this classifier and tumor markers improves the clinical performance of tumor markers. Conclusions: These findings suggest that PFAA profiling, which involves a relatively simple plasma assay and imposes a low physical burden on subjects, has great potential for improving early detection of lung cancer.", "journal": "BMC CANCER", "category": "Oncology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000328604700014", "keywords": "biomechanics; elite triathletes; running pattern; stiffness", "title": "Influence of Microsatellite Instability and KRAS and BRAF Mutations on Lymph Node Harvest in Stage I-III Colon Cancers", "abstract": "Lymph node (LN) harvest is influenced by several factors, including tumor genetics. Microsatellite instability (MSI) is associated with improved node harvest, but the association to other genetic factors is largely unknown. Research methods included a prospective series of stage I-III colon cancer patients undergoing ex vivo sentinel-node sampling. The presence of MSI, KRAS mutations in codons 12 and 13, and BRAF V600E mutations was analyzed. Uni- and multivariate regression models for node sampling were adjusted for clinical, pathological and molecular features. Of 204 patients, 67% had an adequate harvest (>= 12 nodes). Adequate harvest was highest in patients whose tumors exhibited MSI (79%; odds ratio [OR] 2.5, 95% confidence interval [CI] 1.2-4.9; P = 0.007) or were located in the proximal colon (73%; 2.8, 1.5-5.3; P = 0.002). In multiple linear regression, MSI was a significant predictor of the total LN count (P = 0.02). Total node count was highest for cancers with MSI and no KRAS/BRAF mutations. The independent association between MSI and a high LN count persisted for stage I and II cancers (P = 0.04). Tumor location in the proximal colon was the only significant predictor of an adequate LN harvest (adjusted OR 2.4, 95% CI 1.2-4.9; P = 0.01). An increase in the total number of nodes harvested was not associated with an increase in nodal metastasis. In conclusion, number of nodes harvested is highest for cancers of the proximal colon and with MSI. The nodal harvest associated with MSI is influenced by BRAF and KRAS genotypes, even for cancers of proximal location. Mechanisms behind the molecular diversity and node yield should be further explored.", "journal": "MOLECULAR MEDICINE", "category": "Biochemistry & Molecular Biology; Cell Biology; Medicine, Research & Experimental", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000319675700004", "keywords": "Delhi; Housing conditions; Health consequences; Historical background; Geographical location", "title": "Relationship between Housing and Health: A Cross-Sectional Study of an Urban Centre of India", "abstract": "Poor housing is an important public health risk worldwide. Addressing this issue offers an opportunity to highlight a vital social determinant of health. This study aimed to evaluate housing conditions and their potential health consequences in three different zones of National Capital Territory of Delhi, India. A cross-sectional design used for data collection from random sample of 1896 households through door to door survey method was adopted for this study. It is hypothesised that housing conditions and health outcomes vary in different parts of the city according to their geographical location, and historical background of development. A trend of overcrowded housing condition was reported by nearly 65% of the respondents in city zone, while the respective percentage for Najafgarh and Shahdara was 35% and 46%. Regression analysis of household level data controlling individual socio-economic and demographic covariates indicates, respiratory infections were the main health outcomes (p < 0.006) and attached to overcrowding; ARI (OR 1.62, CI 1.42-1.84), tuberculosis (OR 1.26, CI 1.11-1.44) and asthma (OR 1.17, C.I 1.03-1.33). The study concludes that a uniform approach cannot solve the problem. A combination of programmes and policies including implementation of housing codes, renovation work, and generating awareness among the dwellers can lead to a better health.", "journal": "INDOOR AND BUILT ENVIRONMENT", "category": "Construction & Building Technology; Engineering, Environmental; Public, Environmental & Occupational Health", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000315260400021", "keywords": "Ellipsometer; genetic algorithm; ITNLC; liquid crystals; mueller matrix; stokes vector; TNLC; VALC", "title": "Determination of Time Resolved Heat Transfer Coefficient and Adiabatic Effectiveness Waveforms With Unsteady Film", "abstract": "Traditional hot gas path film cooling characterization involves the use of wind tunnel models to measure the spatial adiabatic effectiveness (g) and heat transfer coefficient (h) distributions. Periodic unsteadiness in the flow, however, causes fluctuations in both g and h. In this paper we present a novel inverse heat transfer methodology that may be used to approximate the g(t) and h(t) waveforms. The technique is a modification of the traditional transient heat transfer technique that, with steady flow conditions only, allows the determination of g and h from a single experiment by measuring the surface temperature history as the material changes temperature after sudden immersion in the flow. However, unlike the traditional transient technique, this new algorithm contains no assumption of steadiness in the formulation of the governing differential equations for heat transfer into a semi-infinite slab. The technique was tested by devising arbitrary waveforms for g and h at a point on a film cooled surface and running a computational simulation of an actual experimental model experiencing those flow conditions. The surface temperature history was corrupted with random noise to simulate actual surface temperature measurements and then fed into an algorithm developed here that successfully and consistently approximated the g(t) and h(t) waveforms. [DOI: 10.1115/1.4007545]", "journal": "JOURNAL OF TURBOMACHINERY-TRANSACTIONS OF THE ASME", "category": "Engineering, Mechanical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000317118300001", "keywords": "Soccer; Team sports; Binge; Alcohol; Students; Culture", "title": "Heavy episodic drinking and soccer practice among high school students in Brazil: the contextual aspects of this relationship", "abstract": "Background: Heavy episodic drinking (HED) (consumption of five or more drinks on the same occasion) among adolescents is related to several problems and partaking in sport or physical activities has been suggested as an option to prevent or reduce alcohol consumption among this population. The aim of this study was to investigate the relationship between soccer practice and heavy episodic drinking among high school students from Brazil. Methods: Data were obtained from a cross-sectional study among a representative sample of public and private high school students from all Brazilian state capitals (N=19,132). Only students aged from 14 to 18 who reported having taken part in soccer practice, other team sports or non-practicing sports in the last month were included. Characteristics of sport practice (frequency and motivation) and HED in the last month (type of drink; where and with whom they drank; frequency of HED) were also considered. Regression models were controlled for sociodemographic variables. Results: For all groups studied most of the students reported drinking beer, with friends and at nightclubs or bars. Soccer practice was associated to HED when compared to non-practicing sports and to other team sports. Compared to other team sports, playing soccer for pleasure or profession, but not for keep fit or health reasons, were more associated to HED. Frequency of soccer practice from 1 to 5 days per month and 20 or more days per month, but not from 6 to 19 days per month, were also more associated to HED. Conclusions: The relationship between soccer and HED appears to be particularly stronger than in other team sports among adolescents in Brazil. Induced sociability of team sports practice cannot be assumed as the main reason for HED among soccer players. Possibly these results reflect the importance of a strong cultural association between soccer and beer in Brazil and these findings should be integrated to future prevention or intervention programs.", "journal": "BMC PUBLIC HEALTH", "category": "Public, Environmental & Occupational Health", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000316373400008", "keywords": "Addiction; Astrocyte; Drug abuse; GFAP; Heroin; Quantitative image analysis; Astrogliosis", "title": "Quantitative analysis of astrogliosis in drug-dependent humans", "abstract": "Drug addiction is a chronic, relapsing disease caused by neurochemical and molecular changes in the brain. In this human autopsy study qualitative and quantitative changes of glial fibrillary acidic protein (GFAP)-positive astrocytes in the hippocampus of 26 lethally intoxicated drug addicts and 35 matched controls are described. The morphological characterization of these cells reflected alterations representative for astrogliosis. But, neither quantification of GFAP-positive cells nor the Western blot analysis indicated statistical significant differences between drug fatalities versus controls. However, by semi-quantitative scoring a significant shift towards higher numbers of activated astrocytes in the drug group was detected. To assess morphological changes quantitatively, graph-based representations of astrocyte morphology were obtained from single cell images captured by confocal laser scanning microscopy. Their underlying structures were used to quantify changes in astroglial fibers in an automated fashion. This morphometric analysis yielded significant differences between the investigated groups for four different measures of fiber characteristics (Euclidean distance, graph distance, number of graph elements, fiber skeleton distance), indicating that, e.g., astrocytes in drug addicts on average exhibit significant elongation of fiber structures as well as two-fold increase in GFAP-positive fibers as compared with those in controls. In conclusion, the present data show characteristic differences in morphology of hippocampal astrocytes in drug addicts versus controls and further supports the involvement of astrocytes in human pathophysiology of drug addiction. The automated quantification of astrocyte morphologies provides a novel, testable way to assess the fiber structures in a quantitative manner as opposed to standard, qualitative descriptions. (C) 2013 Elsevier B.V. All rights reserved.", "journal": "BRAIN RESEARCH", "category": "Neurosciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000320485500005", "keywords": "Horizontal ground source heat pump; Tire Derived Aggregate; Non-homogeneous soil; Optimization; Genetic algorithm; Energy efficiency; Control", "title": "A passive design strategy for a horizontal ground source heat pump pipe operation optimization with a non-homogeneous soil profile", "abstract": "The effectiveness of a non-homogeneous soil profile for horizontal ground source heat pumps (GSHPs), defined as natural backfill with an intermediate layer of material having different thermal characteristics, is investigated. Steps toward development of a comprehensive model to consider the effects of the non-homogeneous layer are described. The developed model is utilized successfully in conjunction with a genetic algorithm (GA) search method to obtain the optimized operational parameters for a GSHP in three different climate conditions. A properly sized and engineered non-homogeneous soil profile demonstrated the potential to increase the energy extraction/dissipation rates from/to the ground to a significant level. The potential benefit of a recycled product, Tire Derived Aggregate (TDA), as an insulating non-homogeneous layer is assessed. TDA is demonstrated to be more effective in the cold climate (Buffalo) by increasing the energy extraction rates from the ground approximately 15% annually. TDA's effectiveness is less pronounced in a relatively moderate climate (Dallas) by increasing the energy extraction rates from the ground about 4% annually. For the cooling only scenario (Miami), a high conductive intermediate layer of saturated sand exhibited greater potential to increase the energy dissipation to the ground. Published by Elsevier B.V.", "journal": "ENERGY AND BUILDINGS", "category": "Construction & Building Technology; Energy & Fuels; Engineering, Civil", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000312896700020", "keywords": "Head and neck cancer; Hypoxia; Gene expression; TaqMan Low Density; Array; Pimonidazole; Carbonic anhydrase IX", "title": "Prospective technical validation and assessment of intra-tumour heterogeneity of a low density array hypoxia gene profile in head and neck squamous cell carcinoma", "abstract": "Background and purpose: Tumour hypoxia is associated with a poor prognosis in head and neck squamous cell carcinoma (HNSCC), however there is no accepted method for assessing hypoxia clinically. We aimed to conduct a technical validation of a hypoxia gene expression signature using the TaqMan Low Density Array (TLDA) platform to investigate if this approach reliably identified hypoxic tumours. Materials and methods: Tumour samples (n = 201) from 80 HNSCC patients were collected prospectively from two centres. Fifty-three patients received pimonidazole prior to surgery. TaqMan Low Density Array-Hypoxia Scores (TLDA-HS) were obtained by quantitative real-time PCR (qPCR) using a 25-gene signature and customised TLDA cards. Assay performance was assessed as coefficient of variation (CoV). Results: The assay was sensitive with linear reaction efficiencies across a 4log(10) range of inputted cDNA (0.001-10 ng/mu l). Intra- (CoV = 6.9%) and inter- (CoV = 2.0%) assay reproducibility were excellent. Intra-tumour heterogeneity was lower for TLDA-HS (23.2%) than for pimonidazole (67.2%) or single gene measurements of CA9 (62.2%), VEGFA (45.0%) or HIG2 (39.4%). TLDA-HS in HNSCC cell lines increased with decreasing pO(2). TLDA-HS correlated with Affymetrix U133 Plus 2.0 microarray HS (p < 0.01) and positive pimonidazole scores (p = 0.005). Conclusions: Gene expression measurements of hypoxia using a 25-gene signature and TLDA cards are sensitive, reproducible and associated with lower intra-tumour heterogeneity than assaying individual genes or pimonidazole binding. The approach is suitable for further assessment of prognostic and predictive capability in clinical trial material. (C) 2012 Elsevier Ltd. All rights reserved.", "journal": "EUROPEAN JOURNAL OF CANCER", "category": "Oncology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000318667000046", "keywords": "Food texture; Non-destructive measurement; Sensory evaluation; Texture profile analysis; Food quality", "title": "Texture measurement approaches in fresh and processed foods - A review", "abstract": "Knowledge of textural properties is important for stakeholders in the food value chain including producers, postharvest handlers, processors, marketers and consumers. For fresh foods such as fruit and vegetable, textural properties such as firmness are widely used as indices of readiness to harvest (maturity) to meet requirements for long term handling, storage and acceptability by the consumer. For processed foods, understanding texture properties is important for the control of processing operations such as heating, frying and drying to attain desired quality attributes of the end product. Texture measurement is therefore one of the most common techniques and procedures in food and postharvest research and industrial practice. Various approaches have been used to evaluate the sensory attributes of texture in foods. However, the high cost and time consumption of organizing panelists and preparing food limit their use, and often, sensory texture evaluation is applied in combination with instrumental measurement. Objective tests using a wide range of instruments are the most widely adopted approaches to texture measurement. Texture measurement instruments range from simple hand-held devices to the Instron machine and texture analyzer which provide time-series data of product deformation thereby allowing a wide range of texture attributes to be calculated from force-time or force-displacement data. In recent times, the application of novel and emerging non-invasive technologies such as near-infrared spectroscopy and hyper-spectral imaging to measure texture attributes has increased in both fresh and processed foods. Increasing demand for rapid, cost-effective and non-invasive measurement of texture remains a challenge in the food industry. The relationships between sensory evaluation and instrumental measurement of food texture are also discussed, which shows the importance of multidisciplinary collaboration in this field. (C) 2013 Elsevier Ltd. All rights reserved.", "journal": "FOOD RESEARCH INTERNATIONAL", "category": "Food Science & Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000331050700004", "keywords": "sleep quality; primary care; attendees; northern Nigeria", "title": "SLEEP QUALITY AMONG PRIMARY CARE ATTENDEES IN KADUNA, NORTHERN NIGERIA: A CASE-CONTROL STUDY", "abstract": "Objective: To assess sleep quality and its determinants among primary care patients in a Northern Nigerian setting. Methods: We administered the Pittsburgh Sleep Quality Index (PSQI) and the Hospital Anxiety and Depression Scale to 217 consecutive patients attending the General Outpatient Clinic of Barau Dikko Specialist Hospital, Kaduna, and PSQI to 223 age-matched controls to evaluate their sleep quality and levels of anxiety and depression. A data collection sheet was used to record the sociodemographic characteristics of patients and controls, and the clinical characteristics of the patients. Results: The mean ages of all the subjects, patients, and controls were 33.7 (SD 10.6), 33.5 (SD 10.6), and 34.0 (SD 10.5) years respectively; 54.4% of the patients were females, 54.8% were Muslims, 56.2% admitted they had pain, 60.8% and 46.5% had anxiety and depressive symptoms respectively, while 68.7% had poor sleep quality. The mean global score of sleep quality for patients was 9.2 (SD 3.6) while that of the control was 3.8 (SD 1.4). The difference was statistically significant (t = 20.834, P value < 0.001, 95% CI 4.891-5.910). Islamic religious faith, presence of pain, anxiety symptoms, and depressive symptoms were significantly associated with poor quality of sleep (P value < 0.05). Multiple regression analysis identified being a Muslim (OR 6.422, P value 0.027, 95% CI 0.196-0.907), pain (OR 8.038, P value < 0.001, 95% CI 0.016-0.091), and anxiety symptoms (OR 5.253, P value < 0.001, 95% CI 0.136-0.473) as predictors of poor sleep quality among the patients. Conclusion: Poor quality of sleep is common in primary care patients. Efforts should be made to improve its recognition, identify associated factors, and consider a holistic approach to patients' care.", "journal": "INTERNATIONAL JOURNAL OF PSYCHIATRY IN MEDICINE", "category": "Psychiatry", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000317433300012", "keywords": "Space-time codes; Performance bounds; MMSE algorithm; SNR transfer", "title": "Predicting Approximate Performance Bounds for Space-Time Codes Over Quasi-Static Fading Channels", "abstract": "This paper provides a useful method for predicting the approximate performance of space time codes over quasi-static fading channels. Based on evaluating the signal-to-noise ratios (SNR) of the extrinsic information in the minimum mean-square error (MMSE) detector, we obtain upper and lower bounds on SNR, which are the convergent limits of the SNR evolution of the extrinsic information between the MMSE detector and the a posteriori probability decoder. Based on the SNR bounds the upper and lower bounds of frame-error-rate and bit-error-rate can be assessed by averaging over the fading coefficients. The results of numerical simulations are found to be in excellent agreement with the semi-analytic bounds.", "journal": "WIRELESS PERSONAL COMMUNICATIONS", "category": "Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000315322800015", "keywords": "Prostate biopsy; Infective complications; Urinary tract infection; Antibiotic resistance", "title": "Sleep Duration and Risk of Atrial Fibrillation (from the Physicians' Health Study)", "abstract": "Although sleep quality and duration have been related to cardiovascular end points, little is known about the association between sleep duration and incident atrial fibrillation (AF). Hence, we prospectively examined the association between sleep duration and incident AF in a cohort of 18,755 United States male physicians. Self-reported sleep duration was ascertained during a 2002 annual follow-up questionnaire. Incident AF was ascertained through annual follow-up questionnaires. Cox regression analysis was used to estimate the relative risks of AF. The average age at baseline was 67.7 +/- 8.6 years. During a mean follow-up of 6.9 +/- 2.1 years, 1,468 cases of AF occurred. Using 7 hours of sleep as the reference group, the multivariate adjusted hazard ratio for AF was 1.06 (95% confidence interval 0.92 to 1.22), 1.0 (reference), and 1.13 (95% confidence interval 1.00 to 1.27) from the lowest to greatest category of sleep duration (p for trend = 0.26), respectively. In a secondary analysis, no evidence was seen of effect modification by adiposity (p for interaction = 0.69); however, prevalent sleep apnea modified the relation of sleep duration with AF (p for interaction = 0.01). From the greatest to the lowest category of sleep duration, the multivariate-adjusted hazard ratio for AF was 2.26 (95% confidence interval 1.26 to 4.05), 1.0 (reference), and 1.34 (95% confidence interval 0.73 to 2.46) for those with prevalent sleep apnea and 1.01 (95% confidence interval 0.87 to 1.16), 1.0 (reference), and 1.12 (95% confidence interval 0.99 to 1.27) for those without sleep apnea, respectively. Our data showed a modestly elevated risk of AF with long sleep duration among United States male physicians. Furthermore, a shorter sleep duration was associated with a greater risk of AF in those with prevalent sleep apnea. (C) 2013 Elsevier Inc. All rights reserved. (Am J Cardiol 2013;111:547-551)", "journal": "AMERICAN JOURNAL OF CARDIOLOGY", "category": "Cardiac & Cardiovascular Systems", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000325837500001", "keywords": "linearized shallow water equations; asymptotics; wave front singularity evolution; trapped nonstationary wave; underwater ridge; underwater bank", "title": "Wave Trains Associated with a Cascade of Bifurcations of Space-Time Caustics over Elongated Underwater Banks", "abstract": "We study the behavior of linear nonstationary shallow water waves generated by an instantaneous localized source as they propagate over and become trapped by elongated underwater banks or ridges. To find the solutions of the corresponding equations, we use an earlier-developed asymptotic approach based on a generalization of Maslov's canonical operator, which provides a relatively simple and efficient analytic-numerical algorithm for the wave field computation. An analysis of simple examples (where the bank and source shapes are given by certain elementary functions) shows that the appearance and dynamics of trapped wave trains is closely related to a cascade of bifurcations of space-time caustics, the bifurcation parameter being the bank length-to-width ratio.", "journal": "MATHEMATICAL MODELLING OF NATURAL PHENOMENA", "category": "Mathematical & Computational Biology; Mathematics, Applied; Mathematics, Interdisciplinary Applications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000316643600092", "keywords": "Network theory; Input-output analysis; Structural equivalence; Structural analysis", "title": "Input-output research in structural equivalence: Extracting paths and similarities", "abstract": "The systematic study of the way in which intersectoral transactions are organized is an interesting source of information about the pattern of existing economic relationships in one country that allows comparison of its structural features with those of another economy of reference. A new tool in the input-output context that allows us to reveal the functioning economic structure by determining the structural equivalent sectoral groups is proposed. The aim is to outline a new approach to clustering sectors based on similarities between sector linkages profiles. The obtained set of structurally equivalent sectors defines a reduced model which provides additional information about the main paths of influence and the degree of complexity. The structural equivalent sectors of Spain in 2000 and 2005 are determined. The Spanish economy has evolved to a more connected and oriented-service economy. (C) 2013 Elsevier B.V. All rights reserved.", "journal": "ECONOMIC MODELLING", "category": "Economics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000324405200007", "keywords": "Inverse acoustic scattering; direct sampling method; indicator function; far-field data", "title": "A DIRECT SAMPLING METHOD FOR INVERSE SCATTERING USING FAR-FIELD DATA", "abstract": "This work is concerned with a direct sampling method (DSM) for inverse acoustic scattering problems using far-field data. Using one or few incident waves, the DSM provides quite reasonable profiles of scatterers in time-harmonic inverse acoustic scattering without a priori knowledge of either the physical properties or the number of disconnected components of the scatterer. We shall first present a novel derivation of the DSM using far-field data, then carry out a systematic evaluation of the performances and distinctions of the DSM using both near-field and far-field data. A new interpretation from the physical perspective is provided based on some numerical observations. It is shown from a variety of numerical experiments that the method has several interesting and promising potentials: a) ability to identify not only medium scatterers, but also obstacles, and even cracks, using measurement data from one or few incident directions, b) robustness with respect to large noise, and c) computational efficiency with only inner products involved.", "journal": "INVERSE PROBLEMS AND IMAGING", "category": "Mathematics, Applied; Physics, Mathematical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000317321900017", "keywords": "Aerodynamic admittance function; Cross-wind; Non-stationary; Aerodynamic loads; Wind tunnel tests; Characteristic wind curves", "title": "Implementation of a Continuous-Inextensible-Surface Piezocomposite Airfoil", "abstract": "The theoretical and experimental evaluation of a variable-camber airfoil which employs a continuous inextensible surface and surface-bonded piezoelectric actuators is presented. The partially active surface is designed to have sufficient bending stiffness in the chordwise direction to sustain chordwise shape under aerodynamic loading. In contrast, the in-plane stiffness is relatively high; however, the necessary deformations that are required to change the aerodynamic response can still be attained while maintaining the surface perimeter constant. Coupled with two carefully selected boundary conditions, the proposed piezocomposite airfoil can achieve significant change in aerodynamic response. The surface geometry properties are determined using a genetic algorithm optimization method. The optimization is conducted to achieve maximum change of lift-output-per-square-root-of-drag, which is the difference in the aerodynamic response for the airfoil at maximum excitation with asymmetric profile and zero excitation with symmetric profile. A coupled analysis of the fluid structure interaction is employed assuming static-aeroelastic behavior, which allows the realization of a design that can sustain aerodynamic loads. The theoretical response is supplemented with extensive bench-top and wind-tunnel experiments conducted on a prototype airfoil. The experimental results are compared to the theoretical predictions, highlighting agreements and discrepancies.", "journal": "JOURNAL OF AIRCRAFT", "category": "Engineering, Aerospace", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000320184900013", "keywords": "Fuzzy logic; NPP safety; nuclear power plant safety; safety verification", "title": "Fuzzy-Logic-Based Safety Verification Framework for Nuclear Power Plants", "abstract": "This article presents a practical implementation of a safety verification framework for nuclear power plants (NPPs) based on fuzzy logic where hazard scenarios are identified in view of safety and control limits in different plant process values. Risk is estimated quantitatively and compared with safety limits in real time so that safety verification can be achieved. Fuzzy logic is used to define safety rules that map hazard condition with required safety protection in view of risk estimate. Case studies are analyzed from NPP to realize the proposed real-time safety verification framework. An automated system is developed to demonstrate the safety limit for different hazard scenarios.", "journal": "RISK ANALYSIS", "category": "Public, Environmental & Occupational Health; Mathematics, Interdisciplinary Applications; Social Sciences, Mathematical Methods", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000313408900011", "keywords": "Oligomeric enzyme kinetics; Nonequilibrium steady state; Single trajectory analysis; Fluctuation theorem", "title": "Entropy production of a mechanically driven single oligomeric enzyme: a consequence of fluctuation theorem", "abstract": "In this work we have shown how an applied mechanical force affects an oligomeric enzyme kinetics in a chemiostatic condition where the statistical characteristics of random walk of the substrate molecules over a finite number of active sites of the enzyme plays important contributing factors in governing the overall rate and nonequilibrium thermodynamic properties. The analytical results are supported by the simulation of single trajectory based approach of entropy production using Gillespie's stochastic algorithm. This microscopic numerical approach not only gives the macroscopic entropy production from the mean of the distribution of entropy production which depends on the force but also a broadening of the distribution by the applied mechanical force, a kind of power broadening. In the nonequilibrium steady state (NESS), both the mean and the variance of the distribution increases and then saturates with the rise in applied force corresponding to the situation when the net rate of product formation reaches a limiting value with an activationless transition. The effect of the system-size and force on the entropy production distribution is shown to be constrained by the detailed fluctuation theorem.", "journal": "JOURNAL OF MATHEMATICAL CHEMISTRY", "category": "Chemistry, Multidisciplinary; Mathematics, Interdisciplinary Applications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000317375700003", "keywords": "Urinary bladder, overactive; Urinary incontinence; Botulinum toxins", "title": "Actual treatment of overactive bladder and urge urinary incontinence", "abstract": "Overactive bladder (OAB) is defined by its hallmark symptom, urgency It can be associated with urge urinary incontinence (UUI), and dramatically impact the patients' quality of life. Etiologies of OAB are numerous, and under this common wording, virtually all the population is covered (men as well as women, patients with or without neurogenic disease, and all age categories). OAB and UUI management have been historically based on non-interventional therapies, antimuscarinics, and surgery. In the last decade, innovations in the treatment of this highly prevalent condition have been multiple, and further insights came from various horizons (drug invention, innovative use of existing drugs, new medical devices, tissue engineering, gene and cell therapy). Notably, the use of BoNT and neuromodulation techniques have deeply modified the algorithm of specialized OAB management, delaying surgery indications and offering mini-invasive alternatives to patient refractory to behavioral and medical treatment. Whilst some of these techniques are about to reach maturity, numerous questions remain unsolved about their indications, long term effects, rank in the armamentarium, cost-effectiveness, hypothetical combination or sequential use. The present review depicts the actual wide range of options available for OAB management in adults, focusing on the latest evolutions. When relevant, a distinction was made between genders and OAB subtypes (idiopathic vs neurogenic) regarding treatment outcomes.", "journal": "MINERVA UROLOGICA E NEFROLOGICA", "category": "Urology & Nephrology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000319618300013", "keywords": "Aerosol; trend; satellite; AVHRR", "title": "A global survey of the effect of cloud contamination on the aerosol optical thickness and its long-term trend derived from operational AVHRR satellite observations", "abstract": "Subpixel cloud contamination is one of the major issues plaguing passive satellite aerosol remote sensing. Its impact on the aerosol optical thickness (AOT) retrieval has been analyzed/evaluated by many studies. However, the question of how it influences the AOT trend remains to be answered. In this paper, four long-term advanced very high resolution radiometer (AVHRR) AOT data sets from 1981 to 2009 over global oceans for four different definitions of clear sky, respectively, are produced by applying a two-channel aerosol retrieval algorithm to the AVHRR clear-sky reflectances derived by combining NOAA Pathfinder Atmosphere's Extended AVHRR climate data record level-2b all-sky reflectances with the cloud probability parameter determined from the Bayesian probabilistic cloud detection technique. A global analysis of the effect of cloud contamination on the AVHRR AOT retrieval as well as on its long-term trend is then performed by comparing the results from the four data sets. It was found that cloud contamination imposes not only a positive bias on AOT values but also a positive bias on its long-term trend such that negative trends become less negative and positive trends become more positive. A cloud probability value of 1% has been identified as an optimal criterion for clear-sky definition to minimize the cloud contamination in the AVHRR aerosol retrieval while still retaining strong aerosol signals. In order for a satellite aerosol product to be useful and reliable in aerosol trend detection, the cloud contamination effect on aerosol trends needs to be studied/evaluated carefully along with the effects of calibration error, surface disturbance, and aerosol model assumptions.", "journal": "JOURNAL OF GEOPHYSICAL RESEARCH-ATMOSPHERES", "category": "Meteorology & Atmospheric Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000317878600009", "keywords": "Altimeter calibration; Tide gauges; GPS; Altimeter waveform; Wet tropospheric correction", "title": "GPS-based sea level measurements to help the characterization of land contamination in coastal areas", "abstract": "The Corsica site has been established in 1996 to perform altimeter calibration on TOPEX/Poseidon and then on its successors Jason-1 and Jason-2. The first chosen location was under the #85 ground track that overflight the Senetosa Cape. In 2005, it was decided to develop another location close to Ajaccio, to be able to perform the calibration of Envisat and in a next future of SARAL/AltiKa that will flight over the same ground tracks. Equipped with various instruments (tide gauges, permanent GPS, GPS buoy, weather station...) the Corsica calibration site is able to quantify the altimeter Sea Surface Height bias but also to give an input on the origin of this bias (range, corrections, orbits,...). Due to the size of Corsica (not a tiny island), the altimeter measurement system (range and corrections) can be contaminated by land. The aim of this paper is to evaluate this land contamination by using GPS measurements from a fixed receiver on land and from another receiver onboard a life buoy. Concerning the altimeter land contamination, we have quantify that this effect can reach 8 mm/km and then affects the Sea Surface Height bias values already published in the framework of the Corsica calibration site by 5-8 mm for TOPEX and Jason missions. On the other hand, the radiometer measurements (wet troposphere correction) are also sensitive to land and we have been able to quantify the level of improvement of a dedicated coastal algorithm that reconciles our results with those coming from other calibration sites. Finally, we have also shown that the standard deviation of the GPS buoy sea level measurements is highly correlated (similar to 87%) with the Significant Wave Height derived from the altimeters and can be used to validate such parameter. (C) 2012 COSPAR. Published by Elsevier Ltd. All rights reserved.", "journal": "ADVANCES IN SPACE RESEARCH", "category": "Engineering, Aerospace; Astronomy & Astrophysics; Geosciences, Multidisciplinary; Meteorology & Atmospheric Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000320767600027", "keywords": "optoacoustic imaging; photoacoustic tomography; image reconstruction; quantification; multispectral optoacoustic tomography; inverse problem; light transport; spectroscopic imaging", "title": "Optoacoustic Imaging and Tomography: Reconstruction Approaches and Outstanding Challenges in Image Performance and Quantification", "abstract": "This paper comprehensively reviews the emerging topic of optoacoustic imaging from the image reconstruction and quantification perspective. Optoacoustic imaging combines highly attractive features, including rich contrast and high versatility in sensing diverse biological targets, excellent spatial resolution not compromised by light scattering, and relatively low cost of implementation. Yet, living objects present a complex target for optoacoustic imaging due to the presence of a highly heterogeneous tissue background in the form of strong spatial variations of scattering and absorption. Extracting quantified information on the actual distribution of tissue chromophores and other biomarkers constitutes therefore a challenging problem. Image quantification is further compromised by some frequently-used approximated inversion formulae. In this review, the currently available optoacoustic image reconstruction and quantification approaches are assessed, including back-projection and model-based inversion algorithms, sparse signal representation, wavelet-based approaches, methods for reduction of acoustic artifacts as well as multi-spectral methods for visualization of tissue bio-markers. Applicability of the different methodologies is further analyzed in the context of real-life performance in small animal and clinical in-vivo imaging scenarios.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000314483400016", "keywords": "Snow plowing; Arc routing; Windy postman problem; Precedence", "title": "Plowing with precedence: A variant of the windy postman problem", "abstract": "In winter, a common problem is to determine the route that a snowplow should take in order to minimize the distance traveled. We propose a variant of this arc routing problem that is motivated by the fact that deadhead travel over streets that have already been plowed is significantly faster than the time it takes to plow the street. This problem differs from most arc routing problems because the cost of traversing a street changes depending on the order of the streets on a route. We develop a method that generates near-optimal solutions to instances as large as 200 nodes. (C) 2012 Elsevier Ltd. All rights reserved.", "journal": "COMPUTERS & OPERATIONS RESEARCH", "category": "Computer Science, Interdisciplinary Applications; Engineering, Industrial; Operations Research & Management Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000314153900046", "keywords": "Monte Carlo; Neutron transport; Reactor analysis; Load balancing", "title": "The effect of load imbalances on the performance of Monte Carlo algorithms in LWR analysis", "abstract": "A model is developed to predict the impact of particle load imbalances on the performance of domain-decomposed Monte Carlo neutron transport algorithms. Expressions for upper bound performance \"penalties\" are derived in terms of simple machine characteristics, material characterizations and initial particle distributions. The hope is that these relations can be used to evaluate tradeoffs among different memory decomposition strategies in next generation Monte Carlo codes, and perhaps as a metric for triggering particle redistribution in production codes. (C) 2012 Elsevier Inc. All rights reserved.", "journal": "JOURNAL OF COMPUTATIONAL PHYSICS", "category": "Computer Science, Interdisciplinary Applications; Physics, Mathematical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000327809800003", "keywords": "Gaussian quadrature; Ordered orthogonal Laurent polynomial sequences; Positive definite strong moment functionals", "title": "Traffic Signal Timing Optimization Choosing the Objective Function", "abstract": "Choosing an appropriate objective function in optimizing traffic signals in urban transportation networks is not a simple and straightforward task because the choice likely will affect the set of constraints, modeling variables, obtained outputs, and necessary computer and human resources. A methodology for selection of an appropriate objective function for the problem of signal timing optimization was developed. The methodology was applied to a realistic case study network under four demand patterns (symmetric, asymmetric, undersaturated, oversaturated). Selection is made from a pool of five candidates: minimizing the delay, minimizing the travel time, maximizing the throughput-minus-queue, maximizing the number of completed trips (or trip maximization), and maximizing the weighted number of completed trips (or weighted trip maximization). Findings indicate that for all demand patterns, weighted trip maximization improved network performance compared with the other objective functions. Weighted trip maximization reduced system total delay by 0.1% to 5.2% in symmetric undersaturated demand, by 1.0% to 2.4% in asymmetric undersaturated demand, by 1.2% to 16.6% in symmetric oversaturated demand, and by 11.7% to 27.4% in asymmetric partially oversaturated demand. These figures indicate that the weighted trip maximization objective function is the most suitable of the candidates in oversaturated conditions, especially when demand is not symmetric. Throughput-minus-queue and trip maximization were the second most suitable objective functions for oversaturated conditions, and trip maximization was slightly more suitable when demand was asymmetric.", "journal": "TRANSPORTATION RESEARCH RECORD", "category": "Engineering, Civil; Transportation; Transportation Science & Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000328730800026", "keywords": "Knee osteoarthritis; Electromyography; Gait; Radiographic severity; Biomechanics; Principal component analysis", "title": "AMINO ACID PROFILING OF HUMAN PLASMA SAMPLES USING CAPILLARY ELECTROPHORESIS WITH CONTACTLESS CONDUCTIVITY DETECTION", "abstract": "Quantification of amino acids is a significant task for many scientific areas. Amino acids play a complex role in a living organism thus amino acid profiling of human biological fluids can be used for supplementary diagnostic purposes. Capillary electrophoresis coupled with in-house assembled contactless conductivity detection showed to be reliable and sensitive enough for quantification of 18 amino acids in 26 human plasma samples. Principal component analysis resulted in a partial grouping of the plasma samples with the same diagnosis proving potential use of presented method for supplementary diagnostic purposes.", "journal": "CHEMICKE LISTY", "category": "Chemistry, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000360932800007", "keywords": "Non-stationarity tests; conditional volatility; residual bootstrap; time series; random walk", "title": "Using the Electronic Nose to Identify Airway Infection during COPD Exacerbations", "abstract": "Background The electronic nose (e-nose) detects volatile organic compounds (VOCs) in exhaled air. We hypothesized that the exhaled VOCs print is different in stable vs. exacerbated patients with chronic obstructive pulmonary disease (COPD), particularly if the latter is associated with airway bacterial infection, and that the e-nose can distinguish them. Methods Smell-prints of the bacteria most commonly involved in exacerbations of COPD (ECOPD) were identified in vitro. Subsequently, we tested our hypothesis in 93 patients with ECOPD, 19 of them with pneumonia, 50 with stable COPD and 30 healthy controls in a cross-sectional case-controlled study. Secondly, ECOPD patients were re-studied after 2 months if clinically stable. Exhaled air was collected within a Tedlar bag and processed by a Cynarose 320 e-nose. Breath-prints were analyzed by Linear Discriminant Analysis (LDA) with \"One Out\" technique and Sensor logic Relations (SLR). Sputum samples were collected for culture. Results ECOPD with evidence of infection were significantly distinguishable from non-infected ECOPD (p = 0.018), with better accuracy when ECOPD was associated to pneumonia. The same patients with ECOPD were significantly distinguishable from stable COPD during follow-up (p = 0.018), unless the patient was colonized. Additionally, breath-prints from COPD patients were significantly distinguished from healthy controls. Various bacteria species were identified in culture but the e-nose was unable to identify accurately the bacteria smell-print in infected patients. Conclusion E-nose can identify ECOPD, especially if associated with airway bacterial infection or pneumonia.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000381215600003", "keywords": "Monitoring; photovoltaic; reliability; series resistance", "title": "Automatic segmentation and classification of seven-segment display digits on auroral images", "abstract": "In this paper we describe a new and fully automatic method for segmenting and classifying digits in seven-segment displays. The method is applied to a dataset consisting of about 7 million auroral all-sky images taken during the time period of 1973-1997 at camera stations centred around Sodankyla observatory in northern Finland. In each image there is a clock display for the date and time together with the reflection of the whole night sky through a spherical mirror. The digitised film images of the night sky contain valuable scientific information but are impractical to use without an automatic method for extracting the date-time from the display. We describe the implementation and the results of such a method in detail in this paper.", "journal": "GEOSCIENTIFIC INSTRUMENTATION METHODS AND DATA SYSTEMS", "category": "Geosciences, Multidisciplinary; Meteorology & Atmospheric Sciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000392492500005", "keywords": "augmented reality; AR technology in language learning; educational technology; technology and language learning", "title": "Investigating the Role of Augmented Reality Technology in the Language Classroom", "abstract": "The purpose of this study was to inform about some of the current applications and literature on Augmented Reality (AR) technology in education and to present experimental data about the effectiveness of AR application in a language classroom at the elementary level in Turkey. The research design of the study was quasi-experimental. Sixty-one 5th grade students from a state elementary school participated in this research on a volunteer basis. The participants were divided into the experimental and control group, where new vocabulary items were introduced to the experimental group through augmented reality technology. A post-test and a retention test were administered upon the implementation of the program. The results of the study revealed that participants from the experimental group achieved higher scores than participants in the control group and they also performed better in recalling the learnt information. This study suggested that the use of AR technology in a language classroom at the elementary level increased learners' performances and made vocabulary learning more effective in comparison with the traditional methods.", "journal": "CROATIAN JOURNAL OF EDUCATION-HRVATSKI CASOPIS ZA ODGOJ I OBRAZOVANJE", "category": "Education & Educational Research", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000358095800003", "keywords": "Statistical downscaling; Numerical weather prediction; Complex terrain", "title": "Bayesian Inference aided analog downscaling for near-surface winds in complex terrain", "abstract": "Assessing atmospheric boundary layer flows in complex terrain for short-range real-time applications demands fast and reliable downscaling from coarser-resolution meteorological data to the relevant scale. An ideal statistical downscaling numerical experiment was performed for surface winds above complex terrain in Israel's northern Negev desert region. Dynamical downscaling have been performed by the WRF model to create a historical database by the following two sets. The first set used 5 nested domains from 40.5 km to 0.5 km. The second set used 3 nested domains ranging from 40.5 km to 4.5 km. The 4.5 km data (stage 2) was defined as predictors while data on 0.5 km (stage 1) served as predictands for statistical downscaling. Two statistical downscaling algorithms: minimal distance analog and a Bayesian inference aided analog (hereafter Bayesian algorithm) were tested by the above data. Unlike most analog algorithms, the Bayesian algorithm refers to the probability to get the best analog instead of the minimal differences between predictands. The comparison of the two algorithms shows that the Bayesian approach yields improved results. The Bayesian algorithm reproduces the 0.5 km resolution dynamically downscaled surface winds with an average absolute direction difference of 43 and 20 for calm winds and moderate/strong winds respectively. Its average wind speed error is similar to 1.1 ms(-1). similar to 40 days are sufficient to create a representative database. Given the database, the procedure is extremely fast (a few seconds) and easy to operate, which makes it suitable for real-time non-expert fast-response applications. (C) 2015 Elsevier B.V. All rights reserved.", "journal": "ATMOSPHERIC RESEARCH", "category": "Meteorology & Atmospheric Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000358802900011", "keywords": "Chemical space; Middle space; Bacterial permeability; Fluoroquinolone; DNA gyrase; Antibacterial; Antibiotic; Dimer", "title": "Synthesis of ciprofloxacin dimers for evaluation of bacterial permeability in atypical chemical space", "abstract": "We describe the synthesis and evaluation of a library of variably-linked ciprofloxacin dimers. These structures unify and expand on the use of fluoroquinolones as probes throughout the antibiotic literature. A dimeric analog (19) showed enhanced inhibition of its intracellular target (DNA gyrase), and translation to antibacterial activity in whole cells was demonstrated. Overall, cell permeation was governed by physicochemical properties and bacterial type. A principal component analysis demonstrated that the dimers occupy a unique and privileged region of chemical space most similar to the macrolide class of antibiotics. (C) 2015 Elsevier Ltd. All rights reserved.", "journal": "BIOORGANIC & MEDICINAL CHEMISTRY LETTERS", "category": "Chemistry, Medicinal; Chemistry, Organic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000377036300010", "keywords": "Second-order cone optimization problem; primal-dual interior-point methods; kernel function", "title": "A large-update primal-dual interior-point algorithm for second-order cone optimization based on a new proximity function", "abstract": "In this paper, we propose a large-update primal-dual interior-point algorithm for second-order cone optimization (SOCO) based on a class of kernel functions consisting of a trigonometric barrier term. The algorithm starts from a strictly feasible point and generates a sequence of points converging to an optimal solution of the problem. Using a simple analysis, we show that the algorithm has O(root N log N log N/is an element of) worst case iteration complexity for large-update primal-dual interior point methods which coincides with the so far best-known iteration bound for SOCO.", "journal": "OPTIMIZATION", "category": "Operations Research & Management Science; Mathematics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000362908200027", "keywords": "clinical setting; learning; nursing education; nursing students; patient safety; supportive environment; systems-based approach", "title": "Learning to ensure patient safety in clinical settings: comparing Finnish and British nursing students' perceptions", "abstract": "Aims and objectives. To explore and compare Finnish and British nursing students' perceptions of their learning about patient safety in clinical settings. Background. Patient safety culture and practices in different health care organisations and clinical units varies, posing challenges for nursing students' learning about patient safety during their clinical placements. Patient safety as a growing international concern has challenged health care professionals globally requiring a comprehensive review. International studies comparing nursing education about patient safety are lacking. Design. A cross-sectional comparative study. Method. The participants were final year preregistration nursing students from two universities of applied sciences in Finland (n = 195) and from two universities in England, UK (n = 158). The data were collected with the Patient Safety in Nursing Education Questionnaire and analysed with principal component analysis, Pearson Chi-Square and Mann-Whitney U tests and logistic regression. Results. Finnish nursing students had significantly more critical perceptions on their learning about patient safety in clinical settings than their British peers. A strong predictor for differences was supportive and systems-based approaches in learning to ensure patient safety. Notably, fewer Finnish students had practiced reporting of incidents in clinical settings compared to British students. In both countries, the students held learning about patient safety in higher esteem compared to their learning experiences in clinical settings. Conclusions. Nursing students appear to want more learning opportunities related to patient safety compared to the reality in clinical settings. Learning systematically from errors in a supportive environment and having systems-based approaches to ensure patient safety are essential elements for nursing students' learning about safe practice. Finnish students seem to experience more barriers in learning about safe practices and to report errors than the British students. Relevance to clinical practice. Health care organisations and professionals with responsibilities for patient safety should seek to standardise the preparation of nursing students incorporating requisite international standards and benchmarks.", "journal": "JOURNAL OF CLINICAL NURSING", "category": "Nursing", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000369257400008", "keywords": "promising sour cherries; genetic variation; identification; SSR markers", "title": "Genetic variation and identification of promising sour cherries inferred from microsatellite markers", "abstract": "The aim of this study was to identify the group of highly polymorphic microsatellite markers for identification of promising sour cherries. From among 30 tested microsatellite (SSR) markers, 19 were selected to profile genetic variation in sour cherries due to high polymorphisms. Results indicated a high level of polymorphism of the accessions based on these markers. Totally 148 alleles were generated at 19 SSR loci which 122 alleles were polymorphic. The number of total alleles per locus ranged from 2 to 15 with an average of 7.78 and polymorphism percentage varied from 50 to 100% with an average of 78.76%. Also, PIC varied from 0.47 to 0.89 with an average of 0.79 and heterozygosity ranged from 0.35 to 0.55 with a mean of 0.45. According to these results, these markers specially PMS3, PS12A02, PceGA34, BPPCT021, EMPA004, EMPA018, and Pchgms3 produced good and various levels of amplifications and showed high heterozygosity levels. By the way, the genetic similarity showed a high diversity among the sour cherries. Cluster analysis separated improved cultivars from promising sour cherries, and the PCoA supported the cluster analysis results. Since the studied sour cherries were superior to the improved cultivars and were separated from them in most groups, these sour cherries can be considered as distinct genotypes for further evaluations in the framework of breeding programs and new cultivar identification in cherries. Results also confirmed that the set of microsatellite markers employed in this study demonstrated usefulness of microsatellite markers for the identification of sour cherry genotypes.", "journal": "RUSSIAN JOURNAL OF GENETICS", "category": "Genetics & Heredity", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000366647000022", "keywords": "Graphene; AlGaN/GaN heterostructures; Interface properties; Raman spectroscopy", "title": "Raman spectroscopy of graphene on AlGaN/GaN heterostructures", "abstract": "In this paper, we report Raman mapping of graphene on AlGaN/GaN heterostructure on GaN/Si substrates. Graphene samples are prepared using exfoliation technique and transferred to AlGaN/GaN heterostructures with GaN and SiN cap layers. AlGaN induced charge accumulation is observed in graphene. Significant intensity reduction is observed in the Raman spectra in the AlGaN/GaN heterostructure peaks with graphene. We anticipate that this work provides further insights of graphene, AlGaN/GaN interfaces and can be used to further develop sensors and devices. (C) 2015 Elsevier B.V. All rights reserved.", "journal": "THIN SOLID FILMS", "category": "Materials Science, Multidisciplinary; Materials Science, Coatings & Films; Physics, Applied; Physics, Condensed Matter", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000365825700012", "keywords": "Polycystic ovary syndrome; Metabolic syndrome; Neck circumference", "title": "The prevalence of metabolic syndrome in polycystic ovary syndrome in a South Indian population and the use of neck circumference in defining metabolic syndrome", "abstract": "Polycystic ovary syndrome (PCOS) is a common endocrine disorder in young women with a high prevalence of insulin resistance (IR) and metabolic syndrome (MetS). The prevalence of MetS differs based on the defining criteria used. Neck circumference (NC) has been proposed as a surrogate marker of MetS which is simple and easy to perform in the outpatient setting. The aim of the study was to estimate the prevalence of metabolic syndrome in women with PCOS and to study the use of NC in defining metabolic syndrome. This was a prospective observational cross-sectional study involving 121 PCOS patients over a period of 2 years. The prevalence of metabolic syndrome was estimated using the modified Adult Treatment Panel (ATP) III criteria as well as the International Diabetes Federation (IDF) criteria. The Pearson correlation coefficient was used to find the degree of correlation between NC and waist circumference (WC). The Receiver operating characteristic (ROC) curves of NC were used to predict the metabolic syndrome. The independent sample t test and the Mann-Whitney U test were used for comparing the average NC and WC between the groups of patients with and without MetS. The prevalence of MetS was found to be 30.6 % using the modified ATP III criteria and 52 % using the IDF criteria. There is a statistically significant positive correlation between NC and WC (r = 0.758, p < 0.001). The mean NC is higher in patients who have MetS by both criteria (p < 0.001). Based on ROC curve analysis, the NC cutoff of 33.5 cm detected MetS (by IDF criteria) with a sensitivity of 60.3 % and a specificity of 70.7 % (area under ROC curve = 0.70, p < 0.001) and the NC cutoff of 33.87 cm detected MetS (by ATP III criteria) with a sensitivity of 73 % and a specificity of 69 % (area under ROC curve = 0.722 p < 0.001). The IDF criteria identified a higher number of PCOS subjects with MetS compared to the ATP III criteria. NC correlated very well with MetS as well as WC, and this could replace the waist circumference to define MetS in the future.", "journal": "INTERNATIONAL JOURNAL OF DIABETES IN DEVELOPING COUNTRIES", "category": "Endocrinology & Metabolism", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000369252200003", "keywords": "synchronization; discrete-time network; impulsive control; pinning control", "title": "Impulsive pinning synchronization of discrete-time network", "abstract": "Combining impulsive and pinning control, we investigate the synchronization problem of discrete-time network. In the proposed pinning control scheme, the controlled nodes are chosen according to the norm of the synchronization errors at different impulsive instants. Based on the Lyapunov function method and mathematical analysis technique, two synchronization criteria with respect to the impulsive gains and intervals are analytically derived. Both undirected and directed discrete-time networks coupled with Chirikov standard maps are performed in numerical examples to verify the effectiveness of the derived results.", "journal": "ADVANCES IN DIFFERENCE EQUATIONS", "category": "Mathematics, Applied; Mathematics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000365173100083", "keywords": "face patches; body patches; visual integration; visual context", "title": "Whole-agent selectivity within the macaque face-processing system", "abstract": "The primate brain contains a set of face-selective areas, which are thought to extract the rich social information that faces provide, such as emotional state and personal identity. The nature of this information raises a fundamental question about these face-selective areas: Do they respond to a face purely because of its visual attributes, or because the face embodies a larger social agent? Here, we used functional magnetic resonance imaging to determine whether the macaque face patch system exhibits a whole-agent response above and beyond its responses to individually presented faces and bodies. We found a systematic development of whole-agent preference through the face patches, from subadditive integration of face and body responses in posterior face patches to superadditive integration in anterior face patches. Superadditivity was not observed for faces atop nonbody objects, implying categorical specificity of face-body interaction. Furthermore, superadditivity was robust to visual degradation of facial detail, suggesting whole-agent selectivity does not require prior face recognition. In contrast, even the body patches immediately adjacent to anterior face areas did not exhibit superadditivity. This asymmetry between face-and body-processing systems may explain why observers attribute bodies' social signals to faces, and not vice versa. The development of whole-agent selectivity from posterior to anterior face patches, in concert with the recently described development of natural motion selectivity from ventral to dorsal face patches, identifies a single face patch, AF (anterior fundus), as a likely link between the analysis of facial shape and semantic inferences about other agents.", "journal": "PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000367921500003", "keywords": "Children; Critical illness; Fellowship; Intensive care; Mortality", "title": "Association of house staff training with mortality in children with critical illness", "abstract": "Aim: To evaluate the association of house staff training with mortality in children with critical illness. Methods: Patients <18 years of age in the Virtual PICU Systems (VPS, LLC) Database (2009-2013) were included. The study population was divided in two study groups: hospitals with residency programme only and hospitals with both residency and fellowship programme. Control group constituted hospitals with no residency or fellowship programme. The primary study outcome was mortality before intensive care unit (ICU) discharge. Multivariable logistic regression models were fitted to evaluate association of training programmes with ICU mortality. Results: A total of 336 335 patients from 108 centres were included. Case-mix of patients among the hospitals with training programmes was complex; patients cared for in the hospitals with training programmes had greater severity of illness, had higher resource utilisation and had higher overall admission risk of death compared to patients cared for in the control hospitals. Despite caring for more complex and sicker patients, the hospitals with training programmes were associated with lower odds of ICU mortality. Conclusion: Our study establishes that ICU care provided in hospitals with training programmes is associated with improved adjusted survival rates among the Virtual PICU database hospitals in the United States.", "journal": "ACTA PAEDIATRICA", "category": "Pediatrics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000360344200033", "keywords": "Kinect; Ground detection; Particle filter; Point-cloud; Indoor mobile robot; Map-based position estimation; RGB-depth sensor; Boundary edges estimation; Absolute self-localization", "title": "Method of determining effects of heat-induced irregular refractive index on an optical system", "abstract": "The effects of an irregular refractive index on optical performance are examined. A method was developed to express a lens's irregular refractive index distribution. An optical system and its mountings were modeled by a thermomechanical finite element (FE) program in the predicted operating temperature range, -45 degrees C-50 degrees C. FE outputs were elaborated using a MATLAB optimization routine; a nonlinear least squares algorithm was adopted to determine which gradient equation best fit each lens's refractive index distribution. The obtained gradient data were imported into Zemax for sequential ray-tracing analysis. The root mean square spot diameter, modulation transfer function, and diffraction ensquared energy were computed for an optical system under an irregular refractive index and under thermoelastic deformation. These properties are greatly reduced by the irregular refractive index effect, which is one-third to five-sevenths the size of the thermoelastic deformation effect. Thus, thermal analyses of optical systems should consider not only thermoelastic deformation but also refractive index irregularities caused by inhomogeneous temperature. (C) 2015 Optical Society of America", "journal": "APPLIED OPTICS", "category": "Optics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000368244600002", "keywords": "X-ray diffraction; Molecular beam epitaxy; Nitrides; GaN; Semiconducting gallium compounds", "title": "Anomalous elongation of c-axis of GaN on Al2O3 grown by MBE using NH3-cluster ions", "abstract": "GaN thin films were grown on Al2O3 (0001) by MBE using NH3-clusters either ionized with the energy of 4-7 eV/molecule (ionized Cluster Beam, i-CB) or un-ionized with the energy of about 0.1 eV/molecule (neutral Cluster Beam, n-CB) at growth temperatures ranging from 390 to 960 degrees C. The c-axis is extremely elongated but the a-axis is shrunken at the initial growth stage (up to the film thickness of about 10 nm) in GaN grown by the mixture of n- and i-CB under N-rich condition. The films thicker than 30 nm have the relaxed a- and c-axis lengths close to the unstrained values and obey the Poisson relation. GaN grown by i-CB under Ga-rich condition have the relaxed lattice constants obeying the Poisson relation for the film as thin as 6 nm. In GaN grown by the cluster beam (CB) which is not ionized intentionally, both a- and c-axis lengths are almost independent of the film thickness, having nearly the same values as those of the unstrained samples. These characteristics can be ascribed to the nature of interface between the nitrided Al2O3 substrate and epilayer. It is concluded that the films grown by i-CB bond firmly to underlay AIN than the films by n-CB and CB. (C) 2015 Elsevier B.V. All rights reserved.", "journal": "JOURNAL OF CRYSTAL GROWTH", "category": "Crystallography; Materials Science, Multidisciplinary; Physics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000366750200007", "keywords": "Water pipeline; Replacement planning; Group scheduling; Multiobjective optimization; Service interruption; Replacement decision optimization model for group scheduling (RDOM-GS)", "title": "Optimized Group Replacement Scheduling for Water Pipeline Network", "abstract": "Replacement of deteriorated water pipes is a capital-intensive activity for utility companies. Replacement planning aims to minimize total costs while maintaining a satisfactory level of service and is usually conducted for individual pipes. Scheduling replacement in groups is seen to be a better method and has the potential to provide benefits such as the reduction of maintenance costs and service interruptions. However, developing group replacement schedules is a complex task and often beyond the ability of a human expert, especially when multiple or conflicting objectives need to be catered for, such as minimization of total costs and service interruptions. This paper describes the development of a novel replacement decision optimization model for group scheduling (RDOM-GS), which enables multiple group-scheduling criteria by integrating new cost functions, a service interruption model, and optimization algorithms into a unified procedure. An industry case study demonstrates that RDOM-GS can improve replacement planning significantly and reduce costs and service interruptions. (C) 2015 American Society of Civil Engineers.", "journal": "JOURNAL OF WATER RESOURCES PLANNING AND MANAGEMENT", "category": "Engineering, Civil; Water Resources", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000361797500076", "keywords": "Deep-sea fish; Commercial fish; Carboxylesterases; Biotransformation enzymes; Antioxidant enzymes", "title": "Vitamin D Deficiency and Depressive Symptomatology in Psychiatric Patients Hospitalized with a Current Depressive Episode: A Factor Analytic Study", "abstract": "Background Low vitamin D levels have been associated with depressive symptoms in population-based studies and non-clinical samples as well as with clinical depression. This study aimed to examine the association of vitamin D levels with the severity and dimensions of depressive symptoms in hospitalized patients with a current episode of depression taking into account confounding variables. Methods We investigated 380 patients (mean age 47 +/- 12 years, 70% women) who were consecutively hospitalized with a main diagnosis of an ICD-10 depressive episode. All patients self-rated depressive symptom severity with the Hospital Anxiety and Depression Scale (HADS-D), the Beck Depression Inventory-II (BDI-II), and the Brief Symptom Inventory. A principal component analysis was performed with all 34 items of these questionnaires and serum levels of 25-hydroxyvitamin D3 (25-OH D) were measured. Results Vitamin D deficiency (<50 nmol/l), insufficiency (50-75 nmol/l), and sufficiency (>75 nmol/l) were present in 55.5%, 31.8% and 12.6%, respectively, of patients. Patients with vitamin D deficiency scored higher on the HADS-D scale and on an anhedonia symptom factor than those with insufficient (p-values <= 0.023) or sufficient (p-values <= 0.008) vitamin D. Vitamin D deficient patients also scored higher on the BDI-II scale than those with sufficient vitamin D (p = 0.007); BDI-II cognitive/affective symptoms, but not somatic/affective symptoms, were higher in patients with vitamin D deficiency (p = 0.005) and insufficiency (p = 0.041) relative to those with sufficient vitamin D. Effect sizes suggested clinically relevant findings. Conclusions Low vitamin D levels are frequent in hospitalized patients with a current episode of depression. Especially 25-OH D levels <50 nmol/l were associated with cognitive/affective depressive symptoms, and anhedonia symptoms in particular.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000360507700015", "keywords": "Precipitation; Palaeoclimate; Climate reconstruction; Regional climate modelling; Proxy; PPE", "title": "Establishing the skill of climate field reconstruction techniques for precipitation with pseudoproxy experiments", "abstract": "This study aims at assessing the skill of several climate field reconstruction techniques (CFR) to reconstruct past precipitation over continental Europe and the Mediterranean at seasonal time scales over the last two millennia from proxy records. A number of pseudoproxy experiments are performed within the virtual reality of a regional paleoclimate simulation at 45 km resolution to analyse different aspects of reconstruction skill. Canonical Correlation Analysis (CCA), two versions of an Analog Method (AM) and Bayesian hierarchical modeling (BHM) are applied to reconstruct precipitation from a synthetic network of pseudoproxies that are contaminated with various types of noise. The skill of the derived reconstructions is assessed through comparison with precipitation simulated by the regional climate model. Unlike BHM, CCA systematically underestimates the variance. The AM can be adjusted to overcome this shortcoming, presenting an intermediate behaviour between the two aforementioned techniques. However, a trade-off between reconstruction-target correlations and reconstructed variance is the drawback of all CFR techniques. CCA (BHM) presents the largest (lowest) skill in preserving the temporal evolution, whereas the AM can be tuned to reproduce better correlation at the expense of losing variance. While BHM has been shown to perform well for temperatures, it relies heavily on prescribed spatial correlation lengths. While this assumption is valid for temperature, it is hardly warranted for precipitation. In general, none of the methods outperforms the other. All experiments agree that a dense and regularly distributed proxy network is required to reconstruct precipitation accurately, reflecting its high spatial and temporal variability. This is especially true in summer, when a specifically short de-correlation distance from the proxy location is caused by localised summertime convective precipitation events.", "journal": "CLIMATE DYNAMICS", "category": "Meteorology & Atmospheric Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000372162300012", "keywords": "coronary computed tomography angiography; iterative reconstruction; coronary calcification; stent", "title": "Effect of iterative reconstruction on image quality in evaluating patients with coronary calcifications or stents during coronary computed tomography angiography: a pilot study", "abstract": "Objective: To determine the effect of \"Iterative Reconstruction in Image Space\" (IRIS) on image quality by comparing reconstructions of both medium and sharp kernels when evaluating coronary calcifications or stents during coronary computed tomography (CT) angiography. Methods: Thirty one consecutive patients were scanned with an electrocardiogram-gated helical technique on a dual-source CT system. Image reconstruction was performed using standard filtered back projection (FBP) and IRIS algorithm on both medium and sharp kernels (B26f, I26f, B46f, I46f). Each reconstruction was derived from the same raw data. Two blinded readers graded image quality using a five-point scale. Noise, signal-to-noise ratio (SNR), contrast-to-noise ratio (CNR) were obtained. Noise was derived from the ascending aorta and left ventricle. SNR was obtained from sinus Valsalva, interventricular septum, and coronary vessels. CNR was obtained from septum, coronary vessels, and left ventricle. Comparisons of paired results between FBP and IRIS images were analyzed using the repeated measures analysis of variance method. Interreader correlation was assessed using weighted Kappa statistic. Results: Noise values of the ascending aorta and left ventricle were significantly lower in the images reconstructed with IRIS than those reconstructed with FBP for the evaluation of the same filters. SNR and CNR values were higher in the IRIS images (p<0.05). Interreader agreement for four reconstructions was interpreted as moderate (kappa = 0.40-0.59). Conclusion: IRIS significantly reduced image noise and improved imaging of coronary calcifications or stents. When combined with a sharp kernel, IRIS can improve image quality by reducing the negative effects of decreased signal that may result from using a sharp kernel.", "journal": "ANATOLIAN JOURNAL OF CARDIOLOGY", "category": "Cardiac & Cardiovascular Systems", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000363967900015", "keywords": "Arrhythmia; Cardiopulmonary resuscitation; Heart arrest; Pharmacology", "title": "Volume versus outcome: More emergency medical services personnel on-scene and increased survival after out-of-hospital cardiac arrest", "abstract": "Background and aim: : The large regional variation in survival after treatment of out-of-hospital cardiac arrest (OHCA) is incompletely explained. Communities respond to OHCA with differing number of emergency medical services (EMS) personnel who respond to the scene. The effect of different numbers of EMS personnel on-scene upon outcomes is unclear. We sought to evaluate the association between number of EMS personnel on-scene and survival after OHCA. Methods: We performed a retrospective review of prospectively collected data on 16,122 EMS-treated OHCA events from December 1, 2005 to May 31, 2007 from a combined population over 21 million people residing in an area of over 33,000 square miles in Canada and the United States. Number of EMS personnel on-scene was defined as the number of EMS personnel who responded to the scene of OHCA within 15 min after 9-1-1 call receipt and prior to patient death or transport away from the scene. Associations with survival to hospital discharge were assessed by using generalized estimating equations to construct multivariable logistic regression models. Results: Compared to a reference number of EMS personnel on-scene of 5 or 6, 7 or 8 EMS personnel on-scene was associated with a higher rate of survival to hospital discharge, adjusted odds ratio [OR], 1.35 (95% CI: 1.05, 1.73). There was no significant difference in survival between 5 or 6 personnel on-scene versus fewer. Conclusion: More EMS personnel on-scene within 15 min of 9-1-1 call was associated with improved survival of out-of-hospital cardiac arrest. It is unlikely that this finding was mediated solely by earlier CPR or earlier defibrillation. (C) 2015 Elsevier Ireland Ltd. All rights reserved.", "journal": "RESUSCITATION", "category": "Critical Care Medicine; Emergency Medicine", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000363130900076", "keywords": "face perception; behavioral genetics; cognitive psychology; twin study", "title": "Genetic specificity of face recognition", "abstract": "Specific cognitive abilities in diverse domains are typically found to be highly heritable and substantially correlated with general cognitive ability (g), both phenotypically and genetically. Recent twin studies have found the ability to memorize and recognize faces to be an exception, being similarly heritable but phenotypically substantially uncorrelated both with g and with general object recognition. However, the genetic relationships between face recognition and other abilities (the extent to which they share a common genetic etiology) cannot be determined from phenotypic associations. In this, to our knowledge, first study of the genetic associations between face recognition and other domains, 2,000 18- and 19-year-old United Kingdom twins completed tests assessing their face recognition, object recognition, and general cognitive abilities. Results confirmed the substantial heritability of face recognition (61%), and multivariate genetic analyses found that most of this genetic influence is unique and not shared with other cognitive abilities.", "journal": "PROCEEDINGS OF THE NATIONAL ACADEMY OF SCIENCES OF THE UNITED STATES OF AMERICA", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000371353700512", "keywords": "Self-trapped exciton; Impurity-trapped exciton; Lanthanide; Ytterbium; NaMgF3; VUV spectroscopy", "title": "Prevalence of circulating Tumour Cells in Patients with hormone naive, osseous metastatic Prostate Cancer (PCa). First Results of the Biomarker Analysis of the ProMPT- Study", "abstract": "Results of a vacuum ultraviolet spectroscopic characterization of NaMgF3:Yb2+ are presented. The material demonstrates emission features associated with self-trapped excitons and impurity-trapped excitons. The emission features noticeably overlap giving rise to a broad emission band from 17 000 to 35 000 cm(-1) at a sample temperature of 8 K. To identify the true profiles of the emission features we have used a deconvolution procedure. The deconvolution was possible due to the thermal quenching of self-trapped excitons at room temperature that allowed for direct observations of the impurity trapped exciton emission band. Energy transfer between host electronic excitations (excitons and e-h pairs) and Yb2+ ions leading to the formation of impurity-trapped excitons is evident from excitation spectra. (C) 2015 Elsevier B.V. All rights reserved.", "journal": "ONCOLOGY RESEARCH AND TREATMENT", "category": "Oncology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000370302503099", "keywords": "Nonnegative matrix factorization; Semi-supervised clustering; User-driven clustering; Regularization", "title": "Validation of an Automated Platform to Assess the Total Inflammatory Load in Kidney Transplant Biopsies Using CD45 Immunohistochemistry and Computer-Assisted Quantitative Image Analysis", "abstract": "Mixed ratios of native and released ayu Plecoglossus altivelis altivelis caught in the Takatsu River were estimated for years 2009 to 2012. The stock of ayu in this river consists of three types: native amphidromous and two released hatchery-reared forms bred at different hatcheries. Mixed ratios were estimated by fitting a multiple normal distribution to the frequency distribution of the scale number above the lateral line using the maximum likelihood method. The data of scale numbers above the lateral line of ayu caught in the upper and lower waters of the dam in this river were resampled by the bootstrap method at 2,000 repetitions, and then each bootstrap distribution was fitted to the multiple normal distribution with two peaks. The native ayu ratios were estimated at 26.3-58.8. and 82.6-100., respectively. These estimates reflected upstreaming conditions of native ayu, and were considered to be proper values.", "journal": "MODERN PATHOLOGY", "category": "Pathology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000374668200005", "keywords": "biomass; foods; oleic acid; palm trees", "title": "Hyperspectral Image Denoising with a Combined Spatial and Spectral Weighted Hyperspectral Total Variation Model", "abstract": "Hyperspectral image (HSI) denoising is a prerequisite for many subsequent applications. For an HSI, the level and type of noise often vary with different bands and spatial positions, which make it difficult to effectively remove noise while preserving textures and edges. To alleviate this problem, we propose a new total-variation model. The main contribution of the proposed approach lies in that the adaptive regularization terms in both the spatial and the spectral dimensions are designed separately and then combined into a unified framework. The 2 separate regularization terms allow a better description of the intrinsic nature of the original HSI data and can simultaneously penalize the noise from both the spatial and spectral perspectives. The designed weights for the regularization terms are positively correlated with the magnitude of the noise intensity and negatively correlated with the signal variation; thus, the original signal can be accurately retained and the noise can be effectively suppressed. To efficiently process the HSI, which appears as a huge data cube, a new optimization algorithm based on the alternating direction method of multipliers (ADMM) procedure is proposed to solve the new model. Experiments using HYDICE and AVIRIS images were conducted to validate the effectiveness of the proposed method.Resume. Hyperspectrale l'image (HSI) debruitage est une condition prealable pour de nombreuses applications ulterieures. Pour un HSI, le niveau et le type de bruit varie souvent avec differents groupes et positions spatiales, ce qui rend difficile d'eliminer efficacement le bruit tout en preservant les textures et les bords. Pour pallier ce probleme, nous proposons un nouveau modele de variation totale. Les principales contributions de l'approche proposee mensonge dans la conception des termes de regularisation adaptative dans les deux dimensions spatiales et spectrales, et en les combinant dans un cadre unifie. Les deux termes de regularisation separes permettent une meilleure description de la nature intrinseque des donnees HSI original et peuvent penaliser simultanement le bruit a la fois des perspectives spatiales et spectrales. Les poids concus pour les termes de regularisation sont en correlation positive avec la grandeur de l'intensite du bruit et correlation negative avec la variation de signal; ainsi, le signal d'origine peut etre retenu avec precision et le bruit peut etre efficacement supprimee. Pour traiter efficacement le HSI, qui apparait comme un enorme cube de donnees, un nouvel algorithme d'optimisation base sur la methode de direction alternee de multiplicateurs << alternating direction method of multipliers >> (ADMM) procedure est proposee pour resoudre le nouveau modele. Des experiences utilisant des images AVIRIS et HYDICE et ont ete menees afin de valider l'efficacite de la methode proposee.", "journal": "CANADIAN JOURNAL OF REMOTE SENSING", "category": "Remote Sensing", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000367260000027", "keywords": "Behavioral study; big data analysis; taxis", "title": "Analysis of Taxi Drivers' Behaviors Within a Battle Between Two Taxi Apps", "abstract": "A battle between two Chinese taxi booking mobile apps, namely, Didi and Kuaidadi, had recently occurred in early 2014. These two apps, which are backed by Internet giants Tencent and Alipay, gave promotion fees to taxi drivers for each deal made and also allowed each taxi passenger to save some money, when a customer had taken a taxi through the app and paid the fare through the mobile payment method. As expected, the taxi service pattern had been greatly changed during this battle. To address the debates on social justice, equity, and improvements of taxi service, we collect 37-day trip data of over 9000 taxis in Beijing to study the influence of this pattern change. In the first 18 days, the battle had not occurred and in the remaining 19 days, the battle is white-hot. We quantitatively demonstrate how several important service indices (e.g., the traveling distances and idle time lengths) of taxi drivers had been changed. The spatial-temporal traveling patterns of taxis are then studied. Based on comprehensive analysis, the benefits and drawbacks brought by money promotion are finally discussed. The obtained results indicate that productively employing big data can help answer some important questions attracting the interest of the whole society.", "journal": "IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS", "category": "Engineering, Civil; Engineering, Electrical & Electronic; Transportation Science & Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000365041300064", "keywords": "milling; stability; variable spindle speed; delay-differential equation; numerical integration method; Floquet theory", "title": "Strain-effect transistors: Theoretical study on the effects of external strain on III-nitride high-electron-mobility transistors on flexible substrates", "abstract": "This paper presents strain-effect transistors (SETs) based on flexible III-nitride high-electron-mobility transistors (HEMTs) through theoretical calculations. We show that the electronic band structures of InAlGaN/GaN thin-film heterostructures on flexible substrates can be modified by external bending with a high degree of freedom using polarization properties of the polar semiconductor materials. Transfer characteristics of the HEMT devices, including threshold voltage and transconductance, are controlled by varied external strain. Equilibrium 2-dimensional electron gas (2DEG) is enhanced with applied tensile strain by bending the flexible structure with the concave-side down (bend-down condition). 2DEG density is reduced and eventually depleted with increasing compressive strain in bend-up conditions. The operation mode of different HEMT structures changes from depletion-to enchantment-mode or vice versa depending on the type and magnitude of external strain. The results suggest that the operation modes and transfer characteristics of HEMTs can be engineered with an optimum external bending strain applied in the device structure, which is expected to be beneficial for both radio frequency and switching applications. In addition, we show that drain currents of transistors based on flexible InAlGaN/GaN can be modulated only by external strain without applying electric field in the gate. The channel conductivity modulation that is obtained by only external strain proposes an extended functional device, gate-free SETs, which can be used in electro-mechanical applications. (C) 2015 AIP Publishing LLC.", "journal": "APPLIED PHYSICS LETTERS", "category": "Physics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000369254900008", "keywords": "GEM distribution; GEM diffusion process; Harnack inequality; Heat kernel; Super log-Sobolev inequality", "title": "Harnack Inequality and Applications for Infinite-Dimensional GEM Processes", "abstract": "The dimension-free Harnack inequality and uniform heat kernel upper/lower bounds are derived for a class of infinite-dimensional GEM processes, which was introduced in Feng and Wang (J. Appl. Probab. 44 938-949 2007) to simulate the two-parameter GEM distributions. In particular, the associated Dirichlet form satisfies the super log-Sobolev inequality which strengthens the log-Sobolev inequality derived in Feng and Wang (J. Appl. Probab. 44 938-949 2007). To prove the main results, explicit Harnack inequality and super Poincar, inequality are established for the one-dimensional Wright-Fisher diffusion processes. The main tool of the study is the coupling by change of measures.", "journal": "POTENTIAL ANALYSIS", "category": "Mathematics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000362220500004", "keywords": "corpus callosum; diffusion tensor imaging; mathematical difficulties; mild traumatic brain injury; pediatrics", "title": "Mathematical Difficulties and White Matter Abnormalities in Subacute Pediatric Mild Traumatic Brain Injury", "abstract": "Mathematical difficulties have been documented following pediatric mild traumatic brain injury (mTBI), yet a precise characterization of these impairments and their neural correlates is currently unavailable. We aimed to characterize these impairments by comparing behavioral and neuroimaging (i.e., diffusion tensor imaging [DTI]) outcomes from children with subacute mTBI to typically-developing controls. Twenty subacute pediatric mTBI patients and 20 well-matched controls underwent cognitive assessment and DTI examination. DTI tractography was used to detect white matter abnormalities in the corpus callosum (CC) and superior and inferior longitudinal fasciculi; these tracts are involved in mathematical performance and they are often damaged after mTBI. Behavioral results revealed that children with mTBI performed significantly more poorly on rapid apprehension of small numbers of objects (or subitizing), processing of non-symbolic numerosities, and procedural problem solving. These group differences were explained by differences in visuospatial working memory, which suggests that the observed mathematical difficulties may be a consequence of impairments in visuospatial abilities. DTI analysis revealed subtle group differences in the CC genu and splenium (i.e., higher fractional anisotropy and lower mean and radial diffusivity in children with mTBI) but the observed white matter abnormalities of the CC were not significantly associated with the observed mathematical difficulties in the mTBI patients.", "journal": "JOURNAL OF NEUROTRAUMA", "category": "Critical Care Medicine; Clinical Neurology; Neurosciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000366122300007", "keywords": "FoodWiki; Food knowledge base; Safety food consumption; Mobile E-health systems; Food side effects", "title": "FoodWiki: a Mobile App Examines Side Effects of Food Additives Via Semantic Web", "abstract": "In this article, a research project on mobile safe food consumption system (Food Wiki) is discussed that performs its own inferencing rules in its own knowledge base. Currently, the developed rules examines the side effects that are causing some health risks: heart disease, diabetes, allergy, and asthma as initial. There are thousands compounds added to the processed food by food producers with numerous effects on the food: to add color, stabilize, texturize, preserve, sweeten, thicken, add flavor, soften, emulsify, and so forth. Those commonly used ingredients or compounds in manufactured foods may have many side effects that cause several health risks such as heart disease, hypertension, cholesterol, asthma, diabetes, allergies, alzheimer etc. according to World Health Organization. Safety in food consumption, especially by patients in these risk groups, has become crucial, given that such health problems are ranked in the top ten health risks around the world. It is needed personal e-health knowledge base systems to help patients take control of their safe food consumption. The systems with advanced semantic knowledge base can provide recommendations of appropriate foods before consumption by individuals. The proposed Food Wild system is using a concept based search mechanism that performs on thousands food compounds to provide more relevant information.", "journal": "JOURNAL OF MEDICAL SYSTEMS", "category": "Health Care Sciences & Services; Medical Informatics", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000359173200013", "keywords": "Chronic stress; Oligodendrocytes; Sgk1; Desmosome; Major depression", "title": "Sgk1 regulates desmoglein 1 expression levels in oligodendrocytes in the mouse corpus callosum after chronic stress exposure", "abstract": "Major depression, one of the most prevalent mental illnesses, is thought to be a multifactorial disease related to both genetic and environmental factors. However, the genes responsible for and the pathogenesis of major depression at the molecular level remain unclear. Recently, we reported that stressed mice with elevated plasma corticosterone levels show upregulation and activation of serum glucocorticoid-regulated kinase (Sgk1) in oligodendrocytes. Active Sgk1 causes phosphorylation of N-myc downstream-regulated gene 1 (Ndrg1), and phospho-Ndrg1 increases the expression of N-cadherin, alpha-catenin, and beta-catenin in oligodendrocytes. This activation of the Sgk1 cascade results in morphological changes in the oligodendrocytes of nerve fiber bundles, such as those present in the corpus callosum. However, little is known about the molecular functions of the traditional and/or desmosomal cadherin superfamily in oligodendrocytes. Therefore, in this study, we aimed to elucidate the functions of the desmosomal cadherin superfamily in oligodendrocytes. Desmoglein (Dsg) 1, Dsg2, and desmocollin 1 (Dsc1) were found to be expressed in the corpus callosum of mouse brain, and the expression of a subtype of Dsg1, Dsg1c, was upregulated in oligodendrocytes after chronic stress exposure. Furthermore, Dsg1 proteins were localized around the plasma membrane regions of oligodendrocytes. A study in primary oligodendrocyte cultures also revealed that chronic upregulation of Sgk1 by dexamethasone administration is involved in upregulation of Dsg1c mRNA. These results may indicate that chronic stress induced Sgk1 activation in oligodendrocytes, which increases Dsg1 expression near the plasma membrane. Thus, Dsg1 upregulation may be implicated in the molecular mechanisms underlying the morphological changes in oligodendrocytes in response to chronic stress exposure. (C) 2015 Elsevier Inc. All rights reserved.", "journal": "BIOCHEMICAL AND BIOPHYSICAL RESEARCH COMMUNICATIONS", "category": "Biochemistry & Molecular Biology; Biophysics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000359743300025", "keywords": "polycystic ovary syndrome (PCOS); persistent organic pollutants (POPs); case-control study; endocrine-disrupting chemicals (EDCs); partial least-squares-discriminant analysis (PLS-DA)", "title": "Association of serum levels of typical organic pollutants with polycystic ovary syndrome (PCOS): a case-control study", "abstract": "STUDY QUESTION: Is polycystic ovary syndrome (PCOS) associated with increased serum levels of typical organic pollutants? summary answer: PCOS in Han females from Northern China was significantly associated with elevated serum levels of pollutants, including polychlorinated biphenyls (PCBs), organochlorine pesticides and polycyclic aromatic hydrocarbons (PAHs). what is known already: PCOS is arguably the most common endocrinopathy in females of reproductive age. The etiology of PCOS is thought to be multifactorial. study design, size, duration: This was a preliminary case-control study undertaken at the Division of Reproductive Center, Peking University Third Hospital. Fifty participants affected by PCOS and 30 normal controls were recruited between August and October 2012 from Northern China. All participants were Han women. participants/materials, setting, methods: PCOS participants were diagnosed according to the 2003 Rotterdam criteria. The control participants were non-pregnant females unable to conceive solely due to male azoospermia. Serum levels of a wide range of organic pollutants, including PCBs, organochlorine pesticides, PAHs and more than 20 phenolic pollutants, were analyzed using gas chromatographic mass spectrometry. main results and the role of chance: Serum levels of PCBs, pesticides and PAHs were significantly higher in the PCOS group than the control group. Concentrations of PCBs, p,p'-dichlorodiphenyldichloroethylene (p,p'-DDE) and PAHs in serum above median levels were associated with PCOS with odds ratios of 3.81 [95% confidence interval (CI), 1.45-10.0], 4.89 (95% CI, 1.81-13.2) and 2.39 (95% CI, 0.94-6.05), respectively. Partial least-squares-discriminant analysis (PLS-DA) confirmed that serum levels of organic pollutants were associated with PCOS, especially for p, p'-DDE and PCBs. limitations, reasons for caution: Some other possible covariates (e.g. dietary and income) were missed in this study, although education and occupation have been considered as an indicator of personal income. The PLS-DA model allowed a quasi-exposome analysis with over 60 kinds of typical organic pollutants; however, the possibility of other pollutants involved in the PCOS still could not be excluded. wider implications of the findings: Our study identified that bodily retention of environmental organic pollutants-including PCBs, pesticides (especially p, p'-DDE) and PAHs-was associated with PCOS.", "journal": "HUMAN REPRODUCTION", "category": "Obstetrics & Gynecology; Reproductive Biology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000362668400044", "keywords": "28S rRNA; base substitution rate; BEAST; COI; ecology; MEGA; phylogeny; raxmlGUI", "title": "Carbon as a source for yellow luminescence in GaN: Isolated C-N defect or its complexes", "abstract": "We study three carbon defects in GaN, isolated C-N and its two complexes with donors C-N-O-N, and C-N-Si-Ga, as a cause of the yellow luminescence using accurate hybrid density functional calculation, which includes the semi-core Ga 3d electrons as valence electrons and uses a larger 300-atom supercell. We show that the isolated C-N defect yields good agreement with experiment on the photoluminescence (PL) peak position, zero-phonon line, and thermodynamic defect transition level. We find that the defect state of the complexes that is involved in the PL process is the same as that of the C-N defect. The role of the positively charged donors (O-N or Si-Ga) next to C-N is to blue-shift the PL peak. Therefore, the complexes cannot be responsible for the same PL peak as isolated C-N. Our detailed balance analysis further suggests that under thermal equilibrium at typical growth temperature, the concentration of isolated C-N defect is orders of magnitude higher than the defect complexes, which is a result of the small binding energy in these complexes. (C) 2015 AIP Publishing LLC.", "journal": "JOURNAL OF APPLIED PHYSICS", "category": "Physics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000365686400121", "keywords": "direction of arrival (DOA) estimation; sparse representations; eigenvalue decomposition (EVD); off-grid; Rife algorithm", "title": "A Modified Rife Algorithm for Off-Grid DOA Estimation Based on Sparse Representations", "abstract": "In this paper we address the problem of off-grid direction of arrival (DOA) estimation based on sparse representations in the situation of multiple measurement vectors (MMV). A novel sparse DOA estimation method which changes MMV problem to SMV is proposed. This method uses sparse representations based on weighted eigenvectors (SRBWEV) to deal with the MMV problem. MMV problem can be changed to single measurement vector (SMV) problem by using the linear combination of eigenvectors of array covariance matrix in signal subspace as a new SMV for sparse solution calculation. So the complexity of this proposed algorithm is smaller than other DOA estimation algorithms of MMV. Meanwhile, it can overcome the limitation of the conventional sparsity-based DOA estimation approaches that the unknown directions belong to a predefined discrete angular grid, so it can further improve the DOA estimation accuracy. The modified Rife algorithm for DOA estimation (MRife-DOA) is simulated based on SRBWEV algorithm. In this proposed algorithm, the largest and sub-largest inner products between the measurement vector or its residual and the atoms in the dictionary are utilized to further modify DOA estimation according to the principle of Rife algorithm and the basic idea of coarse-to-fine estimation. Finally, simulation experiments show that the proposed algorithm is effective and can reduce the DOA estimation error caused by grid effect with lower complexity.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000363727900011", "keywords": "CDM; Technology transfer; Emission reduction; Logistic regression; China", "title": "Copy number variations in multiple sclerosis and neuromyelitis optica", "abstract": "ObjectiveTo clarify the potential association of copy number variations (CNVs) with multiple sclerosis (MS) and neuromyelitis optica (NMO) in Japanese cases. MethodsGenome-wide association analyses of CNVs among 277 MS patients, 135 NMO/NMO spectrum disorder (NMOSD) patients, and 288 healthy individuals as a discovery cohort, and among 296 MS patients, 76 NMO/NMOSD patients, and 790 healthy individuals as a replication cohort were performed using high-density single nucleotide polymorphism microarrays. ResultsA series of discovery and replication studies revealed that most identified CNVs were 5 to 50kb deletions at particular T cell receptor (TCR) gamma and alpha loci regions. Among these CNVs, a TCR gamma locus deletion was found in 16.40% of MS patients (p=2.44E-40, odds ratio [OR]=52.6), and deletion at the TCR alpha locus was found in 17.28% of MS patients (p=1.70E-31, OR=13.0) and 13.27% of NMO/NMOSD patients (p=5.79E-20, OR=54.6). These CNVs were observed in peripheral blood T-cell subsets only, suggesting the CNVs were somatically acquired. NMO/NMOSD patients carrying the CNV tended to be seronegative for anti-aquaporin-4 antibody or had significantly lower titers than those without CNV. InterpretationDeletion-type CNVs at specific TCR loci regions contribute to MS and NMO susceptibility. Ann Neurol 2015;78:Ann Neurol 2015;78:679-696", "journal": "ANNALS OF NEUROLOGY", "category": "Clinical Neurology; Neurosciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000369524500004", "keywords": "Maximum likelihood estimator; sub-fractional Brownian motion; Stein's method; Malliavin calculus", "title": "Map-based prediction of organic carbon in headwater streams improved by downstream observations from the river outlet", "abstract": "In spite of the great abundance and ecological importance of headwater streams, managers are usually limited by a lack of information about water chemistry in these headwaters. In this study we test whether river outlet chemistry can be used as an additional source of information to improve the prediction of the chemistry of upstream headwaters (size < 2 km(2)), relative to models based on map information alone. We use the concentration of total organic carbon (TOC), an important stream ecosystem parameter, as the target for our study. Between 2000 and 2008, we carried out 17 synoptic surveys in 9 mesoscale catchments (size 32-235 km(2)). Over 900 water samples were collected in total, primarily from headwater streams but also including each catchment's river outlet during every survey. First we used partial least square regression (PLS) to model the distribution (median, interquartile range (IQR)) of headwater stream TOC for a given catchment, based on a large number of candidate variables including sub-catchment characteristics from GIS, and measured river chemistry at the catchment outlet. The best candidate variables from the PLS models were then used in hierarchical linear mixed models (MM) to model TOC in individual headwater streams. Three predictor variables were consistently selected for the MM calibration sets: (1) proportion of forested wetlands in the sub-catchment (positively correlated with headwater stream TOC), (2) proportion of lake surface cover in the sub-catchment (negatively correlated with headwater stream TOC), and (3) river outlet TOC (positively correlated with headwater stream TOC). Including river outlet TOC improved predictions, with 5-15% lower prediction errors than when using map information alone. Thus, data on water chemistry measured at river outlets offer information which can complement GIS-based modelling of headwater stream chemistry.", "journal": "BIOGEOSCIENCES", "category": "Ecology; Geosciences, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000368577200001", "keywords": "DNA words; DNA vocabulary; Integrity of word", "title": "Extracting DNA words based on the sequence features: non-uniform distribution and integrity", "abstract": "Background: DNA sequence can be viewed as an unknown language with words as its functional units. Given that most sequence alignment algorithms such as the motif discovery algorithms depend on the quality of background information about sequences, it is necessary to develop an ab initio algorithm for extracting the \"words\" based only on the DNA sequences. Methods: We considered that non-uniform distribution and integrity were two important features of a word, based on which we developed an ab initio algorithm to extract \"DNA words\" that have potential functional meaning. A Kolmogorov-Smirnov test was used for consistency test of uniform distribution of DNA sequences, and the integrity was judged by the sequence and position alignment. Two random base sequences were adopted as negative control, and an English book was used as positive control to verify our algorithm. We applied our algorithm to the genomes of Saccharomyces cerevisiae and 10 strains of Escherichia coli to show the utility of the methods. Results: The results provide strong evidences that the algorithm is a promising tool for ab initio building a DNA dictionary. Conclusions: Our method provides a fast way for large scale screening of important DNA elements and offers potential insights into the understanding of a genome.", "journal": "THEORETICAL BIOLOGY AND MEDICAL MODELLING", "category": "Mathematical & Computational Biology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000367070400011", "keywords": "Online algorithms; Relative load; Competitive ratio", "title": "Minimum average relative load for online routing", "abstract": "We are given a network where each edge has an associated capacity or bandwidth. Requests for connections arrive online; each request specifies the source and sink and the requested bandwidth. This paper presents an algorithm for routing of messages. At the time of arrival of each request, if its bandwidth is bigger than associated bound, then the request is rejected. Otherwise, the algorithm assigns it a path to send the message. We define a new routing objective, which is called the average relative load on edges. Our objective is different from the previously studied objective (Aspnes et al. in J ACM 44:486-504, 1997; Buchbinder and Naor 2006), minimizing the maximum relative load. The algorithm assigns the paths such that the competitive ratio of the algorithm is -1/2 beta / (alpha + 1/2 beta) = Omicron(1), where beta < 0 and -1/2 beta < alpha < - 1/beta. Also, an upper bound Omicron(log(a) n) for the relative load of each edge is presented, where 1/2 <= a < 1.", "journal": "WIRELESS NETWORKS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000361750200007", "keywords": "H filters; Markov processes; Gaussian distribution; linear matrix inequalities; finite time H filtering; incomplete transition probabilities; probability approach; discrete Markov jump system; truncated Gaussian distribution; filtering error systems; finite-time stochastic stable; H filter design; linear matrix inequalities", "title": "Finite-time H-infinity filtering of Markov jump systems with incomplete transition probabilities: a probability approach", "abstract": "This paper concerns the finite-time H filtering of discrete Markov jump system with incomplete transition probabilities which cover the cases of known, uncertain and unknown. To include all possible cases, with the probability viewpoint, a truncated Gaussian distribution is employed to describe them. To ensure the filtering error systems to be finite-time stochastic stable with a prescribed noise attenuation level, sufficient conditions for the H filter design are yielded in terms of solvability of a set of linear matrix inequalities. A numerical example is given to illustrate the effectiveness of the proposed method.", "journal": "IET SIGNAL PROCESSING", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000374902300002", "keywords": "Wind speed; direction; frequent patterns; sequential pattern mining; sliding window; coordinated multiple views; interactive visualization", "title": "Interactive discovery of sequential patterns in time series of wind data", "abstract": "Wind speed and direction vary over space and time due to the interactions between different pressures and temperature gradients within the atmospheric layers. Near the earth's surface, these interactions are modulated by topography and artificial structures. Hence, characterizing wind behaviour over large areas and long periods is a complex but essential task for various energy-related applications. In this study, we present a novel approach to discover wind patterns by integrating sequential pattern mining and interactive visualization techniques. The approach relies on the use of the Linear time Closed pattern Miner sequence algorithm in conjunction with a time sliding window that allows the discovery of all sequential patterns present in the data. These patterns are then visualized using integrated 2D and 3D coordinated multiple views and visually explored to gain insight into the characteristics of the wind from a spatial, temporal and attribute (type of wind pattern) point of view. This proposed approach is used to analyse 10years of hourly wind speed and direction data for 29 weather stations in the Netherlands. The results show that there are 15 main sequential patterns in the data. The spatial task shows that weather stations located in the same region do not necessarily experience similar wind pattern. For within the selected time interval, similar wind patterns can be observed in different stations and in the same station at different times of occurrence. The attribute task discovered that the repetitive occurrences of chosen pattern indicate as regular wind behaviour at different weather stations that persisted continuously over time. The results of these tasks show that the proposed interactive discovery facilitates the understanding of wind dynamics in space and time.", "journal": "INTERNATIONAL JOURNAL OF GEOGRAPHICAL INFORMATION SCIENCE", "category": "Computer Science, Information Systems; Geography; Geography, Physical; Information Science & Library Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000368959300017", "keywords": "Profile recovery; LAMOST; Instantaneous frequency; Wigner-Ville distribution", "title": "Profile recovery for LAMOST fiber spectral data with low SNR via improved time-frequency peak filtering", "abstract": "A new profile recovery method which is called time-frequency peak filtering (TFPF) is introduced to low signal-to-noise ratio (SNR) astronomical fiber spectral signals. TFPF is an effective noise removal method, which is essentially instantaneous frequency (IF) estimation based or(Wigner-Ville distribution (WVD). To achieve a better noise suppression result of spectral signal, we propose an improved TFPF, which combines the pseudo WVD (PWVD) and the smoothed pseudo WVD (SPWVD). The proposed method produces a data fusion from the outcomes of PWVD and SPWVD to sharpen the time-frequency distribution and adopts a sine function to eliminate interference points in time-frequency domain. We test our method on both simulated and observed data based on the Large Sky Area Multi-Object Fiber Spectroscopy Telescope (LAMOST) project. The results show good performance of the proposed method. (C) 2015 Elsevier GmbH. All rights reserved.", "journal": "OPTIK", "category": "Optics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000366439500004", "keywords": "Edge tracing; Extension; Voronoi diagram; Additively weighted; Computational geometry", "title": "Extension of the edge tracing algorithm to disconnected Voronoi skeletons", "abstract": "One of the ways how to describe spatial relations among balls in Euclidean space is to use an additively weighted Voronoi diagram, where each ball has a region of closest points assigned to it. The boundary of regions is formed by non-linear faces, edges and vertices. Edges and vertices form a skeleton of the diagram. An edge tracing algorithm exists, which finds an initial vertex and then traces edges until the whole component is discovered. The problem is that the skeleton can have more components but the algorithm does not necessarily discover them all. We introduce an extension of the edge tracing algorithm to discover all components. (C) 2015 Elsevier B.V. All rights reserved.", "journal": "INFORMATION PROCESSING LETTERS", "category": "Computer Science, Information Systems", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000364880300002", "keywords": "Luminescence; Cerium; White LED; Phosphor", "title": "A novel high thermal stability Ce3+-doped Ca-5(SiO4)(2)F-2 blue-emitting phosphor for near UV-excited white light-emitting diodes", "abstract": "A novel high thermal stability Cas(SiO4)(2)F-2:Ce3+,Na+ blue-emitting phosphor was synthesized by solid-state reaction. The excitation spectrum shows a broad band extending from 200 nm to 390 nm, and the emission spectrum shows a broad blue band peaking at 408 nm with a half width of approximately 75 nm. The optimum doping concentration of Ce3+ is found to be 8 mol%. Its luminescence intensity at 420 K still kept by approximately 86% of the initial value at room temperature. The calculated CIE coordinates indicate that the phosphor has good color purity. Given their broadband absorption from 250 nm to 390 nm, these phosphors meet the application requirements for GaN-based light-emitting diodes. (C) 2015 Elsevier B.V. All rights reserved.", "journal": "MATERIALS LETTERS", "category": "Materials Science, Multidisciplinary; Physics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000374464400001", "keywords": "Subgraph isomorphism; Graph class; Polynomial-time algorithm; NP-completeness", "title": "Coordinated Precoding for D2D Communications Underlay Uplink MIMO Cellular Networks", "abstract": "We study the coordinated precoding problem for device-to-device (D2D) communications underlay multiple-input multiple-output (MIMO) cellular networks. The system model considered here constitutes multiple D2D user pairs attempting to share the uplink radio resources of a cellular network. We first formulate the coordinated precoding problem for the D2D user pairs as a sum-rate maximization (SRM) problem, which is subject to a total interference power constraint imposed to protect the base station (BS) and individual transmit power budgets available for each D2D user pair. Since the formulated SRM problem is nonconvex in general, we reformulate it as a difference convex-(DC-) type programming problem, which can be iteratively solved by employing the famous successive convex approximation (SCA) method. Moreover, a proximal-point-based regularization approach is also pursued here to ensure the convergence of the proposed algorithm. Interestingly, the centralized precoding algorithm can also lend itself to a distributed implementation. By introducing a price-based interference management mechanism, we reformulate the coordinated precoding problem as a Stackelberg game. Then, a distributed precoding algorithm is developed based on the concept of Stackelberg equilibrium (SE). Finally, numerical simulations are also provided to demonstrate the proposed algorithms. Results show that our algorithms can converge fast to a satisfactory solution with guaranteed convergence.", "journal": "MOBILE INFORMATION SYSTEMS", "category": "Computer Science, Information Systems; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000379774500003", "keywords": "DASH; Wireless multimedia", "title": "Random Matrix Ensembles with Singularities and a Hierarchy of Painlev, III Equations", "abstract": "We study unitary invariant random matrix ensembles with singular potentials. We obtain asymptotics for the partition functions associated to the Laguerre and Gaussian Unitary Ensembles perturbed with a pole of order at the origin, in the double scaling limit where the size of the matrices grows, and at the same time the strength of the pole decreases at an appropriate speed. In addition, we obtain double scaling asymptotics of the correlation kernel for a general class of ensembles of positive-definite Hermitian matrices perturbed with a pole. Our results are described in terms of a hierarchy of higher order analogs to the PIII equation, which reduces to the PIII equation itself when the pole is simple.", "journal": "INTERNATIONAL MATHEMATICS RESEARCH NOTICES", "category": "Mathematics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000366289600006", "keywords": "hepatitis E virus; seroprevalence; predictors; anti-HEV IgG antibody; children; Nigeria", "title": "Seroprevalence and predictors of hepatitis E infection in Nigerian children", "abstract": "Introduction: Hepatitis E is a hepatotropic virus transmitted through the fecal-oral route and is prevalent in developing countries where sanitation is still a public health issue. There is no epidemiological data about this virus in Nigerian children. All the existing studies are hospital based, with obvious limitations. This study was conducted to establish the seroprevalence and predictors of viral hepatitis E antibody in children in Akpabuyo Local Government Area of Cross River State, Nigeria. Methodology: This was a community-based, cross-sectional study. A multi-staged sampling technique was used to select ten communities from which 406 children were recruited. The study period was April to June 2012. A structured interviewer-administered questionnaire was used for data collection. Blood samples were screened for anti-HEV IgG antibody using the enzyme-linked immunoassay technique. Multivariate logistic regression was used to identify factors that independently predicted the occurrence of the anti-HEV IgG antibody. A p value of < 0.05 was considered significant. Results: The seroprevalence rate of anti-HEV IgG antibody was 7.7% (95% CI = 5.1-10.3). The study population mainly (94.1%) comprised the lower social class. Levels of social amenities in these communities were generally poor, with virtually no piped water and modern sewage disposal systems. After multivariate analysis, the predictor of infection was the duration of residence in the study communities. Conclusions: HEV infection was prevalent in the study population. Educational campaigns and provision of good sewage disposal and piped water are of high necessity.", "journal": "JOURNAL OF INFECTION IN DEVELOPING COUNTRIES", "category": "Infectious Diseases", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000370693800055", "keywords": "Photoluminescence; Luminescence enhancement; Magnetic field; Energy level splitting; Optical materials", "title": "A prototype algorithm to calculate health-based exposure limits for a safe use of pesticides", "abstract": "Sputter deposited thin film AlN:Er (1 at.%) emits at 554 nm and 561 nm as a result of H-2(11/2) -> I-4(11/2) and S-4(3/2) -> I-4(15/2) transitions under 532 nm NdYAG laser and 783.3 nm crystal laser excitation. An external magnetic field of 0.1 T enhances the green emission and splits the S-4(3/2) energy level in two sub-levels with a difference of 0.013 eV. The splitting of energy level produces new emission from Er3+ with a wavelength of 564.5 nm. Infrared emission is also observed at 1552 nm as a result of I-4(13/2) -> I-4(15/2) transition. Enhanced luminescence shows the suitability of Er3+ for high efficiency optical devices. (C) 2015 Elsevier B.V. All rights reserved.", "journal": "TOXICOLOGY LETTERS", "category": "Toxicology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000371784100001", "keywords": "variable memristor; memristor emulator; Chua's circuit; memductance; chaotic dynamics", "title": "Variable cubic-polynomial memristor based canonical Chua's chaotic circuit", "abstract": "In this letter, the Chua's circuit with an active variable memristor is presented for the first time. The central concept behind the variable memristor is that its memductance could be controlled by changing the cubic function which describes a relationship between the flux and charge of the memristor. It was found that chaotic dynamics of the variable-memristor-based Chua's circuit could be easily controlled according to the memductance profile of the memristor. The controllable chaotic dynamics in the circuit were studied thoroughly by the numerical analysis. The rich dynamics of the Chua's circuit using the proposed variable memristor were verified in terms of the time series, frequency spectra, phase states, bifurcation and Lyapunov exponents. These results were also confirmed by laboratory experiments.", "journal": "IEICE ELECTRONICS EXPRESS", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000364891200007", "keywords": "Diffusion magnetic resonance imaging; Limited gradient directions; Sparse reconstruction; Prior directional knowledge; Interdigitated tongue muscles", "title": "A Bayesian approach to distinguishing interdigitated tongue muscles from limited diffusion magnetic resonance imaging", "abstract": "The tongue is a critical organ for a variety of functions, including swallowing, respiration, and speech. It contains intrinsic and extrinsic muscles that play an important role in changing its shape and position. Diffusion tensor imaging (DTI) has been used to reconstruct tongue muscle fiber tracts. However, previous studies have been unable to reconstruct the crossing fibers that occur where the tongue muscles interdigitate, which is a large percentage of the tongue volume. To resolve crossing fibers, multi-tensor models on DTI and more advanced imaging modalities, such as high angular resolution diffusion imaging (HARDI) and diffusion spectrum imaging (DSI), have been proposed. However, because of the involuntary nature of swallowing, there is insufficient time to acquire a sufficient number of diffusion gradient directions to resolve crossing fibers while the in vivo tongue is in a fixed position. In this work, we address the challenge of distinguishing interdigitated tongue muscles from limited diffusion magnetic resonance imaging by using a multi-tensor model with a fixed tensor basis and incorporating prior directional knowledge. The prior directional knowledge provides information on likely fiber directions at each voxel, and is computed with anatomical knowledge of tongue muscles. The fiber directions are estimated within a maximum a posteriori (MAP) framework, and the resulting objective function is solved using a noise-aware weighted l(1)-norm minimization algorithm. Experiments were performed on a digital crossing phantom and in vivo tongue diffusion data including three control subjects and four patients with glossectomies. On the digital phantom, effects of parameters, noise, and prior direction accuracy were studied, and parameter settings for real data were determined. The results on the in vivo data demonstrate that the proposed method is able to resolve interdigitated tongue muscles with limited gradient directions. The distributions of the computed fiber directions in both the controls and the patients were also compared, suggesting a potential clinical use for this imaging and image analysis methodology. (C) 2015 Elsevier Ltd. All rights reserved.", "journal": "COMPUTERIZED MEDICAL IMAGING AND GRAPHICS", "category": "Engineering, Biomedical; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000360299100038", "keywords": "Occupational health; Health risk appraisal; Work ability; Functional capacity evaluation; Meat-packing industry", "title": "Atrial Fibrillation and Colonic Neoplasia in African Americans", "abstract": "Background Colorectal cancer (CRC) and atrial fibrillation/flutter (AF) share several risk factors including increasing age and obesity. However, the association between CRC and AF has not been thoroughly examined, especially in African Americans. In this study we aimed to assess the prevalence of AF and its risk factors in colorectal neoplasia in an African American. Methods We reviewed records of 527 African American patients diagnosed with CRC and 1008 patients diagnosed with benign colonic lesions at Howard University Hospital from January 2000 to December 2012. A control group of 731 hospitalized patients without any cancer or colonic lesion were randomly selected from the same time and age range, excluding patients who had diagnosis of both CRC and/or adenoma. The presence or absence of AF was based upon ICD-9 code documentation. The prevalence of AF in these three groups was compared by multivariate logistic regression. Results The prevalence of AF was highest among CRC patients (10%) followed by adenoma patients (7.2%) then the control group (5.4%, P for trend = 0.002). In the three groups of participants, older age (P<0.008) and heart failure (P<0.001) were significantly associated with higher risk of AF. After adjusting for these risk factors, CRC (OR: 1.4(95% CI): 0.9-2.2, P = 0.2) and adenoma (OR: 1.1(95% CI): 0.7-1.6, P = 0.7) were not significantly associated AF compared to control group. Conclusions AF is highly prevalent among CRC patients; 1 in 10 patients had AF in our study. The predictors of AF in CRC was similar to that in adenoma and other patients after adjustment for potential confounders suggesting that the increased AF risk in CRC is explained by higher prevalence of AF risk factors.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000368596100008", "keywords": "antibiotic; breast reconstruction; TOPS; transverse rectus abdominis myocutaneous; latissimus; free flap", "title": "Impact of Postoperative Antibiotic Prophylaxis Duration on Surgical Site Infections in Autologous Breast Reconstruction", "abstract": "Background Although some surgeons prescribe prolonged postoperative antibiotics after autologous breast reconstruction, evidence is lacking to support this practice. We used the Tracking Operations and Outcomes for Plastic Surgeons database to evaluate the association between postoperative antibiotic duration and the rate of surgical site infection (SSI) in autologous breast reconstruction. Study Design The intervention of interest for this study was postoperative duration of antibiotic prophylaxis: either discontinued 24 hours after surgery or continued beyond 24 hours. The primary outcome variable of interest for this study was the presence of SSI within 30 days of autologous breast reconstruction. Cohort characteristics and 30-day outcomes were compared using (2) and Fischer exact tests for categorical variables and Student t tests for continuous variables. Multivariate logistic regression was used to control for confounders. Results A total of 1036 patients met inclusion criteria for our study. Six hundred fifty-nine patients (63.6%) received antibiotics for 24 hours postoperatively, and 377 patients (36.4%) received antibiotics for greater than 24 hours. The rate of SSI did not differ significantly between patients given antibiotics for only 24 hours and those continued on antibiotics beyond the 24-hour postoperative time period (5.01% vs 2.92%, P = 0.109). Furthermore, antibiotic duration was not predictive of SSI in multivariate regression modeling. Conclusions We did not find a statistically significant difference in the rate of SSI in patients who received 24 hours of postoperative antibiotics compared to those that received antibiotics for greater than 24 hours. These findings held for both purely autologous reconstruction as well as latissimus dorsi reconstruction in conjunction with an implant. Thus, our study does not support continuation of postoperative antibiotics beyond 24 hours after autologous breast reconstruction.", "journal": "ANNALS OF PLASTIC SURGERY", "category": "Surgery", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000370195600001", "keywords": "Orientation inhomogeneity; Real-world distribution; Scale-invariant feature transform; Least discriminability", "title": "HOW DO YOU WRITE AND PRESENT RESEARCH WELL? Q4-DO NOT METASTASIZE WITH METADISCOURSE", "abstract": "Scale-invariant feature transform (SIFT) is an algorithm to detect and describe local features in images. In the last fifteen years, SIFT plays a very important role in multimedia content analysis, such as image classification and retrieval, because of its attractive character on invariance. This paper intends to explore a new path for SIFT research by making use of the findings from neuroscience. We propose a more efficient and compact scale-invariant feature detector and descriptor by simulating visual orientation inhomogeneity in human system. We validate that visual orientation inhomogeneity SIFT (v-SIFT) can achieve better or at least comparable performance with less computation resource and time cost in various computer vision tasks under real world conditions, such as image matching and object recognition. This work also illuminates a wider range of opportunities for integrating the inhomogeneity of visual orientation with other local position-dependent detectors and descriptors. (C) 2015 Elsevier Ltd. All rights reserved.", "journal": "CANADIAN JOURNAL OF CHEMICAL ENGINEERING", "category": "Engineering, Chemical", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000367197100011", "keywords": "Simple diagrams; triangulation; punctured surface", "title": "Deh s algorithm for simple diagrams", "abstract": "Dehn gave an algorithm for deciding if two cyclic words in the standard presentation of the fundamental group of a closed oriented surface of positive genus represent the same conjugacy class. A simple diagram on a surface is a disjoint union of simple closed curves none of which bound a disk. If F is a once punctured closed surface of negative Euler characteristic, simple diagrams are classified up to isotopy by their geometric intersection numbers with the edges of an ideal triangulation of F. Simple diagrams on the unpunctured surface F can be represented by simple diagrams on F. The weight of a simple diagram is the sum of its geometric intersection numbers with the edges of the triangulation. We show that you can pass from any representative to a least weight representative via a sequence of elementary moves, that monotonically decrease weights. This leads to a geometric analog of Dehn's algorithm for simple diagrams.", "journal": "JOURNAL OF KNOT THEORY AND ITS RAMIFICATIONS", "category": "Mathematics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000360513300017", "keywords": "Production scheduling; Single machine; Deterministic; Heuristics; Stability; Robustness", "title": "Improving schedule stability in single-machine rescheduling for new operation insertion", "abstract": "The problem studied here entails inserting a new operation into an existing predictive schedule (preschedule) on a (non-preemptive) single machine by rescheduling its operations, so that the resultant schedule is the most stable one among schedules with minimal maximum tardiness. Stability is measured by the sum of absolute deviations of post-rescheduling start times from the pre-rescheduling start times. In addition to several simple heuristics, this study investigates a hybrid branch-and-bound/local-search algorithm. A large set of instances that include cases with inserted idle times allows for tests of the performance of the heuristics for preschedules with varying degrees of robustness. The results show that algorithms can be developed that significantly improve the stability of schedules with no degradation in T-max. In addition, new insights emerge into the robustness characteristics of a preschedule. Specifically, the number of gaps in the schedule, equal distribution of total slack among these gaps, and the slack introduced beyond the amount enforced by release times all have effects on schedule robustness and stability. (C) 2015 Elsevier Ltd. All rights reserved.", "journal": "COMPUTERS & OPERATIONS RESEARCH", "category": "Computer Science, Interdisciplinary Applications; Engineering, Industrial; Operations Research & Management Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000389471900010", "keywords": "Occupational injuries; Rubber tappers; Sri Lanka", "title": "Prevalence of occupational injury and its contributing factors among rubber tappers in Galle, Sri Lanka", "abstract": "Background: Rubber tapping involves carrying heavy loads, navigating rough terrain, and using sharp tools. However, little is known about occupational injury among this vulnerable working population. Objective: To assesses the prevalence, severity, and contributing factors associated with occupational injury among Sri Lankan rubber tappers and to identify possible interventions to improve occupational safety. Methods: A questionnaire was administered to 300 Sri Lankan rubber tappers. The associations between tapper characteristics and injury within the last year were examined using log-binomial regression models. Short response answers were analyzed using qualitative content analysis. Results: 300 tappers reported 594 injuries in the previous 12 months, and missed 1,080 days of work. The prevalence of one or more injuries was 49%. Factors associated with injury were being female, working an additional job, tapping with a two-handed approach, and depressive symptomology. Qualitative findings suggest three interventions to address injuries: (1) landscaping, (2) personal protective equipment, and (3) provision of eyeglasses. Conclusions: Work-related injuries are common among Sri Lankan rubber tappers. These results highlight the importance of working with and including informal workers in the creation of Sri Lankan occupational health and safety regulations. We believe that the three interventions identified by respondents could help to reduce the risk of occupational injury among rubber tappers.", "journal": "INTERNATIONAL JOURNAL OF OCCUPATIONAL AND ENVIRONMENTAL HEALTH", "category": "Public, Environmental & Occupational Health", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000394632900109", "keywords": "Stochastic fractional integro differential equations; Galerkin approximation; Convergence; Multiplicative noise", "title": "Convergence of Galerkin method for the solution of stochastic fractional integro differential equations", "abstract": "Stochastic fractional integro differential equations arise frequently in theory and applications. Finding exact solution of such equations is difficult in many cases. In this paper a numerical algorithm for approximation of the solution of these equations by means of finite dimensional Galerkin approximations is established. For stochastic equations with multiplicative noise, we prove the convergence of the method. Furthermore, we derive the convergence rate which shows efficiency and accuracy of the computed solutions. We apply the method to some problems which confirm the theoretical results and the performance of the proposed algorithm. (C) 2016 Elsevier GmbH. All rights reserved.", "journal": "OPTIK", "category": "Optics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000363478600004", "keywords": "damage; nonlinear solvers; Newton methods; thermodynamics; integration", "title": "Algorithmically imposed thermodynamic compliance for material models in mechanical simulations using the AIM method", "abstract": "Thermodynamic irreversibility can be imposed on empirical material behaviour by using an appropriate algorithm which takes the path-dependence of the degradation process into account. This new algorithm, Algorithmically Imposed Mechanics (AIM), for algorithmically irreversible mechanics, is described, and the convergence and unicity of the solutions obtained are proven. AIM is applicable to a range of mechanical behaviour and is demonstrated to work in conjunction with non-local damage with rotating cracks as well as a mixed plastic and damage behaviour. Copyright (c) 2015 John Wiley & Sons, Ltd.", "journal": "INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN ENGINEERING", "category": "Engineering, Multidisciplinary; Mathematics, Interdisciplinary Applications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000385282800035", "keywords": "matrix functions; contour integral method; fractional reaction-diffusion equation; fractional Laplacian; exponential Euler method; Krylov subspace methods; preconditioning", "title": "GPU ACCELERATED ALGORITHMS FOR COMPUTING MATRIX FUNCTION VECTOR PRODUCTS WITH APPLICATIONS TO EXPONENTIAL INTEGRATORS AND FRACTIONAL DIFFUSION", "abstract": "The efficient computation of matrix function vector products has recently become an important area of research, driven in particular by two important applications: the numerical solution of fractional partial differential equations and the integration of large systems of ordinary differential equations. In this work we consider a problem that combines these two applications in the form of a numerical solution algorithm for fractional reaction-diffusion equations that, after spatial discretization, is advanced in time using the exponential Euler method. We focus on the efficient implementation of the algorithm on graphics processing units (GPUs), as we wish to make use of the increased computational power available with this hardware. We compute the matrix function vector products using the contour integration method in [N. Hale, N. J. Higham, and L. N. Trefethen, SIAM J. Numer. Anal., 46 (2008), pp. 2505 2523]. Multiple levels of preconditioning are applied to reduce the GPU memory footprint and to further accelerate convergence. We also derive an error bound for the convergence of the contour integral method that allows us to predetermine the appropriate number of quadrature points. Results are presented that demonstrate the effectiveness of the method for large two-dimensional problems, showing a speedup of more than an order of magnitude compared to a CPU-only implementation.", "journal": "SIAM JOURNAL ON SCIENTIFIC COMPUTING", "category": "Mathematics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000386009400041", "keywords": "Opportunity cost; GDP; FDI; Middle East; Regional instability", "title": "The economic opportunity cost for countries located in crisis zones: Evidence from the Middle East", "abstract": "Whether the Middle East is a blessed or damned region is a matter of perspective and evidence. This paper investigates the effect of regional instability on countries caught in such conflict solely because of their location. By use of an interrupted time series model, an unrestricted error correction model, and the incremental capital output ratio (ICOR), the indirect economic costs of regional unrest are estimated for Jordan, as an exemplar of Middle Eastern countries. Jordan has lost during 24 years of regional turmoil the equivalent of 40-72% of its 2012 gross domestic product (GDP), or US$12.6 billion to US$22.7 billion. Furthermore, it has lost US$2.3 billion of foreign direct investment (FDI) and its return, which are higher than the annual FDI inflows in most of the years covered by this study. This substantial loss is a warning sign that should be seriously considered by politicians and economists in the Middle East, especially for countries whose resources are already constrained. (C) 2015 Elsevier B.V. All rights reserved.", "journal": "RESEARCH IN INTERNATIONAL BUSINESS AND FINANCE", "category": "Business, Finance", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000362512200093", "keywords": "computed tomography; misalignment; geometric parameters estimation; calibration", "title": "Geometric Parameters Estimation and Calibration in Cone-Beam Micro-CT", "abstract": "The quality of Computed Tomography (CT) images crucially depends on the precise knowledge of the scanner geometry. Therefore, it is necessary to estimate and calibrate the misalignments before image acquisition. In this paper, a Two-Piece-Ball (TPB) phantom is used to estimate a set of parameters that describe the geometry of a cone-beam CT system. Only multiple projections of the TPB phantom at one position are required, which can avoid the rotation errors when acquiring multi-angle projections. Also, a corresponding algorithm is derived. The performance of the method is evaluated through simulation and experimental data. The results demonstrated that the proposed method is valid and easy to implement. Furthermore, the experimental results from the Micro-CT system demonstrate the ability to reduce artifacts and improve image quality through geometric parameter calibration.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000366670900007", "keywords": "Anemia, sickle cell; Quality of life; Anxiety; Depression; Alcoholism", "title": "The relationship between genotype, psychiatric symptoms and quality of life in adult patients with sickle cell disease in Sao Paulo, Brazil: a cross-sectional study", "abstract": "CONTEXT AND OBJECTIVE: Health-related quality of life (HRQoL) may be worsened in sickle cell patients due to the presence of psychiatric disorders. The aims of this study were to describe the psychiatric symptoms in Brazilian sickle cell patients and to evaluate the relationship of these symptoms to the genotype of the disease and the subject's HRQoL. DESIGN AND SETTING: Cross-sectional study conducted at the hematology outpatient clinic, Hospital Sao Paulo. METHODS: Adult patients with sickle cell disease completed the Medical Outcome Study - Short Form 36 and the Patients' Health Questionnaire. Clinical data were gathered from their medical files. Linear regression models were developed to study the dependency of HRQoL domains on the genotype controlling for psychiatric symptoms. RESULTS: In the study period, 110 patients were evaluated. The most frequent psychiatric symptom was depression (30%), followed by anxiety (12.7%) and alcohol abuse (9.1%). Patients with the more severe genotype (SS and S beta thal(0)) showed lower scores for the \"general health\" and \"role-physical\" HRQoL domains, without interference from psychiatric symptoms. In the \"role-physical\" domain, the more severe genotype operated as a protective factor for HRQoL (beta = 0.255; P = 0.007). CONCLUSION: The more severe genotypes worsened HRQoL in two domains of physical health (general health and role-physical), but they did not have any influence on mental health, thus suggesting that physicians should be more attentive to aspects of HRQoL relating to the functionality of sickle cell disease patients, so as to be aware of the limitations that these patient live with.", "journal": "SAO PAULO MEDICAL JOURNAL", "category": "Medicine, General & Internal", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000367258200015", "keywords": "Automotive engineering; vehicle driving; vehicular and wireless technologies", "title": "Optimal Motion Cueing for Race Cars", "abstract": "The application of optimal control to simulator motion cueing is examined. Existing motion cueing algorithms are hampered by the fact that they do not consider explicitly the optimal usage of simulator workspace. In this paper, numerical optimal control is used to minimize simulator platform acceleration errors, while explicitly recognizing the confines of the workspace. Actuator constraints are included and the impact of restricted actuator performance is thereby facilitated. The solution of open-loop optimal control calculations are also used as a baseline against which to compare the commonly employed linear quadratic Gaussian (LQG) and model predictive control-based techniques. The limitations of these methods are identified and two additional modules are introduced to the LQG algorithm to improve its performance.", "journal": "IEEE TRANSACTIONS ON CONTROL SYSTEMS TECHNOLOGY", "category": "Automation & Control Systems; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000365371500015", "keywords": "Akaike information; Kullback-Leibler distance; model averaging; model selection; prediction", "title": "MODEL AVERAGING BASED ON KULLBACK-LEIBLER DISTANCE", "abstract": "This paper proposes a model averaging method based on Kullback-Leibler distance under a homoscedastic normal error term. The resulting model average estimator is proved to be asymptotically optimal. When combining least squares estimators, the model average estimator is shown to have the same large sample properties as the Mallows model average (MMA) estimator developed by Hansen (2007). We show via simulations that, in terms of mean squared prediction error and mean squared parameter estimation error, the proposed model average estimator is more efficient than the MMA estimator and the estimator based on model selection using the corrected Akaike information criterion in small sample situations. A modified version of the new model average estimator is further suggested for the case of heteroscedastic random errors. The method is applied to a data set from the Hong Kong real estate market.", "journal": "STATISTICA SINICA", "category": "Statistics & Probability", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000361625100019", "keywords": "Low birthweight; Neighborhoods; African Americans; Hispanics; Socioeconomic status; Community level indicators; Alcohol; Drugs", "title": "Community Level Correlates of Low Birthweight Among African American, Hispanic and White Women in California", "abstract": "Racial and ethnic groups in the US exhibit major differences in low birthweight (LBW) rates. While previous studies have shown that community level social indicators associated with LBW vary by race and ethnicity, it is not known whether these differences exist among racial or ethnic groups who live in the same neighborhood or community. To address this question, we examined the association of community level features with LBW among African American, White and Hispanic women who live in similar geographic areas. The analysis is based on geocoded birth certificates for all singleton live births in the year 2000 to women residing in 805 California ZIP codes. Community level social and demographic data were obtained from U.S. Census data files for the year 2000 and surrogate indices of population level alcohol and drug abuse and dependence were derived from hospital discharge data (HDD). Tobit and bootstrap analyses were used to test associations with birth outcomes, maternal characteristics, and community level social and demographic features within and across the three groups of women living in similar geographic areas. The results demonstrate major racial and ethnic differences in community level correlates of LBW. Rates of LBW among African Americans were lower if they lived in areas that were more densely populated, had greater income disparities, were more racially segregated, and had low rates of alcohol abuse or dependence. These associations were different or absent for Hispanic and White women. The results suggest that despite living in the same areas, major differences in neighborhood features and social processes are linked to birth outcomes of African American women compared to Hispanic and White women. Further research, especially using multilevel approaches, is needed to precisely identify these differences to help reduce racial and ethnic disparities in LBW.", "journal": "MATERNAL AND CHILD HEALTH JOURNAL", "category": "Public, Environmental & Occupational Health", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000368018800025", "keywords": "Craniofacial surgery; Le Fort I; medical robotic; surgical navigation", "title": "A Novel System for Navigation- and Robot-Assisted Craniofacial Surgery: Establishment of the Principle Prototype", "abstract": "Purpose: The authors aimed to develop 1 novel navigation-guided robotic system for craniofacial surgery to improve accuracy during operation. Materials and Methods: A new 7-DOF (7-degree-of-freedom) robotic arm was designed and manufactured. Based on our self-developed navigation system TBNAVIS-CMFS, the key technique of integration was studied. A phantom skull model was manufactured based on computed tomography image data and used for the preexperimental study. Firstly, virtual planning was achieved through the TBNAVIS-CMFS, where the Le Fort I procedure was executed through simulation. Then, the actual Le Fort 1 osteotomy was expected to perform with the use of the robotic arm following the instructions from the navigation system. Results: The theoretical prototype of navigation-guided robotic system for craniofacial surgery was established successfully, which performed the planned Le Fort I procedure with the whole process visible on the screen. Conclusions: The technical method of navigation-guided robotics system, allowing the operator to practice the virtual planning procedure through navigation system as well as perform the actual operation thru the robotic arm, could be regarded as a valuable option for benefiting craniofacial surgeons.", "journal": "JOURNAL OF CRANIOFACIAL SURGERY", "category": "Surgery", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000380237100058", "keywords": "arteriosclerosis; risk factors; peripheral arterial disease; RADIOLOGY & IMAGING", "title": "Retrospective angiographic study to determine the effect of atherosclerotic stenoses of upstream arteries on the degree of atherosclerosis in distal vascular territories", "abstract": "Objective Experimental coarctation of the aorta prevents the development of downstream atherosclerosis. The aim of this study was to find out whether or not atherosclerotic stenoses protect distal vascular territories from developing atherosclerosis in humans. Design and setting A total of 2125 vascular segments from angiographies of 101 patients were evaluated by calculating the maximum degree of stenosis (NASCET criteria), the degree of calcification, the degree of collaterals and the Friesinger score. Results Stenosis 30-49% was found in 685 vascular segments (32.2%), 50-69% in 490 (23.1%), 70-89% in 373 (17.6%) and 90% in 265 (12.5%). If a stenosis of at least 70-89% was present in the common iliac, the external iliac or the common femoral artery, the degrees of stenosis distal to it were lower than those on the contralateral side (19.822.3% (CI 11.7 to 28.0) vs 25.2 +/- 20.7% (CI 21.2 to 29.1); Friesinger scores 1.1 +/- 1.2 (CI 0.6 to 1.5) vs 1.4 +/- 1.1 (CI 1.2 to 1.6); degrees of calcification 0.8 +/- 1.0 (CI 0.4 to 1.1) vs 1.2 +/- 1.1 (CI 1.2 to 1.6); p<0.05 each). This effect depended on the degree of proximal stenosis, but not on collaterals, and was most pronounced distal to stenoses of the common iliac, the superficial femoral and the popliteal artery. In regression models, stenoses of the pelvic arteries were shown to be an independent protective factor for the distal vascular territories. Conclusions Atherosclerotic stenoses seem to protect distal vascular territories from developing atherosclerosis. The underlying pathophysiological mechanism of this phenomenon remains to be determined. It could be based on pulse pressure reduction.", "journal": "BMJ OPEN", "category": "Medicine, General & Internal", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000359033400024", "keywords": "mid-ocean ridges; CO2; incompatible elements; glacial cycles; sea level; mantle", "title": "Variations in mid-ocean ridge CO2 emissions driven by glacial cycles", "abstract": "The geological record documents links between glacial cycles and volcanic productivity, both subaerially and, tentatively, at mid-ocean ridges. Sea-level-driven pressure changes could also affect chemical properties of mid-ocean ridge volcanism. We consider how changing sea-level could alter the CO2 emissions rate from mid-ocean ridges on both the segment and global scale. We develop a simplified transport model for a highly incompatible trace element moving through a homogeneous mantle; variations in the concentration and the emission rate of the element are the result of changes in the depth of first silicate melting. The model predicts an average global mid-ocean ridge CO2 emissions rate of 53 Mt/yr or 91 Mt/yr for an average source mantle CO2 concentration of 125 or 215 ppm by weight, in line with other estimates. We show that falling sea level would cause an increase in ridge CO2 emissions about 100 kyrs after the causative sea level change. The lag and amplitude of the response are sensitive to mantle permeability and plate spreading rate. For a reconstructed sea-level time series of the past million years, we predict variations of up to 12% in global mid-ocean ridge CO2 emissions. (C) 2015 The Authors. Published by Elsevier B.V. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).", "journal": "EARTH AND PLANETARY SCIENCE LETTERS", "category": "Geochemistry & Geophysics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000370541400007", "keywords": "Frequency-dependent reflection error (FDRE); ground penetrating radar (GPR); perfectly matched layer (PML); rotated staggered grid (RSG); time-dependent reflection error (TDRE)", "title": "A New Approach of Rotated Staggered Grid FD Method With Unsplit Convolutional PML for GPR", "abstract": "We introduce a novel implementation of a rotated staggered grid (RSG) finite difference (FD) method based on the unsplit convolutional perfectly matched layer (UCPML). The method is carried out using rotated staggered FD operators and a general complex frequency shifted stretching factor. It can offer a number of advantages over the RSG with split convolutional perfectly matched layer (SCPML), such as improving computational efficiency, overcoming the numerical dispersions resulting from the high-frequency propagation and strong contrasts in material, and specifically absorbing the evanescent modes at grazing incidence. A half-space numerical example with a lossy medium attached demonstrates that large-scale spurious reflections and evanescent modes at grazing incidence are absorbed efficiently by the UCPML in RSG scheme. Meanwhile, the comparison of the reflection error between the proposed algorithm and traditional approaches [using time-dependent reflection error (TDRE) and frequency-dependent reflection error (FDRE)] suggests that the proposed algorithm of RSG provides a preferable improvement in absorbing low-frequency coda wave of evanescent modes, particularly in the lossy medium that possesses inhomogeneous discontinuities.", "journal": "IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING", "category": "Engineering, Electrical & Electronic; Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000369254700008", "keywords": "Renal tubular damage; Hypoalbuminemia; Heart failure; Cardiac prognosis", "title": "Comorbid renal tubular damage and hypoalbuminemia exacerbate cardiac prognosis in patients with chronic heart failure", "abstract": "Renal tubular damage (RTD) and hypoalbuminemia are risks for poor prognosis in patients with chronic heart failure (CHF). Renal tubules play a pivotal role in amino acid and albumin reabsorption, which maintain serum albumin levels. The aims of the present study were to (1) examine the association of RTD with hypoalbuminemia, and (2) assess the prognostic importance of comorbid RTD and hypoalbuminemia in patients with CHF. We measured N-acetyl-beta-D-glucosamidase (NAG) levels and the urinary beta 2-microglobulin to creatinine ratio (UBCR) in 456 patients with CHF. RTD was defined as UBCR a parts per thousand yen300 mu g/g or NAG a parts per thousand yen14.2 U/g. There were moderate correlations between RTD markers and serum albumin (NAG, r = -0.428, P < 0.0001; UBCR, r = -0.399, P < 0.0001). Multivariate logistic analysis showed that RTD was significantly related to hypoalbuminemia in patients with CHF. There were 134 cardiac events during a median period of 808 days. The comorbidity of RTD and hypoalbuminemia was increased with advancing New York Heart Association functional class. Multivariate Cox proportional hazard regression analysis showed that the presence of RTD and hypoalbuminemia was associated with cardiac events. The net reclassification index was significantly improved by adding RTD and hypoalbuminemia to the basic risk factors. Comorbid RTD and hypoalbuminemia are frequently observed and increase the risk for extremely poor outcome in patients with CHF.", "journal": "CLINICAL RESEARCH IN CARDIOLOGY", "category": "Cardiac & Cardiovascular Systems", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000385359700001", "keywords": "diabetes mellitus; geriatric; quality of life; hospitalized; rehabilitation", "title": "Predictors of quality of life among hospitalized geriatric patients with diabetes mellitus upon discharge", "abstract": "Purpose: Diabetes mellitus is prevalent among older adults, and affects their quality of life. Furthermore, the number is growing as the elderly population increases. Thus, this study aims to explore the predictors of quality of life among hospitalized geriatric patients with diabetes mellitus upon discharge in Malaysia. Methods: A total of 110 hospitalized geriatric patients aged 60 years and older were selected using convenience sampling method in a cross-sectional study. Sociodemographic data and medical history were obtained from the medical records. Questionnaires were used during the in-person semistructured interviews, which were conducted in the wards. Linear regression analyses were used to determine the predictors of each domain of quality of life. Results: Multiple regression analysis showed that activities of daily living, depression, and appetite were the determinants of physical health domain of quality of life (R-2=0.633, F(3, 67)=38.462; P<0.001), whereas depression and instrumental activities of daily living contributed to 55.8% of the variability in psychological domain (R-2=0.558, F(2, 68)=42.953; P<0.001). Social support and cognitive status were the determinants of social relationship (R-2=0.539, F(2, 68)=39.763; P<0.001) and also for the environmental domain of the quality of life (R-2=0.496, F(2, 68)=33.403; P<0.001). Conclusion: The findings indicated different predictors for each domain in the quality of life among hospitalized geriatric patients with diabetes mellitus. Nutritional, functional, and psychological aspects should be incorporated into rehabilitation support programs prior to discharge in order to improve patients' quality of life.", "journal": "CLINICAL INTERVENTIONS IN AGING", "category": "Geriatrics & Gerontology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000362513100052", "keywords": "Stroke; Prospective cohort; Case-fatality rates; Prognostic study; Resource limited environments; Sub-Saharan Africa", "title": "Preoperative Chronic Kidney Disease Status is an Independent Prognostic Factor in Patients with Renal Cell Carcinoma", "abstract": "Preoperative chronic kidney disease (CKD) status may affect disease outcomes in patients with renal cell carcinoma (RCC). This study evaluated the influence of preoperative CKD status on clinicopathological features and prognosis in patients with RCC. Between 1999 and 2011, a total of 1855 patients underwent radical nephrectomy at various centers throughout Korea. Of these patients, 1655 had an estimated glomerular filtration rate (eGFR) a parts per thousand yen60 ml/min/1.73 m(2) (non-CKD group) and 200 patients had an eGFR a parts per thousand yen30 but < 60 ml/min/1.73 m(2) (CKD group). To reduce the effects of selection bias and potential confounding factors, 600 patients in the non-CKD group were selected by propensity-score matching. The median age of all patients was 57.3 years (range 20-94 years) and the median follow-up was 35.0 months (range 1-154 months). Comparisons of the propensity-score-matched cohorts showed that T and N stages were more advanced and tumor size was larger in the CKD group than in the non-CKD group (p < 0.05 each). Kaplan-Meier analyses showed that recurrence-free survival, cancer-specific survival (CSS), and overall survival (OS) were significantly lower in the CKD group (p < 0.01 each). Multivariate regression analysis showed that preoperative CKD status was an independent predictor of CSS and OS in patients with RCC (p < 0.05 each). Preoperative CKD may be associated with more aggressive features and poorer prognosis in patients with RCC. RCC patients with preoperative CKD should be carefully and frequently followed-up after nephrectomy.", "journal": "ANNALS OF SURGICAL ONCOLOGY", "category": "Oncology; Surgery", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000385339800002", "keywords": "Prenatal nicotine; Intravenous exposure; Locomotor sensitization; Methamphetamine; Conditioned hyperactivity; BDNF; Rat", "title": "Intravenous Prenatal Nicotine Exposure Alters METH-Induced Hyperactivity, Conditioned Hyperactivity, and BDNF in Adult Rat Offspring", "abstract": "In the USA, approximately 15% of women smoke tobacco cigarettes during pregnancy. In utero tobacco smoke exposure produces somatic growth deficits like intrauterine growth restriction and low birth weight in offspring, but it can also negatively influence neuro developmental outcomes in later stages of life, such as an increased incidence of obesity and drug abuse. Animal models demonstrate that prenatal nicotine (PN) alters the development of the mesocorticolimbic system, which is important for organizing goal-directed behavior. In the present study, we determined whether intravenous (IV) PN altered the initiation and/or expression of methamphetamine (METH)-induced locomotor sensitization as a measure of mesocorticolimbic function in adult rat offspring. We also determined whether PN and/or METH exposure altered protein levels of BDNF (brain-derived neurotrophic factor) in the nucleus accumbens, the dorsal striatum, and the prefrontal cortex of adult offspring. BDNF was of interest because of its role in the development and maintenance of the mesocorticolimbic pathway and its ability to modulate neural processes that contribute to drug abuse, such as sensitization of the dopamine system. Dams were injected with IV nicotine (0.05 mg/kg/injection) or saline, 3x/day on gestational days 8-21. Testing was conducted when offspring reached adulthood (around postnatal day 90). Following 3 once daily habituation sessions the animals received a saline injection and baseline locomotor activity was measured. PN and prenatal saline (PS)-exposed offspring then received 10 once daily injections of METH (0.3 mg/kg) to induce locomotor sensitization. The animals received a METH injection (0.3 mg/kg) to assess the expression of sensitization following a 14-day period of no injections. A day later, all animals were injected with saline and conditioned hyperactivity was assessed. Brain tissue was harvested 24 h later. PN animals habituated more slowly to the activity chambers compared to PS controls. PN rats treated with METH showed significant enhancement of locomotor behavior compared to PS rats following acute and repeated injections; however, PN did not produce differential initiation or expression of behavioral sensitization. METH produced conditioned hyperactivity, and PN rats exhibited a greater conditioned response of hyperactivity relative to controls. PN and METH exposure produced changes in BDNF protein levels in all three regions, and complex interactions were observed between these two factors. Logistic regression revealed that BDNF protein levels, throughout the mesocorticolimbic system, significantly predicted the difference in the conditioned hyperactive response of the animals: both correlations were significant, but the predicted relationship between BDNF and context-elicited activity was stronger in the PN (r = 0.67) compared to the PS rats (r = 0.42). These findings indicate that low-dose PN exposure produces long-term changes in activity and enhanced sensitivity to the locomotor effects of METH. The enhanced METH-induced contextual conditioning shown by the PN animals suggests that offspring of in utero tobacco smoke exposure have greater susceptibility to learn about drug-related conditional stimuli, such as the context. The PN-induced alterations in mesocorticolimbic BDNF protein lend further support for the hypothesis that maternal smoking during pregnancy produces alterations in neuronal plasticity that contribute to drug abuse vulnerability. The current findings demonstrate that these changes are persistent into adult-hood. (C) 2016 S. Karger AG, Basel", "journal": "DEVELOPMENTAL NEUROSCIENCE", "category": "Developmental Biology; Neurosciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000389914600021", "keywords": "Functional data; Gaussian processes; longitudinal data; variational Bayes", "title": "Functional models for longitudinal data with covariate dependent smoothness", "abstract": "This paper considers functional models for longitudinal data with subject and group specific trends modelled using Gaussian processes. Fitting Gaussian process regression models is a computationally challenging task, and various sparse approximations to Gaussian processes have been considered in the literature to ease the computational burden. This manuscript builds on a fast non-standard variational approximation which uses a sparse spectral representation and is able to treat uncertainty in the covariance function hyperparameters. This allows fast variational computational methods to be extended to models where there are many functions to be estimated and where there is a hierarchical model involving the covariance function parameters. The main goal of this paper is to implement this idea in the context of functional models for longitudinal data by allowing individual specific smoothness related to covariates for different subjects. Understanding the relationship of smoothness to individual specific covariates is of great interest in some applications. The methods are illustrated with simulated data and a dataset of streamflow curves generated by a rainfall runoff model, and compared with MCMC. It is also shown how these methods can be used to obtain good proposal distributions for MCMC analyses.", "journal": "ELECTRONIC JOURNAL OF STATISTICS", "category": "Statistics & Probability", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000360994500002", "keywords": "body-condition index; capture probability; causality; population estimation; time-lagged weather", "title": "Avoiding verisimilitude when modelling ecological responses to climate change: the influence of weather conditions on trapping efficiency in European badgers (Meles meles)", "abstract": "The signal for climate change effects can be abstruse; consequently, interpretations of evidence must avoid verisimilitude, or else misattribution of causality could compromise policy decisions. Examining climatic effects on wild animal population dynamics requires ability to trap, observe or photograph and to recapture study individuals consistently. In this regard, we use 19years of data (1994-2012), detailing the life histories on 1179 individual European badgers over 3288 (re-) trapping events, to test whether trapping efficiency was associated with season, weather variables (both contemporaneous and time lagged), body-condition index (BCI) and trapping efficiency (TE). PCA factor loadings demonstrated that TE was affected significantly by temperature and precipitation, as well as time lags in these variables. From multi-model inference, BCI was the principal driver of TE, where badgers in good condition were less likely to be trapped. Our analyses exposed that this was enacted mechanistically via weather variables driving BCI, affecting TE. Notably, the very conditions that militated for poor trapping success have been associated with actual survival and population abundance benefits in badgers. Using these findings to parameterize simulations, projecting best-/worst-case scenario weather conditions and BCI resulted in 8.6% +/- 4.9 SD difference in seasonal TE, leading to a potential 55.0% population abundance under-estimation under the worst-case scenario; 38.6% over-estimation under the best case. Interestingly, simulations revealed that while any single trapping session might prove misrepresentative of the true population abundance, due to weather effects, prolonging capture-mark-recapture studies under sub-optimal conditions decreased the accuracy of population estimates significantly. We also use these projection scenarios to explore how weather could impact government-led trapping of badgers in the UK, in relation to TB management. We conclude that population monitoring must be calibrated against the likelihood that weather conditions could be altering trap success directly, and therefore biasing model design.", "journal": "GLOBAL CHANGE BIOLOGY", "category": "Biodiversity Conservation; Ecology; Environmental Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000367500300003", "keywords": "time fractional sub-diffusion equation; semi-infinite domain; Chebyshev-Gauss-Radau collocation scheme; Laguerre-Gauss-Radau collocation scheme; Caputo derivatives", "title": "A CHEBYSHEV-LAGUERRE-GAUSS-RADAU COLLOCATION SCHEME FOR SOLVING A TIME FRACTIONAL SUB-DIFFUSION EQUATION ON A SEMI-INFINITE DOMAIN", "abstract": "We propose a new efficient spectral collocation method for solving a time fractional sub-diffusion equation on a semi-infinite domain. The shifted Chebyshev-Gauss-Radau interpolation method is adapted for time discretization along with the Laguerre-Gauss-Radau collocation scheme that is used for space discretization on a semi-infinite domain. The main advantage of the proposed approach is that a spectral method is implemented for both time and space discretizations, which allows us to present a new efficient algorithm for solving time fractional sub-diffusion equations.", "journal": "PROCEEDINGS OF THE ROMANIAN ACADEMY SERIES A-MATHEMATICS PHYSICS TECHNICAL SCIENCES INFORMATION SCIENCE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000366956900010", "keywords": "Neurogenesis; Neural progenitor cell; Radial glia; Microarray; Morpholino; Candidate gene; In vivo imaging; Proliferation; Differentiation", "title": "An in vivo screen to identify candidate neurogenic genes in the developing Xenopus visual system", "abstract": "Neurogenesis in the brain of Xenopus laevis continues throughout larval stages of development. We developed a 2-tier screen to identify candidate genes controlling neurogenesis in Xenopus optic tectum in vivo. First, microarray and NanoString analyses were used to identify candidate genes that were differentially expressed in Sox2-expressing neural progenitor cells or their neuronal progeny. Then an in vivo, time-lapse imaging-based screen was used to test whether morpholinos against 34 candidate genes altered neural progenitor cell proliferation or neuronal differentiation over 3 days in the optic tectum of intact Xenopus tadpoles. We co-electroporated antisense morpholino oligonucleotides against each of the candidate genes with a plasmid that drives GFP expression in Sox2-expressing neural progenitor cells and quantified the effects of morpholinos on neurogenesis. Of the 34 morpholinos tested, 24 altered neural progenitor cell proliferation or neuronal differentiation. The candidates which were tagged as differentially expressed and validated by the in vivo imaging screen include: actn1, arl9, eif3a, elk4, ephb1, fmr1-a, fxr1-1, fbxw7, fgf2, gstp1, hat1, hspa5, lsm6, mecp2, mmp9, and prkaca. Several of these candidates, includingfe2 and elk4, have known or proposed neurogenic functions, thereby validating our strategy to identify candidates. Genes with no previously demonstrated neurogenic functions, gstp1, hspa5 and lsm6, were identified from the morpholino experiments, suggesting that our screen successfully revealed unknown candidates. Genes that are associated with human disease, such as such as mecp2 and fmr1-a, were identified by our screen, providing the groundwork for using Xenopus as an experimental system to probe conserved disease mechanisms. Together the data identify candidate neurogenic regulatory genes and demonstrate that Xenopus is an effective experimental animal to identify and characterize genes that regulate neural progenitor cell proliferation and differentiation in vivo. (C) 2015 The Authors. Published by Elsevier Inc.", "journal": "DEVELOPMENTAL BIOLOGY", "category": "Developmental Biology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000364492700003", "keywords": "mass spectrometry; Fourier transform; noise filtering; Markov Chain Monte Carlo (MCMC)", "title": "Accurate Identification of Mass Peaks for Tandem Mass Spectra Using MCMC Model", "abstract": "In proteomics, many methods for the identification of proteins have been developed. However, because of limited known genome sequences, noisy data, incomplete ion sequences, and the accuracy of protein identification, it is challenging to identify peptides using tandem mass spectral data. Noise filtering and removing thus play a key role in accurate peptide identification from tandem mass spectra. In this paper, we employ a Bayesian model to identify proteins based on the prior information of bond cleavages. A Markov Chain Monte Carlo (MCMC) algorithm is used to simulate candidate peptides from the posterior distribution and to estimate the parameters for the Bayesian model. Our simulation and computational experimental results show that the model can identify peptide with a higher accuracy.", "journal": "TSINGHUA SCIENCE AND TECHNOLOGY", "category": "Computer Science, Information Systems; Computer Science, Software Engineering; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000379950800014", "keywords": "Probability distributions; Marine electromagnetics; Continental margins: convergent; North America; Pacific Ocean", "title": "A new water-resistant snow index for the detection and mapping of snow cover on a global scale", "abstract": "An up-to-date spatio-temporal change analysis of global snow cover is essential for better understanding of climate-hydrological interactions. The normalized difference snow index (NDSI) is a widely used algorithm for the detection and estimation of snow cover. However, NDSI cannot discriminate between snow cover and water bodies without use of an external water mask. A stand-alone methodology for robust detection and mapping of global snow cover is presented by avoiding external dependency on the water mask. A new spectral index called water-resistant snow index (WSI) with the capability of exhibiting significant contrast between snow cover and other cover types, including water bodies, was developed. WSI uses the normalized difference between the value and hue obtained by transforming red, green, and blue, (RGB) colour composite images comprising red, green, and near-infrared bands into a hue, saturation, and value (HSV) colour model. The superiority of WSI over NDSI is confirmed by case studies conducted in major snow regions globally. Snow cover was mapped by considering monthly variation in snow cover and availability of satellite data at the global scale. A snow cover map for the year 2013 was produced at the global scale by applying the random walker algorithm in the WSI image supported by the reference data collected from permanent snow-covered and non-snow-covered areas. The resultant snow-cover map was compared to snow cover estimated by existing maps: MODIS Land Cover Type Product (MCD12Q1 v5.1, 2012), Global Land Cover by National Mapping Organizations (GLCNMO v2.0, 2008), and European Space Agency's GlobCover 2009. A significant variation in snow cover as estimated by different maps was noted, and was was attributed to methodological differences rather than annual variation in snow cover. The resultant map was also validated with reference data, with 89.46% overall accuracy obtained. The WSI proposed in the research is expected to be suitable for seasonal and annual change analysis of global snow cover.", "journal": "INTERNATIONAL JOURNAL OF REMOTE SENSING", "category": "Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000359378600004", "keywords": "time-dependent quickest path; lower bound", "title": "The time-dependent quickest path problem: Properties and bounds", "abstract": "The fast computation of point-to-point quickest paths on very large time-dependent road networks will allow next-generation web-based travel information services to take into account both congestion patterns and real-time traffic informations. The contribution of this article is threefold. First, we prove that, under special conditions, the Time-Dependent-Quickest Path Problem (QPP) can be solved as a static QPP with suitable-defined (constant) travel times. Second, we show that, if these special conditions do not hold, the static quickest path provides a heuristic solution for the original time-dependent problem with a worst-case guarantee. Third, we develop a time-dependent lower bound on the time-to-target which is both accurate and fast to compute. We show the potential of this bound by embedding it into a unidirectional A* algorithm which is tested on large metropolitan graphs. Computational results show that the new lower bound allows to reduce the computing time by 27% on average. (c) 2015 Wiley Periodicals, Inc.", "journal": "NETWORKS", "category": "Computer Science, Hardware & Architecture; Operations Research & Management Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000359875200005", "keywords": "Inverse analysis; Round panel test; Three-points beam test; Fibre reinforced concrete", "title": "Development of an inverse analysis procedure for the characterisation of softening diagrams for FRC beams and panels", "abstract": "The flexural performance of fibre reinforced concrete (FRC) materials is directly related to the stress-crack opening diagram (COD). This diagram is needed for structural design applications because it represents the energy change in the fracture process zone at the material level. In this paper, a novel inverse analysis procedure for the determination of the COD for FRC standard beams (EN14651) and round panels (ASTM C1550) is presented and validated. This procedure is based on a data fitting algorithm, for which the shape of the COD is not fixed a priori (free to vary). The efficiency of this procedure is demonstrated through experimental results for the three-points notched beam and the round panel standardised FRC characterisation tests. It is worthy to mention that inverse analysis procedures for panels are, to author's knowledge, very scarce, and this procedure can be easily extended to cover several kinds of beams and panels. (C) 2015 Elsevier Ltd. All rights reserved.", "journal": "CONSTRUCTION AND BUILDING MATERIALS", "category": "Construction & Building Technology; Engineering, Civil; Materials Science, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000401314600001", "keywords": "Wireless body area network; data forwarding; energy effciency; collaborative forwarding", "title": "An Energy-Efficiet Data Forwarding Strategy for Heterogeneous WBANs", "abstract": "Monitoring human bodies and collecting physiological information are the major healthcare applications for wireless body area networks (WBANs). Due to the limited energy resource of body sensors, their energy depletions will cause severe network performance degradation, such as latency and energy effciency. The heterogeneity of body sensors can result in different sensor energy consumption rates. Besides, the important degrees of monitored physiological data could vary hugely. Targeting at the above-mentioned problems, an energy-effcient data forwarding strategy (EDFS) is proposed in this paper to balance sensor energy consumption and improve network lifetime and collaborative operations of heterogeneous WBANs. Our major contributions include: 1) the original physiological data are processed by compressed sensing to reduce the data size to be transmitted and 2) the remaining energy levels, sampling frequency, and sensor importance are jointly considered by EDFS for the optimal relay sensor selection. With the EDFS, the energy-effciency and reliability of WBAN data transmission can be improved. In addition, our simulation results demonstrate that the proposed EDFS can effectively deal with the frequently changing WBAN topologies while providing balanced energy consumption and energy effciency.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000358942700020", "keywords": "fall detection; depth images; Signal-Gauss-Model; Dense spatio-temporal-context", "title": "Cerebral Perfusion Measurements in Elderly with Hypertension Using Arterial Spin Labeling", "abstract": "Purpose The current study assesses the feasibility and value of crushed cerebral blood flow (CBFcrushed) and arterial transit time (ATT) estimations for large clinical imaging studies in elderly with hypertension. Material and Methods Two pseudo-continuous arterial spin labeling (ASL) scans with (CBFcrushed) and without flow crushers (CBFnon-crushed) were performed in 186 elderly with hypertension, from which CBF and ATT maps were calculated. Standard flow territory maps were subdivided into proximal, intermediate and distal flow territories, based on the measured ATT. The coefficient of variation (CV) and physiological correlations with age and gender were compared between the three perfusion parameters. Results There was no difference in CV between CBFcrushed and CBFnon-crushed (15-24%, p>0.4) but the CV of ATT (4-9%) was much smaller. The total gray matter correlations with age and gender were most significant with ATT (p =.016 and p<.001 respectively), in between for CBFcrushed (p =.206 and p =.019) and least significant for CBFnon-crushed (p =.236 and p =.100). Conclusion These data show the feasibility and added value of combined measurements of both crushed CBF and ATT for group analyses in elderly with hypertension. The obtained flow territories provide knowledge on vascular anatomy of elderly with hypertension and can be used in future studies to investigate regional vascular effects.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000368070800003", "keywords": "lung ventilation model; airway network; airway constriction; asthma; reverse flow", "title": "Dynamic flow characteristics in normal and asthmatic lungs", "abstract": "Complex flow patterns exist within the asymmetric branching airway network in the lungs. These flow patterns are known to become increasingly heterogeneous during disease as a result of various mechanisms such as bronchoconstriction or alterations in lung tissue compliance. Here, we present a coupled model of tissue deformation and network airflow enabling predictions of dynamic flow properties, including temporal flow rate, pressure distribution, and the occurrence of reverse flows. We created two patient-specific airway geometries, one for a healthy subject and one for a severe asthmatic subject, derived using a combination of high-resolution CT data and a volume-filling branching algorithm. In addition, we created virtually constricted airway geometry by reducing the airway radii of the healthy subject model. The flow model was applied to these three different geometries to solve the pressure and flow distribution over a breathing cycle. The differences in wave phase of the flows in parallel airways induced by asymmetric airway geometry and bidirectional interaction between intra-acinar and airway network pressures were small in central airways but were more evident in peripheral airways. The asthmatic model showed elevated ventilation heterogeneity and significant flow disturbance. The reverse flows in the asthmatic model not only altered the local flow characteristics but also affected total lung resistance. The clinical significance of temporal flow disturbance on lung ventilation in normal airway model is obscure. However, increased flow disturbance and ventilation heterogeneity observed in the asthmatic model suggests that reverse flow may be an important factor for asthmatic lung function. Copyright (C) 2015 John Wiley & Sons, Ltd.", "journal": "INTERNATIONAL JOURNAL FOR NUMERICAL METHODS IN BIOMEDICAL ENGINEERING", "category": "Engineering, Biomedical; Mathematical & Computational Biology; Mathematics, Interdisciplinary Applications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000363075900020", "keywords": "Seismology; Mexico; Subduction; Rivera; Cocos; Middle America Trench; Jalisco block", "title": "Shallow seismicity patterns in the northwestern section of the Mexico Subduction Zone", "abstract": "This study characterizes subduction related seismicity with local deployments along the northwestern section of the Mexico Subduction Zone where 4 portions of the plate interface have ruptured in 1973, 1985, 1995, and 2003. It has been proposed that the subducted boundary between the Cocos and Rivera plates occurs beneath this region, as indicated by inland volcanic activity, a gap in tectonic tremor, and the Manzanillo Trough and Colima Graben, which are depressions thought to be associated with the splitting of the two plates after subduction. Data from 50 broadband stations that comprised the MARS seismic array, deployed from January 2006 to June 2007, were processed with the software program Antelope and its generalized source location algorithm, genloc, to detect and locate earthquakes within the network. Slab surface depth contours from the resulting catalog indicate a change in subduction trajectory between the Rivera and Cocos plates. The earthquake locations are spatially anti-correlated with tectonic tremor, supporting the idea that they represent different types of fault slip. Hypocentral patterns also reveal areas of more intense seismic activity (clusters) that appear to be associated with the 2003 and 1973 megathrust rupture regions. Seismicity concentrated inland of the 2003 rupture is consistent with slip on a shallowly dipping trajectory for the Rivera plate interface as opposed to crustal faulting in the overriding North American plate. A prominent cluster of seismicity within the suspected 1973 rupture zone appears to be a commonly active portion of the megathrust as it has been active during three previous deployments. We support these interpretations by determining focal mechanisms and detailed relocations of the largest events within the 1973 and inland 2003 clusters, which indicate primarily thrust mechanisms near the plate interface. (C) 2015 Elsevier Ltd. All rights reserved.", "journal": "JOURNAL OF SOUTH AMERICAN EARTH SCIENCES", "category": "Geosciences, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000382051600001", "keywords": "Device-to-device communication; content sharing; collaborative caching", "title": "Positive-Unlabeled Learning for Pupylation Sites Prediction", "abstract": "Pupylation plays a key role in regulating various protein functions as a crucial posttranslational modification of prokaryotes. In order to understand the molecular mechanism of pupylation, it is important to identify pupylation substrates and sites accurately. Several computational methods have been developed to identify pupylation sites because the traditional experimental methods are time-consuming and labor-sensitive. With the existing computational methods, the experimentally annotated pupylation sites are used as the positive training set and the remaining nonannotated lysine residues as the negative training set to build classifiers to predict new pupylation sites from the unknown proteins. However, the remaining nonannotated lysine residues may contain pupylation sites which have not been experimentally validated yet. Unlike previous methods, in this study, the experimentally annotated pupylation sites were used as the positive training set whereas the remaining nonannotated lysine residues were used as the unlabeled training set. A novel method named PUL-PUP was proposed to predict pupylation sites by using positive-unlabeled learning technique. Our experimental results indicated that PUL-PUP outperforms the other methods significantly for the prediction of pupylation sites. As an application, PUL-PUP was also used to predict the most likely pupylation sites in nonannotated lysine sites.", "journal": "BIOMED RESEARCH INTERNATIONAL", "category": "Biotechnology & Applied Microbiology; Medicine, Research & Experimental", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000365051100022", "keywords": "Digital elevation model; Drainage network; Hierarchical pyramid; Horton-Strahler order; River basin", "title": "A hierarchical pyramid method for managing large-scale high-resolution drainage networks extracted from DEM", "abstract": "The increasing resolution of Digital Elevation Models (DEMs) and the development of drainage network extraction algorithms make it possible to develop high-resolution drainage networks for large river basins. These vector networks contain massive numbers of river reaches with associated geographical features, including topological connections and topographical parameters. These features create challenges for efficient map display and data management. Of particular interest are the requirements of data management for multi-scale hydrological simulations using multi-resolution river networks. In this paper, a hierarchical pyramid method is proposed, which generates coarsened vector drainage networks from the originals iteratively. The method is based on the Horton-Strahler's (H-S) order schema. At each coarsening step, the river reaches with the lowest H-S order are pruned, and their related sub-basins are merged. At the same time, the topological connections and topographical parameters of each coarsened drainage network are inherited from the former level using formulas that are presented in this study. The method was applied to the original drainage networks of a watershed in the Huangfuchuan River basin extracted from a 1-m-resolution airborne LiDAR DEM and applied to the full Yangtze River basin in China, which was extracted from a 30-m-resolution ASTER GDEM. In addition, a map-display and parameter-query web service was published for the Mississippi River basin, and its data were extracted from the 30-m-resolution ASTER GDEM. The results presented in this study indicate that the developed method can effectively manage and display massive amounts of drainage network data and can facilitate multi-scale hydrological simulations. (C) 2015 Published by Elsevier Ltd.", "journal": "COMPUTERS & GEOSCIENCES", "category": "Computer Science, Interdisciplinary Applications; Geosciences, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000373722400016", "keywords": "distributed analytics; federated research network; privacy-preserving network infrastructure; comparative effectiveness research", "title": "A system to build distributed multivariate models and manage disparate data sharing policies: implementation in the scalable national network for effectiveness research", "abstract": "Background Centralized and federated models for sharing data in research networks currently exist. To build multivariate data analysis for centralized networks, transfer of patient-level data to a central computation resource is necessary. The authors implemented distributed multivariate models for federated networks in which patient-level data is kept at each site and data exchange policies are managed in a study-centric manner. Objective The objective was to implement infrastructure that supports the functionality of some existing research networks (e.g., cohort discovery, workflow management, and estimation of multivariate analytic models on centralized data) while adding additional important new features, such as algorithms for distributed iterative multivariate models, a graphical interface for multivariate model specification, synchronous and asynchronous response to network queries, investigator-initiated studies, and study-based control of staff, protocols, and data sharing policies. Materials and Methods Based on the requirements gathered from statisticians, administrators, and investigators from multiple institutions, the authors developed infrastructure and tools to support multisite comparative effectiveness studies using web services for multivariate statistical estimation in the SCANNER federated network. Results The authors implemented massively parallel (map-reduce) computation methods and a new policy management system to enable each study initiated by network participants to define the ways in which data may be processed, managed, queried, and shared. The authors illustrated the use of these systems among institutions with highly different policies and operating under different state laws. Discussion and Conclusion Federated research networks need not limit distributed query functionality to count queries, cohort discovery, or independently estimated analytic models. Multivariate analyses can be efficiently and securely conducted without patient-level data transport, allowing institutions with strict local data storage requirements to participate in sophisticated analyses based on federated research networks.", "journal": "JOURNAL OF THE AMERICAN MEDICAL INFORMATICS ASSOCIATION", "category": "Computer Science, Information Systems; Computer Science, Interdisciplinary Applications; Health Care Sciences & Services; Information Science & Library Science; Medical Informatics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000359964100021", "keywords": "Fluid-structure interaction; Arbitrary Lagrangian Eulerian; Combined field formulation; Stable explicit interface advancing scheme; Locomotion of articulated structures", "title": "A second-order stable explicit interface advancing scheme for FSI with both rigid and elastic structures and its application to fish swimming simulations", "abstract": "In this paper, we present a temporally second-order numerical scheme for fluid-structure interaction (FSI) problems in which the structure may be rigid or elastic. The explicit treatment of the interface motion and the semi-implicit treatment of all the other terms make the scheme very efficient. We prove an energy inequality of the scheme which shows that the explicit treatment of the interface motion does not damage the stability. An exact solution for FSI is derived. We use it to numerically check that our scheme converges at rate O(Delta t(2) + h(m+1)) when we use P-m/Pm-1/P-m finite elements for fluid velocity, fluid pressure and structure velocity. We also test its performance on the benchmark problem of a laminar incompressible channel flow around a compressible elastic structure. As our fluid-structure system can model both active motion and passive deformation of structures, we apply our scheme to study the locomotion of articulated structures in viscous fluids. We propose an elastic-rigid fish model which obeys all the local balance laws at the deforming interfaces. It can faithfully capture the vorticity generation and drag/thrust generation at these deforming interfaces. Our computation shows that a planar three-link fish can propel itself in a viscous fluid by periodically change its shape variables. Mesh refinement study is also performed. (C) 2015 Elsevier Ltd. All rights reserved.", "journal": "COMPUTERS & FLUIDS", "category": "Computer Science, Interdisciplinary Applications; Mechanics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000351763800017", "keywords": "Acoustic measurements; active microwave remote sensing; sea surface; underwater acoustics", "title": "Measuring the Variance of the Vertical Orbital Velocity Component by an Acoustic Wave Gauge With a Single Transceiver Antenna", "abstract": "The capabilities of a new instrument for measuring the surface wave parameters are studied in detail. An acoustic wave gauge with a single antenna is designed to measure the backscatter intensity and the Doppler spectrum of the reflected acoustic signal and to retrieve the variance of the vertical orbital velocity component. The new instrument is numerically simulated, and field experiments using a prototype acoustic wave gauge are carried out. The comparison of the variances of the vertical orbital velocity component measured by acoustic and string wave gauges under field conditions, as well as the comparison of measurements and numerical simulation results, has confirmed the high accuracy of the retrieval algorithms. The advantage of the instrument is its capability of making measurements in any water basin without the use of a fixed platform.", "journal": "IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING", "category": "Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000435923500001", "keywords": "Consensus; multiagent systems; interactive protocol; graph topology", "title": "On non-consensus motions of dynamical linear multiagent systems", "abstract": "The non-consensus problems of high-order linear time-invariant dynamical homogeneous multiagent systems are studied. Based on the conditions of consensus achievement, the mechanisms that lead to non-consensus motions are analysed. Besides, a comprehensive classification of diverse types of non-consensus phases corresponding to different conditions is conducted, which is jointly depending on the self-dynamics of the agents, the interactive protocol and the graph topology. A series of numerical examples are explained to illustrate the theoretical analysis.", "journal": "PRAMANA-JOURNAL OF PHYSICS", "category": "Physics, Multidisciplinary", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000447508700001", "keywords": "Underactuated autonomous underwater vehicle; trajectory tracking control; bioinspired velocity control; adaptive integral sliding mode control; robustness", "title": "Robust trajectory tracking control for an underactuated autonomous underwater vehicle based on bioinspired neurodynamics", "abstract": "This article investigates the three-dimensional trajectory tracking control problem for an underactuated autonomous underwater vehicle in the presence of parameter perturbations and external disturbances. An adaptive robust controller is proposed based on the velocity control strategy and adaptive integral sliding mode control algorithm. First, the desired velocities are developed using coordinate transformation and the backstepping method, which is the necessary velocities for autonomous underwater vehicle to track the time-varying desired trajectory. The bioinspired neurodynamics is used to smooth the desired velocities, which effectively avoids the jump problem of the velocity and simplifies the derivative calculation. Then, the dynamic control laws are designed based on the adaptive integral sliding mode control to drive the underactuated autonomous underwater vehicle to sail at the desired velocities. At the same time, the auxiliary control laws and the adaptive laws are introduced to eliminate the influence of parameter perturbations and external disturbances, respectively. The stability of the control system is guaranteed by the Lyapunov theorem, which shows that the system is asymptotically stable and all tracking errors are asymptotically convergent. At the end, numerical simulations are carried out to demonstrate the effectiveness and robustness of the proposed controller.", "journal": "INTERNATIONAL JOURNAL OF ADVANCED ROBOTIC SYSTEMS", "category": "Robotics", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000443162600004", "keywords": "Genomic prediction; Wheat breeding Marker density; Training set design; GenPred; Shared Data Resources; Genomic Selection", "title": "Optimising Genomic Selection in Wheat: Effect of Marker Density, Population Size and Population Structure on Prediction Accuracy", "abstract": "Genomic selection applied to plant breeding enables earlier estimates of a line's performance and significant reductions in generation interval. Several factors affecting prediction accuracy should be well understood if breeders are to harness genomic selection to its full potential. We used a panel of 10,375 bread wheat (Triticum aestivum) lines genotyped with 18,101 SNP markers to investigate the effect and interaction of training set size, population structure and marker density on genomic prediction accuracy. Through assessing the effect of training set size we showed the rate at which prediction accuracy increases is slower beyond approximately 2,000 lines. The structure of the panel was assessed via principal component analysis and K-means clustering, and its effect on prediction accuracy was examined through a novel cross-validation analysis according to the K-means clusters and breeding cohorts. Here we showed that accuracy can be improved by increasing the diversity within the training set, particularly when relatedness between training and validation sets is low. The breeding cohort analysis revealed that traits with higher selection pressure (lower allelic diversity) can be more accurately predicted by including several previous cohorts in the training set. The effect of marker density and its interaction with population structure was assessed for marker subsets containing between 100 and 17,181 markers. This analysis showed that response to increased marker density is largest when using a diverse training set to predict between poorly related material. These findings represent a significant resource for plant breeders and contribute to the collective knowledge on the optimal structure of calibration panels for genomic prediction.", "journal": "G3-GENES GENOMES GENETICS", "category": "Genetics & Heredity", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000442706100009", "keywords": "Anaerobic digestion; Bio-methane; Fixed-bed reactor; Fuzzy-logic control; Optimization; Winery wastewater", "title": "Development and pilot-scale validation of a fuzzy-logic control system for optimization of methane production in fixed-bed reactors", "abstract": "The objective of this study was to develop an advanced control system for optimizing the performance of fixed-bed anaerobic reactors. The controller aimed at maximizing the bio-methane production whilst controlling the volatile fatty acids content in the effluent. For this purpose, a fuzzy-logic controller was developed, tuned and validated in an anaerobic fixed-bed reactor at pilot scale (350 L) treating raw winery wastewater. The results showed that the controller was able to adequately optimize the process performance, maximizing the methane production in terms of methane flow rate, resulting in an average methane yield of about 0.29 L-CH4 g(-1) COD. On the other hand, the controller maintained the volatile fatty acids content in the effluent close to the established maximum limit (750 mg COD L-1). The outcomes of this study are expected to facilitate plant engineers to establish an optimal control strategy that enables an adequate process performance with the maximum bio-methane productivity. (C) 2018 Elsevier Ltd. All rights reserved.", "journal": "JOURNAL OF PROCESS CONTROL", "category": "Automation & Control Systems; Engineering, Chemical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000430968300003", "keywords": "Biological systems; computational methods; Lyapunov methods; stability of nonlinear systems", "title": "Computation of Lyapunov Functions for Nonlinear Differential Equations via a Massera-Type Construction", "abstract": "An approach for computing Lyapunov functions for nonlinear continuous-time differential equations is developed via a new, Massera-type construction. This construction is enabled by imposing a finite-time criterion on the integrated function. By means of this approach, we relax the assumptions of exponential stability on the system's equilibrium, while still allowing integration over a finite time interval. The resulting Lyapunov function can be computed based on any K-infinity-function of the norm of the solution of the system. In addition, we show how the developed converse theorem can be used to construct an estimate of the domain of attraction. Finally, a range of examples from the literature and biological applications, such as the genetic toggle switch, the repressilator, and the HPA axis, are worked out to demonstrate the efficiency and improvement in computation of the proposed approach.", "journal": "IEEE TRANSACTIONS ON AUTOMATIC CONTROL", "category": "Automation & Control Systems; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000436828200002", "keywords": "Mallows model combination (MMA); predictive ability; model selection; perturbation instability measure; forecast strain", "title": "Difference or not to difference an integrated time series? An empirical investigation", "abstract": "This paper uses the gross domestic product growth rates of Malaysia, Thailand, Indonesia and China in an empirical examination to determine whether an integrated time series should be differenced before it is used for forecasting. The results reveal that Mallows model combination (M.M.A.) of original and differenced series is a better choice than just differencing the series only if the perturbation instability measure is more than 1.25 for autoregressive (A.R.) model, and 1.105 for moving average (M.A.) model and autoregressive fractional integrated moving average (A.R.F.I.M.A.) model. Furthermore, it is found that M.M.A. performs better in forecasting with better model stability for the case of M.A. and A.R.F.I.M.A. than A.R. However, M.M.A. is very sensitive in financial crisis.", "journal": "ECONOMIC RESEARCH-EKONOMSKA ISTRAZIVANJA", "category": "Economics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000436850000003", "keywords": "big data; radiation oncology; comparative effectiveness research; rapid-learning health care system; personalized radiation therapy", "title": "Cooperative Robot Localization Using Event-Triggered Estimation", "abstract": "This paper describes a novel communication-spare cooperative localization algorithm for a team of mobile unmanned robotic vehicles. Exploiting an event-based estimation paradigm, robots only send measurements to neighbors when the expected innovation for state estimation is high. Because agents know the event-triggering condition for measurements to be sent, the lack of a measurement is thus also informative and fused into state estimates. The robots use a covariance intersection mechanism to occasionally synchronize their local estimates of the full network state. In addition, heuristic balancing dynamics on the robots' covariance-intersection-triggering thresholds ensure that, in large-diameter networks, the local error covariances remains below desired bounds across the network. Simulations on both linear and nonlinear dynamics/measurement models show that the event-triggering approach achieves nearly optimal state estimation performance in a wide range of operating conditions, even when using only a fraction of the communication cost required by conventional full data sharing. The robustness of the proposed approach to lossy communications as well as the relationship between network topology and covariance-intersection-based synchronization requirements are also examined.", "journal": "JOURNAL OF AEROSPACE INFORMATION SYSTEMS", "category": "Engineering, Aerospace", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000449125500008", "keywords": "Urban Stormwater Drainage Systems; Optimization; AMALGAM; Hypervolume Indicator; Tehran", "title": "Assessment of different MOEAs for rehabilitation evaluation of Urban Stormwater Drainage Systems - Case study: Eastern catchment of Tehran", "abstract": "Design/rehabilitation of urban stormwater drainage systems has become a challenging issue due to increasing frequency and severity of floods in urbanized areas. Optimization frameworks can provide a proficient computational tool for stormwater management. In this study, using four different optimization algorithms and EPA-SWMM (Environmental Protection Agency-StorrnWater Management Model) software, a coupled numerical and optimization model was developed to rehabilitate the drainage system in eastern Tehran, Iran. The current drainage network suffers from a significant lack of hydraulic capacity. Thus, combinations of relief tunnels and/or storage units were evaluated and optimal rehabilitation strategies were suggested according to minimizing conflicting objective functions of costs and flooding. Results have revealed that AMALGAM (A Multi-Algorithm, Genetically Adaptive Multi-objective) outperformed three other algorithms, NSGA-II (Non-dominated Sorting Genetic Algorithm-II), NSHS (Non-dominated Sorting Harmony Search), and NSDE (Non-dominated Sorting Differential Evolution) for the evaluation of rehabilitation of Urban Stormwater Drainage Systems (USDSs) in terms of convergence and diversity criteria.", "journal": "JOURNAL OF HYDRO-ENVIRONMENT RESEARCH", "category": "Engineering, Civil; Environmental Sciences; Water Resources", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000455288500051", "keywords": "Binary response models; Smoothed maximum score estimation; Bandwidth selection", "title": "Small target detection using Pinpoint FISH", "abstract": "Binary response regression models are useful in many economic and statistical applications. Horowitz (1992) proposes a semi-parametric estimation method, which is a smoothed version of, and has a faster convergence rate than, Manski's maximum score estimator. The method for selecting the smoothing parameter (bandwidth) here is analogous to the plug-in method in kernel density estimation. It requires initial \"pilot\" values of the bandwidth to obtain the optimal bandwidth. However, this method has the disadvantage of not being fully data-driven. In this paper, we propose a data-driven bandwidth selection method by minimizing a cross-validated criterion function. Our simulation results show that the proposed method performs better than some existing methods. (C) 2018 Elsevier B.V. All rights reserved.", "journal": "CANCER GENETICS", "category": "Oncology; Genetics & Heredity", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000447786100012", "keywords": "Atrophy; cerebral small vessel diseases; cognition; image processing; computer-assisted; magnetic resonance imaging; stroke", "title": "The brain health index: Towards a combined measure of neurovascular and neurodegenerative structural brain injury", "abstract": "Background A structural magnetic resonance imaging measure of combined neurovascular and neurodegenerative burden may be useful as these features often coexist in older people, stroke and dementia. Aim We aimed to develop a new automated approach for quantifying visible brain injury from small vessel disease and brain atrophy in a single measure, the brain health index. Materials and methods We computed brain health index in N=288 participants using voxel-based Gaussian mixture model cluster analysis of T1, T2, T2*, and FLAIR magnetic resonance imaging. We tested brain health index against a validated total small vessel disease visual score and white matter hyperintensity volumes in two patient groups (minor stroke, N=157; lupus, N=51) and against measures of brain atrophy in healthy participants (N=80) using multiple regression. We evaluated associations with Addenbrooke's Cognitive Exam Revised in patients and with reaction time in healthy participants. Results The brain health index (standard beta=0.20-0.59, P<0.05) was significantly and more strongly associated with Addenbrooke's Cognitive Exam Revised, including at one year follow-up, than white matter hyperintensity volume (standard beta=0.04-0.08, P>0.05) and small vessel disease score (standard beta=0.02-0.27, P>0.05) alone in both patient groups. Further, the brain health index (standard beta=0.57-0.59, P<0.05) was more strongly associated with reaction time than measures of brain atrophy alone (standard beta=0.04-0.13, P>0.05) in healthy participants. Conclusions The brain health index is a new image analysis approach that may usefully capture combined visible brain damage in large-scale studies of ageing, neurovascular and neurodegenerative disease.", "journal": "INTERNATIONAL JOURNAL OF STROKE", "category": "Clinical Neurology; Peripheral Vascular Disease", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000451814000354", "keywords": "supervisory control; sliding mode control; non-linear control; robust control; More Electric Aircraft", "title": "Multi-Objective Supervisory Control for DC/DC Converters in Advanced Aeronautic Applications", "abstract": "In this paper, an intelligent control strategy for DC/DC converters is proposed. The converter connects two DC busses, a high-voltage and a low-voltage bus. The control scheme is composed by a two-layer architecture, a low-level control based on the concept of sliding manifold, and high gain control, and a high-level control used to guarantee the achievement of various objectives. The proposed control strategies are based on solid mathematical arguments, with stability proofs for the non-linear case, and decision trees for parameter selection. The paper results are analyzed and discussed by using simulation at different detail levels in MATLAB/Stateflow/PowerSystem, and validated by experimental results, also considering MIL standard performance indices.", "journal": "ENERGIES", "category": "Energy & Fuels", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000439649900010", "keywords": "AlGaN/GaN; ferroelectric; high-electron-mobility transistors (HEMTs); polarization switching", "title": "Polarization Engineering in PZT/AlGaN/GaN High-Electron-Mobility Transistors", "abstract": "Polarization engineering in 30-nm Pb (Zr, Ti) O-3 (PZT)/AlGaN/GaN high-electron-mobility transistors (HEMTs) is demonstrated. The coupling between ferroelectric polarization and 2-D electron gas (2DEG) showed a remarkable increase in transconductance during the down-sweep process; this can be attributed to the switching of ferroelectric polarization. With the increase in maximumgate voltage, the transfer curve showed a positive shift; this is because of the increase in polarization charges induced by gate voltage. The drain electric field affected the polarization of ferroelectric layer, leading to different hysteresis behaviors and peak transconductance trends of transfer curves. To decrease the effect of drain electric field on gate, a small drain voltage was used to evaluate the characteristics of PZT/AlGaN/GaN. The variation in peak transconductance was used to estimate the recovery and pinning degree of polarization. The ferroelectric layer modulates the 2DEG in ferroelectric/AlGaN/GaN HEMTs; this mechanism provides guidelines for designing and electrically controlling ferroelectric/AlGaN/GaN HEMTs.", "journal": "IEEE TRANSACTIONS ON ELECTRON DEVICES", "category": "Engineering, Electrical & Electronic; Physics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000441274200012", "keywords": "PID control; bidirectional control; active vibration control; stability; building structure", "title": "A method for bidirectional active control of structures", "abstract": "Proportional-derivative (PD) and proportional-integral-derivative (PID) controllers are popular control algorithms in industrial applications, especially in structural vibration control. In this paper, the designs of two dampers, namely the horizontal actuator and torsional actuator, are combined for the lateral and torsional vibrations of the structure. The standard PD and PID controllers are utilized for active vibration control. The sufficient conditions for asymptotic stability of these controllers are validated by utilizing the Lyapunov stability theorem. An active vibration control system with two floors equipped with a horizontal actuator and a torsional actuator is installed to carry out the experimental analysis. The experimental results show that bidirectional active control has been achieved.", "journal": "JOURNAL OF VIBRATION AND CONTROL", "category": "Acoustics; Engineering, Mechanical; Mechanics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000429599800003", "keywords": "Network simulator; Wireless big data networks; Multi-path multi-hop communications; Transport layer; Network coding", "title": "Simulation methodology and performance analysis of network coding based transport protocol in wireless big data networks", "abstract": "The Multi-Path, Multi-Hop (MPMH) communications have been extensively used in wireless network. It is especially suitable to big data transmissions due to its high throughput. To provide congestion and end-to-end reliability control, two types of transport layer protocols have been proposed in the literature: the TCP-based protocols and the rateless coding based protocols. However, the former is too conservative to explore the capacity of the MPMH networks, and the latter is too aggressive in filling up the communication capacity and performs poorly when dealing with congestions. To overcome their drawbacks, a novel network coding scheme, namely, Adjustable Batching Coding (ABC), was proposed by us, which uses redundancy coding to overcome random loss and uses retransmissions and window size shrink to relieve congestion. The stratified congestion control strategy makes the ABC scheme especially suitable for big data transmissions. However, there is no simulation platform built so far that can accurately test the performance of the network coding based transport protocols. We have built a modular, easy-to-customize simulation system based on an event-based programming method, which can simulate the ABC-based MPMH transport layer behaviors. Using the proposed simulator, the optimal parameters of the protocol can be fine-tuned, and the performance is superior to other transport layer protocols under the same settings. Furthermore, the proposed simulation methodology can be easily extended to other variants of MPMH communication systems by adjusting the ABC parameters. (C) 2018 Elsevier B.V. All rights reserved.", "journal": "SIMULATION MODELLING PRACTICE AND THEORY", "category": "Computer Science, Interdisciplinary Applications; Computer Science, Software Engineering", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000449842300014", "keywords": "onconephrology; kidney function; CKD-EPI; MDRD; Cockcroft-Gault; Janowitz equation", "title": "Evaluation of chronic kidney disease in cancer patients: is there a preferred estimation formula?", "abstract": "Background: The evaluation of chronic kidney disease (CKD) in cancer patients seems to rely mostly on the Cockcroft-Gault (CG) formula or the creatinine levels to adjust treatment dosages which is a practice refuted by internists. Aims: We evaluate the overall agreement of the CG, modification of diet in renal disease (MDRD) and CKD-epidemiology collaboration equations (CKD-EPI) equation with the newly devised Janowitz and Williams' (JW) equation. Methods: The renal function was estimated in 235 cancer patients according to the CG, MDRD, body surface area (BSA)-adjusted MDRD, CKD-EPI, BSA-adjusted CKDEPI and JW formulae. Results: JW equation was more in agreement with CG and CKD-EPI estimations than the other equations. Taking JW equation as reference, receiver operating characteristic curve analysis showed that CG eGFR had the higher area under the curve when compared with other equations. Hierarchical cluster analysis showed more proximity between CG and JW equations than the other equations. Conclusion: The newly proposed JW eGFR estimation was more in agreement with CG equation than the other equations.", "journal": "INTERNAL MEDICINE JOURNAL", "category": "Medicine, General & Internal", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000448955300006", "keywords": "stars: activity; methods: statistical", "title": "Estimating activity cycles with probabilistic methods II. The Mount Wilson Ca H&K data", "abstract": "Context. Debate over the existence of branches in the stellar activity-rotation diagrams continues. Application of modern time series analysis tools to study the mean cycle periods in chromospheric activity index is lacking. Aims. We develop such models, based on Gaussian processes (GPs), for one-dimensional time series and apply it to the extended Mount Wilson Ca H&K sample. Our main aim is to study how the previously commonly used assumption of strict harmonicity of the stellar cycles as well as handling of the linear trends affect the results. Methods. We introduce three methods of different complexity, starting with Bayesian harmonic regression model, followed by GP regression models with periodic and quasi-periodic covariance functions. We also incorporate a linear trend as one of the components. We construct rotation to magnetic cycle period ratio-activity (RCRA) diagrams and apply a Gaussian mixture model to learn the optimal number of clusters explaining the data. Results. We confirm the existence of two populations in the RCRA diagram; this finding is robust with all three methods used. We find only one significant trend in the inactive population, namely that the cycle periods get shorter with increasing rotation, leading to a positive slope in the RCRA diagram. This is in contrast with earlier studies, that postulate the existence of trends of different types in both of the populations. Our data is consistent with only two activity branches (inactive, transitional) instead of three (inactive, active, transitional) such that the active branch merges together with the transitional one. The retrieved stellar cycles are uniformly distributed over the < R'(HK)> activity index, indicating that the operation of stellar large-scale dynamos carries smoothly over the Vaughan-Preston gap. At around the solar activity index, however, indications of a disruption in the cyclic dynamo action are seen. Conclusions. Our study shows that stellar cycle estimates from time series the length of which is short in comparison to the searched cycle itself depend significantly on the model applied. Such model-dependent aspects include the improper treatment of linear trends, while the assumption of strict harmonicity can result in the appearance of double cyclicities that seem more likely to be explained by the quasi-periodicity of the cycles. In the case of quasi-periodic GP models, which we regard the most physically motivated ones, only 15 stars were found with statistically significant cycles against red noise model. The periodicities found have to, therefore, be regarded as suggestive.", "journal": "ASTRONOMY & ASTROPHYSICS", "category": "Astronomy & Astrophysics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000444246400005", "keywords": "Disturbance attenuation; Fault sensitivity; Lipschitz constant; Robust fault detection; Uncertain Lipschitz nonlinear system; Weighted multi-objective optimization", "title": "Robust Fault Detection of Uncertain Lipschitz Nonlinear Systems with Simultaneous Disturbance Attenuation Level and Enhanced Fault Sensitivity and Lipschitz Constant", "abstract": "In this paper, the problem of optimal robust fault detection (FD) for uncertain Lipschitz nonlinear systems is considered. Arobust active fault detection approach for a class of the Lipschitz nonlinear systems in the presence of disturbances and parametric uncertainties is proposed, wherein the Lipschitz constant is assumed as one of the optimization parameters in the observer design. In addition to disturbance attenuation level, the fault sensitivity criterion based on H-index is also defined in the FD system design. Different criteria are defined as a weighted multi-objective linear matrix inequality optimization problem, and the optimal variables of the FD system are derived based on a newly defined cost function. Anumerical example is provided to demonstrate the effectiveness of the proposed FD system. The results show the robustness of the proposed method against parametric uncertainty and nonlinear uncertainty as well.", "journal": "CIRCUITS SYSTEMS AND SIGNAL PROCESSING", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000430388500010", "keywords": "polymicrobial infection; Alzheimer's disease; infection in aging brain; fungal infection; next generation sequencing; bacteria and fungal co-infections", "title": "Global microRNA expression profiling in the liver biopsies of hepatitis B virus-infected patients suggests specific microRNA signatures for viral persistence and hepatocellular injury", "abstract": "Hepatitis B virus (HBV) can manipulate the microRNA (miRNA) regulatory networks in infected cells to create a permissive environment for viral replication, cellular injury, disease onset, and its progression. The aim of the present study was to understand the miRNA networks and their target genes in the liver of hepatitis B patients involved in HBV replication, liver injury, and liver fibrosis. We investigated differentially expressed miRNAs by microarray in liver biopsy samples from different stages of HBV infection and liver disease (immune-tolerant [n=8], acute viral hepatitis [n=8], no fibrosis [n=16], early [F1+F2, n=19] or late [F3+F4, n=14] fibrosis, and healthy controls [n=7]). miRNA expression levels were analyzed by unsupervised principal component analysis and hierarchical clustering. Analysis of miRNA-mRNA regulatory networks identified 17 miRNAs and 18 target gene interactions with four distinct nodes, each representing a stage-specific gene regulation during disease progression. The immune-tolerant group showed elevated miR-199a-5p, miR-221-3p, and Let-7a-3p levels, which could target genes involved in innate immune response and viral replication. In the acute viral hepatitis group, miR-125b-5p and miR-3613-3p were up, whereas miR-940 was down, which might affect cell proliferation through the signal transducer and activator of transcription 3 pathway. In early fibrosis, miR-34b-3p, miR-1224-3p, and miR-1227-3p were up, while miR-499a-5p was down, which together possibly mediate chronic inflammation. In advanced fibrosis, miR-1, miR-10b-5p, miR-96-5p, miR-133b, and miR-671-5p were up, while miR-20b-5p and miR-455-3p were down, possibly allowing chronic disease progression. Interestingly, only 8 of 17 liver-specific miRNAs exhibited a similar expression pattern in patient sera. Conclusion: miRNA signatures identified in this study corroborate previous findings and provide fresh insight into the understanding of HBV-associated liver diseases which may be helpful in developing early-stage disease diagnostics and targeted therapeutics. (Hepatology 2018;67:1695-1709)", "journal": "HEPATOLOGY", "category": "Gastroenterology & Hepatology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000439233200002", "keywords": "Intersubband transition; Crystal planes; Strain state; Nitrides", "title": "Strain effect on intersubband transition in a GaN/AlGaN single quantum well on arbitrary crystal planes", "abstract": "III-nitride intersubband transition has attracted much attention due to their great potential to fabricate ultrafast optoelectronic devices with broad spectrum. However, under most conditions, the transition properties are adversely affected by the polarization effect and strain effect existing in nitride materials. In this work, the optical properties of intersubband transition in a GaN/Al0.28Ga0.72N single quantum well on arbitrary crystal planes are theoretically compared for the structure strained on GaN or AlN. Under the conditions of two strain states, the difference of transition energy is gradually diminishing with the weakening of polarization intensity, while for the magnitude of intersubband dipole moment, inverse relationships are obviously observed between two groups of crystal planes when theta < 45A degrees and 45A degrees < theta < 90A degrees tilted from c-plane. These findings indicate that an appropriate template layer should be chosen when designing an orientation-related intersubband device with high efficiency.", "journal": "OPTICAL AND QUANTUM ELECTRONICS", "category": "Engineering, Electrical & Electronic; Quantum Science & Technology; Optics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000446066600013", "keywords": "Liver registration; Monocular laparoscopy; Biomechanical simulation", "title": "Preoperative liver registration for augmented monocular laparoscopy using backward-forward biomechanical simulation", "abstract": "Augmented reality for monocular laparoscopy from a preoperative volume such as CT is achieved in two steps. The first step is to segment the organ in the preoperative volume and reconstruct its 3D model. The second step is to register the preoperative 3D model to an initial intraoperative laparoscopy image. To date, there does not exist an automatic initial registration method to solve the second step for the liver in the de facto operating room conditions of monocular laparoscopy. Existing methods attempt to solve for both deformation and pose simultaneously, leading to nonconvex problems with no optimal solution algorithms. We propose in contrast to break the problem down into two parts, solving for (i) deformation and (ii) pose. Part (i) simulates biomechanical deformations from the preoperative to the intraoperative state to predict the liver's unknown intraoperative shape by modeling gravity, the abdominopelvic cavity's pressure and boundary conditions. Part (ii) rigidly registers the simulated shape to the laparoscopy image using contour cues. Our formulation leads to a well-posed problem, contrary to existing methods. This is because it exploits strong environment priors to complement the weak laparoscopic visual cues. Quantitative results with in silico and phantom experiments and qualitative results with laparosurgery images for two patients show that our method outperforms the state-of-the-art in accuracy and registration time.", "journal": "INTERNATIONAL JOURNAL OF COMPUTER ASSISTED RADIOLOGY AND SURGERY", "category": "Engineering, Biomedical; Radiology, Nuclear Medicine & Medical Imaging; Surgery", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000454014503176", "keywords": "fluorescence imaging; three dimension image analysis; sublobar resection", "title": "The Application of 3D Medical Image Analyzer and a Fluorescence Guided Surgery for Pulmonary Sublobar Resection", "abstract": "In order to improve the accuracy and flexibility of forestry robot mechanical arm, the mechanical arm mathematical model is constructed in this paper by Denavit-Hartenberg parameter method, and the inverse solution algorithm based on Newton iteration method is designed. On this basis, in the joint space, three times interpolation and five times interpolation algorithm are designed for path planning. Then, the hardware framework and software system of mechanical arm control system are designed, and the simulation experiment is carried out, to test the precision and error of mechanical arm. The results showed that the motor rotation error of the seven freedom mechanical arm control system is 0.025%, the position error of the mechanical arm is 5%, and the repetitive positioning error of the mechanical arm is less than 7 mm. Based on the above findings, it is concluded that the error of the end motion is within the allowable range, and the desired design requirements are completed.", "journal": "JOURNAL OF THORACIC ONCOLOGY", "category": "Oncology; Respiratory System", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000455903700013", "keywords": "Biomarkers; Component-based structural equation modeling; Drug response; Liver cancer; Multiple reaction monitoring mass spectrometry (MRM-MS); Prediction model; Sorafenib", "title": "Military Hospital Spending on Environmental Services and Inpatient Satisfaction Ratings", "abstract": "To assess the impact of military hospital expenditures on environmental services (EVS) on inpatient satisfaction, the authors collected Defense Health Agency TRICARE Inpatient Satisfaction Survey data from fiscal years 2011 through 2013, military hospital EVS spending and workload data, facility construction/renovation data, and military health system inpatient administrative claims data. Multivariate logistic regression for panel data was performed independently for medical/surgical and obstetric product lines and each satisfaction question. A statistically significant positive relationship was found between hospital EVS spending and patient satisfaction, with the highest expenditure levels generally exhibiting a greater association with satisfaction. Statistically significant increases in satisfaction with cleanliness were associated with higher levels of hospital expenditures on EVS.", "journal": "JOURNAL OF HEALTHCARE MANAGEMENT", "category": "Health Policy & Services", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000441305300003", "keywords": "surface topography measurement; surface texture; transfer behavior; transfer function; filter design; time series modeling", "title": "Modeling of topography measuring instrument transfer functions by time series models", "abstract": "The transfer function of topography measuring instruments contains important information for an understanding of the metrological characteristics. There are different methods for the estimation of the transfer function. For example, a measurement of the transfer function can be conducted with the aid of material measures that feature defined properties in the frequency domain. Another possibility is to determine the transfer function with the utilization of virtual measurements. However, these methods either require the development of a material measure specifically for this purpose or are only theoretical. We propose an approach that is common in signal processing and time series analysis for the application towards measuring instruments for geometrical product specification (GPS): filter design is used to estimate the instrument's transfer function. With this approach, the transfer function can be determined with the aid of a measuring object with any well-known stochastic surface structure, as long as the manufacturing and measurement of the structures are possible. The general suitability of the approach for both stylus and optical measuring instruments is demonstrated, and the proposed time series model and the required signal pre-processing are optimized. Based on the results, a comparison of the model results with measured transfer functions is conducted and virtual measurements are performed in order to evaluate the accuracy of the determined models. In doing so, it was observed that the surface structure of the measuring object used has an influence on the quality of the results.", "journal": "MEASUREMENT SCIENCE AND TECHNOLOGY", "category": "Engineering, Multidisciplinary; Instruments & Instrumentation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000447131100032", "keywords": "API usage pattern; Code example; Code search system", "title": "A Comment-Driven Approach to API Usage Patterns Discovery and Search", "abstract": "Considerable effort has gone into the discovery of API usage patterns or examples. However, how to enable programmers to search for discovered API usage examples using natural language queries is still a significant research problem. This paper presents an approach, referred to as Codepus, to facilitate the discovery of API usage examples based on mining comments in open source code while permitting searches using natural language queries. The approach includes two key features: API usage patterns as well as multiple keywords and tf-idf values are discovered by mining open source comments and code snippets; and a matchmaking function is devised for searching for API usage examples using natural language queries by aggregating scores related to semantic similarity, correctness, and the number of APIs. In a practical application, the proposed approach discovered 43,721 API usage patterns with 641,591 API usage examples from 15,814 open source projects. Experiment results revealed the following: (1) Codepus reduced the browsing time required for locating API usage examples by 46.5%, compared to the time required when using a web search engine. (2) The precision of Codepus is 91% when using eleven real-world frequently asked questions, which is superior to those of Gists and Open Hub.", "journal": "JOURNAL OF INTERNET TECHNOLOGY", "category": "Computer Science, Information Systems; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000437115100017", "keywords": "Tobacco leaf stems; Biomass byproducts; Online detection; Separation; X-ray imaging", "title": "Online Detection in the Separation Process of Tobacco Leaf Stems as Biomass Byproducts Based on Low Energy X-Ray Imaging", "abstract": "Stems often need to be separated from leaves and used alone as biomass byproducts in the agro-processing industry involving utilization of crops leaves. However, an effective method to detect stems from leaves is needed. In this study, X-ray imaging combined with digital photography analysis was applied to online detection of stems from threshed tobacco leaves. The optimal conditions for the inspection system were: X-ray radiation intensity of 70 keV, detection belt speed of 1.6 m/s, and gray threshold of 40,000. The free stems and leaf strips containing stems were used as target samples to validate the method. The test results showed that identification accuracies for both free stems and leaf strips containing stems were >98.0%. When leaf strips containing stems were mixed with pure leaves, identification accuracy of stems was 94.5%, and rejecting accuracy was 92.0%. These results indicate that this method is a reliable classification solution for threshed tobacco leaves.", "journal": "WASTE AND BIOMASS VALORIZATION", "category": "Environmental Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000431863200008", "keywords": "Intimate partner violence; Forced first sex; Maternal and newborn health outcomes; Zimbabwe", "title": "Intimate partner violence, forced first sex and adverse pregnancy outcomes in a sample of Zimbabwean women accessing maternal and child health care", "abstract": "Background: Intimate partner violence (IPV) remains a serious problem with a wide range of health consequences including poor maternal and newborn health outcomes. We assessed the relationship between IPV, forced first sex (FFS) and maternal and newborn health outcomes. Methods: A cross sectional study was conducted with 2042 women aged 15-49 years attending postnatal care at six clinics in Harare, Zimbabwe, 2011. Women were interviewed on IPV while maternal and newborn health data were abstracted from clinic records. We conducted logistic regression models to assess the relationship between forced first sex (FFS), IPV (lifetime, in the last 12 months and during pregnancy) and maternal and newborn health outcomes. Results: Of the recent pregnancies 27.6% were not planned, 50.9% booked (registered for antenatal care) late and 5.6% never booked. A history of miscarriage was reported by 11.5%, and newborn death by 9.4% of the 2042 women while 8.6% of recent livebirths were low birth weight (LBW) babies. High prevalence of emotional (63,9%, 40.3%, 43.8%), physical (373%, 21.3%, 15.8%) and sexual (51.7%, 35.6%, 38.8%) IPV ever, 12 months before and during pregnancy were reported respectively. 15.7% reported forced first sex (FFS). Each form of lifetime IPV (emotional, physical, sexual, physical/sexual) was associated with a history of miscarrying (aOR ranges: 1.26-1.38), newborn death (aOR ranges: 1.13-2.05), and any negative maternal and newborn health outcome in their lifetime (aOR ranges: 1.32-1.55). FFS was associated with a history of a negative outcome (newborn death, miscarriage, stillbirth) (aOR1.45 95%CI: 1.06-1.98). IPV in the last 12 months before pregnancy was associated with unplanned pregnancy (aOR ranges 1.31-2.02) and booking late for antenatal care. Sexual IPV (aOR 2.09 CI1.31-3.34) and sexual/physical IPV (aOR2.13, 95%CI: 1.32-3.42) were associated with never booking for antenatal care. Only emotional IPV during pregnancy was associated with low birth weight (aOR1.78 95%CI1.26-2.52) in the recent pregnancy and any recent pregnancy negative outcomes including LBW, premature baby, emergency caesarean section (aOR1.38,95%CI:1.03-1.83). Conclusions: Forced first sex (FFS) and intimate partner violence (IPV) are associated with adverse maternal and newborn health outcomes. Strengthening primary and secondary violence prevention is required to improve pregnancy-related outcomes.", "journal": "BMC PUBLIC HEALTH", "category": "Public, Environmental & Occupational Health", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000431758800005", "keywords": "Dengue; Time series analysis; Anima; Brazil; Forecasting; America region", "title": "Investigation of the flow inside an urban canopy immersed into an atmospheric boundary layer using laser Doppler anemometry", "abstract": "Laser Doppler anemometry (LDA) is used to investigate the flow inside an idealized urban canopy consisting of a staggered array of cubes with a 25% density immersed into an atmospheric boundary layer with a Reynolds number of delta(+)= 32,300. The boundary layer thickness to cube height ratio (delta/h = 22.7) is large enough to be representative of atmospheric surface layer in neutral conditions. The LDA measurements give access to pointwise time-resolved data at several positions inside the canopy (z = h/4, h/2, and h). Synchronized hot-wire measurements above the canopy (inertial region and roughness sublayer) are also realized to get access to interactions between the different flow regions. The wall-normal mean velocity profile and Reynolds stresses show a good agreement with available data in the literature, although some differences are observed on the standard deviation of the spanwise component. A detailed spectral and integral time scale analysis inside the canopy is then carried out. No clear footprint of a periodic vortex shedding on the sides of the cubes could be identified on the power spectra, owing to the multiple cube-to-cube interactions occuring within a canopy with a building density in the wake interference regime. Results also suggest that interactions between the most energetics scales of the boundary layer and those related to the cube canopy take place, leading to a broadening of the energy peak in the spectra within the canopy. This is confirmed by the analysis of coherence results between the flow inside and above the canopy. It is shown that linear interactions mechanisms are significant, but reduced compared to smooth-wall boundary-layer flow. To our knowledge, this is the first time such results are shown on the dynamics of the flow inside an urban canopy.", "journal": "EXPERIMENTS IN FLUIDS", "category": "Engineering, Mechanical; Mechanics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000434743700015", "keywords": "Automatic Search Query Enhancement; Query drift; Information retrieval; Wikipedia", "title": "A Wikipedia powered state-based approach to automatic search query enhancement", "abstract": "This paper describes the development and testing of a novel Automatic Search Query Enhancement (ASQE) algorithm, the Wikipedia N Sub-state Algorithm (WNSSA), which utilises Wikipedia as the sole data source for prior knowledge. This algorithm is built upon the concept of iterative states and sub-states, harnessing the power of Wikipedia's data set and link information to identify and utilise reoccurring terms to aid term selection and weighting during enhancement. This algorithm is designed to prevent query drift by making callbacks to the user's original search intent by persisting the original query between internal states with additional selected enhancement terms. The developed algorithm has shown to improve both short and long queries by providing a better understanding of the query and available data. The proposed algorithm was compared against five existing ASQE algorithms that utilise Wikipedia as the sole data source, showing an average Mean Average Precision (MAP) improvement of 0.273 over the tested existing ASQE algorithms.", "journal": "INFORMATION PROCESSING & MANAGEMENT", "category": "Computer Science, Information Systems; Information Science & Library Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000449635000004", "keywords": "Pediatric neck surgery; Retroauricular approach; Robotic surgery; Robotic pediatric surgery; Transaxillary approach", "title": "Robotic Neck Surgery in the Pediatric Population", "abstract": "Introduction: Thyroid, parathyroid, and thymus surgeries are traditionally performed via a cervical approach. However, robot-assisted procedures can provide a safe alternative for neck surgeries. We report our experiences with robotic transaxillary and retroauricular approaches in pediatric patients. Case Presentation: We conducted a retrospective review of pediatric patients who underwent robot-assisted neck surgery by a single surgeon between April 2010 and May 2017. Patient demographics and surgical outcomes including operative time, incidence of complications, and length of hospital stay were evaluated. Management and Outcomes: Nine surgeries in 7 female patients were reviewed (mean age, 16.0 +/- 1.58 years; mean body mass index, 22.5 +/- 0.75). Two thyroid lobectomies, 2 complete thyroidectomies, 1 subtotal thyroidectomy, 1 thyroid lobectomy with thymectomy, 2 subtotal parathyroidectomies with thymectomy, and 1 dermoid cyst excision were performed. Two surgeries with the retroauricular approach had a mean surgical time of 142.0 +/- 6.13 minutes. Seven surgeries with the transaxillary approach had a mean surgical time of 146.1 +/- 21.01 minutes. There were no reported conversions, permanent vocal cord paralysis, permanent hypoparathyroidism, hematoma, or seroma. There was 1 case (11%) of temporary shoulder hypoesthesia and 2 cases of temporary vocal cord paresis (22%). Discussion: This series on robot-assisted neck surgeries in children describes procedures performed with robotic transaxillary and retroauricular approaches. In the hands of a high-volume surgeon the techniques are feasible and safe options for operations in the neck in a select group of pediatric patients.", "journal": "JSLS-JOURNAL OF THE SOCIETY OF LAPAROENDOSCOPIC SURGEONS", "category": "Surgery", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000440257000001", "keywords": "Diabetes mellitus; Hospital medicine; Patient readmission; Epidemiological methods", "title": "Derivation and validation of a predictive model for the readmission of patients with diabetes mellitus treated in internal medicine departments", "abstract": "Objectives: We developed a predictive model for the hospital readmission of patients with diabetes. The objective was to identify the frail population that requires additional strategies to prevent readmissions at 90 days. Methods: Using data collected from 1977 patients in 3 studies on the national prevalence of diabetes (2015-2017), we developed and validated a predictive model of readmission at 90 days for patients with diabetes. Results: A total of 704 (36%) readmissions were recorded. There were no differences in the readmission rates over the course of the 3 studies. The hospitals with more than 500 beds showed significantly (p=.02) higher readmission rates than those with fewer beds. The main reasons for readmission were infectious diseases (29%), cardiovascular diseases (24) and respiratory diseases (14%). Readmissions directly related to diabetic decompensations accounted for only 2% of all readmissions. The independent variables associated with hospital readmission were patient's age, degree of comorbidity, estimated glomerular filtration rate, degree of disability, presence of previous episodes of hypoglycaemia, use of insulin in treating diabetes and the use of systemic glucocorticoids. The predictive model showed an area under the ROC curve (AUC) of 0.676 (95% confidence interval [95% cI] 0.642-0.709; p=.001) in the referral cohort. In the validation cohort, the model showed an AUC of 0.661 (95% CI 0.612-0.710; p=.001). Conclusion: The model we developed for predicting readmissions for hospitalised patients with type 2 diabetes helps identify a subgroup of frail patients with a high risk of readmission. (C) 2018 Elsevier Espana, S.L.U. and Sociedad Espanola de Medicina Interna (SEMI). All rights reserved.", "journal": "REVISTA CLINICA ESPANOLA", "category": "Medicine, General & Internal", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000443547600025", "keywords": "syphilis serodiagnosis; treponemal test; nontreponemal test; US CDC-recommended algorithm; ECDC-recommended algorithm", "title": "Analysis of 2 Reverse Syphilis Testing Algorithms in Diagnosis of Syphilis: A Large-Cohort Prospective Study", "abstract": "Background. Two serologic syphilis screening algorithms recommended by the US Centers for Disease Control and Prevention (US CDC) and the European Centre for Disease Prevention and Control (ECDC), respectively, are commonly used for syphilis screening; however, which one is optimal remains to be determined. Methods. We conducted a prospective study of 119 891 subjects to analyze the consistency of the US CDC- and ECDC-recommended algorithms. The US CDC-recommended algorithm begins with a treponemal immunoassay, followed by a rapid plasma reagin (RPR) test. RPR-nonreactive samples are confirmed by the Treponema pallidum particle agglutination assay (TPPA). The ECDC-recommended algorithm begins with a treponemal immunoassay, followed by a confirmatory treponemal test. If the confirmatory test is reactive, a quantitative nontreponemal assay is used to assess the disease activity and treatment response. In the present study, a total of 119 891 serum samples from a large hospital (sixth largest in China) were included, and each sample was screened with a chemiluminescent immunoassay (CIA). CIA-reactive samples were then simultaneously tested with RPR and TPPA. The consistency of these 2 algorithms was determined by calculating the percentage of agreement and. coefficient. Results. The overall percentage of agreement and kappa value between these 2 algorithms were 99.996% and 0.999, respectively. The positivity rate for syphilis as determined by the US CDC- and ECDC-recommended algorithms was 1.43% and 1.42%, respectively. Conclusions. Our results suggest that the US CDC-recommended algorithm and the ECDC-recommended algorithm have comparable performances for syphilis screening in low-prevalence populations.", "journal": "CLINICAL INFECTIOUS DISEASES", "category": "Immunology; Infectious Diseases; Microbiology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000447673000007", "keywords": "Fault location; Parallel transmission line; Source impedance estimation", "title": "Extended Fault Location Algorithm Using the Estimated Remote Source Impedance for Parallel Transmission Lines", "abstract": "This paper describes extended fault location algorithm using estimated remote source impedance. The method uses data only at the local end and the sequence current distribution factors for more accurate estimation. The proposed algorithm can respond to variation of the local and remote source impedance. Therefore, this method is especially useful for transmission lines interconnected to a wind farm that the source impedance varies continuously. The proposed algorithm is very insensitive to the variation in fault distance and fault resistance. The simulation results have shown the accuracy and effectiveness of the proposed algorithm.", "journal": "JOURNAL OF ELECTRICAL ENGINEERING & TECHNOLOGY", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000435151300042", "keywords": "apoptosis; cytology; genes; image analysis; pterygia", "title": "Deregulation of p53/survivin apoptotic markers correlated to PTEN expression in pterygium neoplastic cells", "abstract": "Purpose: Pterygium is a distinct clinicopathological entity characterized by degenerated and neoplastic-like features. Concerning its rise on normal conjunctiva epithelia, the role of specific gene deregulations including apoptotic/anti-apoptotic factors and significant suppressor genes in signaling transduction pathways is under investigation. In the current study, we co-analyzed p53, survivin and PTEN proteins in pterygia and normal conjunctiva. Methods: Using a liquid-based cytology assay, 50 cell specimens were obtained by a smooth scraping on conjunctiva epithelia and fixed accordingly. Among them, 38 were pterygia and the remaining (n=12) normal epithelia (control group). Immunocytochemistry assays were implemented on the corresponding slides by applying ani-p53, survivin, and PTEN antibodies. Digital image analysis was performed for evaluating objectively the corresponding immunostaining intensity levels. Results: The majority of the examined pterygia cases overexpressed the markers p53:22/38-57.9%, survivin:30/38-78.9%, and PTEN:25/38-65.7%. Interestingly, overall p53/PTEN co-expression was found to be statistically significant (p=0.022). Conclusions: Survivin overexpression leads to an increased anti-apoptotic activity playing a central molecular role in the pathogenesis and progression of pterygia. Furthermore, although p53 expression is observed in these lesions, its impact seems to be low compared to survivin's influence on them. Additionally, the role of PTEN in the process is potentially significant providing a suppressor balance to the p53/survivin complex.", "journal": "JOURNAL OF BUON", "category": "Oncology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000439178100001", "keywords": "free energy principle; conflict; psychoanalysis; defense; systems theory; neuropsychoanalysis", "title": "Expected Free Energy Formalizes Conflict Underlying Defense in Freudian Psychoanalysis", "abstract": "Freud's core interest in the psyche was the dynamic unconscious: that part of the psyche which is unconscious due to conflict (Freud, 1923/1961). Over the course of his career, Freud variously described conflict as an opposition to the discharge of activation (Freud, 1950), opposition to psychic activity due to the release of unpleasure (Freud, 1990/1991), opposition between the primary principle and the reality principle (Freud, 1911/1963), structural conflict between id, ego, and superego (Freud, 1923/1961), and ambivalence (Freud, 1912/1963). Besides this difficulty of the shifting description of conflict,an underlying question remained the specific shared terrain in which emotions, thoughts, intentions or wishes could come into conflict with one another (the neuronal homolog of conflict), and most especially how they may exist as quantities in opposition within that terrain. Friston's free-energy principle (FEP henceforth) connected to the work of Friston (Friston et al., 2006; Friston, 2010) has provided the potential for a powerful unifying theory in psychology, neuroscience, and related fields that has been shown to have tremendous consilience with psychoanalytic concepts (Hopkins, 2012). Hopkins (2016), drawing on a formulation by Hobson et al. (2014), suggests that conflict may be potentially quantifiable as free energy from a FEP perspective. More recently, work by Friston et al. (2017a) has framed the selection of action as a gradient descent of expected free energy under different policies of action. From this perspective, the article describes how conflict could potentially be formalized as a situation where opposing action policies have similar expected free energy, for example between actions driven by competing basic prototype emotion systems as described by Panksepp (1998). This conflict state may be avoided in the future through updating the relative precision of a particular set of prior beliefs about outcomes: this has the result of tending to favor one of the policies of action over others in future instances, a situation analogous to defense. Through acting as a constraint on the further development of the person, the defensive operation can become entrenched, and resistant to alteration. The implications that this formalization has for psychoanalysis is explored.", "journal": "FRONTIERS IN PSYCHOLOGY", "category": "Psychology, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000430674500002", "keywords": "Fractional order system; Positive real uncertainty; Linear matrix inequality (LMI); Singular value decomposition (SVD)", "title": "An indirect Lyapunov approach to robust stabilization for a class of linear fractional-order system with positive real uncertainty", "abstract": "The paper is concerned with the problem of the robust stabilization for a class of fractional order linear systems with positive real uncertainty under Riemann-Liouville (RL) derivatives. Firstly, by utilizing the continuous frequency distributed model of the fractional integrator, the fractional order system is expressed as an infinite dimensional integral order system. And via using indirect Lyapunov approach and linear matrix inequality techniques, sufficient condition for robust asymptotic stability of the fractional order systems and design methods of the state feedback controller are presented. Secondly, by using matrixs singular value decomposition technique the static output feedback controller and observer-based controller for asymptotically stabilizing the fractional order systems are derived. Finally, the validity of the proposed methods are demonstrated by numerical examples.", "journal": "JOURNAL OF APPLIED MATHEMATICS AND COMPUTING", "category": "Mathematics, Applied; Mathematics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000434068100002", "keywords": "Censored data; heteroscedasticity; transformation regression model", "title": "Analysis of censored data under heteroscedastic transformation regression models with unknown transformation function", "abstract": "Consider a censored heteroscedastic transformation regression model where both the transformation function and the error distribution function are completely unknown. A method is developed to estimate the transformation function, the regression parameter vector, and the single index parameter vector of the variance function by establishing an expression for the transformation function and two estimating equations for both the parameter vectors. It is shown that the estimator of the transformation function converges weakly to a mean zero Gaussian process, and the parametric estimators are asymptotically normal. All the estimators converge to their true values in probability at a rate proportional to n-1/2. Simulation studies are conducted to evaluate the finite sample behaviour of the proposed estimators, and a real data analysis is used to illustrate the proposed estimating method. The Canadian Journal of Statistics 46: 233-245; 2018 (c) 2017 Statistical Society of Canada", "journal": "CANADIAN JOURNAL OF STATISTICS-REVUE CANADIENNE DE STATISTIQUE", "category": "Statistics & Probability", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000443049900018", "keywords": "Algorithm design and analysis; algorithms; collision avoidance; computer simulation; wireless sensor networks (WSNs)", "title": "An Efficient Minimum-Latency Collision-Free Scheduling Algorithm for Data Aggregation in Wireless Sensor Networks", "abstract": "Data collection is one of the most important operations in applications of wireless sensor networks (WSNs). In many emerging WSN applications, it is urgent to achieve a guarantee for the latency involved in collecting data. Many researchers have studied collecting data in WSNs with minimum latency but without data collision while assuming that any (or no) data are allowed to he aggregated into one packet. In addition, tree structures are often used for solutions. However, in some cases, a fixed number of data are allowed to be aggregated into one packet. This motivates us to study the problem of minimizing the latency for data aggregation without data collision in WSNs when a fixed number of data are allowed to be aggregated into one packet, termed the minimum-latency collision-avoidance multiple-data-aggregation scheduling (MLCAMDAS) problem. The MLCAMDAS problem is shown to be NP-complete here. In addition, a nontree-based method, termed the independent-set-based collision-avoidance scheduling (ISBCAS) algorithm, is proposed accordingly. The ISBCAS is demonstrated via simulations to have good performance.", "journal": "IEEE SYSTEMS JOURNAL", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Operations Research & Management Science; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000447261800001", "keywords": "container relocation problem; block relocation problem; combinatorial optimization; multistage stochastic models; decision tress", "title": "The Stochastic Container Relocation Problem", "abstract": "The container relocation problem (CRP) is concerned with finding a sequence of moves of containers that minimizes the number of relocations needed to retrieve all containers, while respecting a given order of retrieval. However, the assumption of knowing the full retrieval order of containers is particularly unrealistic in real operations. This paper studies the stochastic CRP, which relaxes this assumption. A new multistage stochastic model, called the batch model, is introduced, motivated, and compared with an existing model (the online model). The two main contributions are an optimal algorithm called Pruning-Best-First-Search (PBFS) and a randomized approximate algorithm called PBFS-Approximate with a bounded average error. Both algorithms, applicable in the batch and online models, are based on a new family of lower bounds for which we show some theoretical properties. Moreover, we introduce two new heuristics outperforming the best existing heuristics. Algorithms, bounds, and heuristics are tested in an extensive computational section. Finally, based on strong computational evidence, we conjecture the optimality of the \"leveling\" heuristic in a special \"no information\" case, where, at any retrieval stage, any of the remaining containers is equally likely to be retrieved next.", "journal": "TRANSPORTATION SCIENCE", "category": "Operations Research & Management Science; Transportation; Transportation Science & Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000444248700009", "keywords": "Fault detection; linear time-varying system; observer; operator; uncertain", "title": "In vivo toxicology of carbon dots by H-1 NMR-based metabolomics", "abstract": "Owing to the promising applications of C-dots in biomedical engineering, concerns about their safety have drawn increasing attention recently. In this study, mice were intraperitoneally injected at different C-dot concentrations (0, 6.0, 12.0 and 24.0 mg kg(-1)) once every 2 days for 30 days. A H-1 NMR-based metabolic approach supplemented with biochemical analysis and histopathology was used for the first time to explore the toxicity of C-dots in vivo. Histopathological inspection revealed that C-dots did not induce any obvious impairment in tissues. Biochemical assays showed no significant alterations of most measured biochemical parameters in tissues and serum, except for a slight reduction of the albumin level in serum as well as AChE activity in the liver and kidneys. Orthogonal signal correction-partial least squares-discriminant analysis (OSC-PLS-DA) of NMR profiles supplemented with correlation network analysis and SUS-plots disclosed that C-dots not only triggered the immune system but also disturbed the function of cell membranes as well as the normal liver clearance, indicating that the H-1 NMR based metabolomics approach provided deep insights into the toxicity of C-dots in vivo and gained an advantage over traditional toxicological means, and should be helpful for the understanding of its toxic mechanism.", "journal": "TOXICOLOGY RESEARCH", "category": "Toxicology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000445318700004", "keywords": "Docking; multiple regression; AM1 method; antipsychotic drugs; SAR", "title": "Structure-activity relationship in phenothiazine antipsychotic drugs: Molecular orbital calculation, in silico molecular docking and physico-chemical parameters", "abstract": "A series of antipsychotic phenothiazine drugs, whose results of clinical trials are known, have been subjected to all valence molecular orbital calculations using AM1 method. Phenothiazines are well known antipsychotic drugs and many molecules of this group are studied for their activity. These molecules are docked in dopamine receptor and the energies of the drug receptor complexes are obtained. The physico-chemical parameters like logP, PSA, Volume, BBB and drug likeness score have also been found. A regression analysis has been carried out to obtained dependency of antipsychotic activity with different parameters derived from MO calculations, molecular docking and physico-chemical studies.", "journal": "INDIAN JOURNAL OF CHEMISTRY SECTION B-ORGANIC CHEMISTRY INCLUDING MEDICINAL CHEMISTRY", "category": "Chemistry, Organic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000444957900027", "keywords": "erectile dysfunction; functional outcome; quality of life; radical prostatectomy; urinary incontinence", "title": "Long-term functional outcome analysis in a large cohort of patients after radical prostatectomy", "abstract": "IntroductionGoal of the study was an analysis of functional outcomes after radical prostatectomy (RP) in a large cohort of patients. MethodsFunctional outcomes were assessed with the ICIQ-SF questionnaire and daily pad-usage for the evaluation of stress urinary incontinence (SUI) as well as with the IIEF-5 score for the evaluation of erectile dysfunction (ED). Statistical analysis included log-rank test, Mann-Whitney-Test, ANOVA test and logistic regression (P<0.05). ResultsIn total 4003 patients were included in the study. Median follow-up was 42 months (min. 2-max. 147 mo.). Regarding ED, an IIEF-5 score of 20 was reached by 39% of patients. Regarding SUI, 55% stated that they needed no pads, 21% of patients needed one pad per day. 33% of patients reported of no incontinence (0 p. in ICIQ), 26% of patients reported of a mild incontinence (1-5 p. in ICIQ). Patient global impression of their overall health respectively patient subjective quality of life were assessed with the EORTC QLC-C30 and were both high with a median of six points (on a scale of 1-7). In multivariate analysis time since surgery could be identified as independent risk factors on QOL (P=0.0028), IIEF-5 (P=0.0149), ICIQ (P<0.001), and pads per day (P<0.001). ConclusionsOur data show a good continence status, a clearly impaired erectile function but overall a good quality of life after surgery. In summary older patientswith an advanced tumor and adjuvant radiation therapywere at highest risk for an impaired functional outcome.", "journal": "NEUROUROLOGY AND URODYNAMICS", "category": "Urology & Nephrology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000451246100071", "keywords": "FV-BPM; MMI; WDM; Slot waveguide", "title": "Analysis of Isocratic-Chromatographic-Retention Data using Bayesian Multilevel Modeling", "abstract": "The objective of this work was to develop a multilevel (hierarchical) model based on isocratic-reversed-phase-high-performance-chromatographic data collected in methanol and acetonitrile for 58 chemical compounds. Such a multilevel model is a regression model of the analyte-specific chromatographic measurements, in which all the regression parameters are given a probability model. It is a fundamentally different approach from the most common approach, where parameters are separately estimated for each analyte (without sharing information across analytes and different organic modifiers). The statistical analysis was done with Stan software implementing the Bayesian-statistics inference with Markov-chain Monte Carlo sampling. During the model building process, a series of multilevel models of different complexity were obtained, such as (1) a model with no pooling (separate models were fitted for each analyte), (2) a model with partial pooling (a common distribution was used for analyte-specific parameters), and (3) a model with partial pooling as well as a regression model relating analyte-specific parameters and analyte-specific properties (QSRR equations). All the models were compared with each other using 10-fold cross-validation. The benefits of multilevel models in inference and predictions were shown. In particular the obtained models allowed us to (i) better understand the data and (ii) solve many routine analytical problems, such as obtaining well-calibrated predictions of retention factors for an analyte in acetonitrile-containing mobile phases given zero, one, or several measurements in methanol containing mobile phases and vice versa.", "journal": "ANALYTICAL CHEMISTRY", "category": "Chemistry, Analytical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000441514600007", "keywords": "authorisation; smart phones; shoulder-surfing attacks; contrary to common belief; automated algorithm; smartphone passcode prediction; documents; financial statements; unauthorised access; sensitive information protection; time 75; 0 s", "title": "Smartphone passcode prediction", "abstract": "Many people now own smartphones and store all their documents such as pictures and financial statements on their phone. To protect this sensitive information, people generally use a passcode to prevent unauthorised access to their phone. Shoulder-surfing attacks are well known. However, contrary to common belief, they are not easy to carry out. Shoulder-surfing attacks to predict the passcode by humans are shown to not be accurate. The authors thus propose an automated algorithm to accurately predict the passcode entered by a victim on her smartphone by recording the video. Their proposed algorithm is able to predict over 92% of numbers entered in fewer than 75s with training performed once.", "journal": "IET INFORMATION SECURITY", "category": "Computer Science, Information Systems; Computer Science, Theory & Methods", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000441652400015", "keywords": "Air transport demand; Brexit; Forecasting; Co-integration; Regression model", "title": "A new direct demand model of long-term forecasting air passengers and air transport movements at German airports", "abstract": "The German Aerospace Center has developed and applied a \"classical\" four-step model of forecasting passenger and flight volume at German airports for many years. However, it has become increasingly difficult to update and verify the model because of a lack of specific data. We have therefore developed a more versatile model based upon co-integration theory, which directly forecasts passenger and flight volume at German airports. The paper describes the model approaches and discusses the advantages and disadvantages of both the classical and new model approaches. The model includes demand shocks and estimated GDP-elasticity is 1.31. The model has been employed to estimate the effects of Brexit on traffic volume at German airports for the years 2016-2018.", "journal": "JOURNAL OF AIR TRANSPORT MANAGEMENT", "category": "Transportation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000440415500064", "keywords": "perinatology; paediatrics; obstetrics; neonatology; epidemiology", "title": "The Multiple Object Test as a performance- based tool to assess the decline of ADL function in Parkinson's disease", "abstract": "Introduction As cognitive-driven worsening of activities of the daily living (ADL) in Parkinson's disease (PD) is the core feature of PD dementia (PDD), there is great need for sensitive quantitative assessment. Aim of our study was the evaluation of cognitive-driven worsening of ADL by the performance-based Multiple Object Test (MOT), offering an essential clinical advantage as it is quick and easy to apply in a clinical context even on severely impaired patients. Methods 73 PD patients were assessed longitudinally over a period of 37 (6-49) months. According to their neuropsychological profile the sample was divided into two groups: PD patients with (n = 34, PD-CI) and without cognitive impairment (n = 39, PD-noCl). The MOT comprises five routine tasks (e.g. to make coffee) quick and easy to apply. Quantitative (total error number, processing time) and qualitative parameters (error type) were analyzed using non-parametric test statistic (e.g.Wilcoxon signed-rank test, binary logistic regression). Results Median number of total errors (p = 0.001), processing time (p<0.001), perplexity (p = 0.035), and omission errors (p<0.001) increased significantly from baseline to follow-up in the total sample. Worsening of MOT performance was correlated to cognitive decline in the attention/executive function and visuo-constructive domain. PD-CI showed an increase in omission errors (p = 0.027) compared to PD-noCl over time. This increase in omission errors between visits was further identified as a risk marker for PDD conversion. Conclusion The MOT, especially frequency of omission errors, is a promising tool to rate PD patients objectively and might help to identify patients with a high risk for having mild cognitive impairment or dementia.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000443834100001", "keywords": "Autoinflammation; autoimmunity; cytokine; nerve growth factor; pain; autonomic; peripheral nervous system", "title": "Autoinflammatory and autoimmune contributions to complex regional pain syndrome", "abstract": "Complex regional pain syndrome (CRPS) is a highly enigmatic syndrome typically developing after injury or surgery to a limb. Severe pain and disability are common among those with chronic forms of this condition. Accumulating evidence suggests that CRPS may involve both autoinflammatory and autoimmune components. In this review article, evidence for dysfunction of both the innate and adaptive immune systems in CRPS is presented. Findings from human studies in which cytokines and other inflammatory mediators were measured in the skin of affected limbs are discussed. Additional results from studies of mediator levels in animal models are evaluated in this context. Similarly, the evidence from human, animal, and translational studies of the production of autoantibodies and the potential targets of those antibodies is reviewed. Compelling evidence of autoinflammation in skin and muscle of the affected limb has been collected from CRPS patients and laboratory animals. Cytokines including IL-1 beta, IL-6, TNF alpha, and others are reliably identified during the acute phases of the syndrome. More recently, autoimmune contributions have been suggested by the discovery of self-directed pain-promoting IgG and IgM antibodies in CRPS patients and model animals. Both the autoimmune and the autoinflammatory components of CRPS appear to be regulated by neuropeptide-containing peripheral nerve fibers and the sympathetic nervous system. While CRPS displays a complex neuroimmunological pathogenesis, therapeutic interventions could be designed targeting autoinflammation, autoimmunity, or the neural support for these phenomena.", "journal": "MOLECULAR PAIN", "category": "Neurosciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000446181900121", "keywords": "epilepsy; patient-reported outcomes; test-retest reliability; clinical decision support; epidemiology", "title": "Patient-reported outcome (PRO) measure-based algorithm for clinical decision support in epilepsy outpatient follow-up: a test-retest reliability study", "abstract": "Objectives Patient-reported outcome (PRO) measures have been used in epilepsy outpatient clinics in Denmark since 2011. The patients' self-reported PRO data are used by clinicians as a decision aid to support whether a patient needs contact with the outpatient clinic or not based on a PRO algorithm. Validity and reliability are fundamental to any PRO measurement used at the individual level in clinical practice. The aim of this study was to evaluate the test-retest reliability of the PRO algorithm used in epilepsy outpatient clinics and to analyse whether the method of administration (web and paper) would influence the result. Design and setting Test-retest reliability study conducted in three epilepsy outpatient clinics in Central Denmark Region, Denmark. Participants A total of 554 epilepsy outpatients aged 15 years or more were included from August 2016 to April 2017. The participants completed questionnaires at two time points and were randomly divided into four test-retest groups: web-web, paper-paper, web-paper and paper-web. In total, 166 patients completed web-web, 112 paper-paper, 239 web-paper and 37 paper-web. Results Weighted kappa with squared weight was 0.67 (95% CI 0.60 to 0.74) for the pooled PRO algorithm, and perfect agreement was observed in 82% (95% CI 78% to 85%) of the cases. There was a tendency towards higher test-retest reliability and agreement estimates within same method of administration (web-web or paper-paper) compared with a mixture of methods (web-paper and paper-web). Conclusions The PRO algorithm used for clinical decision support in epilepsy outpatient clinics showed moderate to substantial test-retest reliability. Different methods of administration produced similar results, but an influence of change in administration method cannot be ruled out.", "journal": "BMJ OPEN", "category": "Medicine, General & Internal", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000450319100001", "keywords": "Complex natural gas pipeline network; Vulnerability analysis; Supply service; Mart-flow algorithm; Floyd algorithm; Criticality analysis", "title": "Comparative biosensing of glycosaminoglycan hyaluronic acid oligo- and polysaccharides using aerolysin and -hemolysin nanopores(ai dagger)", "abstract": "Seeking new tools for the analysis of glycosaminoglycans, we have compared the translocation of anionic oligosaccharides from hyaluronic acid using aerolysin and -hemolysin nanopores. We show that pores of similar channel length and diameter lead to distinct translocation behavior of the same macromolecules, due to different structural properties of the nanopores. When passing from the vestibule side of the nanopores, short hyaluronic acid oligosaccharides could be detected during their translocation across an aerolysin nanopore but not across an -hemolysin nanopore. We were however able to detect longer oligosaccharide fragments, resulting from the in situ enzymatic depolymerization of hyaluronic acid polysaccharides, with both nanopores, meaning that short oligosaccharides were crossing the -hemolysin nanopore with a speed too high to be detected. The translocation speed was an order of magnitude higher across -hemolysin compared to aerolysin. These results show that the choice of a nanopore to be used for resistive pulse sensing experiments should not rely only on the diameter of the channel but also on other parameters such as the charge repartition within the pore lumen.", "journal": "EUROPEAN PHYSICAL JOURNAL E", "category": "Chemistry, Physical; Materials Science, Multidisciplinary; Physics, Applied; Polymer Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000457690500027", "keywords": "Religiosity", "title": "Halal products consumption in international chain restaurants among global Moslem consumers", "abstract": "Purpose The purpose of this paper is to investigate the determinants of global Moslem consumers on consuming halal products in international chain restaurants. The hypotheses are proposed based on the integration of theory of planned behavior (TPB) and identity theory-religiosity. Design/methodology/approach The survey method was used to test the proposed hypotheses by using PLS. A total of 296 out of 407 questionnaires were collected among global Moslem students in a big city of Indonesia. Findings The results indicate that perceived behavioral control and religiosity is the significant predictor of the intention to consume halal products in international chain restaurants. Surprisingly, attitudes toward halal products and subjective norms have no significant effects on their intention. Research limitations/implications This study mainly investigates from international students' perspectives, and future studies could diversify the respondents. Further, although the studies were done in the biggest Moslem populated country, conducting a multi-country study further validates the results of this study. Additional variables, such as personality or cross-cultural variables, could enhance the prediction of the developed model. Originality/value This study proposes religiosity as an important predictor of halal products consumption among global consumers, which increases the predictive capability of TPB. The results suggest that it is important for managers and governments all products targeted for global Moslem consumers to be halal certified.", "journal": "INTERNATIONAL JOURNAL OF EMERGING MARKETS", "category": "Business; Economics; Management", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000514094500001", "keywords": "Cropland; Land pressure; Livestock; Population growth; Production dynamics", "title": "Competition for land resources: driving forces and consequences in crop-livestock production systems of the Ethiopian highlands", "abstract": "Introduction: Ethiopia has made efforts to tackle the challenges of low crop and livestock productivity and degradation of land resources through various rural development strategies. However, increasing demands for food, animal feed, fuel, and income-generating activities are putting pressure on the land. In this paper, we describe the production pressure and competition between crop and livestock production, quantify rates of land-use/cover (LULC) changes, and examine driving forces and consequences of land conversion. Methods: The study was conducted in Gudo Beret watershed, North Shewa Zone of Amhara region, Ethiopia. It used a combination of methods including remote sensing, household interviews, field observations, focus group discussions, and key informant interviews. Supervised and unsupervised image classification methods were employed to map LULC classes for 31 years (1984-2016). Results: The results of satellite remote sensing revealed that 51% of the land in the study area was subject to accelerated land conversions. The household survey results indicated that feed resources and grain production pressures were 1.43 and 1.34 t ha(-1) respectively. The observed annual changes in plantation and settlement areas were 2.6% and 2.9%. This was mainly at the expense of bushland and grazing land systems. Cropland increased (0.4% year(-1)) while grazing land reduced (3.5% year(-1)) under contrasting dynamics and competitive changes. An increase in human and livestock populations and farm expansion were major drivers of land conversion that adversely affected household livelihoods and the natural ecosystem. The consequences of these pressures resulted in a lack of animal feed, low crop-livestock productivity, and a reduction in natural vegetation coverage. Conclusions: We suggest that sustainable land resource management, more integrated crop-livestock production, and the use of productivity-enhancing technologies could play a role in managing competition for land resources.", "journal": "ECOLOGICAL PROCESSES", "category": "Ecology; Environmental Sciences", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000445620200002", "keywords": "chikungunya; dengue; multiplex real-time reverse-transcriptase polymerase chain reaction; yellow fever; Zika", "title": "Development of multiplex real-time reverse-transcriptase polymerase chain reaction assay for simultaneous detection of Zika, dengue, yellow fever, and chikungunya viruses in a single tube", "abstract": "Zika virus (ZIKV), dengue virus (DENV), chikungunya virus (CHIKV) and yellow fever virus (YFV) share the same mosquito vectors and have similar clinical manifestations early stage of infection. Therefore, simultaneously differentiating these viruses from each other is necessary. We developed a multiplex real-time reverse-transcriptase polymerase chain reaction (RT-PCR) assay for the differentiation of these four viruses in a single tube. The linear range was established by regression analysis, and the R-2 value for each viruswas 0.98, and the 95% lower limit of detectionfor each virus was as follows (copies/reaction): ZIKV-Asian, 9; ZIKV-Africa, 15; CHIKV, 11; DENV-1, 19; DENV-2, 13; DENV-3, 24; DENV-4, 36; and YFV, 17. Meanwhile, our multiplex real-time RT-PCR has a good consistency with the commercial singleplex assay. In summary, the developed assay can be effectively used for the diagnosis of ZIKV, DENV, CHIKV, and YFV infections.", "journal": "JOURNAL OF MEDICAL VIROLOGY", "category": "Virology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000435328500012", "keywords": "cancer screening; intimate partner violence; preventive services; health disparities", "title": "Healthcare Access and Cancer Screening Among Victims of Intimate Partner Violence", "abstract": "Background: Intimate partner violence (IPV) victims often experience substantial and persistent mental and physical health problems, including increased risk for chronic disease and barriers to healthcare access. This study investigated the association between IPV and cancer screening. Materials and Methods: Behavioral Risk Factor Surveillance System data from the eight states and one U.S. territory that administered the optional IPV module in 2006 were analyzed to examine demographic characteristics, health behaviors, health status, healthcare coverage, use of health services, and cancer screening among men and women who reported IPV victimization compared with those among men and women who did not. IPV victimization included physical violence, threats, and sexual violence. Results: In the nine jurisdictions that administered the IPV module, 23.6% of women and 11.3% of men experienced IPV. Fewer women and men reporting IPV victimization had health insurance, a personal doctor or healthcare provider, or regular checkups within the past 2 years than nonvictims. More male and female IPV victims were current tobacco users and engaged in binge drinking in the past month. IPV victims of both sexes also had poorer health status, lower life satisfaction, less social and emotional support, and more days with poor physical and mental health in the past month than nonvictims. IPV victimization was associated with lower rates of mammography and colorectal cancer screening but not cervical cancer screening in women and was not associated with colorectal cancer screening in men. In multivariable logistic regression results presented as adjusted proportions controlling for demographics, health status, and healthcare access, only the association with mammography screening remained significant, and the magnitude of this association was modest. Conclusions: There were consistent differences between IPV victims and nonvictims in nearly every measure of healthcare access, health status, and preventive service use. Much of this association seems explained by population characteristics associated with both IPV and lower use of preventive service use, including differences in demographic characteristics, health status, and healthcare access. Healthcare providers could take steps to identify populations at high risk for lack of access or use of preventive services and IPV victimization.", "journal": "JOURNAL OF WOMENS HEALTH", "category": "Public, Environmental & Occupational Health; Medicine, General & Internal; Obstetrics & Gynecology; Women's Studies", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000438858100001", "keywords": "Bi-objective scheduling; hybrid flow shop; learning effect; meta-heuristic", "title": "The Effect of Worker Learning on Scheduling Jobs in a Hybrid Flow Shop: A Bi-Objective Approach", "abstract": "This paper studies learning effect as a resource utilization technique that can model improvement in worker's ability as a result of repeating similar tasks. By considering learning of workers while performing setup times, a schedule can be determined to place jobs that share similar tools and fixtures next to each other. The purpose of this paper is to schedule a set of jobs in a hybrid flow shop (HFS) environment with learning effect while minimizing two objectives that are in conflict: namely maximum completion time (makespan) and total tardiness. Minimizing makespan is desirable from an internal efficiency viewpoint, but may result in individual jobs being scheduled past their due date, causing customer dissatisfaction and penalty costs. A bi-objective mixed integer programming model is developed, and the complexity of the developed bi-objective model is compared against the bi-criteria one through numerical examples. The effect of worker learning on the structure of assigned jobs to machines and their sequences is analyzed. Two solution methods based on the hybrid water flow like algorithm and non-dominated sorting and ranking concepts are proposed to solve the problem. The quality of the approximated sets of Pareto solutions is evaluated using several performance criteria. The results show that the proposed algorithms with learning effect perform well in reducing setup times and eliminate the need for setups itself through proper scheduling.", "journal": "JOURNAL OF SYSTEMS SCIENCE AND SYSTEMS ENGINEERING", "category": "Operations Research & Management Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000437469800018", "keywords": "Wireless networks; Worldwide Interoperability for Microwave Access; Quality of service; Admission control algorithm; Scheduling algorithm", "title": "Downlink Scheduling Algorithm for WiMAX Protocol to Improve QoS", "abstract": "The growing demand for multimedia communication has resulted in tougher requirements of quality of service (QoS). Today, QoS necessitates the deployment of powerful and efficient networks. Worldwide Interoperability for Microwave Access (WiMAX) is regarded as a promising technology in the field of wireless communication. In fact, WiMAX network is considered the best network to support real-time as well as non-real-time applications in varied conditions of a simulated environment. Wireless communication requires uplink and downlink scheduling for communication among base station subscribers. Scheduling is still a challenging task for researchers. In this work, we propose an evolutionary computational scheme for downlink scheduling that brings in substantial improvisations in the QOS of a network system. The proposed approach simplifies the scheduling scheme for varied service schemes such as UGS, rtPS, nrtPS. We extend some improved computational strategies to our proposed approach in order to control data communication as well as route formation in signal information. We also use a computational approach, i.e., passage relocation admission control to perform automatic selection of base station with similar data operations. We further seek to analyze the role of data communication and packet dropping in wireless network communication. Our experimental study shows an improved performance of the proposed model in terms of slot/success ratio, throughput and energy consumption. As it happens, we succeed in recording 7% improvement in throughput performance, 10.34% improvement in slot/success ratio performance, and quite significantly, a 28% reduction in energy consumption based on the simulation time.", "journal": "ARABIAN JOURNAL FOR SCIENCE AND ENGINEERING", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000454975400007", "keywords": "Quantum field theory; Chemical potential; Phase diagram; Bose-Einstein condensation; Quantum simulation; Worm algorithm", "title": "SU(3) quantum spin ladders as a regularization of the CP(2) model at non-zero density: From classical to quantum simulation", "abstract": "Quantum simulations would be highly desirable in order to investigate the finite density physics of QCD. (1 + 1)-d CP(N - 1) quantum field theories are toy models that share many important features of QCD: they are asymptotically free, have a non-perturbatively generated massgap, as well as theta-vacua. SU(N) quantum spin ladders provide an unconventional regularization of CP(N - 1) models that is well-suited for quantum simulation with ultracold alkaline-earth atoms in an optical lattice. In order to validate future quantum simulation experiments of CP(2) models at finite density, here we use quantum Monte Carlo simulations on classical computers to investigate SU(3) quantum spin ladders at non-zero chemical potential. This reveals a rich phase structure, with single- or double-species Bose-Einstein \"condensates\", with or without ferromagnetic order. (C) 2018 Elsevier Inc. All rights reserved.", "journal": "ANNALS OF PHYSICS", "category": "Physics, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000438198300022", "keywords": "lung cancer; nicotine dependence; smoking; smoking cessation", "title": "Tobacco Dependence Predicts Higher Lung Cancer and Mortality Rates and Lower Rates of Smoking Cessation in the National Lung Screening Trial", "abstract": "BACKGROUND: Incorporating tobacco treatment within lung cancer screening programs has the potential to influence cessation in high-risk smokers. We aimed to better understand the characteristics of smokers within a screening cohort, correlate those variables with downstream outcomes, and identify predictors of continued smoking. METHODS: This study is a secondary analysis of the National Lung Screening Trial randomized clinical study. Tobacco dependence was evaluated by using the Fagerstrom Test for Nicotine Dependence, the Heaviness of Smoking Index, and time to first cigarette (TTFC); descriptive statistics were performed. Clinical outcomes (smoking cessation, lung cancer, and mortality) were assessed with descriptive statistics and chi(2) tests stratified according to nicotine dependence. Logistic and Cox regression models were used to study the influence of dependence on smoking cessation and mortality, respectively. RESULTS: Patients with high dependence scores were less likely to quit smoking compared with low dependence smokers (TTFC OR, 0.50 [95% CI, 0.42-0.60]). Indicators of high dependence, as measured according to all three metrics, were associated with worsening clinical outcomes. TTFC showed that patients who smoked within 5 min of waking (indicating higher dependence) had higher rates of lung cancer (2.07% for > 60 min after waking vs 5.92% <= 5 min after waking; hazard ratio [HR], 2.56 [95% CI, 1.49-4.41]), all-cause mortality (5.38% for > 60 min vs 11.21% <= 5 min; HR, 2.19 [95% CI, 1.55-3.09]), and lung cancer-specific mortality (0.55% for > 60 min vs 2.92% for <= 5 min; HR, 4.46 [95% CI, 1.63-12.21]). CONCLUSIONS: Using TTFC, a one-question assessment of tobacco dependence, at the time of lung cancer screening has implications for personalizing tobacco treatment and improving risk assessment.", "journal": "CHEST", "category": "Critical Care Medicine; Respiratory System", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000431398300013", "keywords": "Mediterranean sea; Deep sea; Trawl survey; Red shrimps; GAMs", "title": "Effects of environmental and anthropogenic drivers on the spatial distribution of deep-sea shrimps in the Ligurian and Tyrrhenian Seas (NW Mediterranean)", "abstract": "In the Mediterranean Sea, Aristaeomorpha foliacea and Aristeus antennatus are the most important target species of deep-sea trawl fisheries. Previous studies performed in several areas of the Mediterranean highlighted the key role played by both environmental factors, such as temperature, and anthropogenic activities, such as fishing, in affecting the abundance and distribution of the two species. The present study is aimed at investigating the effects of environmental and anthropogenic drivers on the abundance and spatial distribution of A. foliacea and A. antennatus in the Ligurian and northern and central Tyrrhenian Seas (NW Mediterranean). To this end, the time series of MEDITS trawl survey data (1994-2015) were analyzed together with environmental variables, namely sea surface temperature, current speed, and fishing effort by means of GAM. The results show that fishing plays an important role in shaping the spatial distribution of the two species. A. antennatus is prevalent where fishing effort is higher; this can be related to the higher resilience of blue and red shrimp to fishery impact. In contrast, high temperatures are associated to the prevalence of A. foliacea. Therefore, the abundance and spatial distribution of this species is mainly driven by temperature.", "journal": "HYDROBIOLOGIA", "category": "Marine & Freshwater Biology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000438479700004", "keywords": "Change point; Linear processes; CUSUM estimator", "title": "An efficient algorithm to estimate the change in variance", "abstract": "This article conducts a more exact analysis of the probability of the absolute bias of the popular CUSUM estimator of the variance change, we conclude that the precision of the CUSUM estimator is effected by the location of the variance change and the variances before and behind the change date. And a more efficient estimation is proposed. Simulations demonstrate that the improvement of the proposed method is nontrivial. (C) 2018 Elsevier B.V. All rights reserved.", "journal": "ECONOMICS LETTERS", "category": "Economics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000441334300062", "keywords": "degree-of-presence ratio; ensemble empirical mode decomposition; fault diagnosis; feature extraction; intrinsic mode function selection; Kullback-Leibler divergence; rotating machinery; rub-impact faults", "title": "Rub-Impact Fault Diagnosis Using an Effective IMF Selection Technique in Ensemble Empirical Mode Decomposition and Hybrid Feature Models", "abstract": "The complex nature of rubbing faults makes it difficult to use traditional signal analysis methods for feature extraction. Various time-frequency analysis approaches based on signal decomposition, such as empirical mode decomposition (EMD) and ensemble EMD (EEMD), have been widely utilized recently to analyze rub-impact faults. However, traditional EMD suffers from \"mode-mixing\", and in both EMD and EEMD the relevance of the extracted components to rubbing processes must be determined. In this paper, we introduce a new informative intrinsic mode function (IMF) selection method for EEMD and a hybrid feature model for diagnosing rub-impact faults of various intensities. Our method uses a novel selection procedure that combines the degree-of-presence ratio of rub impact and a Kullback Leibler divergence-based similarity measure into an IMF quality metric with adaptive threshold-based selection to pick the meaningful signal-dominant modes. Signals reconstructed using the selected IMFs contained explicit information about the rubbing faults and are used for hybrid feature extraction. Experimental results demonstrated that the proposed approach effectively defines meaningful IMFs for rubbing processes, and the presented hybrid feature model allows for the classification of rub-impact faults of various intensities with good accuracy.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000437742100005", "keywords": "accelerometry; sitting/standing; epidemiology; gerontology", "title": "Segregating the Distinct Effects of Sedentary Behavior and Physical Activity on Older Adults' Cardiovascular Structure and Function: Part 1-Linear Regression Analysis Approach", "abstract": "Background: Physical behavior [PB, physical activity (PA), and sedentary behavior (SB)] can adjust cardiovascular mortality risk in older adults. The aim of this study was to predict cardiovascular parameters (CVPs) using 21 parameters of PB. Methods: Participants [n = 93, 73.8 (6.23) y] wore a thigh-mounted accelerometer for 7 days. Phenotype of the carotid, brachial, and popliteal arteries was conducted using ultrasound. Results: Sedentary behavior was associated with one of the 19 CVPs. Standing and light-intensity PA was associated with 3 and 1 CVP, respectively. Our prediction model suggested that an hourly increase in light-intensity PA would be negatively associated with popliteal intima-media thickness [0.09 mm (95% confidence interval, 0.15 to 0.03)]. sMVPA [moderate-vigorous PA (MVPA), accumulated in bouts <10 min] was associated with 1 CVP. (10)MVPA (MVPA accumulated in bouts >= 10 min) had no associations. W50% had associations with 3 CVP. SB%, alpha, true mean PA bout, daily sum of PA bout time, and total week (10)MVPA each were associated with 2 CVP. Conclusions: Patterns of PB are more robust predictors of CVP than PB (hours per day). The prediction that popliteal intima-media thickness would be negatively associated with increased standing and light-intensity PA engagement suggests that older adults could obtain health benefits without MVPA engagement.", "journal": "JOURNAL OF PHYSICAL ACTIVITY & HEALTH", "category": "Public, Environmental & Occupational Health", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000447479400037", "keywords": "Building wall; Thermal impulse response; Dynamical thermal characteristics; Tikhonov regularization; Energy efficiency", "title": "Estimation of thermal impulse response of a multi-layer building wall through in-situ experimental measurements in a dynamic regime with applications", "abstract": "The dynamic thermal characteristics of the components of a building have a primary influence on the energy performance of the building's envelope under real environmental conditions. In this study, a novel approach for estimation of the thermal impulse response (TIR) functions and determination of the dynamic thermal characteristics of a multilayer facade wall with unknown thermal properties, structure, and dimensions is proposed. Unlike existing approaches, such as those presented by Luo et al. (2010) and Fernandes et al. (2015), which are based on the use of known physical parameters and dimensions of the considered structure for determination of the transfer function, the proposed framework is based solely on data from in-situ experimental measurements of surface temperatures and thermal fluxes through the inner and outer wall surfaces in a dynamic regime. Consequently, the estimated TIR functions and dynamic thermal characteristics reflect the actual physical conditions of the considered building wall. The building wall is modelled as a two-input, two-output linear time invariant (LTI) dynamic system where the surface temperatures and fluxes from both sides are used as system inputs and outputs, respectively. The input and output quantities are related by the convolution integrals and TIR functions. The TIR functions are obtained using the measured data and the least square estimator. As the corresponding system of linear equations is ill-posed, the Tikhonov regularization technique with a single parameter is implemented to overcome the numerical difficulties. The optimal regularization parameter is obtained using the L-curve approach. The estimated TIR functions are validated by comparison with the analytical solutions. The dynamic thermal characteristics of the considered building wall with unknown parameters are determined using the Fourier transform (FT) of the estimated TIR functions. The practical applications of the estimated TIR functions related to the energy performance of buildings (EPB) and energy efficiency, along with additional validation, are demonstrated by the evaluation of the dynamic thermal characteristics, cumulative heat losses, heat accumulation, conductive part of thermal transmittance (U-value), and surface heat fluxes, using only the estimated TIR functions and a control set of the experimental data.", "journal": "APPLIED ENERGY", "category": "Energy & Fuels; Engineering, Chemical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000437763900005", "keywords": "echocardiography; global longitudinal strain; heart failure; left ventricular noncompaction; noncompaction cardiomyopathy", "title": "Echocardiographic and clinical markers of left ventricular ejection fraction and moderate or greater systolic dysfunction in left ventricular noncompaction cardiomyopathy", "abstract": "BackgroundLeft ventricular noncompaction (LVNC) is associated with progressive LV systolic dysfunction and dilated cardiomyopathy. We aimed to investigate the echocardiographic and clinical characteristics associated with LV ejection fraction (LVEF) and moderate or greater systolic dysfunction in patients with LVNC. MethodsOur institutional echocardiography database was retrospectively reviewed between 2008 and 2014, and 62 patients with LVNC were identified. Forty-three (69%) had moderate or greater LV systolic dysfunction (LVEF40%) and were compared with 19 (31%) patients with preserved or mildly reduced LVEF (>40%). Linear regression analyses were utilized to identify markers associated with LVEF. ResultsThe mean age was 6317years and noncompacted-to-compacted ratio was 2.3 +/- 0.5, and was larger in patients with LVEF40% (2.4 vs 2.1; P=.02). Patients with LVEF40% were older, had more congestive heart failure, significant QRS interval prolongation, and greater LV remodeling and worse mean global longitudinal strain (GLS). Multivariate regression analysis revealed increased age (standardized regression coefficient ()=-0.17; P=.04) and QRS duration (=-0.13; P=.08), congestive heart failure (=-0.18; P=.04), and worsened GLS (=-0.40; P=.001) were independently associated with decreased LVEF in the cohort (overall model fit R-2=0.71; P<.0001). Increased age (=-0.49; P=.01) and QRS duration (=-0.50; P=.002), and worsened GLS (=-0.33; P=.04), were also associated with a lower LVEF in patients with LVEF>40%. ConclusionsThe independent markers associated with LVEF and moderate or greater LV systolic dysfunction in patients with LVNC, in particular GLS and QRS duration, may detect high-risk candidates for more aggressive clinical surveillance and medical therapy.", "journal": "ECHOCARDIOGRAPHY-A JOURNAL OF CARDIOVASCULAR ULTRASOUND AND ALLIED TECHNIQUES", "category": "Cardiac & Cardiovascular Systems", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000444845100013", "keywords": "Pediatric obstructive sleep apnea; Magnetic resonance imaging; Adenotonsillectomy", "title": "Characterization of upper airway obstruction using cine MRI in children with residual obstructive sleep apnea after adenotonsillectomy", "abstract": "Objectives/Background: Tonsillectomy and adenoidectomy (T&A) lead to resolution of obstructive sleep apnea (OSA) in most children. However, OSA persists in about 25-40% of children. Cinematic magnetic resonance imaging (cine MRI) can aid the management of persistent OSA by localizing airway obstruction. We describe our experience in implementing and optimizing a cine MRI protocol by using a 3 Tesla MRI scanner, and the use of dexmedetomidine for sedation to improve reproducibility, safety, and diagnostic accuracy. Patients/Methods: Patients aged 3-18 years who underwent cine MRI for the evaluation of persistent OSA after T&A and failed positive airway pressure (PAP) therapy were included. Clinical data and the apnea-hyponea index were compared with quantitative and qualitative estimates of airway obstruction from imaging sequences. Results: A total of 36 children were included with a mean age of 9.6 +/- 4.6 (SD) years with 40% over 12 years of age. Two-thirds of them were boys. Seventeen out of 36 children (47%) had Down syndrome. Single site and multilevel obstruction were identified in 21 of 36 patients (58%) and in 12 of 36 patients (33%), respectively. All cine MRIs were performed without complications. Multiple regression analysis demonstrated that a combination of the minimum airway diameter and body mass index z-score best predicted OSA severity (P = 0.002). Conclusions: Cine MRI is a sensitive, safe, and noninvasive modality for visualizing upper airway obstruction in children with persistent OSA after T&A. Accurate identification of obstruction can assist in surgical planning in children who fail PAP therapy. (C) 2017 Elsevier B.V. All rights reserved.", "journal": "SLEEP MEDICINE", "category": "Clinical Neurology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000439907600010", "keywords": "", "title": "Sustained Virologic Response and Costs Associated with Direct-Acting Antivirals for Chronic Hepatitis C Infection in Oklahoma Medicaid", "abstract": "BACKGROUND: Outcomes involving newer direct-acting antiviral (DAA) hepatitis C virus (HCV) regimens have not been studied extensively among the Medicaid population. OBJECTIVE: To assess clinical (treatment failure) and economic outcomes for chronic HCV-infected Oklahoma Medicaid members following treatment with DAAs and to measure associations with patient, treatment, and clinical characteristics. METHODS: This cross-sectional study used Oklahoma Medicaid pharmacy and medical claims data for adult members who used a newer DAA agent and had reported a successful or failed sustained virological response rate 12 weeks after therapy completion (SVR12) from January 1, 2014, to June 30, 2016. Multivariable logistic and gamma regressions assessed predictors of SVR12 failure and costs controlling for member demographics (i.e., age, sex, race, rural residence); type of DAA and adherence; clinical characteristics (e.g., comorbid conditions, advanced liver disease); and the implementation of changes to a prior authorization program. RESULTS: Of 934 Medicaid members eligible for treatment with DAAs between January 1, 2014, and June 30, 2016, 906 received DAA treatment, 40.6% (368/906) had reported SVR12 outcomes, and 59.4% (n=538) did not have a reported SVR recorded. Of those with reported SVR12 outcomes, patients were 53.1 +/- 9.7 years of age, 51.1% were male, 8.4% had SVR12 failure, and each member had mean costs of $140,283 +/-$52,779. Multivariable analyses indicated higher odds of SVR12 failure was independently associated with cirrhosis (OR [decompensated] = 6.69 and OR [compensated] =3.52, P<0.001), while males had higher odds of failure than females (0R = 3.34, P<0.010). No significant difference in SVR12 failure was noted, according to DAA type or a medication adherence threshold of >95%. Ledipasvir/sofosbuvir was independently associated with lower costs (exp[b] = 0.81; P<0.001) compared with sofosbuvir, while higher costs were associated with decompensated cirrhosis (exp[b] = 1.22; P<0.001) and treatment failure (exp[b] = 1.18, P<0.010). In an analysis including members without reported SVR12 outcomes, decompensated and compensated cirrhosis had lower odds (P<0.001) of no reported SVR12 from ambulatory clinic settings. CONCLUSIONS: Almost 60% of Medicaid members receiving DAA treatment did not have a final reported SVR12 outcome. Among those with viral load measurements, treatment success was high and both decompensated and compensated cirrhosis were independently associated with significantly higher odds of treatment failure. Addressing a loss to follow-up among HCV patients and curtailing the development of cirrhosis to improve treatment success may warrant interventions that improve access to care and remove barriers that impede treatment initiation and completion. Copyright (C) 2018, Academy of Managed Care Pharmacy. All rights reserved.", "journal": "JOURNAL OF MANAGED CARE & SPECIALTY PHARMACY", "category": "Health Care Sciences & Services; Pharmacology & Pharmacy", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000444973300009", "keywords": "Pathophysiology; fatty acids; myocardial infarction; lipolysis", "title": "Early increase in serum fatty acid binding protein 4 levels in patients with acute myocardial infarction", "abstract": "Background: Acute myocardial infarction (AMI) induces marked activation of the sympathetic nervous system. Fatty acid binding protein 4 (FABP4) is not only an intracellular protein, but also a secreted adipokine that contributes to obesity-related metabolic complications. Here, we examined the role of serum FABP4 as a pathophysiological marker in patients with AMI. Methods and results: We studied 106 patients presenting to the emergency unit with a final diagnosis of AMI, including 12 patients resuscitated from out-of-hospital cardiac arrest (OHCA) caused by ventricular fibrillation. FABP4 levels peaked on admission or just after percutaneous coronary intervention and declined thereafter. Regression analysis revealed no significant correlation between peak FABP4 and peak cardiac troponin T determined by Roche high-sensitive assays (hs-TnT). Notably, FABP4 levels were particularly elevated in AMI patients who were resuscitated from OHCA (median 130.2 ng/mL, interquartile range (IQR) 51.8-243.9 ng/mL) compared with those without OHCA (median 26.1 ng/ml, IQR 17.1-43.4 ng/mL), while hs-TnT levels on admission were not associated with OHCA. Immunohistochemistry of the human heart revealed that FABP4 is abundantly present in adipocytes within myocardial tissue and epicardial adipose tissue. An in vitro study using cultured adipocytes showed that FABP4 is released through a 3-adrenergic receptor (AR)-mediated mechanism. Conclusions: FABP4 levels were significantly elevated during the early hours after the onset of AMI and were robustly increased in OHCA survivors. Together with the finding that FABP4 is released from adipocytes via 3-AR-mediated lipolysis, our data provide a novel hypothesis that serum FABP4 may represent the adrenergic overdrive that accompanies acute cardiovascular disease, including AMI.", "journal": "EUROPEAN HEART JOURNAL-ACUTE CARDIOVASCULAR CARE", "category": "Cardiac & Cardiovascular Systems", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000454895500045", "keywords": "detector arrays; digital processing; noise; image analysis; image processing", "title": "Stratigraphic and lithofacies control on pore characteristics of Mississippian limestone and chert reservoirs of north-central Oklahoma", "abstract": "We have determined how stratigraphy and lithofacies control pore structures in the Mississippian limestone and chert reservoir of north-central Oklahoma There are 17 lithofacies and 29 high-frequency cycles documented in the Mississippian interval of this study. The high-frequency cycles have thicknesses ranging from 0.3 to 30.5 m (1-100 ft) and are mainly asymmetric regressive phases. The pore characteristics, measured through digital-image analysis (DIA) of thin-sections photomicrographs (N > 3100), exhibit unique correlations with core porosity, permeability, and lithofacies within a sequence-stratigraphic framework. There are five fundamental correlations observed. First, porosity from DIA and laboratory core measurements has a strong positive relationship (R-2 = 0.94). However, some values from DIA porosity yield relatively higher values, specifically in spiculitic mudstone wackestones and argillaceous spiculitic mudstone wackestones. The difference is hypothesized due to the presence of isolated nanopores that are not accessible by helium during measurement of core porosity. Second, the relationship between pore circularity and permeability is indeterminate. The indeterminate relationship is related to a complex internal pore network, intensive diagenetic alteration, an unconnected microfracture network, and isolated pores. Third, positive moderate to strong correlations (R-2 = 0.46 - 0.85) between porosity and permeability are observed only in four lithofacies. Fourth, coarse-grained lithofacies within the uppermost depositional sequence of the Mississippian interval have a heterogeneous pore-size distribution, whereas fine-grained lithofacies tend to exhibit a homogeneous pore-size distribution. Fifth, higher reservoir quality is associated with the upper intervals of high-frequency shallowing-upward cycles. This confirms that the sequence-stratigraphic variability of lithofacies is important to predict reservoir quality and its distribution. An alternative graphical method of pore-size distribution is also developed. To be a useful \"technique,\" examples of the plot are demonstrated using samples in this study. The plot successfully provides simple identification of pore-size classes, quantitative percentage of pore-size class, dominant pore class, and approximate minimum and maximum pore size.", "journal": "INTERPRETATION-A JOURNAL OF SUBSURFACE CHARACTERIZATION", "category": "Geochemistry & Geophysics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000435198400123", "keywords": "image segmentation; region merging; within- and between-segment heterogeneity; watershed transformation; geographic object-based image analysis (GEOBIA)", "title": "Region Merging Considering Within- and Between-Segment Heterogeneity: An Improved Hybrid Remote-Sensing Image Segmentation Method", "abstract": "Image segmentation is an important process and a prerequisite for object-based image analysis, but segmenting an image into meaningful geo-objects is a challenging problem. Recently, some scholars have focused on hybrid methods that employ initial segmentation and subsequent region merging since hybrid methods consider both boundary and spatial information. However, the existing merging criteria (MC) only consider the heterogeneity between adjacent segments to calculate the merging cost of adjacent segments, thus limiting the goodness-of-fit between segments and geo-objects because the homogeneity within segments and the heterogeneity between segments should be treated equally. To overcome this limitation, in this paper a hybrid remote-sensing image segmentation method is employed that considers the objective heterogeneity and relative homogeneity (OHRH) for MC during region merging. In this paper, the OHRH method is implemented in five different study areas and then compared to our region merging method using the objective heterogeneity (OH) method, as well as the full lambda-schedule algorithm (FLSA). The unsupervised evaluation indicated that the OHRH method was more accurate than the OH and FLSA methods, and the visual results showed that the OHRH method could distinguish both small and large geo-objects. The segments showed greater size changes than those of the other methods, demonstrating the superiority of considering within- and between-segment heterogeneity in the OHRH method.", "journal": "REMOTE SENSING", "category": "Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000433074400011", "keywords": "dynamic information; traffic flow model; traffic control; guidance information; traffic congestion", "title": "Study on the Reduced Traffic Congestion Method Based on Dynamic Guidance Information", "abstract": "This paper studies how to generate the reasonable information of travelers' decision in real network. This problem is very complex because the travelers' decision is constrained by different human behavior. The network conditions can be predicted by using the advanced dynamic OD (Origin-Destination, OD) estimation techniques. Based on the improved mesoscopic traffic model, the predictable dynamic traffic guidance information can be obtained accurately. A consistency algorithm is designed to investigate the travelers' decision by simulating the dynamic response to guidance information. The simulation results show that the proposed method can provide the best guidance information. Further, a case study is conducted to verify the theoretical results and to draw managerial insights into the potential of dynamic guidance strategy in improving traffic performance.", "journal": "COMMUNICATIONS IN THEORETICAL PHYSICS", "category": "Physics, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000441367600009", "keywords": "Screen light; Artificial visible light; Normal human fibroblasts; Transcriptome; Mitochondrial network; Actin cytoskeleton", "title": "Mitochondrial damage and cytoskeleton reorganization in human dermal fibroblasts exposed to artificial visible light similar to screen-emitted light", "abstract": "Background: Artificial visible light is everywhere in modern life. Social communication confronts us with screens of all kinds, and their use is on the rise. We are therefore increasingly exposed to artificial visible light, the effects of which on skin are poorly known. Objective: The purpose of this study was to model the artificial visible light emitted by electronic devices and assess its effect on normal human fibroblasts. Methods: The spectral irradiance emitted by electronic devices was optically measured and equipment was developed to accurately reproduce such artificial visible light. Effects on normal human fibroblasts were analyzed on human genome microarray-based gene expression analysis. At cellular level, visualization and image analysis were performed on the mitochondrial network and F-actin cytoskeleton. Cell proliferation, ATP release and type I procollagen secretion were also measured. Results: We developed a device consisting of 36 LEDs simultaneously emitting blue, green and red light at distinct wavelengths (450 nm, 525 nm and 625 nm) with narrow spectra and equivalent radiant power for the three colors. A dose of 99 J/cm(2) artificial visible light was selected so as not to induce cell mortality following exposure. Microarray analysis revealed 2984 light-modulated transcripts. Functional annotation of light-responsive genes revealed several enriched functions including, amongst others, the \"mitochondria\" and \"integrin signaling\" categories. Selected results were confirmed by real-time quantitative PCR, analyzing 24 genes representing these two categories. Analysis of micro-patterned culture plates showed marked fragmentation of the mitochondrial network and disorganization of the F-actin cytoskeleton following exposure. Functionally, there was considerable impairment of cell growth and spread, ATP release and type I procollagen secretion in exposed fibroblasts. Conclusion: Artificial visible light induces drastic molecular and cellular changes in normal human fibroblasts. This may impede normal cellular functions and contribute to premature skin aging. The present results extend our knowledge of the effects of the low-energy wavelengths that are increasingly used to treat skin disorders. (C) 2018 Japanese Society for Investigative Dermatology. Published by Elsevier B.V. All rights reserved.", "journal": "JOURNAL OF DERMATOLOGICAL SCIENCE", "category": "Dermatology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000448545700064", "keywords": "economic complexity; non-linear map; bipartite networks", "title": "A New and Stable Estimation Method of Country Economic Fitness and Product Complexity", "abstract": "We present a new metric estimating fitness of countries and complexity of products by exploiting a non-linear non-homogeneous map applied to the publicly available information on the goods exported by a country. The non homogeneous terms guarantee both convergence and stability. After a suitable rescaling of the relevant quantities, the non homogeneous terms are eventually set to zero so that this new metric is parameter free. This new map almost reproduces the results of the original homogeneous metrics already defined in literature and allows for an approximate analytic solution in case of actual binarized matrices based on the Revealed Comparative Advantage (RCA) indicator. This solution is connected with a new quantity describing the neighborhood of nodes in bipartite graphs, representing in this work the relations between countries and exported products. Moreover, we define the new indicator of country net-efficiency quantifying how a country efficiently invests in capabilities able to generate innovative complex high quality products. Eventually, we demonstrate analytically the local convergence of the algorithm involved.", "journal": "ENTROPY", "category": "Physics, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000440404700001", "keywords": "antioxidant activity; benzimidazole; cholinesterase inhibitors; molecular modeling", "title": "Multifunctional cholinesterase inhibitors for Alzheimer's disease: Synthesis, biological evaluations, and docking studies of o/p-propoxyphenylsubstituted-1H-benzimidazole derivatives", "abstract": "This study indicates the synthesis, cholinesterase (ChE) inhibitory activity, and molecular modeling studies of 48 compounds as o- and p-(3-substitutedethoxyphenyl)-1H-benzimidazole derivatives. According to the ChE inhibitor activity results, generally, para series are more active against acetylcholinesterase (AChE) whereas ortho series are more active against butyrylcholinesterase (BuChE). The most active compounds against AChE and BuChE are compounds A12 and B14 with IC50 values of 0.14 and 0.22M, respectively. Additionally, the most active 16 compounds against AChE/BuChE were chosen to investigate the neuroprotective effects, and the results indicated that most of the compounds have free radical scavenging properties and show their effects by reducing free radical production; moreover, some of the compounds significantly increased the viability of SH-SY5Y cells exposed to H2O2. Overall, compounds A12 and B14 with potential AChE and BuChE inhibitory activities, high neuroprotection against H2O2-induced toxicity, free radical scavenging properties, and metal chelating abilities may be considered as lead molecules for the development of multi-target-directed ligands against Alzheimer's disease.", "journal": "ARCHIV DER PHARMAZIE", "category": "Chemistry, Medicinal; Chemistry, Multidisciplinary; Pharmacology & Pharmacy", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000441334300174", "keywords": "GF-3; InSAR; DEM; baseline estimation; real-time orbit", "title": "InSAR Baseline Estimation for Gaofen-3 Real-Time DEM Generation", "abstract": "For Interferometry Synthetic Aperture Radar (InSAR), the normal baseline is one of the main factors that affects the accuracy of the ground elevation. For Gaofen-3 (GF-3) InSAR processing, the poor accuracy of the real-time orbit determination results in a large baseline error, leads to a modulation error in azimuth and a slope error in the range for timely Digital Elevation Model (DEM) generation. In order to address this problem, a novel baseline estimation approach based on Shuttle Radar Topography Mission (SRTM) DEM is proposed in this paper. Firstly, the orbit fitting is executed to remove the non-linear error factor, which is different from traditional methods. Secondly, the height errors are obtained in a slant-range plane between SRTM DEM and the GF-3 generated DEM, which can be used to estimate the baseline error with a linear variation. Then, the real-time orbit can be calibrated by the baseline error. Finally, the DEM generation is performed by using the modified baseline and orbit. This approach has the merit of spatial and precise orbital free ability. Based on the results of GF-3 interferometric SAR data for Hebei, the effectiveness of the proposed algorithm is verified and the accuracy of GF-3 real-time DEM products can be improved extensively.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000450361100031", "keywords": "Cognitive radio; Dynamic spectrum access; Cooperative relaying; Coalitional game; Energy efficiency", "title": "A New Energy Efficiency/Spectrum Efficiency Model for Cooperative Cognitive Radio Network", "abstract": "In this paper we study the resources access problem in cognitive radio networks, especially we are interested in the large number of secondary users (SUs). We establish a model based on channel access process when the PU (Primary User) is active, respecting the level of interference authorized by the operator. We study a system of cooperation between the SUs and the PUs to increase the performance of the system. SUs pass through an negotiation phase with the PUs for the acquisition of the underutilized channels with exceeded interference caused to the PU. The PU will support additional interference Delta but will benefit from the cooperation of SUs to relay its data. We model this cooperation as coalitional game.The utility function depends on two main parameters which are: transmission power and noise level. A distributed coalition formation algorithm is also proposed, which can be used by SUs to decide whether to join or leave a coalition. Such a decision is based on whether it can increase the maximal coalition utility value. We consider also the trade off between energy efficiency and the target throughput in the proposed cooperative relay network. The objective of this work is to validate the expected enhancement of the overall throughput of the network and also the energy efficiency while increasing the opportunity for SUs to access the licensed spectrum owned by PUs.", "journal": "MOBILE NETWORKS & APPLICATIONS", "category": "Computer Science, Hardware & Architecture; Computer Science, Information Systems; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000444601700010", "keywords": "Spine; Vertebral fractures; Osteoporosis; Dual-layer spectral computed tomography", "title": "Three-material decomposition with dual-layer spectral CT compared to MRI for the detection of bone marrow edema in patients with acute vertebral fractures", "abstract": "ObjectivesTo assess whether bone marrow edema in patients with acute vertebral fractures can be accurately diagnosed based on three-material decomposition with dual-layer spectral CT (DLCT).Materials and methodsAcute (n=41) and chronic (n=18) osteoporotic thoracolumbar vertebral fractures as diagnosed by MRI (hyperintense signal in STIR sequences) in 27 subjects (7211years; 17 women) were assessed with DLCT. Spectral data were decomposed into hydroxyapatite, edema-equivalent, and fat-equivalent density maps using an in-house-developed algorithm. Two radiologists, blinded to clinical and MR findings, assessed DLCT and conventional CT independently, using a Likert scale (1=no edema; 2=likely no edema; 3=likely edema; 4=edema). For DLCT and conventional CT, accuracy, sensitivity, and specificity for identifying acute fractures (Likert scale, 3 and 4) were analyzed separately using MRI as standard of reference.ResultsFor the identification of acute fractures, conventional CT showed a sensitivity of 0.73-0.76 and specificity of 0.78-0.83, whereas the sensitivity (0.93-0.95) and specificity (0.89) of decomposed DLCT images were substantially higher. Accuracy increased from 0.76 for conventional CT to 0.92-0.93 using DLCT. Interreader agreement for fracture assessment was high in conventional CT (weighted [95% confidence interval]; 0.81 [0.70; 0.92]) and DLCT (0.96 [0.92; 1.00]).Conclusionsp id=\"Par4\"Material decomposition of DLCT data substantially improved accuracy for the diagnosis of acute vertebral fractures, with a high interreader agreement. This may spare patients additional examinations and facilitate the diagnosis of vertebral fractures.", "journal": "SKELETAL RADIOLOGY", "category": "Orthopedics; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000430882300004", "keywords": "Floodrisk analysis; Floodcontrol and sediment transport; Copula; Uncertainty; Loess Plateau", "title": "Flood risk analysis for flood control and sediment transportation in sandy regions: A case study in the Loess Plateau, China", "abstract": "Traditional flood risk analysis focuses on the probability of flood events exceeding the design flood of downstream hydraulic structures while neglecting the influence of sedimentation in river channels on regional flood control systems. This work advances traditional flood risk analysis by proposing a univariate and copula-based bivariate hydrological risk framework which incorporates both flood control and sediment transport. In developing the framework, the conditional probabilities of different flood events under various extreme precipitation scenarios are estimated by exploiting the copula-based model. Moreover, a Monte Carlo-based algorithm is designed to quantify the sampling uncertainty associated with univariate and bivariate hydrological risk analyses. Two catchments located on the Loess plateau are selected as study regions: the upper catchments of the Xianyang and Huaxian stations (denoted as UCX and UCH, respectively). The univariate and bivariate return periods, risk and reliability in the context of uncertainty for the purposes of flood control and sediment transport are assessed for the study regions. The results indicate that sedimentation triggers higher risks of damaging the safety of local flood control systems compared with the event that AMF exceeds the design flood of downstream hydraulic structures in the UCX and UCH. Moreover, there is considerable sampling uncertainty affecting the univariate and bivariate hydrologic risk evaluation, which greatly challenges measures of future flood mitigation. In addition, results also confirm that the developed framework can estimate conditional probabilities associated with different flood events under various extreme precipitation scenarios aiming for flood control and sediment transport. The proposed hydrological risk framework offers a promising technical reference for flood risk analysis in sandy regions worldwide. (C) 2018 Elsevier B.V. All rights reserved.", "journal": "JOURNAL OF HYDROLOGY", "category": "Engineering, Civil; Geosciences, Multidisciplinary; Water Resources", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000447239300003", "keywords": "Sepsis; Therapy; Intensive care medicine; PCT; Therapy management", "title": "Procalcitonin as a tool for the assessment of successful therapy of severe sepsis. An analysis using clinical routine data", "abstract": "Introduction. Procalcitonin (PCT) is awell-evaluated biomarker for the detection of severe bacterial infections and monitoring effectiveness of antibiotic therapy. This study aims to evaluate the usefulness of PCT in aclinical routine setting. Materials and methods. Of 358,763 clinical cases from 7 German hospitals in 2012 and 2013, 3854 cases had an ICD-10 code representing sepsis. A total of 1778 cases had pathologic PCT and one episode of infection. Of those, 671 showed aseries of measures that was suitable to assess treatment success using PCT reduction. Propensity score matching was used to create two comparable groups with 211 patients in each group. Results. The group with PCT reduction within 12 days showed ahighly significant better proportion of survival (146/211 vs. 17/211; p < 0.0001). The odds ratio for death according to PCT reduction vs. nonreduction is 25.64 (p < 0.0001; 95% CI: 14.49-45.45). PCT was normalized after an average of 6.2 days. Discussion. The difference in survival implicates that PCT reduction is asuitable surrogate parameter to indicate successful antimicrobial therapy. Successful antibiotic therapy is aproven predictor for survival in sepsis. This study also showed concordant results in the group of patients with sepsis after abdominal surgery. Results from subgroup analyses confirm the initial findings. PCT reduction was used as surrogate for therapy success, as the antimicrobial therapy was not electronically available. Conclusion. PCT reduction is astrong predictor for survival. However, the data show that overall use of PCT to monitor sepsis therapy is not yet routinely established. Hospitals should establish algorithms for sepsis treatment that include PCT for the assessment of adequacy and the monitoring of success of the antimicrobial therapy.", "journal": "MEDIZINISCHE KLINIK-INTENSIVMEDIZIN UND NOTFALLMEDIZIN", "category": "Medicine, General & Internal", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000443312600003", "keywords": "Dense mobile wireless sensor networks; Periodical boundary detection; Traffic reduction", "title": "Efficient periodical boundary detection through boundary crossing record and sensor data overhearing in dense MWSNs", "abstract": "In dense mobile wireless sensor networks (MWSNs), a number of major applications require monitoring of the geographical boundaries of sensor readings among physical phenomena with as low traffic as possible. In this paper, we propose a periodical boundary detection method reducing traffic by exchanging sensor data only among nodes near the boundaries. Our proposed method achieves this by incorporating recording the time when a node has crossed the boundary and overhearing sensor data. We confirmed the effectiveness of our proposed method through simulation experiments. (C) 2018 The Authors. Published by Elsevier B.V.", "journal": "PERVASIVE AND MOBILE COMPUTING", "category": "Computer Science, Information Systems; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000437830200008", "keywords": "evolutionary soft robotics; physical simulation; soft locomotion; evolved soft robots; optimization; material properties", "title": "Data analysis and uncertainty estimation in supercontinuum laser absorption spectroscopy", "abstract": "A set of algorithms is presented that facilitates the evaluation of super continuum laser absorption spectroscopy (SCLAS) measurements with respect to temperature, pressure and species concentration without the need for simultaneous background intensity measurements. For this purpose a non-linear model fitting approach is employed. A detailed discussion of the influences on the instrument function of the spectrometer and a method for the in-situ determination of the instrument function without additional hardware are given. The evaluation procedure is supplemented by a detailed measurement precision assessment by applying an error propagation through the non-linear model fitting approach. While the algorithms are tailored to SCLAS, they can be transferred to other spectroscopic methods, that similarly require an instrument function. The presented methods are validated using gas cell measurements of methane in the near infrared region at pressures up to 8.7 bar.", "journal": "SCIENTIFIC REPORTS", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000431924000003", "keywords": "Congenital myasthenic syndromes; Lambert-Eaton myasthenic syndrome", "title": "How to Spot Congenital Myasthenic Syndromes Resembling the Lambert-Eaton Myasthenic Syndrome? A Brief Review of Clinical, Electrophysiological, and Genetics Features", "abstract": "Congenital myasthenic syndromes (CMS) are heterogeneous genetic diseases in which neuromuscular transmission is compromised. CMS resembling the Lambert-Eaton myasthenic syndrome (CMS-LEMS) are emerging as a rare group of distinct presynaptic CMS that share the same electrophysiological features. They have low compound muscular action potential amplitude that increment after brief exercise (facilitation) or high-frequency repetitive nerve stimulation. Although clinical signs similar to LEMS can be present, the main hallmark is the electrophysiological findings, which are identical to autoimmune LEMS. CMS-LEMS occurs due to deficits in acetylcholine vesicle release caused by dysfunction of different components in its pathway. To date, the genes that have been associated with CMS-LEMS are AGRN, SYT2, MUNC13-1, VAMP1, and LAMA5. Clinicians should keep in mind these newest subtypes of CMS-LEMS to achieve the correct diagnosis and therapy. We believe that CMS-LEMS must be included as an important diagnostic clue to genetic investigation in the diagnostic algorithms to CMS. We briefly review the main features of CMS-LEMS.", "journal": "NEUROMOLECULAR MEDICINE", "category": "Neurosciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000485675100003", "keywords": "Big data; data preprocessing; on-line monitoring", "title": "DESIGN AND IMPLEMENTATION OF INTELLIGENT SEAWATER AUTOMATIC ON-LINE MONITORING SYSTEM BASED ON BIG DATA", "abstract": "Based on large data analysis method and automatic detection technology, this paper designs a test system, which can realize intelligent online monitoring of seawater. Based on the theory of large data, the data preprocessing method of large data is applied by relying on the information transmitted by integrated sensors. Using data cleaning, data integration, data conversion and data reduction technology, a large number of data collected by marine monitoring devices are processed accurately. An automatic seawater monitoring system is designed on a software platform. Finally, combined with the experimental data of a certain sea area, the test results are analyzed, which proves the feasibility and effectiveness of the designed seawater online monitoring system. It has achieved the effect of seawater environmental analysis and early warning.", "journal": "LATIN AMERICAN APPLIED RESEARCH", "category": "Engineering, Chemical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000426181300001", "keywords": "Drosophila adult brain; anatomical atlas; confocal microscopy; brain mapping; average brain template; diffeomorphic image registration; atlas-based image segmentation", "title": "Virtual and Augmented Reality Put a Twist on Medical Education", "abstract": "Imaging the expression patterns of reporter constructs is a powerful tool to dissect the neuronal circuits of perception and behavior in the adult brain of Drosophila, one of the major models for studying brain functions. To date, several Drosophila brain templates and digital atlases have been built to automatically analyze and compare collections of expression pattern images. However, there has been no systematic comparison of performances between alternative atlasing strategies and registration algorithms. Here, we objectively evaluated the performance of different strategies for building adult Drosophila brain templates and atlases. In addition, we used state-of-the-art registration algorithms to generate a new group-wise inter-sex atlas. Our results highlight the benefit of statistical atlases over individual ones and show that the newly proposed inter-sex atlas outperformed existing solutions for automated registration and annotation of expression patterns. Over 3,000 images from the Janelia Farm FlyLight collection were registered using the proposed strategy. These registered expression patterns can be searched and compared with a new version of the BrainBaseWeb system and BrainGazer software. We illustrate the validity of our methodology and brain atlas with registration-based predictions of expression patterns in a subset of clock neurons. The described registration framework should benefit to brain studies in Drosophila and other insect species.", "journal": "JAMA-JOURNAL OF THE AMERICAN MEDICAL ASSOCIATION", "category": "Medicine, General & Internal", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000426428100002", "keywords": "Insulin resistance; Obesity; Triglycerides; Dyslipidemias; Lipids; Glycated hemoglobin", "title": "Triglycerides and glycated hemoglobin for screening insulin resistance in obese patients", "abstract": "Objective: Assessment of insulin resistance (IR) is essential in non-diabetic patients with obesity. Thus study aims to identify the best determinants of IR and to propose an original approach for routine assessment of IR in obesity. Design and patients: All adult with obesity defined by a body mass index >= 30 kg/m(2), evaluated in the Nutrition Department between January 2010 and January 2015 were included in this cross-sectional study. Patients with diabetes were excluded. IR was diagnosed according to the HOMA-IR. Based on a logistic regression, we determined a composite score of IR. We then tested the variables with a principal component analysis and a hierarchical clustering analysis. Results: A total of 498 patients with obesity were included. IR was associated with grade III obesity (OR = 2.6[1.6-4.4], p < 0.001), HbA1c >= 5.7% (OR = 2.6[ 1.7-4.0], p < 0.001), hypertriglyceridemia > 1.7 mmol/l (OR = 3.0[ 2.0-4.5], p < 0.001) and age (OR = 0.98[ 0.96-0.99], p = 0.002). Exploratory visual analysis using factor map and clustering analysis revealed that lipid and carbohydrates metabolism abnormalities were correlated with insulin resistance but not with excessive fat accumulation and low-grade inflammation. Conclusions: Our results highlight the interest of simple blood tests such as HbA1c and triglyceride determination, which associated with BMI, may be widely available tools for screening IR in obese patients.", "journal": "CLINICAL BIOCHEMISTRY", "category": "Medical Laboratory Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000429541801053", "keywords": "interstitial lung disease; molecularly-targeted drug; adverse reaction; time-to-onset", "title": "JUMPING TO CONCLUSIONS AND FACIAL EMOTION RECOGNITION IMPAIRMENT IN FIRST EPISODE PSYCHOSIS ACROSS EUROPE", "abstract": "The aim of this study was to investigate the time-to-onset of drug-induced interstitial lung disease (DILD) following the administration of small molecule molecularly-targeted drugs via the use of the spontaneous adverse reaction reporting system of the Japanese Adverse Drug Event Report database. DILD datasets for afatinib, alectinib, bortezomib, crizotinib, dasatinib, erlotinib, everolimus, gefitinib, imatinib, lapatinib, nilotinib, osimertinib, sorafenib, sunitinib, temsirolimus, and tofacitinib were used to calculate the median onset times of DILD and the Weibull distribution parameters, and to perform the hierarchical cluster analysis. The median onset times of DILD for afatinib, bortezomib, crizotinib, erlotinib, gefitinib, and nilotinib were within one month. The median onset times of DILD for dasatinib, everolimus, lapatinib, osimertinib, and temsirolimus ranged from 1 to 2 months. The median onset times of the DILD for alectinib, imatinib, and tofacitinib ranged from 2 to 3 months. The median onset times of the DILD for sunitinib and sorafenib ranged from 8 to 9 months. Weibull distributions for these drugs when using the cluster analysis showed that there were 4 clusters. Cluster 1 described a subgroup with early to later onset DILD and early failure type profiles or a random failure type profile. Cluster 2 exhibited early failure type profiles or a random failure type profile with early onset DILD. Cluster 3 exhibited a random failure type profile or wear out failure type profiles with later onset DILD. Cluster 4 exhibited an early failure type profile or a random failure type profile with the latest onset DILD.", "journal": "SCHIZOPHRENIA BULLETIN", "category": "Psychiatry", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000418379300009", "keywords": "intersectionality; invisibility; nonprototypicality; stereotyping", "title": "Invisibility of Black women: Drawing attention to individuality", "abstract": "We examine nonprototypicality as an antecedent to invisibility (lack of individuation) of Black women. Study 1 varied numerical representation of Black women within the group women to be low/equal to White women, and Study 2 varied the trait overlap of Black women to be low/high relative to White women and/or Black men. Invisibility was measured by a face recognition task. Rather than invisibility being reduced under conditions of equal numerical representation and high trait overlap, low numerical representation and low trait overlap increased recognition for Black female faces. In Studies 3-4 participants primed to focus on differences showed better recognition for Black women's faces than those primed to focus on similarities. We suggest a difference focus reduces reliance on categorical information, increasing individuation and visibility. But nonprototypicality matters: Study 5 perceivers who saw less overlap between women and Black women on gender stereotypes showed worse recognition of Black women.", "journal": "GROUP PROCESSES & INTERGROUP RELATIONS", "category": "Psychology, Social", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000424835100002", "keywords": "Kalman filtering (KF); nonlinear Bayesian estimation; state estimation; stochastic estimation", "title": "Nonlinear Bayesian Estimation: From Kalman Filtering to a Broader Horizon", "abstract": "This article presents an up-to-date tutorial review of nonlinear Bayesian estimation. State estimation for nonlinear systems has been a challenge encountered in a wide range of engineering fields, attracting decades of research effort. To date, one of the most promising and popular approaches is to view and address the problem from a Bayesian probabilistic perspective, which enables estimation of the unknown state variables by tracking their probabilistic distribution or statistics (e.g., mean and covariance) conditioned on a system's measurement data. This article offers a systematic introduction to the Bayesian state estimation framework and reviews various Kalman filtering (KF) techniques, progressively from the standard KF for linear systems to extended KF, unscented KF and ensemble KF for nonlinear systems. It also overviews other prominent or emerging Bayesian estimation methods including Gaussian filtering, Gaussian-sum filtering, particle filtering and moving horizon estimation and extends the discussion of state estimation to more complicated problems such as simultaneous state and parameter/input estimation.", "journal": "IEEE-CAA JOURNAL OF AUTOMATICA SINICA", "category": "Automation & Control Systems", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000424721000005", "keywords": "Remote sensing data; Satellite images time series; Clustering; Object based image analyses", "title": "Object-oriented satellite image time series analysis using a graph-based representation", "abstract": "Nowadays, remote sensing technologies produce huge amounts of satellite images that can be helpful to monitor geographical areas over time. A satellite image time series (SITS) usually contains spatio-temporal phenomena that are complex and difficult to understand. Conceiving new data mining tools for SITS analysis is challenging since we need to simultaneously manage the spatial and the temporal dimensions at the same time. In this work, we propose a new clustering framework specifically designed for SITS data. Our method firstly detects spatio-temporal entities, then it characterizes their evolutions by mean of a graph-based representation, and finally it produces clusters of spatio-temporal entities sharing similar temporal behaviors. Unlike previous approaches, which mainly work at pixel-level, our framework exploits a purely object-based representation to perform the clustering task. Object-based analysis involves a segmentation step where segments (objects) are extracted from an image and constitute the element of analysis. We experimentally validate our method on two real world SITS datasets by comparing it with standard techniques employed in remote sensing analysis. We also use a qualitative analysis to highlight the interpretability of the results obtained.", "journal": "ECOLOGICAL INFORMATICS", "category": "Ecology", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000426424100002", "keywords": "Check valve; Multiphase pump; Computational fluid dynamics; Probabilistic model; Gaussian process regression", "title": "Integrated probabilistic modeling method for transient opening height prediction of check valves in oil-gas multiphase pumps", "abstract": "To ensure the reliability of oil-gas multiphase pumps, it is necessary to model the relationship between the transient opening height of the check valve and multiphase transportation conditions. However, the choice of a suitable turbulence model is not straightforward for the widely used computational fluid dynamics (CFD) method. Additionally, the CFD models are still not easy to be validated experimentally because the transient opening motion is difficult to be accurately measured for its quick and nonlinear characteristics. In this work, an integrated probabilistic modeling (IPM) method is proposed to predict the transient opening height of the check valve in oil-gas multiphase pumps. First, candidate CFD transient models with different turbulence models are adopted to provide initial training data for several Gaussian process regression (GPR) models. Then, a probabilistic index of the trained GPR models is formulated to assess their reliability. Consequently, without knowing the actual values, a suitable GPR model is chosen from the candidates for online prediction of a new condition. Moreover, instead of the time-consuming CFD design process, a better CFD turbulence model can be selected efficiently. The superiority of IPM is demonstrated using simulation and experiments.", "journal": "ADVANCES IN ENGINEERING SOFTWARE", "category": "Computer Science, Interdisciplinary Applications; Computer Science, Software Engineering; Engineering, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000427482500035", "keywords": "CT; deformable; MRI; multimodality; registration", "title": "Validation of a deformable MRI to CT registration algorithm employing same day planning MRI for surrogate analysis", "abstract": "PurposeValidating deformable multimodality image registrations is challenging due to intrinsic differences in signal characteristics and their spatial intensity distributions. Evaluating multimodality registrations using these spatial intensity distributions is also complicated by the fact that these metrics are often employed in the registration optimization process. This work evaluates rigid and deformable image registrations of the prostate in between diagnostic-MRI and radiation treatment planning-CT by utilizing a planning-MRI after fiducial marker placement as a surrogate. The surrogate allows for the direct quantitative analysis that can be difficult in the multimodality domain. MethodsFor thirteen prostate patients, T2 images were acquired at two different time points, the first several weeks prior to planning (diagnostic-MRI) and the second on the same day as the planning-CT (planning-MRI). The diagnostic-MRI was deformed to the planning-CT utilizing a commercially available algorithm which synthesizes a deformable image registration (DIR) algorithm from local rigid registrations. The planning-MRI provided an independent surrogate for the planning-CT for assessing registration accuracy using image similarity metrics, including Pearson correlation and normalized mutual information (NMI). A local analysis was performed by looking only within the prostate, proximal seminal vesicles, penile bulb, and combined areas. ResultsThe planning-MRI provided an excellent surrogate for the planning-CT with residual error in fiducial alignment between the two datasets being submillimeter, 0.78 mm. DIR was superior to the rigid registration in 11 of 13 cases demonstrating a 27.37% improvement in NMI (P < 0.009) within a regional area surrounding the prostate and associated critical organs. Pearson correlations showed similar results, demonstrating a 13.02% improvement (P < 0.013). ConclusionBy utilizing the planning-MRI as a surrogate for the planning-CT, an independent evaluation of registration accuracy is possible. This population provides an ideal testing ground for MRI to CT DIR by obviating the need for multimodality comparisons which are inherently more challenging.", "journal": "JOURNAL OF APPLIED CLINICAL MEDICAL PHYSICS", "category": "Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000428345200032", "keywords": "Central Europe; self-assessed health status; self-ranked health status; self-rated health status; social trust; the former Soviet Union", "title": "The contextual-level effects of social trust on health in transitional countries: Instrumental variable analysis of 26 countries", "abstract": "We analyse the effect of contextual-level social capital on health status in a sample of 26 transitional countries of Central and South Europe, Mongolia, and the former Soviet Union for 2006-2010 (N=51911). Contextual-level social capital is conceptualized as country-level social trust, while health status is conceptualized as self-rated health. We use ordinary least squares and instrumental variable regressions to address endogeneity and especially to rule out reverse causality. Both instrumental variable and ordinary least squares regressions suggest a strong positive effect of country-level trust on health. This finding is consistent for the whole sample as well as separate regional estimations.", "journal": "INTERNATIONAL JOURNAL OF HEALTH PLANNING AND MANAGEMENT", "category": "Health Policy & Services; Public, Environmental & Occupational Health", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000453302300001", "keywords": "Complex network; diffusion; information dynamics; overlapping community detection", "title": "Overlapping Community Detection Based on Information Dynamics", "abstract": "Identifying overlapping communities is essential for analyzing network structures, exploring the interactions of groups, studying network functions, and obtaining insight into the dynamics of networks. Many algorithms have been proposed for detecting overlapping communities but identifying the intrinsic communities is still a non-trivial problem because of the difficulties with parameter tuning, user bias criteria, and the lack of ground truth information. In this paper, we propose a new model called OCDID (Overlapping Community Detection based on Information Dynamics) to uncover the overlapping communities, which treats the network as a dynamical system that allows an individual to communicate and share information with its neighbors. The information flow in the network is controlled by the underlying topology structure (e.g., the community structure), and the community structure is also reflected by the information dynamics. Overlapping nodes act as bridges between multiple communities and the information from multiple communities flows through these nodes. Thus, the overlapping nodes can be identified by analyzing the information flow among communities. In addition, we use the monotone convergence theorem to confirm the convergence of our model. Experiments based on synthetic and real-world networks demonstrate that in most cases, our proposed approach is superior to other representative algorithms in terms of the quality of overlapping community detection.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000441905600004", "keywords": "Big data; Tourism; Value creation; Customer knowledge management", "title": "Analyzing Big Data through the lens of customer knowledge management Evidence from a set of regional tourism experiences", "abstract": "Purpose - This paper aims to demonstrate how customer knowledge management (CKM) can opportunely support the process of value creation from Big Data. Focusing on tourism as a knowledge-intensive industry, the paper tries to contribute to the debate on management of Big Data by proposing CKM as a meaningful approach for transforming the huge amount of data available on social networks into valuable assets for competitiveness of tourism destinations. Design/methodology/approach - The paper adopts a qualitative research methodology based on multiple exploratory case studies identified in a set of digital local events related to the Apulia destination (southern Italy). Findings - Research findings demonstrate that the three dimensions of CKM (knowledge for, from and about customers) could be adopted as lens for analyzing the huge amount of data created for, from and about tourist experiences and for transforming them into valuable assets supporting the competitiveness of tourism destinations. Research limitations/implications - Limitations are related to the industry and the regional dimension of the sample. Accordingly, more research is necessary to prove the validity of the approach and to assure its larger replicability. Practical implications - Implications for the agenda of organizations and destinations' makers for designing and implementing knowledge-based services and products arise. Originality/value - Elements of originality reside into the adoption of CKM as framework to analyze Big Data in the tourism industry.", "journal": "KYBERNETES", "category": "Computer Science, Cybernetics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000426508900011", "keywords": "diversification rate; energetic constraints; mountains; multiple colonizations; niche diversity; Sino-Himalayas; time-for-speciation effect", "title": "What makes the Sino-Himalayan mountains the major diversity hotspots for pheasants?", "abstract": "AimThe Sino-Himalayas have higher species richness than adjacent regions, making them a global biodiversity hotspot. Various mechanisms, including ecological constraints, energetic constraints, diversification rate (DivRate) variation, time-for-speciation effect and multiple colonizations, have been posited to explain this pattern. We used pheasants (Aves: Phasianidae) as a model group to test these hypotheses and to understand the ecological and evolutionary processes that have generated the extraordinary diversity in these mountains. LocationSino-Himalayas and adjacent regions. TaxonPheasants. MethodsUsing distribution maps predicted by species distribution models (SDMs) and a time-calibrated phylogeny for pheasants, we examined the relationships between species richness and predictors including net primary productivity (NPP), niche diversity (NicheDiv), DivRate, evolutionary time (EvolTime) and colonization frequency using Pearson's correlations and structural equationmodelling (SEM). We reconstructed ancestral ranges at nodes and examined basal/derived species patterns to reveal the mechanisms underlying species richness gradients in the Sino-Himalayas. ResultsWe found that ancestral pheasants originated in Africa in the early Oligocene (similar to 33Ma), and then colonized the Sino-Himalayan mountains and other regions. In the Sino-Himalayas, species richness was strongly related to DivRate, NPP, NicheDiv and colonization frequency, but weakly correlated with EvolTime. The direct effects of NicheDiv and DivRate on richness were stronger than NPP and EvolTime. NPP indirectly influenced species richness via DivRate, but its effect on richness via NicheDiv was relatively weak. Main conclusionsHigher species diversity in the Sino-Himalayas was generated by both ecological and evolutionary mechanisms. An increase in available niches, rapid diversifications and multiple colonizations was found to be key direct processes for the build-up of the diversity hotspots of pheasants in the Sino-Himalayan mountains. Productivity had an important but indirect effect on species richness, which worked through increased DivRate. Our study offers new insights on species accumulation in the Sino-Himalayas and provides a useful model for understanding other biodiversity hotspots.", "journal": "JOURNAL OF BIOGEOGRAPHY", "category": "Ecology; Geography, Physical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000435527600009", "keywords": "Disease ecology; Environment; Epidemiology; Norway rat; Rattus; Zoonotic", "title": "Environmental Factors Associated with the Carriage of Bacterial Pathogens in Norway Rats", "abstract": "Worldwide, Norway rats (Rattus norvegicus) carry a number of zoonotic pathogens. Many studies have identified rat-level risk factors for pathogen carriage. The objective of this study was to examine associations between abundance, microenvironmental and weather features and Clostridium difficile, antimicrobialresistant (AMR) Escherichia coli and methicillin-resistant Staphylococcus aureus (MRSA) carriage in urban rats. We assessed city blocks for rat abundance and 48 microenvironmental variables during a trap-removal study, then constructed 32 time-lagged temperature and precipitation variables and fitted multivariable logistic regression models. The odds of C. difficile positivity were significantly lower when mean maximum temperatures were high (>= 12.89 degrees C) approximately 3 months before rat capture. Alley pavement condition was significantly associated with AMR E. coli. Rats captured when precipitation was low (<49.40 mm) in the 15 days before capture and those from blocks that contained food gardens and institutions had increased odds of testing positive for MRSA. Different factors were associated with each pathogen, which may reflect varying pathogen ecology including exposure and environmental survival. This study adds to the understanding of how the microenvironment and weather impacts the epidemiology and ecology of zoonotic pathogens in urban ecosystems, which may be useful for surveillance and control activities.", "journal": "ECOHEALTH", "category": "Biodiversity Conservation; Ecology; Environmental Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000429682500002", "keywords": "3D object recognition; evoked potentials; local and global shape; stereo disparity", "title": "Stereo Viewing Modulates Three-Dimensional Shape Processing During Object Recognition: A High-Density ERP Study", "abstract": "The role of stereo disparity in the recognition of 3-dimensional (3D) object shape remains an unresolved issue for theoretical models of the human visual system. We examined this issue using high-density (128 channel) recordings of event-related potentials (ERPs). A recognition memory task was used in which observers were trained to recognize a subset of complex, multipart, 3D novel objects under conditions of either (bi-) monocular or stereo viewing. In a subsequent test phase they discriminated previously trained targets from untrained distractor objects that shared either local parts, 3D spatial configuration, or neither dimension, across both previously seen and novel viewpoints. The behavioral data showed a stereo advantage for target recognition at untrained viewpoints. ERPs showed early differential amplitude modulations to shape similarity defined by local part structure and global 3D spatial configuration. This occurred initially during an N1 component around 145-190 ms poststimulus onset, and then subsequently during an N2/P3 component around 260-385 ms poststimulus onset. For mono viewing, amplitude modulation during the N1 was greatest between targets and distracters with different local parts for trained views only. For stereo viewing, amplitude modulation during the N2/P3 was greatest between targets and distracters with different global 3D spatial configurations and generalized across trained and untrained views. The results show that image classification is modulated by stereo information about the local part, and global 3D spatial configuration of object shape. The findings challenge current theoretical models that do not attribute functional significance to stereo input during the computation of 3D object shape. Public Significance Statement The aim of this research is to elucidate how the human visual system processes sensory information about shapes of 3-dimensional (3D) objects so that we can perceive, and recognize, them. We asked whether these processes are sensitive to both monocular and stereo visual input. To answer this question we measured electrophysiological responses generated in the brain while people viewed, and made recognition judgments about, mono or stereo images of 3D objects. The objects could differ from each in terms of their part structure, or overall 3D spatial configuration. The results showed that the visual system processes these sorts of shape properties differently, and that how it does so is influenced differently by mono and stereo visual input. The findings shed new light on the role of stereo information in the visual perception and recognition of 3D object shape.", "journal": "JOURNAL OF EXPERIMENTAL PSYCHOLOGY-HUMAN PERCEPTION AND PERFORMANCE", "category": "Psychology; Psychology, Experimental", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000441483000014", "keywords": "Interactive communication; Smart grid; Big data traffic; Electrical devices; Cloud center", "title": "An interactive smart grid communication approach for big data traffic", "abstract": "As smart grid technologies begin to mature, the kinds of electrical devices have grown increasingly popular. Powerful computing capacity is required to analyze the power data fetched from electronic devices to recognize the device status for smarter applications of tomorrow. To ensure the accuracy and reliability of the analysis results, all the power data has to be uploaded to the cloud data center. This means that the cloud data center must take the extra data transmission load from the electronic devices simultaneously. The development of a lightweight communication approach for big data traffic that can prevent or quickly respond to the occurrence of network congestion presents an interesting challenge with respect to computing power and bandwidth limits. The main purpose of this study is to design an adaptive dynamic eigenvalue transmission approach that can dynamically minimize the uploaded power data through the intersection dynamic eigenvalue decision-making model, to achieve the optimal benefit between the transmission load and analysis accuracy. (C) 2018 Elsevier Ltd. All rights reserved.", "journal": "COMPUTERS & ELECTRICAL ENGINEERING", "category": "Computer Science, Hardware & Architecture; Computer Science, Interdisciplinary Applications; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000415262300009", "keywords": "Aerial plantation; system identification; image processing; dynamic simulation", "title": "Identifying pitching mode of aerial planting projectile by the use of image processing", "abstract": "Aerial planting is a new and affordable method, which is used for purposes of reforestation and restoring pastures. In this study, flight dynamics of an aerial planting projectile has been simulated using data of the wind tunnel test. By creating the projectile graphical model and using the results of dynamic equations for this model, pitching angle has been simulated graphically by recording the pitch attitude of the graphical model and, by applying spectral filtration the rate of pitch angle at any point of the trajectory is calculated. Primary dynamic model parameters including the coefficients of regression equation of motion are estimated by applying the least squares estimator. Comparison of the estimated coefficients from image processing and coefficients given to the simulation program reveals high accuracy of the model resulting from the image processing data used in this method and approves wind tunnel test data by graphical model and calibration of camera measurement. Also, an experimental test setup has been created and the images of projectile falling down in the presence of fan flow have been captured with high-speed digital camera. By decreasing the ambient light intensity, the images and measurement noise of theta angle increases. By applying recursive least square algorithm, sensitivity of coefficients and robustness of this algorithm has been analyzed. The analysis results indicate that the estimation of coefficients using image processing data has great accuracy.", "journal": "PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART G-JOURNAL OF AEROSPACE ENGINEERING", "category": "Engineering, Aerospace; Engineering, Mechanical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000427057700008", "keywords": "Gestational diabetes; Adipose tissue; Ultrasound; Glucose tolerance; Pregnancy", "title": "Maternal Subcutaneous and Visceral Adipose Ultrasound Thickness in Women with Gestational Diabetes Mellitus at 24-28 Weeks' Gestation", "abstract": "Objective: To compare the sonographic measurement of maternal subcutaneous and visceral adipose thickness between pregnant women with gestational diabetes mellitus (GDM) and patients with nondiabetic pregnancies. Methods: Adipose thickness was measured by transabdominal ultrasound in pregnant women attending our antenatal clinics at 24-28 weeks' gestation. All patients underwent a 75-g oral glucose challenge as a diagnostic test for GDM. Results: The study population comprised 56 women with a positive glucose challenge test and 112 nondiabetic pregnancies. Measurements of subcutaneous and visceral adipose tissues were converted into multiples of the median (MoM), adjusted for gestational age. The mean subcutaneous thickness MoM in patients with GDM was significantly higher compared to nondiabetic pregnancies (1.31 vs. 1.07; p = 0.011). Similarly, the mean visceral thickness MoM was higher in women with a positive oral glucose tolerance test compared to controls (1.61 vs. 1.06; p < 0.001). Multivariate logistic regression analysis demonstrated that visceral adipose thickness, but not subcutaneous thickness, was significantly and independently associated with GDM (odds ratio 34.047, 95% confidence interval 9.489-122.166). Conclusions: Sonographic thickness of maternal visceral adipose tissue at 24-28 weeks' gestation was higher in women with GDM compared to nondiabetic pregnancies, independently from other known risk factors associated with GDM. (C) 2017 S. Karger AG, Basel", "journal": "FETAL DIAGNOSIS AND THERAPY", "category": "Obstetrics & Gynecology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000413950400019", "keywords": "Biologically-inspired robots; marine robotics; swarms", "title": "Robust Maneuverability of a Miniature, Low-Cost Underwater Robot Using Multiple Fin Actuation", "abstract": "In this letter, we present the design of a miniature (100 mm) autonomous underwater robot that is low-cost ($ 100), easy to manufacture, and highly maneuverable. A key aspect of the robot design that makes this possible is the use of low-cost magnet-in-coil actuators, which have a small profile and minimal sealing requirements. This allows us to create a robot with multiple flapping fin propulsors that independently control robot motions in surge, heave, and yaw. We present several results on the robot, including 1) quantified open-loop swimming characteristics; 2) autonomous behaviors using a pressure sensor and an inertial measurement unit (IMU) to achieve controlled swimming of prescribed trajectories; 3) feedback from an optic sensor to enable homing to a light source. The robot is designed to form the basis for underwater swarm robotics testbeds, where low cost and ease of manufacture are critical, and three-dimensional (3-D) maneuverability allows testing complex coordination inspired by natural fish schools. Individually, miniature and low-cost underwater robots can also provide a platform for the study of 3-D autonomy and marine vehicle dynamics in educational and resource-constrained settings.", "journal": "IEEE ROBOTICS AND AUTOMATION LETTERS", "category": "Robotics", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000416866900002", "keywords": "Evolutionary algorithm; Synthetic biology; Network design", "title": "Designing synthetic networks in silico: a generalised evolutionary algorithm approach", "abstract": "Background: Evolution has led to the development of biological networks that are shaped by environmental signals. Elucidating, understanding and then reconstructing important network motifs is one of the principal aims of Systems & Synthetic Biology. Consequently, previous research has focused on finding optimal network structures and reaction rates that respond to pulses or produce stable oscillations. In this work we present a generalised in silico evolutionary algorithm that simultaneously finds network structures and reaction rates (genotypes) that can satisfy multiple defined objectives (phenotypes). Results: The key step to our approach is to translate a schema/binary-based description of biological networks into systems of ordinary differential equations (ODEs). The ODEs can then be solved numerically to provide dynamic information about an evolved networks functionality. Initially we benchmark algorithm performance by finding optimal networks that can recapitulate concentration time-series data and perform parameter optimisation on oscillatory dynamics of the Repressilator. We go on to show the utility of our algorithm by finding new designs for robust synthetic oscillators, and by performing multi-objective optimisation to find a set of oscillators and feed-forward loops that are optimal at balancing different system properties. In sum, our results not only confirm and build on previous observations but we also provide new designs of synthetic oscillators for experimental construction. Conclusions: In this work we have presented and tested an evolutionary algorithm that can design a biological network to produce desired output. Given that previous designs of synthetic networks have been limited to subregions of network-and parameter-space, the use of our evolutionary optimisation algorithm will enable Synthetic Biologists to construct new systems with the potential to display a wider range of complex responses.", "journal": "BMC SYSTEMS BIOLOGY", "category": "Mathematical & Computational Biology", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000426747300008", "keywords": "census microdata; health-selective migration; internal migration; limiting long-term illness; multilevel modelling", "title": "How far is a long distance? An assessment of the issue of scale in the relationship between limiting long-term illness and long-distance migration in England and Wales", "abstract": "Research consistently shows that those in poor health are less likely to migrate over long distances, but analyses rarely consider what constitutes a long distance in this context. Additionally, the migration literature often fails to account for place of residence effects on migration behaviour. This paper addresses these issues through analysis of the distance of residential moves by working age adults in the year preceding the 2011 Census. Multilevel logistic regression models predict the odds of having moved long-distance relative to short distance, for different definitions of long distance: 10 km, 20 km and 50 km. We test whether those reporting a limiting long-term illness (LLTI) are less likely to move long distance in all models, controlling for local authority at the time of the 2011 Census. We find no evidence for health selection in long-distance migration in the 10 and 20km models, but uncover a significant effect in the 50km model. By age, the odds of having moved long distance do not vary for middle-working age adults (25-54) by LLTI, whereas those with an LLTI in the pre-retirement age group (55-64) are less likely to move long distance in all models. We uncover clusters of local authorities where those with an LLTI are more likely to have moved long distance in the 10 and 20km models, but in the 50km model, only two of these areas remain significantly positive. We conclude that health selection in distances moved occurs above a cut-off somewhere between 20 and 50km.", "journal": "POPULATION SPACE AND PLACE", "category": "Demography; Geography", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000430808200001", "keywords": "Generalized space-shift keying (GSSK); visible light communication (VLC); physical layer security (PLS); secrecy rate analysis; optimal LED pattern selection", "title": "Secrecy Analysis of Generalized Space-Shift Keying Aided Visible Light Communication", "abstract": "This paper investigates the physical layer security problem of visible light communication (VLC) systems relying on generalized space-shift keying (GSSK) termed as GSSK-VLC. The GSSK-VLC system considered is assumed to be comprised of three nodes: a transmitter equipped with multiple light-emitting diodes, a legitimate receiver, and a passive eavesdropper. Each of them is equipped with a single photo-detector. Specifically, the average mutual information (AMI) of a GSSK-VLC system is derived. We also obtain both a lower bound and an accurate closed-form expression of the approximate AMI, which can be employed for efficiently estimating the achievable secrecy rate of GSSK-VLC systems. Furthermore, the pairwise error probability and bit error rate of GSSK-VLC systems are analyzed, and again some closed-form expressions are obtained. Additionally, in order to enhance the secrecy performance of the GSSK-VLC system, an optimal LED pattern selection algorithm is proposed under the minimax criterion. We show that the proposed LED pattern selection algorithm is capable of enhancing both the AMI between the transmitter and legitimate user and the achievable secrecy rate of the GSSK-VLC system.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000429036700004", "keywords": "economic growth; growth empirics; spatial econometrics; convergence; Lotka-Volterra; C31; O47", "title": "Cross-country convergence in a general Lotka-Volterra model", "abstract": "This paper uses a general Lotka-Volterra model to estimate convergence for 93 countries over the period 1960-2007. It employs an equation with a spatial time lag and common factors. The spatial lag controls for spatial dependence, while the common factors control for strong cross-sectional dependence. As spatial weights matrices, the shares of high-skilled migrants, trade shares and foreign direct investments are used. A simultaneous least squares estimator and a dynamic common correlated effects (DCCE) estimator are employed. The DCCE estimator finds conditional convergence. The paper highlights the importance of controlling for both types of cross-sectional dependence.", "journal": "SPATIAL ECONOMIC ANALYSIS", "category": "Economics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000440102402133", "keywords": "oral anticancer agents; PK/PD relationship; renal-cell carcinoma; sunitinib; therapeutic drug monitoring; toxicity management", "title": "ANTI-OSTEOPOROSIS MEDICATION PRESCRIPTIONS AND INCIDENCE OF SECONDARY FRACTURE AMONGST HIP FRACTURE PATIENTS IN ENGLAND AND WALES: AN AGE STRATIFIED INTERRUPTED TIME SERIES ANALYSIS", "abstract": "Therapeutic drug monitoring (TDM) could be helpful in oral targeted therapies. Data are sparse to evaluate its impact on treatment management. This study aimed to determine a threshold value of plasma drug exposure associated with the occurrence of grade 3-4 toxicity, then the potential impact of TDM on clinical decision. Consecutive outpatients treated with sunitinib were prospectively monitored between days 21 and 28 of the first cycle, then monthly until disease progression. At each consultation, the composite AUC(T,ss) (sunitinib + active metabolite SU12662) was assayed. The decisions taken during each consultation were matched with AUC(T,ss) and compared to the decisional algorithm based on TDM. A total of 105 cancer patients and 288 consultations were matched with the closest AUC(T,ss) measurement. The majority (60%) of the patients had metastatic renal clear-cell carcinoma (mRCC). Fifty-five (52%) patients experienced grade 3-4 toxicity. Multivariate analysis identified composite AUC(T,ss) as a parameter independently associated with grade 3-4 toxicity (P < 0.0001). Using the ROC curve, the threshold value of composite AUC(T,ss) predicting grade >= 3 toxicity was 2150 ng/mL/h (CI 95%, 0.6-0.79%; P < 0.0001). At disease progression in patients with mRCC, AUC(T,ss) tended to be lower than the one assayed during the first cycle (1678 vs. 2004 ng/mL/h, respectively, P = 0.072). TDM could have changed the medical decision for sunitinib dosing in 30% of patients at the first cycle of treatment, and in 46% of the patients over the whole treatment course. TDM is routinely feasible and may both contribute to improve toxicity management and to identify sunitinib underexposure at the time of disease progression.", "journal": "OSTEOPOROSIS INTERNATIONAL", "category": "Endocrinology & Metabolism", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000431039900002", "keywords": "Uncertainty; Intact rock strength; Bayesian analysis; Hoek-Brown criterion", "title": "Bayesian data analysis to quantify the uncertainty of intact rock strength", "abstract": "One of the main difficulties in the geotechnical design process lies in dealing with uncertainty. Uncertainty is associated with natural variation of properties, and the imprecision and unpredictability caused by insufficient information on parameters or models. Probabilistic methods are normally used to quantify uncertainty. However, the frequentist approach commonly used for this purpose has some drawbacks. First, it lacks a formal framework for incorporating knowledge not represented by data. Second, it has limitations in providing a proper measure of the confidence of parameters inferred from data. The Bayesian approach offers a better framework for treating uncertainty in geotechnical design. The advantages of the Bayesian approach for uncertainty quantification are highlighted in this paper with the Bayesian regression analysis of laboratory test data to infer the intact rock strength parameters sigma(ci) and m(i) used in the Hoek-Brown strength criterion. Two case examples are used to illustrate different aspects of the Bayesian methodology and to contrast the approach with a frequentist approach represented by the nonlinear least squares (NLLS) method. The paper discusses the use of a Student's t-distribution versus a normal distribution to handle outliers, the consideration of absolute versus relative residuals, and the comparison of quality of fitting results based on standard errors and Bayes factors. Uncertainty quantification with confidence and prediction intervals of the frequentist approach is compared with that based on scatter plots and bands of fitted envelopes of the Bayesian approach. Finally, the Bayesian method is extended to consider two improvements of the fitting analysis. The first is the case in which the Hoek-Brown parameter, a, is treated as a variable to improve the fitting in the triaxial region. The second is the incorporation of the uncertainty in the estimation of the direct tensile strength from Brazilian test results within the overall evaluation of the intact rock strength. (C) 2018 Institute of Rock and Soil Mechanics, Chinese Academy of Sciences. Production and hosting by Elsevier B.V.", "journal": "JOURNAL OF ROCK MECHANICS AND GEOTECHNICAL ENGINEERING", "category": "Engineering, Geological", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000429888800009", "keywords": "destination image; change; Athens; web content mining; sentiment analysis; recession", "title": "Is Xenios Zeus Still Alive? Destination Image of Athens in the Years of Recession", "abstract": "This study examines the evolution of the destination image of Athens from 2005 to 2015, in order to exploit the impact of the recent economic recession on individual perceptions. It uses advanced web content mining to analyze TripAdvisor messages that were posted in Athens Travel Forum. The findings show that the image of Athens has remained positive, facing a significant, but short-term, shift during the first years of the crisis. The findings also reveal that the destination image of Athens is only partially shared by individuals residing inside and outside Greece, and that non-Greek residents have more favorable perceptions toward the destination. The study expands understanding on the destination image literature by demonstrating the normative nature of destination images, whichonce established can be particularly resistant to change, even during sustained crises.", "journal": "JOURNAL OF TRAVEL RESEARCH", "category": "Hospitality, Leisure, Sport & Tourism", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000428760200031", "keywords": "visual working memory; orientation; precision; maintenance", "title": "A statewide evaluation of seven strategies to reduce opioid overdose in North Carolina", "abstract": "Background In response to increasing opioid overdoses, US prevention efforts have focused on prescriber education and supply, demand and harm reduction strategies. Limited evidence informs which interventions are effective. We evaluated Project Lazarus, a centralised statewide intervention designed to prevent opioid overdose. Methods Observational intervention study of seven strategies. 74 of 100 North Carolina counties implemented the intervention. Dichotomous variables were constructed for each strategy by county-month. Exposure data were: process logs, surveys, addiction treatment interviews, prescription drug monitoring data. Outcomes were: unintentional and undetermined opioid overdose deaths, overdose-related emergency department (ED) visits. Interrupted time-series Poisson regression was used to estimate rates during preintervention (2009-2012) and intervention periods (2013-2014). Adjusted IRR controlled for prescriptions, county health status and time trends. Time-lagged regression models considered delayed impact (0-6 months). Results In adjusted immediate-impact models, provider education was associated with lower overdose mortality (IRR 0.91; 95% CI 0.81 to 1.02) but little change in overdose-related ED visits. Policies to limit ED opioid dispensing were associated with lower mortality (IRR 0.97; 95% CI 0.87 to 1.07), but higher ED visits (IRR 1.06; 95% CI 1.01 to 1.12). Expansions of medication-assisted treatment (MAT) were associated with increased mortality (IRR 1.22; 95% CI 1.08 to 1.37) but lower ED visits in time-lagged models. Conclusions Provider education related to pain management and addiction treatment, and ED policies limiting opioid dispensing showed modest immediate reductions in mortality. MAT expansions showed beneficial effects in reducing ED-related overdose visits in time-lagged models, despite an unexpected adverse association with mortality.", "journal": "INJURY PREVENTION", "category": "Public, Environmental & Occupational Health", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000425133700001", "keywords": "Postnatal care; Reproductive age; Women", "title": "Postnatal care utilization and associated factors among women of reproductive age Group in Halaba Kulito Town, Southern Ethiopia", "abstract": "Background: Despite postnatal care services significant role in improving maternal and new-born health, services are underutilized in most developing countries including Ethiopia. Hence, it is important to identify factors that facilitate or impede postnatal care services utilization. The aim of this study was to assess postnatal care services utilization and associated factors among reproductive age women who gave live birth in 2015 at Halaba kulito town, Southern Ethiopia. Methods: A community-based cross-sectional study was conducted on 401 reproductive age women who gave live birth a year prior to the survey. Data were collected by using structured questionnaire. Bivariate and multivariable logistic regression analysis were carried out to identify factors associated with postnatal care services utilization. A significant association was declared when p-value is less than 0.05. The strength of association was determined by calculating odds ratio at 95% confidence interval. Result: In this study, postnatal care services utilization by reproductive age women was 47.9%. Multivariable analysis revealed that government employed (AOR = 3.01, 95% CI = 1.36, 6.67), have three ANC visits (AOR = 4.29, 95% CI = 1.59, 11.55), have four ANC visits (AOR = 9.55, 95% CI = (3.46, 26.39), gave last birth at Health Centre (AOR = 10.76, 95% CI = 3.26, 35.57), gave last birth at Hospital (AOR = 13.15, 95% CI = (3.64, 47.50), didn't aware of at least one postpartum danger signs (AOR = 0.06, 95% CI = (0.01, 0.37), didn't know child care and had three ANC visits (AOR = 0.14, 95% CI (0.02, 0.8), and didn't know child care and had four or more ANC visits (AOR = 0.13, 95% CI (0.02, 0.79) were significantly associated with postnatal care services utilization. Conclusion: This study assessed PNC services utilization and associated factors among reproductive age women. The study results provided a basic understanding of factors that associated with PNC services utilization by reproductive age women. The findings of this study showed direct association between postnatal care utilization and maternal employment, awareness to postpartum danger signs, frequency of ANC and attending birth at health institution. Therefore, the results suggested context-specific evidence which might be taken into consideration when rethinking policies to increase PNC utilization.", "journal": "ARCHIVES OF PUBLIC HEALTH", "category": "Public, Environmental & Occupational Health", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000453003700001", "keywords": "ultrasound; thyroid nodule; fine-needle aspiration; pain score; local anesthesia", "title": "Force Analysis and Evaluation of a Pelvic Support Walking Robot with Joint Compliance", "abstract": "The force analysis of a pelvic support walking robot with joint compliance is discussed in this paper. During gait training, pelvic motions of hemiplegic patients may be excessively large or out of control; however, restriction of pelvic motions is not likely to facilitate successful rehabilitation. A robot-assisted pelvic balance trainer (RAPBT) is proposed to help patients control the range of motion via force field, and force analysis is necessary for the control of the compliant joints. Thus, kinematic model and static model are developed to derive the Jacobian and the relation between the interaction forces and the pelvic movements, respectively. Since the joint compliance is realized through a nontorsional spring, a conventional (linear) Jacobian method and a piecewise linear method are derived to relate the interaction forces with the pelvis movements. Three preliminary experiments are carried out to evaluate the effectiveness of the proposed methods and the feasibility of the RAPBT. The experiment results indicate that the piecewise linear method is effective in the calculation of the interaction forces. Gait with pelvic brace strongly resembles free overground walking and partly decreases motion range via force field. The findings of this research demonstrate that the pelvic brace with joint compliance may provide effective interventions.", "journal": "JOURNAL OF HEALTHCARE ENGINEERING", "category": "Health Care Sciences & Services", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000417107500008", "keywords": "passive voice; engineering writing; workplace writing; corpus linguistics", "title": "Asymmetric nanofluidic grating detector for differential refractive index measurement and biosensing", "abstract": "Measuring small changes in refractive index can provide both sensitive and contactless information on molecule concentration or process conditions for a wide range of applications. However, refractive index measurements are easily perturbed by non-specific background signals, such as temperature changes or non-specific binding. Here, we present an optofluidic device for measuring refractive index with direct background subtraction within a single measurement. The device is comprised of two interdigitated arrays of nanofluidic channels designed to form an optical grating. Optical path differences between the two sets of channels can be measured directly via an intensity ratio within the diffraction pattern that forms when the grating is illuminated by a collimated laser beam. Our results show that no calibration or biasing is required if the unit cell of the grating is designed with an appropriate built-in asymmetry. In proof-of-concept experiments we attained a noise level equivalent to similar to 10(-5) refractive index units (30 Hz sampling rate, 4 min measurement interval). Furthermore, we show that the accumulation of biomolecules on the surface of the nanochannels can be measured in real-time. Because of its simplicity and robustness, we expect that this inherently differential measurement concept will find many applications in ultra-low volume analytical systems, biosensors, and portable devices.", "journal": "LAB ON A CHIP", "category": "Biochemical Research Methods; Chemistry, Multidisciplinary; Chemistry, Analytical; Nanoscience & Nanotechnology; Instruments & Instrumentation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000418365200042", "keywords": "domestic violence; emergency; femicide; healthcare professionals; healthcare settings; intimate partner homicide; nursing; partner abuse; strangulation", "title": "Validation and adaptation of the danger assessment-5: A brief intimate partner violence risk assessment", "abstract": "Aims: The aim of this study was to assess the predictive validity of the DA-5 with the addition of a strangulation item in evaluating the risk of an intimate partner violence (IPV) victim being nearly killed by an intimate partner. Background: The DA-5 was developed as a short form of the Danger Assessment for use in healthcare settings, including emergency and urgent care settings. Analyzing data from a sample of IPV survivors who had called the police for domestic violence, the DA-5 was tested with and without an item on strangulation, a potentially fatal and medically damaging IPV tactic used commonly by dangerous abusers. Design: Researchers interviewed a heterogeneous sample of 1,081 women recruited by police between 2009-2013 at the scene of a domestic violence call; 619 (57.3%) were contacted and re-interviewed after an average of 7 months. Methods: The predictive validity of the DA-5 was assessed for the outcome of severe or near lethal IPV re-assault using sensitivity, specificity and ROC curve analysis techniques. Results: The original DA-5 was found to be accurate (AUC =.68), equally accurate with the strangulation item from the original DA substituted (AUC =.68) and slightly more accurate (but not a statistically significant difference) if multiple strangulation is assessed. Conclusion: We recommend that the DA-5 with the strangulation item be used for a quick assessment of homicide or near homicide risk among IPV survivors. A protocol for immediate referral and examination for further injury from strangulation should be adopted for IPV survivors at high risk.", "journal": "JOURNAL OF ADVANCED NURSING", "category": "Nursing", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000427562400011", "keywords": "Chronic kidney disease; Composite likelihood; Empirical process; Self-consistency; U-process", "title": "A pairwise likelihood augmented Cox estimator for left-truncated data", "abstract": "Survival data collected from a prevalent cohort are subject to left truncation and the analysis is challenging. Conditional approaches for left-truncated data could be inefficient as they ignore the information in the marginal likelihood of the truncation times. Length-biased sampling methods may improve the estimation efficiency but only when the underlying truncation time is uniform; otherwise, they may generate biased estimates. We propose a semiparametric method for left-truncated data under the Cox model with no parametric distributional assumption about the truncation times. Our approach is to make inference based on the conditional likelihood augmented with a pairwise likelihood, which eliminates the truncation distribution, yet retains the information about the regression coefficients and the baseline hazard function in the marginal likelihood. An iterative algorithm is provided to solve for the regression coefficients and the baseline hazard function simultaneously. By empirical process and U-process theories, it has been shown that the proposed estimator is consistent and asymptotically normal with a closed-form consistent variance estimator. Simulation studies show substantial efficiency gain of our estimator in both the regression coefficients and the cumulative baseline hazard function over the conditional approach estimator. When the uniform truncation assumption holds, our estimator enjoys smaller biases and efficiency comparable to that of the full maximum likelihood estimator. An application to the analysis of a chronic kidney disease cohort study illustrates the utility of the method.", "journal": "BIOMETRICS", "category": "Biology; Mathematical & Computational Biology; Statistics & Probability", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000447258800018", "keywords": "Salt intake; Impaired kidney function; Chronic kidney disease; Estimated glomerular filtration rate", "title": "Dietary Salt Intake is a Significant Determinant of Impaired Kidney Function in the General Population", "abstract": "Background/Aims: Kidney dysfunction is an important risk factor for cardiovascular disease and end-stage renal disease. This study investigated whether dietary salt intake predicts deterioration of kidney function in the general population. Methods: In all, 12 126 subjects with a normal estimated glomerular filtration rate (eGFR mL >= 60/min per 1.73m(2)) attending an annual check-up were enrolled in the study and were followed-up for a median of 1754 days; the endpoint was the development of impaired kidney function (eGFR <60 mL/min per 1.73m(2)). Individual salt intake was estimated using spot urine analysis. Results: At baseline, mean (+/- SD) salt intake and eGFR were 10.6 +/- 3.4 g/day and 80.8 +/- 12.9 mL/min per 1.73m(2), respectively. During the follow-up period, 1384 subjects (25.2 per 1000 person-years) developed impaired kidney function. Multivariate Cox hazard regression analysis revealed salt intake as a significant predictor of the new onset of kidney impairment (hazard ratio 1.045; 95% confidence interval 1.025-1.065). Subjects were divided into two groups based on salt intake; the incidence of impaired kidney function was higher in the group with high than low salt intake (P < 0.001, log-rank test). Multivariate Cox hazard regression analysis indicated a 29% increased risk of developing impaired kidney function in the high-salt group. Multivariate linear regression analysis showed a significant correlation between salt intake and yearly decline in eGFR (13 = 0.060, P < 0.001). Conclusion: Salt intake is associated with the development of impaired kidney function in the general population, independent of its effects on blood pressure. Salt restriction may help prevent the development of impaired kidney function. (C) 2018 The Author(s) Published by S. Karger AG, Basel", "journal": "KIDNEY & BLOOD PRESSURE RESEARCH", "category": "Physiology; Urology & Nephrology; Peripheral Vascular Disease", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000427582100006", "keywords": "concrete structures; maintenance & inspection; railway tracks", "title": "Damage detection of ballastless railway tracks by the impact-echo method", "abstract": "The objectives of this study were to investigate the impact response of ballastless railway tracks and to study the feasibility of using the impact-echo method for the detection of typical damage to ballastless tracks. Numerical studies were carried out to acquire the transient responses of ballastless tracks subjected to impact. The numerical results were verified by experimental studies on ballastless tracks with and without damage. It was found that there is a predominant frequency whose value depends on the dimensions of the ballastless track for a given P-wave speed in concrete. The presence of damage was found to disrupt the modes of vibration. A shift of the predominant frequency to a lower value was found to be a key indication of the presence of damage. Multiple wave reflections between the impact surface and the damage surface produced a large-amplitude peak in the spectrum at a frequency corresponding to the depth of the damage.", "journal": "PROCEEDINGS OF THE INSTITUTION OF CIVIL ENGINEERS-TRANSPORT", "category": "Engineering, Civil; Transportation Science & Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000434868800269", "keywords": "e-cigarette; vaping; knowledge; beliefs", "title": "The Role of Knowledge and Risk Beliefs in Adolescent E-Cigarette Use: A Pilot Study", "abstract": "The use of e-cigarettes and other vaping devices among adolescents is an urgent public health problem due to the concern about adolescent exposure to nicotine. This study examined: (1) adolescents' knowledge and beliefs about e-cigarette risks; and (2) whether knowledge and risk beliefs were associated with e-cigarette use. N = 69 adolescents completed a cross-sectional survey about e-cigarette knowledge, attitudes (i.e., risk beliefs), and behavior (KAB). Nearly half (47%) of the sample reported ever using e-cigarettes. The majority of adolescents knew about many of the risks of e-cigarettes, with no differences between never- and ever-users. However, risk beliefs, such as worrying about health risks of using e-cigarettes, varied across groups. Compared to never-users, e-cigarette ever-users were significantly less likely to worry about e-cigarette health risks, less likely to think that e-cigarettes would cause them negative health consequences, and less likely to believe that e-cigarette use would lead to addiction. In a multivariable logistic regression, prior combustible cigarette use, mother's education, and addiction risk beliefs about e-cigarettes emerged as significant predictors of adolescents' e-cigarette use. This study reveals that while knowledge is not associated with adolescent e-cigarette use, risk beliefs do predict use.", "journal": "INTERNATIONAL JOURNAL OF ENVIRONMENTAL RESEARCH AND PUBLIC HEALTH", "category": "Environmental Sciences; Public, Environmental & Occupational Health", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000423772600002", "keywords": "Directed ortho-metalation; Anionic ortho-Fries rearrangement; O-Carbamates; Substituted aromatics; Benzofurans", "title": "Directed ortho-Metalation of O-Aryl N,N-Dialkylcarbamates: Methodology, Anionic ortho-Fries Rearrangement, and Lateral Metalation", "abstract": "The directed ortho-lithiation reactions of O-aryl N,N-dialkylcarbamates as well as O-1-naphthyl and O-2-naphthyl N,N-dialkylcarbamates with sec-butyllithium/tetramethylethylenediamine (sBuLi/TMEDA) followed by quenching with various electrophiles afford a range of polysubstituted aromatic compounds. If the solutions of the ortho-lithiated carbamates are warmed to room temperature without the addition of external electrophiles, salicylamide and 1- and 2-hydroxynaphthamide derivatives are formed through anionic ortho-Fries rearrangements. The relative stabilities and reactivities of different O-aryl N,N-dialkylcarbamates were investigated. The lateral metalation of 2-tolyl carbamates with lithium diisopropylamide (LDA) provides a route to benzo[b]furan-2(3H)-ones. Previously reported results are used in a comparison of seven O-based directed metalation groups in reactions with several electrophiles. The described methodology is useful for the preparation of 1,2,3-substituted aromatic compounds.", "journal": "EUROPEAN JOURNAL OF ORGANIC CHEMISTRY", "category": "Chemistry, Organic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000427257100002", "keywords": "Appalachian region; risk factors; smoking; social-contextual model; women; USA", "title": "A social-contextual investigation of smoking among rural women: multi-level factors associated with smoking status and considerations for cessation", "abstract": "Introduction: The social-contextual model of tobacco control and the potential mechanisms of the maintenance or cessation of smoking behavior among disadvantaged women, including rural residents, have yet to be comprehensively studied. The purpose of this study was to determine the association between selected individual, interpersonal, workplace, and neighborhood characteristics and smoking status among women in Appalachia, a US region whose residents experience a disproportionate prevalence of tobacco-related health disparities. These findings may assist in efforts to design and test scientifically valid tobacco control interventions for this and other disadvantaged populations. Methods: Women, 18 years of age and older, residing in three rural Ohio Appalachian counties, were recruited using a two-phase address-based sampling methodology for a cross-sectional interview-administered survey between August 2012 and October 2013 (N=408). Multinomial logistic regression was employed to determine associations between select multilevel factors (independent variables) and smoking status (dependent variable). The sample included 82 (20.1%) current smokers, 92 (22.5%) former smokers, and 234 (57.4%) women reporting never smoking (mean age 51.7 years). Results: In the final multivariable multinomial logistic regression model, controlling for all other significant associations, constructs at multiple social-contextual levels were associated with current versus either former or never smoking. At the individual level, for every additional year in age, the odds of being a former or never smoker increased by 7% and 6% (odds ratio (OR) (95% confidence interval(CI)): 1.07 (1.0-1.11) and 1.06 (1.02-1.09)), respectively, as compared to the odds of being a current smoker. With regard to depression, for each one unit increase in the Center for Epidemiologic Studies Depression Scale score, the odds of being a former or never smoker were 5% and 7% lower (OR(95%CI): 0.95(0.91-0.999) and 0.93(0.88-0.98)), respectively. Five interpersonal factors were associated with smoking status. As the social influence injunctive norm score increased by one unit, indicating perception of smoking to be more acceptable, the odds of being a former or never smoker decreased by 23% and 30%, respectively. For every one unit increase in the social participation score, indicating past-year engagement in one additional activity type, the odds of being a former or never smoker increased by 17% and 36%, respectively. For every 10% increase in the percentage of social ties in the participant's advice network who smoked, the odds of being a former or never smoker were 24% and 28% less, respectively. For every 0.1 unit increase in the E/I index, indicating increasing homophily on smoking in one's social network, the odds of being a former or never smoker were 20% and 24% less, respectively, in the time network, and 18% and 20% less, respectively, in the advice network. At the neighborhood level, for every one unit increase in neighborhood cohesion score, indicating increasing cohesion, the odds of being a former smoker or never smoker were 12% and 14% less, respectively. Conclusions: These findings indicate that a social-contextual approach to tobacco control may be useful for narrowing a widening trajectory of smoking disparity for rural women. Interpersonal context, in particular, must be considered in the development of culturally targeted cessation interventions for Ohio Appalachian women.", "journal": "RURAL AND REMOTE HEALTH", "category": "Public, Environmental & Occupational Health", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000419359500003", "keywords": "structural health monitoring; damage detection; acoustoelasticity; Lamb waves; environment and operational conditions; stress mitigation", "title": "Implication of changing loading conditions on structural health monitoring utilising guided waves", "abstract": "Structural health monitoring systems based on guided waves typically utilise a network of embedded or permanently attached sensors, allowing for the continuous detection of damage remote from a sensor location. The presence of damage is often diagnosed by analysing the residual signals from the structure after subtracting damage-free reference data. However, variations in environmental and operational conditions such as temperature, humidity, applied or thermally-induced stresses affect the measured residuals. A previously developed acoustoelastic formulation is here extended and employed as the basis for a simplified analytical model to estimate the effect of applied or thermally-induced stresses on the propagation characteristics of the fundamental Lamb wave modes. It is noted that there are special combinations of frequency, biaxial stress ratio and direction of wave propagation for which there is no change in the phase velocity of the fundamental anti-symmetric mode. The implication of these results in devising effective strategies to mitigate the effect of stress induced variations in guided-wave damage diagnostics is briefly discussed.", "journal": "SMART MATERIALS AND STRUCTURES", "category": "Instruments & Instrumentation; Materials Science, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000423735900002", "keywords": "High-risk adults; Pneumococcal vaccine; Influenza vaccine; Tdap vaccine; Primary care; Adult vaccination", "title": "Cost-effectiveness of increasing vaccination in high-risk adults aged 18-64 Years: a model-based decision analysis", "abstract": "Background: Adults aged 18-64 years with comorbid conditions are at high risk for complications of certain vaccine-preventable diseases, including influenza and pneumococcal disease. The 4 Pillars T Practice Transformation Program (4 Pillars Program) increases uptake of pneumococcal polysaccharide vaccine, influenza vaccine and tetanus-diphtheria-acellular pertussis vaccine by 5-10% among adults with high-risk medical conditions, but its cost-effectiveness is unknown. Methods: A decision tree model estimated the cost-effectiveness of implementing the 4 Pillars Program in primary care practices compared to no program for a population of adults 18-64 years of age at high risk of illness complications over a 10 year time horizon. Vaccination rates and intervention costs were derived from a randomized controlled cluster trial in diverse practices in 2 U. S. cities. One-way and probabilistic sensitivity analyses were conducted. Results: From a third-party payer perspective, which considers direct medical costs, the 4 Pillars Program cost $ 28,301 per quality-adjusted life year gained; from a societal perspective, which adds direct nonmedical and indirect costs, the program was cost saving and more effective than no intervention. Cost effectiveness results favoring the program were robust in sensitivity analyses. From a public health standpoint, the model predicted that the intervention reduced influenza cases by 1.4%, with smaller decreases in pertussis and pneumococcal disease cases. Conclusion: The 4 Pillars Practice Transformation Program is an economically reasonable, and perhaps cost saving, strategy for protecting the health of adults aged < 65 years with high-risk medical conditions", "journal": "BMC INFECTIOUS DISEASES", "category": "Infectious Diseases", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000417111500006", "keywords": "Asymptotic normality; GARCH model; Laplace (1,1); Quasi-maximum likelihood estimator; Strong consistency", "title": "Asymmetric Relationship between Investors' Sentiment and Stock Returns: Evidence from a Quantile Non-causality Test", "abstract": "This study investigates the causal relationship between investor sentiment and stock returns in the USA by conducting a quantile Granger non-causality test. Employing two proxies for investor sentiment - the sentiment index developed by Baker and Wurgler in 2007 and the University of Michigan Consumer Survey, a consumer confidence index - we find that the causal relationship between investor sentiment and stock returns strengthens when a tail quantile interval is considered. This finding implies that the investor sentiment could provide the incremental predictability for the stock returns under the extreme market situation, which cannot be found using a traditional Granger causality test. Interestingly, the findings can be explained by investors' loss aversion and herding behavior.", "journal": "INTERNATIONAL REVIEW OF FINANCE", "category": "Business, Finance", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000431342900003", "keywords": "Migrants; Social organizations; HIV/AIDS; Political party; Alumni association", "title": "The impact of social organizations on HIV/AIDS prevention knowledge among migrants in Hefei, China", "abstract": "Background: There is a growing recognition of the need to provide HIV/AIDS prevention and care to migrant workers. Social involvement, a type of social capital, is considered a 'critical enabler' of effective HIV/AIDS prevention. Designated participation in formal community groups by the government (e.g., political parties) and informal, voluntary local networks by NGOs (e.g., alumni association, cultural & sports club) play different roles in HIV prevention. The objective of this study is to assess the impact of different types of social organizations on HIV/AIDS prevention knowledge among migrant workers. Methods: A cross-sectional study of 758 migrants was conducted in Hefei, Anhui Province, China. Data were collected through a self-reported questionnaire. Logistic regression was used to assess associations between different social organizations and HIV/AIDS prevention. Results: Migrants who participated in social organizations had a higher awareness of HIV/AIDS knowledge than migrants who do not participate in social organizations. Higher levels of HIV/AIDS knowledge is associated with positive HIV/AIDS behaviors for people who attended political parties (odds ratio [OR] = 3.49, 95% CI: 1.22-9.99). This effect is not significant for alumni association. For both political parties and alumni association members (OR = 0.19, 95% CI: 0.06-0.66, OR = 0.20, 95% CI: 0.08-0.61, respectively), people who exhibited higher levels of HIV/AIDS knowledge had more negative attitudes than those with less knowledge. Conclusion: Social organizations play an important role in improving HIV/AIDS knowledge and behavior in migrants, providing a great opportunity for HIV/AIDS prevention.", "journal": "GLOBALIZATION AND HEALTH", "category": "Public, Environmental & Occupational Health", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000422811500007", "keywords": "Large yellow croaker; Innate antiviral immunity; Interferon-stimulated gene; poly (I:C); RT-qPCR", "title": "Molecular characterization and expression analyses of the Viperin gene in Larimichthys crocea (Family: Sciaenidae)", "abstract": "In this study, we sequenced and characterized an interferon-stimulated gene Viperin homologue, LcViperin, from large yellow croaker (Larimichthys crocea). The LcViperin encodes 354 amino acids and contains an N-terminal amphipathic alpha-helix domain, a radical S-adenosyl-L-methionine (SAM) domain and a highly conserved C-terminal domain. The analyses of LcViperin promoter region revealed nine kinds of putative transcriptional factor binding sites, including five putative ICSBP (IRF-8) binding sites and one putative IRF-1 binding site, indicating that the expression of LcViperin might be induced by the type I IFN response. Phylogenetic analyses based on amino acid sequences showed that the Viperin of large yellow croaker is clustered together with its counterparts from other teleost fishes. The Real-time PCR analyses showed that the LcViperin was found to be ubiquitously expressed in ten examined tissues in large yellow croaker, with predominant expression in peripheral blood, followed by heart and gill. Expression analyses showed that the LcViperin was rapidly and significantly upregulated in vivo after poly (I:C) challenge in peripheral blood, head kidney, spleen and liver tissues. The results indicate that the LcViperin might play a pivotal role in antiviral immune responses. (C) 2017 Elsevier Ltd. All rights reserved.", "journal": "DEVELOPMENTAL AND COMPARATIVE IMMUNOLOGY", "category": "Fisheries; Immunology; Veterinary Sciences; Zoology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000418314300009", "keywords": "Perishable products; Order selection; Vehicle routing problem; Time-dependent; Ant colony algorithm; Large neighborhood search", "title": "A combined order selection and time-dependent vehicle routing problem with time widows for perishable product delivery", "abstract": "This paper addresses a real-life delivery problem often encountered by urban perishable product deliverers, in which the providers suffer losses from failed delivery, such as product deterioration or violating customers' time windows, especially when delivery orders accepted are beyond the providers' delivery capacity. To the best of our knowledge, traditional delivery models are not applicable in such cases and few papers are related. Hence, we develop a new model that combines order selection and time-dependent vehicle routing problem with time windows in the same framework of perishable product delivery, to decide the delivery order, the service sequence and the timing to start a delivery task with the objective of profit maximization. Furthermore, a hybrid ant colony algorithm comprising local search operators is proposed. The effectiveness of our model and algorithm is demonstrated with several computational experiments, and some management insights are provided to guild practical operations.", "journal": "COMPUTERS & INDUSTRIAL ENGINEERING", "category": "Computer Science, Interdisciplinary Applications; Engineering, Industrial", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000428991400018", "keywords": "HIV-1; epidemiology; Yunnan; recombinant", "title": "Comments on a new classification, treatment algorithm and prognosis-estimating system for sigmoid volvulus: the role of percutaneous endoscopic colostomy", "abstract": "Currently, complex HIV-1 recombinations among the B, C, and CRF01_AE genotypes have frequently arisen in Yunnan, China. A novel HIV-1 complex circulating recombinant form (CRF) consisting of B, C, and CRF01_AE (CRF96_cpx) was recently characterized from three epidemiologically unlinked individuals. Two strains of them were isolated from the injecting drug users in this study, the remaining one strain (JL. RL01) was obtained from a previous report in Jilin province. Phylogenetic analysis based on near full-length genome revealed that CRF96_cpx formed a distinct monophyletic cluster supported by a high bootstrap value of 100%, distantly related to all known HIV-1 subtypes/CRFs. CRF96_cpx had a CRF01_AE backbone with three subtype B and C segments inserted, respectively, in the gag and pol region. Furthermore, subregion tree analysis showed that CRF01_AE backbone and subtype B segment inserted originated from a Thai-CRF01_AE lineage, whereas subtype C fragment inserted was from an India C lineage. They are different from previously documented CRF01_AE/B/C forms in its distinct backbone, inserted fragment size, and breakpoints. This highlighted the importance of continual monitoring of genetic diversity and complexity of HIV-1 strains in Yunnan, China.", "journal": "COLORECTAL DISEASE", "category": "Gastroenterology & Hepatology; Surgery", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000417257900008", "keywords": "Communication; Bobbing displays; Galapagos Islands; Sexual selection; Tropiduridae; Visual signals", "title": "Responses of Galapagos Lava Lizards (Microlophus bivittatus) to Manipulation of Female Nuptial Coloration on Lizard Robots", "abstract": "Females of some lizard species exhibit conspicuous coloration during the breeding cycle (\"nuptial coloration'') that elicits male courtship. We conducted two field experiments with San Cristobal Lava Lizards (Microlophus bivittatus) in the Gala ' pagos Islands to determine how the presence and extent of nuptial coloration on lizard robots affected responses of adult males and females. Robots programmed to perform conspecific bobbing displays had a morphological appearance that mimicked (1) a conspecific female without nuptial coloration (non-red control stimulus); (2) a conspecific female with nuptial coloration (normal red stimulus); or (3) a female with an extent of nuptial coloration beyond the range of conspecific variation (super-normal red stimulus). In Experiment 1, subjects witnessed two stimuli in sequence, being presented first with the side of a robot that exhibited conspecific nuptial coloration or with the opposite side of the same robot that lacked nuptial coloration. Results showed no effect of subject sex or stimulus order, but subjects exhibited more display and shorter display latencies in response to the normal red stimulus than to the non-red stimulus. In Experiment 2, new subjects were shown either non-red, normal red, or super-normal red stimulus. In contrast to Experiment 1, results of this experiment revealed sex differences in the amount of display elicited from subjects. Among the findings, males exhibited less display to the super-normal stimulus than to the non-red and normal red stimuli whereas the quantity of display elicited from females by the super-normal stimulus was similar to that evoked by the other two stimuli. We discuss our results in the context of prior studies and offer suggestions for future research.", "journal": "HERPETOLOGICA", "category": "Zoology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000424068600007", "keywords": "Cytomegalovirus; Herpes simplex virus-1; Helicobacter pylori; Mood disorders; Depression; Dysthymia; Bipolar disorder; Gender", "title": "Differences in the association between persistent pathogens and mood disorders among young- to middle-aged women and men in the US", "abstract": "Background: A growing literature supports the role of immune system alterations in the etiology of mood regulation, yet there is little population-based evidence regarding the association between persistent pathogens, inflammation and mood disorders among younger women and men in the U.S. Methods: We used data from the National Health and Nutrition Examination Survey III on individuals 15-39 years of age assessed for major depression, dysthymia, and/or bipolar disorder I and tested for cytomegalovirus (N = 6825), herpes simplex virus (HSV)-1 (N = 5618) and/or Helicobacter pylori (H. pylori) (N = 3167) seropositivity as well as C-reactive protein (CRP) level (N = 6788). CMV immunoglobulin G (IgG) antibody level was also available for a subset of women (N = 3358). We utilized logistic regression to estimate the odds ratio (OR) and 95% confidence interval (CI) for the association between pathogens, CRP levels and each mood disorder overall and among women and men, separately. Results: H. pylori seropositivity was associated with increased odds of dysthymia (OR 2.37, 95% confidence interval (CI): 1.07, 5.24) among women, but decreased odds among men (OR 0.51, 95% CI: 0.28, 0.92). CMV seropositivity was also associated with lower odds of depression (OR 0.54, 95% CI: 032, 0.91) among men, while elevated CMV IgG level was marginally associated with increased odds of mood disorders among women. Associations were not mediated by CRP level. Conclusions: Our findings suggest that persistent pathogens such as CMV and H. pylori may differentially influence mood disorders among women and men, warranting further investigation into biological and/or sociocultural explanations for the contrasting associations observed. (C) 2017 Elsevier Inc. All rights reserved.", "journal": "BRAIN BEHAVIOR AND IMMUNITY", "category": "Immunology; Neurosciences; Psychiatry", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000418836200004", "keywords": "Infant formula; Fingerprint; Establishment; Principal component analysis (PCA); RP-HPLC", "title": "Establishment and Application of Infant Formula Fingerprints by RP-HPLC", "abstract": "An efficient method for rapid and simple identification of infant formula quality was investigated so as to prevent brand counterfeit of infant formulas. Reversed-phase, high-performance liquid chromatography (RP-HPLC) method combined with chemometric method was developed for fingerprints analysis. To optimize the RP-HPLC conditions, the RP-HPLC analysis was performed on a C-8 column by using gradient elution with 0.1% (v/v) aqueous trifluoroacetic acid (TFA) in water as mobile phase A and 0.1% (v/v) TFA in acetonitrile as mobile phase B, the flow rate was 0.5 mL min(-1), and the detection wavelength was set at 214 nm. Three different stages of infant formulas of two series (R-1 and R-2) of R brand were analyzed to establish infant formula fingerprints of each series at different stages under optimal conditions. Five major common peaks in each chromatogram of the infant formulas were used in fingerprint analysis. The results showed that the RP-HPLC fingerprints and principal component analysis (PCA) method can effectively differentiate the infant formula of different brands, as well as the different series of infant formulas of the same brand, which provide an effective testing method for authenticity identification and brand protection in infant formulas.", "journal": "FOOD ANALYTICAL METHODS", "category": "Food Science & Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000431743500066", "keywords": "Dry-cow therapy; somatic cell count; cloxacillin; ampicillin; long-acting; short-acting", "title": "Role of organisational factors on the 'weekend effect' in critically ill patients in Brazil: a retrospective cohort analysis", "abstract": "Introduction Higher mortality for patients admitted to intensive care units (ICUs) during the weekends has been occasionally reported with conflicting results that could be related to organisational factors. We investigated the effects of ICU organisational and staffing patterns on the potential association between weekend admission and outcomes in critically ill patients. Methods We included 59 614 patients admitted to 78 ICUs participating during 2013. We defined 'weekend admission' as any ICU admission from Friday 19:00 until Monday 07:00. We assessed the association between weekend admission with hospital mortality using a mixed logistic regression model controlling for both patient level (illness severity, age, comorbidities, performance status and admission type) and ICU-level (decrease in nurse/bed ratio on weekend, full-time intensivist coverage, use of checklists on weekends and number of institutional protocols) confounders. We performed secondary analyses in the subgroup of scheduled surgical admissions. Results A total of 41 894 patients (70.3%) were admitted on weekdays and 17 720 patients (29.7%) on weekends. In univariable analysis, weekend admitted patients had higher ICU (10.9% vs 9.0%, P<0.001) and hospital (16.5% vs 13.5%, P<0.001) mortality. After adjusting for confounders, weekend admission was not associated with higher hospital mortality (OR 1.05, 950/0CI 0.99 to 1.12, P=0.095). However, a 'weekend effect' was still observed in scheduled surgical admissions, as well as in ICUs riot using checklists during the weekends. For unscheduled admissions, no 'weekend effect' was observed regardless of ICU's characteristics. For scheduled surgical admissions, a 'weekend effect' was present only in ICUs with a low number of implemented protocols and those with a reduction in the nurse/bed ratio and not applying checklists during weekends. Conclusions ICU organisational factors, such as decreased nurse-to-patient ratio, absence of checklists and fewer standardised protocols, may explain, in part, increases in mortality in patients admitted to the ICU mortality on weekends.", "journal": "BMJ OPEN", "category": "Medicine, General & Internal", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000468272500033", "keywords": "motivic homotopy theory; E-infinity-ring spectra; obstruction theory; finite motivic cell complex", "title": "THE MOTIVIC COFIBER OF tau", "abstract": "Consider the Tate twist tau is an element of H-0,H- 1(S-0,S- 0) in the mod 2 cohomology of the motivic sphere. After 2-completion, the motivic Adams spectral sequence realizes this element as a map tau: S- (0, -1)-> S-0,S- 0, with cofiber C tau. We show that this motivic 2-cell complex can be endowed with a unique E-infinity ring structure. Moreover, this promotes the known isomorphism pi(*),C- *tau congruent to Ext(BP*BP)*(,)*(BP*, BP*) to an isomorphism of rings which also preserves higher products. We then consider the closed symmetric monoidal category of C tau-modules (C tau MoD, - Lambda(C tau) -) which lives in the kernel of Betti realization. Given a motivic spectrum X, the C tau-induced spectrum X Lambda C tau is usually better behaved and easier to understand than X itself. We specifically illustrate this concept in the examples of the mod 2 Eilenberg-Maclane spectrum HF2, the mod 2 Moore spectrum S-0,S-0/2 and the connective hermitian K-theory spectrum kq.", "journal": "DOCUMENTA MATHEMATICA", "category": "Mathematics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000414162600014", "keywords": "Body mass index; Lung transplantation; Physical activity; Physiotherapy; Sarcopenia", "title": "Physical activity level significantly affects the survival of patients with end-stage lung disease on a waiting list for lung transplantation", "abstract": "Purpose Our objective was to investigate the factors predicting the survival of patients on the waiting list for lung transplantation (LT) during the waiting period, with a special emphasis on the physical activity level. Methods The study included 70 patients with end-stage pulmonary disease who were on the waiting list for LT at Kyoto University Hospital. We examined the association between the baseline characteristics, including the body mass index and body composition, serum albumin, serum C-reactive protein (CRP), steroid administration, physical activity level (calculated by the food frequency questionnaire) and survival during the waiting period using Kaplan-Meier curves and Cox proportional hazard regression models. Results A physical activity level of <= 1.2 was correlated with significantly decreased survival (1-year survival: 68 vs. 90.9%, p = 0.0089), with a hazard ratio (HR) of 2.24 (95% confidence interval (CI) 1.22-4.19, p = 0.0001). Hypo-albumin (HR 2.024, 95% CI 1.339-6.009, p = 0.004), a high level of CRP (HR 2.551, CI 1.229-4.892, p = 0.02), and the administration of steroids (HR 2.258, CI 1.907-5.032, p = 0.024) were also significant predictors of survival. Conclusions Low levels of physical activity during the waiting period for LT led to decreased survival times among LT candidates.", "journal": "SURGERY TODAY", "category": "Surgery", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000424180100031", "keywords": "Diffuse radiation; Light use efficiency; Eddy covariance; Net ecosystem exchange; Deciduous forest; Canopy photosynthesis", "title": "Model-based analysis of the impact of diffuse radiation on CO2 exchange in a temperate deciduous forest", "abstract": "Clouds and aerosols increase the fraction of global solar irradiance that is diffuse light. This phenomenon is known to increase the photosynthetic light use efficiency (LUE) of closed-canopy vegetation by redistributing photosynthetic photon flux density (400-700 nm) from saturated, sunlit leaves at the top of the canopy, to shaded leaves deeper in the canopy. We combined a process-based carbon cycle model with 10 years of eddy covariance carbon flux measurements and other ancillary data sets to assess 1) how this LUE enhancement influences interannual variation in carbon uptake, and 2) how errors in modeling diffuse fraction affect predictions of carbon uptake. Modeled annual gross primary productivity (GPP) increased by approximate to 0.94% when observed levels of diffuse fraction were increased by 0.01 (holding total irradiance constant). The sensitivity of GPP to increases in diffuse fraction was highest when the diffuse fraction was low to begin with, and lowest when the diffuse fraction was already high. Diffuse fraction also explained significantly more of the interannual variability of modeled net ecosystem exchange (NEE), than did total irradiance. Two tested radiation partitioning models yielded over- and underestimates of diffuse fraction at our site, which propagated to over- and underestimates of annual NEE, respectively. Our findings highlight the importance of incorporating LUE enhancement under diffuse light into models of global primary production, and improving models of diffuse fraction.", "journal": "AGRICULTURAL AND FOREST METEOROLOGY", "category": "Agronomy; Forestry; Meteorology & Atmospheric Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000425940900015", "keywords": "children; diagnosis procedure combination inpatient database; influenza; underweight", "title": "Dose-response relationship between weight status and clinical outcomes in pediatric influenza-related respiratory infections", "abstract": "BackgroundAssociations between underweight/obesity and manifestations of influenza infection remain unclear, especially in children. This study investigated the dose-response relationships between weight status and clinical outcomes among children hospitalized with influenza-related respiratory infections. MethodsWe obtained hospital discharge records of inpatients aged under 18 years with diagnoses of bronchitis/pneumonia and influenza, using a Japanese national inpatient database. The patients were classified as underweight, normal-weight, overweight, or obese groups using weight-for-length, weight-for-height, and body-mass-index for age following World Health Organization criteria. We compared need for intensive care, 30-day readmission, mean total hospitalization costs, and length of hospital stay across the four groups using multivariable regression models and restricted cubic spline functions. ResultsOverall, 27771 patients were identified, including 2637 underweight, 19701 normal-weight, 2675 overweight, and 2758 obese patients. The underweight group showed a significantly higher 30-day readmission (adjusted odds ratio, 1.68; 95% confidence interval, 1.28-2.18) and a longer length of stay (adjusted difference, 0.23 days; 95% confidence interval, 0.12-0.23 days) than the normal-weight group did. No significant differences in the need for intensive care or hospitalization costs were observed across the four weight status groups. The threshold for a statistically significant association between weight status and 30-day readmission was a z-score for weight-for-length, weight-for-height, or BMI for age of -0.95 (17th percentile). ConclusionThese findings demonstrated that underweight status is a risk factor for repeated hospital admissions because of influenza-related respiratory infections in children.", "journal": "PEDIATRIC PULMONOLOGY", "category": "Pediatrics; Respiratory System", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000423584700007", "keywords": "Atrial fibrillation; D-dimer; monocyte-platelet aggregates; soluble CD40 ligand; soluble P-selectin; stroke; thromboembolism; thrombogenicity", "title": "Comparison of diverse platelet activation markers as indicators for left atrial thrombus in atrial fibrillation", "abstract": "Atrial fibrillation (AF) is well known for being a major risk factor of thromboembolic stroke. We could recently demonstrate an association of monocyte-platelet aggregates (MPAs) with the degree of thrombogenicity in patients with AF. This study investigated platelet activation markers, as potential biomarkers for the presence of left atrial (LA) thrombus in patients with AF. One hundred and eight patients with symptomatic AF underwent transesophageal echocardiography (TEE) before scheduled cardioversion or pulmonary vein isolation. In order to determine the content of MPAs by flow-cytometric quantification analyses, blood was drawn on the day of TEE. The soluble CD40 Ligand (sCD40L) and soluble P-selectin (sP-selectin) were obtained by Cytometric Bead Arrays (CBA). D-dimer levels were detected by quantitative immunological determination of fibrin degradation products. Clinical, laboratory, and echocardiographic standard parameters were obtained from all patients, including the determination of the flow in the left atrial appendage (LAA). Patients with detected LA thrombus (n = 28) compared with patients without thrombus (n = 80) showed an increased number of common risk factors, such as age, diabetes, heart failure, and coronary artery disease (CAD). The presence of LA thrombus was associated with significantly increased levels of MPAs (147 +/- 12 vs. 304 +/- 29 per mu l; p < 0.00), sCD40L (106.3 +/- 31.0 vs. 33.5 +/- 2.1 pg/ml, p = 0.027), and D-dimer (0.13 +/- 0.02 vs. 0.69 +/- 0.21 mg FEU/l, p = 0.015). In contrast, sP-selectin showed no association with LA thrombus. A multivariate regression analysis showed that MPAs, sCD40L as well as D-dimers were independent indicators for the existence of LA thrombus. MPAs above 170 cells/mu l indicated LA thrombus with a high sensitivity of 93% and a specificity of 73% (OR 62, 95% CI. 6.9-557.2, p < 0.001) in patients with AF, whereas the D-dimer lost their quality as independent indicator by using the conventional cut-off of 0.5 mg/l within the regression analysis. MPAs, as well as the D-dimer, correlated significantly negatively with the flow in the LAA measured during TEE. The content of MPAs, sCD40L, and D-dimer, but not sP-selectin showed an increased dependence on LA thrombus in patients with AF. In our study group, MPAs showed the best diagnostic test accuracy of the compared platelet markers. The different results of the examined platelet activation markers could be an indication of diverse mechanisms of LA thrombus in AF. Further studies should evaluate whether determination of MPAs in clinical routine may suffice to indicate the presence of LA thrombus in patients with AF.", "journal": "PLATELETS", "category": "Cell Biology; Hematology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000431430000006", "keywords": "ARCH; linearity tests; smooth transition models; spurious inference", "title": "The spurious effect of ARCH errors on linearity tests: a theoretical note and an alternative maximum likelihood approach", "abstract": "Linearity tests against smooth transition nonlinearity are typically based on the standard least-squares (LS) covariance matrix estimator. We derive an expression for the bias of the LS estimator in the presence of ARCH errors. We show that the bias is downward, and increases dramatically with the persistence of the variance process. As a consequence, conventional tests spuriously indicate nonlinearity. Next, we examine an alternative maximum likelihood approach. Our findings suggest that this approach has substantially better size properties than tests based on least-squares and heteroskedasticity-consistent matrix estimators, and performs comparably to a bootstrap technique.", "journal": "STUDIES IN NONLINEAR DYNAMICS AND ECONOMETRICS", "category": "Economics; Social Sciences, Mathematical Methods", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000419754300017", "keywords": "Singular integro-differential equations; Shifted Jacobi polynomials; Operational matrices of integration and product", "title": "Sequential fitting-and-separating reflectance components for analytical bidirectional reflectance distribution function estimation", "abstract": "We present a sequential fitting-and-separating algorithm for surface reflectance components that separates individual dominant reflectance components and simultaneously estimates the corresponding bidirectional reflectance distribution function (BRDF) parameters from the separated reflectance values. We tackle the estimation of a Lafortune BRDF model, which combines a nonLambertian diffuse reflection and multiple specular reflectance components with a different specular lobe. Our proposed method infers the appropriate number of BRDF lobes and their parameters by separating and estimating each of the reflectance components using an interval analysis-based branch-and-bound method in conjunction with iterative K-ordered scale estimation. The focus of this paper is the estimation of the Lafortune BRDF model. Nevertheless, our proposed method can be applied to other analytical BRDF models such as the Cook-Torrance and Ward models. Experiments were carried out to validate the proposed method using isotropic materials from the Mitsubishi Electric Research Laboratories-Massachusetts Institute of Technology (MERL-MIT) BRDF database, and the results show that our method is superior to a conventional minimization algorithm. (c) 2018 Optical Society of America", "journal": "APPLIED OPTICS", "category": "Optics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000427155300002", "keywords": "Trauma care management; Children; Algorithm; Pediatric emergencies; Seriously injured patients", "title": "iTRAP(s) - interdisciplinary trauma room algorithm in pediatric surgery", "abstract": "Accidents are one of the main causes of death in children and adolescents in industrialized countries. The prognosis of trauma patients is directly dependent on the time interval between the accident and arrival at the hospital. In addition to shortened rescue and transport times, selection of the specific trauma center is essential. The treatment of polytraumatized children in the trauma department requires a rapid and systematic assessment of acute life-threatening injuries and thus the interplay of all pediatric specialties. At the level one trauma center Dr. von Hauner'schen Kinderspital, the iTRAP(s) concept for the care of seriously injured children was developed by an interdisciplinary group. It includes the priority-oriented treatment algorithm and the assignment to specialists. The goal was to establish a trauma department concept for severely injured children, including graded diagnostic and treatment instructions, which also resulted in a clearly structured task distribution. The phases of the iTRAP(s) have been adapted to the special needs of severely injured children requiring trauma care. In order to ensure a priority-oriented and structured care ranging from admission to the trauma department to surgery or to the pediatric intensive care station, the process has been divided into four phases, which are named accordingly: phase 1: life-saving emergency measures, phase 2: the stabilization phase, phase 3: urgent diagnostics/therapy, phase 4: completion of diagnostics/therapy. The Munich concept, designated by the acronym iTRAPs, is intended to be a basic framework for the trauma department care of children. In addition to structured trauma department care, the extremely positive acceptance should encourage even more clinics to develop and maintain specific pediatric protocols for the trauma department in order to optimize the structural and process quality in the treatment of severely injured children.", "journal": "NOTFALL & RETTUNGSMEDIZIN", "category": "Emergency Medicine", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000423632800003", "keywords": "Parkinson's disease; driving; cognition; neuropsychological assessment; fitness to drive", "title": "Relationship between neuropsychological tests and driver's license renewal tests in Parkinson's disease", "abstract": "Objective: To determine whether the standard Spanish driving test (ASDE test) was able to identify patients with Parkinson's disease (PD) at risk of unsafe driving and to examine the relationship between the ASDE test and the Useful Field of View (UFOV) as well as with a battery of neuropsychological tests in drivers with PD.Methods: Thirty-seven patients with PD and 33 controls matched by age and education level were included in an observational study. All participants were active drivers and patients with PD underwent study procedures after taking the medication in the on period. Subjects with a Mini-Mental State Examination (MMSE) score 24 were excluded. Neuropsychological tests (Repeatable Battery for Neuropsychological Status [RBANS], Trail Making Test [TMT-A and -B], and Block Design test), driving performance tests (ASDE Driver Test and UFOV), and daytime sleepiness (Epworth Sleepiness Scale) were assessed.Results: The PD group performed significantly worse than healthy controls in the ASDE Motor Coordination tests. No significant differences were observed in anticipation speed, multiple reaction time, concentrated attention, and resistance to monotony. All participants successfully completed the UFOV tests. Statistically significant differences between patients with PD and controls were found in processing speed (UFOV1; P =.03) and more patients with PD were found in the categories of higher driving risk levels (P =.03). In addition, patients with PD showed worse scores than healthy controls in visuospatial capacities (Line Orientation), psychomotor speed (Coding and TMT-A), memory (List Recognition, Story Recall), and executive function (TMT-B). The driving tests (ASDE and UFOV) showed a low sensitivity and a high specificity but a higher percentage of patients in the PD group failed in multiple reaction time, concentrated attention, and resistance to monotony. In addition, 18.9% of patients with PD showed a cutoff of 4 for UFOV risk. In the discriminant analysis, Line Orientation (visuospatial/constructive domain) and Figure Recall (delayed memory) were found to be statistically significant with a rate of correct classification of unsafe drivers with PD of 78.2%. In addition, normal results on the Line Orientation item were associated with a 1.5times higher probability of non-risky driving in the multivariate analysis.Conclusions: At early stages of the disease, about 19% of patients with PD showed difficulties that may affect their driving capabilities. Line Orientation and Figure Recall are useful to alert clinicians to the risk of unsafe driving. For this reason, patients with PD should be evaluated for driving abilities more regularly to determine the extent of deficits that may influence driving performance.", "journal": "TRAFFIC INJURY PREVENTION", "category": "Public, Environmental & Occupational Health; Transportation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000425733600037", "keywords": "comparative study; depression; high-risk pregnancy; normal pregnancy; religious coping", "title": "Association of spiritual/religious coping with depressive symptoms in high- and low-risk pregnant women", "abstract": "Aims and objectivesTo investigate the role of spiritual/religious coping (SRC) on depressive symptoms in high- and low-risk pregnant women. BackgroundSpiritual/religious coping is associated with physical and mental health outcomes. However, only few studies investigated the role of these strategies during pregnancy and whether low- and high-risk pregnant women have different coping mechanisms. DesignThis study is a cross-sectional comparative study. MethodsThis study included a total of 160 pregnant women, 80 with low-risk pregnancy and 80 with high-risk pregnancy. The Beck Depression Inventory, the brief SRC scale and a structured questionnaire on sociodemographic and obstetric aspects were used. General linear model regression analysis was used to identify the factors associated with positive and negative SRC strategies in both groups of pregnant women. ResultsPositive SRC use was high, whereas negative SRC use was low in both groups. Although we found no difference in SRC strategies between the two groups, negative SRC was associated with depression in women with high-risk pregnancy, but not in those with low-risk pregnancy. Furthermore, positive SRC was not associated with depressive symptoms in both groups. ConclusionsResults showed that only the negative SRC strategies of Brazilian women with high-risk pregnancies were associated with worsened mental health outcomes. Relevance to clinical practiceHealthcare professionals, obstetricians and nurse midwives should focus on the use of negative SRC strategies in their pregnant patients.", "journal": "JOURNAL OF CLINICAL NURSING", "category": "Nursing", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000433261300005", "keywords": "Intracranial aneurysm (IA); CXCL12; rs1746048; LDL-C; lipid concentration; immunoglobulins", "title": "Significant Association of CXCL12 rs1746048 with LDL-C Level in Intracranial Aneurysms", "abstract": "Background: C-X-C motif chemokine ligand 12 (CXCL12) may play an important role in the development of Intracranial Aneurysm (IA). Objective: The goal of this study was to explore the association between CXCL12 rs1746048 genotypes and circulating lipid concentrations along with the risk of IA. Methods: A total of 256 IA patients and 361 healthy volunteers were included in the case-control study. The genotypes of CXCL12 rs1746048 were detected by Melting Temperature shift (Tmshift) Polymerase Chain Reaction (PCR). Results: Significant higher levels were seen in Total Cholesterol (TC) (p(adjusted) < 0.001), High-density Lipoprotein Cholesterol (HDL-C) (p(adjusted) < 0.001), Low-density Lipoprotein Cholesterol (LDL-C) (p(adjusted) < 0.001), Apolipoprotein AI (ApoA-I) (p(adjusted) = 0.040), and Apolipoprotein B (ApoB) (p(adjusted) < 0.001) in IAs compared with controls. CXCL12 rs1746048 T allele frequency showed significant association with the risk of IA in the female group aged 65 or above (p = 0.019, Odds Ratio (OR) = 2.15, 95% confidence interval (95%CI) = 1.13 - 4.11, power = 64.8%). Moreover, CXCL12 rs1746048 was likely to be a risk variant of IA under the recessive model in females older than 65 years. (p = 0.030, OR = 3.77, 95%CI = 1.08 - 13.12, power = 81.8%). Additionally, we also found that the levels of LDL-C were significantly different among three genotypes (CC vs. CT vs. TT = 2.75 +/- 0.73 vs. 3.03 +/- 0.89 vs. 2.82 +/- 0.72,p = 0.035) in IA patients. Conclusion: Our results suggest that CXCL12 rs1746048 is significantly associated with IA risk in Han Chinese females aged 65 years and older. Additionally, the genotypes of CXCL12 rs1746048 may affect the LDL-C concentrations in IA patients.", "journal": "CURRENT NEUROVASCULAR RESEARCH", "category": "Clinical Neurology; Neurosciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000411496400006", "keywords": "curve fitting; entropy; MMW imaging; standard deviation; time gating", "title": "Development of an efficient approach for MMW imaging system to identify concealed targets inside the book", "abstract": "The most puzzling task faces by security agency is detection and identification of the targets like, blade, and knife inside the book and capability to differentiate with book mark. Millimeter wave (MMW) imaging system may have good potential for this because of its resolution and penetration capability. Therefore, in this paper a statistics based adaptive algorithm is proposed for detection and identification of targets like blade, knife and book mark inside the book for MMW imaging system.", "journal": "MICROWAVE AND OPTICAL TECHNOLOGY LETTERS", "category": "Engineering, Electrical & Electronic; Optics", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000451415100005", "keywords": "Dynamic site index model; Autoregressive modeling; Crimean pine", "title": "SITE QUALITY ESTIMATIONS BASED ON THE GENERALIZED ALGEBRAIC DIFFERENCE APPROACH: A CASE STUDY IN CANKIRI FORESTS", "abstract": "In this study, it is aimed that the dynamic site index models were developed for Crimean Pine stands in Sarikaya-Cankiri forests located in middle northern Turkey. The data for this study are 153 sample trees obtained from the Crimean Pine stands. In modeling relationships between height and age of dominant or co-dominant trees, some dynamic site index equations such as Chapman-Richards (M1, M2, M3), Lundqvist (M4 and M6), Hossfeld (M5), Weibull (M7) and Schumacher (M8) based on the Generalized Algebraic Difference Approach (GADA) were used. The estimations for these eight-dynamic site index model parameters with well as various statistical values were obtained using the nonlinear regression technique. Among these equations, the Chapman-Richards's equation, M3, was determined to be the most successful model, with accounted for 89.03 % of the total variance in height-age relationships with MSE: 1.7633, RMSE: 1.3279, SSE: 1165.6, Bias: -0.0380. After determination of the best predictive model, ARMA (1, 1) autoregressive prediction technique was used to account autocorrelation problems for time-series height measurements. When ARMA autoregressive prediction technique was applied to the Chapman-Richards function for solving autocorrelation problem, these success statistics were improved as SSE: 868.7, MSE: 1.3183, RMSE: 1.1482, Bias: -0.06369, R2: 0.918. Also, Durbin-Watson statistics displayed that autocorrelation problem was solved by the use of ARMA autoregressive prediction technique; DW test value=1.99, DW< P=0.5622, DW> P=0.4378. The dynamic site index model that was developed has provided results compatible with the growth characteristics expected in the modeling of height-age relations, such as polymorphism, multiple asymptote, and base-age invariance.", "journal": "REVISTA ARVORE", "category": "Forestry", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000441658300009", "keywords": "Sieve prior; posterior contraction rate; approximation rate; rate adaptive; change-point; reversible jump MCMC; 62C10; 62G20; 62G05", "title": "Bayesian sieve methods: approximation rates and adaptive posterior contraction rates", "abstract": "In the last 20 years, a lot of achievements have been made in the study of posterior contraction rates of nonparametric Bayesian methods, and plenty of them involve sieve priors, but mainly for specific models or sieves. We provide a posterior contraction theorem for general parametric sieve priors. The theorem has weaker and simpler conditions compared with the existing results, and indicates that the sieve prior is rate adaptive. We apply the general theorem to density estimations and nonparametric regression with jumps. We also provided a reversible jump MCMC (Markov Chain Monte Carlo) algorithm for the sieve prior.", "journal": "JOURNAL OF NONPARAMETRIC STATISTICS", "category": "Statistics & Probability", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000425936500021", "keywords": "Fatty liver; SGLT2 inhibitors; Type 2 diabetes mellitus", "title": "Utility of Magnetic Resonance Enterography For Small Bowel Endoscopic Healing in Patients With Crohn's Disease", "abstract": "OBJECTIVES: Small bowel (SB) endoscopic healing has not been well studied in patients with Crohn's disease (CD). This study aims to evaluate the utility of magnetic resonance (MR) enterography (MRE) for SB lesions in comparison with balloon-assisted enteroscopy (BAE) findings. METHODS: In total, 139 patients with CD in clinical-serological remission were prospectively followed after BAE and MRE procedures. We applied a modified version of the Simple Endoscopic Score for CD (SES-CD) for an endoscopic evaluation of the SB, called the Simple Endoscopic Active Score for CD (SES-CDa). We also used the MR index of activity (MaRIA) for MR evaluations. The primary end points were time to clinical relapse (CD activity index of > 150 with an increase of > 70 points) and serological relapse (abnormal elevation of C-reactive protein). RESULTS: Clinical and serological relapses occurred in 30 (21.6%) and 62 (44.6%) patients, respectively. SB endoscopic healing (SES-CDa<5) was observed in 76 (54.7%) patients. A multiple regression analysis showed that the lack of SB endoscopic healing was an independent risk factor for clinical relapses (hazard ratio (HR): 5.34; 95% confidence interval (CI): 2.06-13.81) and serological relapses (HR: 3.02; 95% CI: 1.65-5.51), respectively. MR ulcer healing (MaRIA score < 11) demonstrated a high diagnostic accuracy (90.9%; 95% CI: 87.9-93.2%) for endoscopic healing. The kappa coefficient between BAE and MRE for longitudinal responsiveness was 0.754 (95% CI: 0.658-0.850) for clinical relapse and 0.783 (95% CI: 0.701-0.865) for serological relapse. CONCLUSIONS: SB inflammation was associated with a poor prognosis in patients with clinical-serological remission. MRE is a valid and reliable examination for SB inflammatory activity both for cross-sectional evaluations and prognostic prediction.", "journal": "AMERICAN JOURNAL OF GASTROENTEROLOGY", "category": "Gastroenterology & Hepatology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000423417700016", "keywords": "Centrocercus urophasianus; density-dependent growth; density-independent growth; finite population growth rate; generalized linear model; greater sage-grouse; population viability analysis; Wyoming", "title": "Greater sage-grouse population trends across Wyoming", "abstract": "The scale at which analyses are performed can have an effect on model results and often one scale does not accurately describe the ecological phenomena of interest (e.g., population trends) for wide-ranging species: yet, most ecological studies are performed at a single, arbitrary scale. To best determine local and regional trends for greater sage-grouse (Centrocercus urophasianus) in Wyoming, USA, we modeled density-independent and -dependent population growth across multiple spatial scales relevant to management and conservation (Core Areas [habitat encompassing approximately 83% of the sage-grouse population on approximate to 24% of surface area in Wyoming], local Working Groups [7 regional areas for which groups of local experts are tasked with implementing Wyoming's statewide sage-grouse conservation plan at the local level], Core Area status (Core Area vs. Non-Core Area) by Working Groups, and Core Areas by Working Groups). Our goal was to determine the influence of fine-scale population trends (Core Areas) on larger-scale populations (Working Group Areas). We modeled the natural log of change in population size by time to calculate the finite rate of population growth () for each population of interest from 1993 to 2015. We found that in general when Core Area status (Core Area vs. Non-Core Area) was investigated by Working Group Area, the 2 populations trended similarly and agreed with the overall trend of the Working Group Area. However, at the finer scale where Core Areas were analyzed separately, Core Areas within the same Working Group Area often trended differently and a few large Core Areas could influence the overall Working Group Area trend and mask trends occurring in smaller Core Areas. Relatively close fine-scale populations of sage-grouse can trend differently, indicating that large-scale trends may not accurately depict what is occurring across the landscape (e.g., local effects of gas and oil fields may be masked by increasing larger populations). Published 2017. This article is a U.S. Government work and is in the public domain in the USA.", "journal": "JOURNAL OF WILDLIFE MANAGEMENT", "category": "Ecology; Zoology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000427725000007", "keywords": "Millimeter wave; MIMO; Hybrid precoding; Beamforming; VLSI design", "title": "A 16 x 8 Hybrid Beamforming and Precoding Processor for 2D Planar Antenna Array in mmWave MIMO Systems", "abstract": "In the next generation 5G mobile communication systems, the massive multiple-input multiple-output (MIMO) system uses large antenna arrays to improve transmission throughput and reliability. Millimeter wave (mmWave) is very suitable for massive MIMO systems because its small wavelength allows the tightly package of large antenna array. This paper presents a low-complexity hybrid MIMO beamforming and precoding processor for 16x8 2D planar antenna array in mmWave systems. This study proposes to use the antenna selection technique to reduce the MIMO dimension and presents a sliding-window-based index selection for RF beamforming array vector. These techniques not only reduce the computational complexity but also enable highly paralleled hardware architecture. The proposed algorithm was designed and implemented by using TSMC 90 nm CMOS Technology for the MIMO baseband processing in 16 x8 2D planar antenna array MIMO systems. The processor achieves 2.9 M, 3.3 M, 4 M, and 4.7 M channel matrices per second for four, three, two, and one stream, respectively.", "journal": "JOURNAL OF SIGNAL PROCESSING SYSTEMS FOR SIGNAL IMAGE AND VIDEO TECHNOLOGY", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000433150600001", "keywords": "EOR; FCM algorithm; interfacial tension; hydrocarbon; predicting model", "title": "Utilization of Fuzzy C-means algorithm as a novel predictive tool for estimation of interfacial tension of hydrocarbon and brine", "abstract": "The interfacial tension that exists between brine and hydrocarbon is known as one of major properties in petroleum industries because it extremely affects oil trapping in reservoirs and consequently oil recovery. Due to aforementioned reasons the importance of investigation of this parameter has been highlighted. In the present study, Fuzzy C-means (FCM) algorithm was developed to predict interfacial tension between hydrocarbon and brine as function of different parameters such as pressure, temperature, carbon number of hydrocarbon and ionic strength of brine. The obtained results of predicting algorithm expressed its low relative error and deviation from the experimental data which gathered from the literature. Also the coefficients of determination (R-2) for training and testing data were calculated 0.9508 and 0.9309 respectively. This predictive tool is simple and user friend to utilize and can be helpful for petroleum engineers to estimate interfacial tension between hydrocarbons and brine.", "journal": "PETROLEUM SCIENCE AND TECHNOLOGY", "category": "Energy & Fuels; Engineering, Chemical; Engineering, Petroleum", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000460230900001", "keywords": "Big data; Graphical nonlinear knockoffs; High-dimensional nonlinear models; Large-scale inference and FDR; Power; Reproducibility; Robustness", "title": "Adaptive Fuzzy Super-Twisting Sliding Mode Control for Microgyroscope", "abstract": "This paper proposes a novel adaptive fuzzy super-twisting sliding mode control scheme for microgyroscopes with unknown model uncertainties and external disturbances. Firstly, an adaptive algorithm is used to estimate the unknown parameters and angular velocity of microgyroscopes. Secondly, in order to improve the performance of the system and the superiority of the super-twisting algorithm, this paper utilizes the universal approximation characteristic of the fuzzy system to approach the gain of the super-twisting sliding mode controller and identify the gain of the controller online, realizing the adaptive adjustment of the controller parameters. Simulation results verify the superiority and the effectiveness of the proposed approach, compared with adaptive super-twisting sliding mode control without fuzzy approximation; the proposed method is more effective.", "journal": "COMPLEXITY", "category": "Mathematics, Interdisciplinary Applications; Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000457506700042", "keywords": "Trenchless pipe laying; Optimization; Fuzzy logic; Mathematical model", "title": "The selection of the optimum trenchless pipe laying technology with the use of fuzzy logic", "abstract": "Presently, several trenchless pipe laying technologies are used. Those are applied under different conditions and show different, specific values of technical parameters. With a great variety of technologies available, key challenges are the selection of appropriate technology for a given project and the choice of the criteria that should be taken into consideration. The use of inappropriate technology can cause major problems in the maintenance of laid pipelines, or can even result in the failure of pipe laying. For that reason, the mathematical model which allows the choice of the optimal trenchless pipe laying technology was developed. In the model, fuzzy logic was used. Assumptions and input quantities were determined and presented. Limiting conditions in the form of linguistic variables were given. Those include the following: the possibility of pipe laying in a given soil type using a given trenchless technology; the possibility of laying the pipeline of a given diameter using a given trenchless technology; the lengths of pipelines laid in one drive, characteristic of each technology-, the possibility of laying the pipeline of a given material for a given technology and the deviation of an installed pipeline slope from the designed slope, characteristic of each technology. For those variables, linguistic term-sets and universes of discourse were determined and presented. Membership functions, grades of membership attributed to them, and also fuzzy sets qualified for particular linguistic variables were introduced. The algorithm of the mathematical model was constructed. Finally, the verification of the model for real trenchless pipe laying projects, was presented.", "journal": "TUNNELLING AND UNDERGROUND SPACE TECHNOLOGY", "category": "Construction & Building Technology; Engineering, Civil", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000462961900001", "keywords": "High speed maglev train; relative position sensor; adaptive filter; Kalman filter; normalization", "title": "Research on Decoupling Problem of Suspension Gap and Location of Relative Position Sensor in High Speed Maglev Train", "abstract": "The relative position sensor of a high-speed maglev train is an important part of train location and speed detection for motor traction, but its output signal is not only related to position but also related to the suspension gap of the maglev train. The fluctuation of the suspension gap will affect the amplitude of the output signal of the sensor (i.e., the suspension wave signal is coupled with the output signal). The prediction normalization method currently used can eliminate the effect of the suspension fluctuation to a certain extent, but there is a limitation. Aiming at this problem, this paper analyzes the frequency characteristics of the suspension gap fluctuation caused by track irregularities and proposes a gap estimation algorithm based on the adaptive filter. The Kalman filter is used to estimate the gap and then the output signal is compensated according to the numerical relationship between the gap and the output signal of the sensor, so as to achieve the decoupling between the gap and the position measurement. Finally, the effectiveness of the method is proved by the comparison experiments.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000467316400181", "keywords": "helical gearbox; spectral subtraction; empirical wavelet transform; feature extraction", "title": "Fault Diagnosis of a Helical Gearbox Based on an Adaptive Empirical Wavelet Transform in Combination with a Spectral Subtraction Method", "abstract": "Featured Application This work can extract fault features of vibration signals of rotating machinery. Abstract Fault characteristic extraction is attracting a great deal of attention from researchers for the fault diagnosis of rotating machinery. Generally, when a gearbox is damaged, accurate identification of the side-band features can be used to detect the condition of the machinery equipment to reduce financial losses. However, the side-band feature of damaged gears that are constantly disturbed by strong jamming is embedded in the background noise. In this paper, a hybrid signal-processing method is proposed based on a spectral subtraction (SS) denoising algorithm combined with an empirical wavelet transform (EWT) to extract the side-band feature of gear faults. Firstly, SS is used to estimate the real-time noise information, which is used to enhance the fault signal of the helical gearbox from a vibration signal with strong noise disturbance. The empirical wavelet transform can extract amplitude-modulated/frequency-modulated (AM-FM) components of a signal using different filter bands that are designed in accordance with the signal properties. The fault signal is obtained by building a flexible gear for a helical gearbox with ADAMS software. The experiment shows the feasibility and availability of the multi-body dynamics model. The spectral subtraction-based adaptive empirical wavelet transform (SS-AEWT) method was applied to estimate the gear side-band feature for different tooth breakages and the strong background noise. The verification results show that the proposed method gives a clearer indication of gear fault characteristics with different tooth breakages and the different signal-noise ratio (SNR) than the conventional EMD and LMD methods. Finally, the fault characteristic frequency of a damaged gear suggests that the proposed SS-AEWT method can accurately and reliably diagnose faults of a gearbox.", "journal": "APPLIED SCIENCES-BASEL", "category": "Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials Science, Multidisciplinary; Physics, Applied", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000462530300207", "keywords": "Foxing; fungi; XRF; LIBS; SEM", "title": "Audit of fetal cardiac anomaly detection at six obstetric units in the South West", "abstract": "The subject of this present work is a group of nine historical pictures shot in Palermo by the Sicilian photographer E. Interguglielmi in 1912. They are nine matte-collodion prints mounted on the original cardboard supports and all of them show foxing stains affecting the paper surface. In order to characterise the chemical composition of the supports and investigate foxing spots, non-destructive and micro-destructive analysis were carried out. X-rays fluorescence (XRF) analysis was used to characterise the elemental composition of all the mounting boards, allowing a comparison between the foxing spots and non-affected areas. Laser-Induced Breakdown Spectroscopy was used to investigate the presence of lower atomic number elements, not detectable by XRF, while SEM imaging allowed the investigation of surface appearance and nature of original paper samples from the cardboards. [GRAPHICS]", "journal": "BJOG-AN INTERNATIONAL JOURNAL OF OBSTETRICS AND GYNAECOLOGY", "category": "Obstetrics & Gynecology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000457859800025", "keywords": "Deformable image registration; Breathing motion; Adaptive radiotherapy; Lung cancer", "title": "Evaluation of deformable image registration accuracy for CT images of the thorax region", "abstract": "Purpose: Evaluate the performance of three commercial deformable image registration (DIR) solutions on computed tomography (CT) image-series of the thorax. Methods: DIRs were performed on CT image-series of a thorax phantom with tumor inserts and on six 4-dimensional patient CT image-series of the thorax. The center of mass shift (CMS), dice similarity coefficient (DSC) and dose-volume-histogram (DVH) parameters were used to evaluate the accuracy. Dose calculations on deformed patient images were compared to calculations on un-deformed images for the gross tumor volume (GTV) (D-mean, D-98%), lung (V-20Gy,V-12Gy), heart and spinal cord (D-2%). Results: Phantom structures with constant volume and shifts <= 30 mm were reproduced with visually acceptable accuracy (DSC >= 0.91, CMS <= 0.9 mm) for all software solutions. Deformations including volume changes were less accurate with 9/12 DIRs considered visually unacceptable. In patients, organs were reproduced with DSC >= 0.83. GTV shifts <= 1.6 cm were reproduced with visually acceptable accuracy by all software while larger shifts resulted in failures for at least one of the software. In total, the best software succeeded in 18/25 DIRs while the worst succeeded in 12/25 DIRs. Visually acceptable DIRs resulted in deviations <= 3.0% of the prescribed dose and <= 3.6% of the total structure volume in the evaluated DVH-parameters. Conclusions: The take home message from the results of this study is the importance to have a visually acceptable registration. DSC and CMS are not predictive of the associated dose deviation. Visually acceptable DIRs implied dose deviations <= 3.0%.", "journal": "PHYSICA MEDICA-EUROPEAN JOURNAL OF MEDICAL PHYSICS", "category": "Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000469306400014", "keywords": "toilet training; preterm birth; parents", "title": "The Differences Between Preterm and Term Birth Affecting Initiation and Completion of Toilet Training Among Children: A Retrospective Case-Control Study", "abstract": "Purpose: This study seeks to investigate the possibility the existence of a difference in terms of start and end dates of toilet training between term and preterm children as well as the possible determining factors. Materials and Methods: This study was conducted as a 5-year retrospective case (children born preterm-(32 to <37 weeks) - and control (children born at term (>37 weeks + 1 day)) study. The data were collected with a form consisted of questions about demographic data (12 questions) and toilet traning features (10 questions) through face-to-face interviews with the mothers. A chi-square test and logistic regression analysis were conducted to examine the data. Odds ratio was used as a measure of the relation between levels of the dependent variable. Results: The study examined a total of 133 children including 59 preterm children and 74 children born at term including 60 (45.1%) boys and 73 (54.9%) girls. The possibility of starting toilet training at or before 24 months was found to be 6.4 times greater in full-term children than preterm children (OR = 6.493). The logistic regression analysis, which aimed at identifying any variables that might affect end date of toilet training, found that despite the tendency to consider preterm birth as a factor prolonging the duration of toilet training, the difference was not found to be statistically significant (P = .07). Conclusion: This study compared full-term and preterm children in terms of start and end dates of toilet training and found that preterm children start toilet training later than full-term children. Based on the results of the study, it is possible to say that preterm birth, gender and birth order affect start date of toilet training. However there is no difference between term and preterm babies on the end date of toilet training.", "journal": "UROLOGY JOURNAL", "category": "Urology & Nephrology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000455019800006", "keywords": "Finance Big Data; Granger causality directed network; global stock markets; financial network analysis; data visualization; trading strategy; market regulation; risk management", "title": "How Do the Global Stock Markets Influence One Another? Evidence from Finance Big Data and Granger Causality Directed Network", "abstract": "The recent financial network analysis approach reveals that the topologies of financial markets have an important influence on market dynamics. However, the majority of existing Finance Big Data networks are built as undirected networks without information on the influence directions among prices. Rather than understanding the correlations, this research applies the Granger causality test to build the Granger Causality Directed Network for 33 global major stock market indices. The paper further analyzes how the markets influence one another by investigating the directed edges in the different filtered networks. The network topology that evolves in different market periods is analyzed via a sliding window approach and Finance Big Data visualization. By quantifying the influences of market indices, 33 global major stock markets from the Granger causality network are ranked in comparison with the result based on PageRank centrality algorithm. Results reveal that the ranking lists are similar in both approaches where the U.S. indices dominate the top position followed by other American, European, and Asian indices. The lead-lag analysis reveals that there is lag effects among the global indices. The result sheds new insights on the influences among global stock markets with implications for trading strategy design, global portfolio management, risk management, and markets regulation.", "journal": "INTERNATIONAL JOURNAL OF ELECTRONIC COMMERCE", "category": "Business; Computer Science, Software Engineering", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000464461100014", "keywords": "ADC; breast lesions; diagnosis; DWI; MRI", "title": "The diagnostic value of MRI multi-parameter combination for breast lesions with ring enhancement", "abstract": "Purpose: To investigate the diagnostic value of MRI multi-parameter combination for breast lesions with ring enhancement (internal enhancement pattern) on dynamic contrast-enhanced (DCE)-MRI. Methods: 149 patients with histologically confirmed breast lesions underwent DCE-MRI and diffusion-weighted imaging (DWI) examinations were analyzed Sixty-seven lesions were found and were allocated into the benign group and the malignant group. The pathological results were used as dependent variables, indexes with statistical differences were used as independent variables, and logistic regression model was performed to construct the newly combined parameters and to evaluate the diagnostic efficacy of the stepwise combined parameters. Results: There were significant differences in the number of cases with different wall shapes concerning\"enhanced ring\" and the number of cases with wall nodules between the benign and the malignant group. Significant differences were found in the number of cases with different distribution locations of limited diffusion on DWI between the benign and the malignant group. There were significant differences in the semi-quantitative parameters including early enhancement ratio (EER) and maximum enhancement time (Tmax) between the benign group and the malignant group. There were significant differences in apparent diffusion coefficient (ADC)(ring inner) and ADC(ring wall )between the benign and the malignant group. The maximum Youden index of a newly-constructed parameter combination: morphological indexes of \"enhanced ring\" + distribution locations of limited diffusion on DWI + Tmax + ADC(ring inner )was 0.732 for combined diagnosis, the area under the ROC curve (Az) was 0.887, and the diagnostic sensitivity and specificity were 85.78 and 87.37%, respectively. Conclusions: MRI multi-parameter combination can improve the diagnostic efficacy of breast lesions with ring enhancement.", "journal": "JOURNAL OF BUON", "category": "Oncology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000463096800001", "keywords": "Biome-BGC MuSo; Soil temperature; Soil moisture; Ensemble Kalman filter; Data assimilation", "title": "Improved simulation of carbon and water fluxes by assimilating multi-layer soil temperature and moisture into process-based biogeochemical model", "abstract": "BackgroundSoil temperature and moisture are sensitive indicators in soil organic matter decomposition because they control global carbon and water cycles and their potential feedback to climatic variations. Although the Biome-Biogeochemical Cycles (Biome-BGC) model is broadly applied in simulating forest carbon and water fluxes, its single-layer soil module cannot represent vertical variations in soil moisture. This study introduces the Biome-BGC MuSo model, which is composed of a multi-layer soil module and new modules pertaining to phenology and management for simulations of carbon and water fluxes. Although this model considers soil processes among active layers, estimates of soil-related variables might be biased, leading to inaccurate estimates of carbon and water fluxes.MethodsTo improve the estimations of soil-related processes in Biome-BGC MuSo, this study assimilates ground-measured multi-layer daily soil temperature and moisture at the Changbai Mountains forest flux site by using the Ensemble Kalman Filter algorithm. The modeled estimates of water and carbon fluxes were evaluated with measurements using determination coefficient (R-2) and root mean square error (RMSE). The differences in the RMSEs from Biome-BGC MuSo and the assimilated Biome-BGC MuSo were calculated (RMSE), and the relationships between RMSE and the climatic and biophysical factors were analyzed.ResultsCompared with the original Biome-BGC model, Biome-BGC MuSo improved the simulations of ecosystem respiration (ER), net ecosystem exchange (NEE) and evapotranspiration (ET). Data assimilation of the soil-related variables into Biome-BGC MuSo in real time improved the accuracies of the simulated carbon and water fluxes (ET: R-2=0.81, RMSE=0.70mmd(-1); ER: R-2=0.85, RMSE=1.97 gCm(-2)d(-1); NEE: R-2=0.70, RMSE=1.16 gCm(-2)d(-1)).ConclusionsThis study proved that seasonal simulation of carbon and water fluxes are more accurate when using Biome-BGC MuSo with a multi-layer soil module than using Biome-BGC with a single-layer soil module. Moreover, assimilating the observed soil temperature and moisture data into Biome-BGC MuSo improved the modeled estimates of water and carbon fluxes via calibrated soil-related simulations. The assimilation strategy is applicable to various climatic and biophysical conditions, particularly densely forested areas, and for local or regional simulation.", "journal": "FOREST ECOSYSTEMS", "category": "Forestry", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000467001900009", "keywords": "Premature birth; Nomogram; High risk; Asymptomatic; Bacterial vaginosis; Short cervix; Late miscarriage", "title": "Development of a nomogram for individual preterm birth risk evaluation", "abstract": "Objective. - This study aimed to develop a new tool for personalised preterm birth risk evaluation in high-risk population. Study design. - 813 high-risk asymptomatic pregnant women included in a French multicentric prospective study were analysed. Clinical and paraclinical variables, including screening for bacterial vaginosis with molecular biology, cervical length, have been used to create the nomogram, based on the logistic regression model. The validity was checked by bootstrap. A downloadable calculator was build. Results. - Nine risk factors were included in this model: history of late miscarriage and/or preterm delivery, active smoking, ultrasound cervical length, term of pregnancy at screening, bacterial vaginosis, premature rupture of membranes, daily travel more than 30 min. Discrimination and calibration of the nomogram revealed good predictive abilities. The area under the receiver operating characteristic curve was 0.77 (95% CI; 0.72-0.81). The mean absolute error was 0.018, which showed proper calibration. The optimal risk threshold was 23.2% with a sensitivity of 74%, a specificity of 72.7% and a predictive negative value of 90.6%. Conclusion. - The nomogram can help to better define individual preterm birth risk in high-risk pregnancies. (C) 2018 Elsevier Masson SAS. All rights reserved.", "journal": "JOURNAL OF GYNECOLOGY OBSTETRICS AND HUMAN REPRODUCTION", "category": "Obstetrics & Gynecology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000459070600023", "keywords": "community assembly; environmental gradient; habitat filtering; limiting similarity; niche differentiation; spatial partitioning; trait convergence; trait divergence", "title": "Disentangling the processes driving plant assemblages in mountain grasslands across spatial scales and environmental gradients", "abstract": "1. Habitat filtering and limiting similarity are well-documented ecological assembly processes that hierarchically filter species across spatial scales, from a regional pool to local assemblages. However, information on the effects of fine-scale spatial partitioning of species, working as an additional mechanism of coexistence, on community patterns is much scarcer. 2. In this study, we quantified the importance of fine-scale spatial partitioning, relative to habitat filtering and limiting similarity in structuring grassland communities in the western Swiss Alps. To do so, 298 vegetation plots (2 mx2 m) each with five nested subplots (20 cmx20 cm) were used for trait-based assembly tests (i.e., comparisons with several alternative null expectations), examining the observed plot and subplot level alpha-diversity (indicating habitat filtering and limiting similarity) and the among-subplot beta-diversity of traits (indicating fine-scale spatial partitioning). We further assessed variations in the detected signatures of these assembly processes along a set of environmental gradients. 3. We found habitat filtering was the dominating assembly process at the plot level with diminished effect at the subplot level, whereas limiting similarity prevailed at the subplot level with weaker average effect at the plot level. Plot-level limiting similarity was positively correlated with fine-scale partitioning, suggesting that the trait divergence resulted from a combination of competitive exclusion between functionally similar species and environmental micro-heterogeneities. Overall, signatures of assembly processes only marginally changed along environmental gradients, but the observed trends were more prominent at the plot than at the subplot scale. 4. Synthesis. Our study emphasises the importance of considering multiple assembly processes and traits simultaneously across spatial scales and environmental gradients to understand the complex drivers of plant community composition.", "journal": "JOURNAL OF ECOLOGY", "category": "Plant Sciences; Ecology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000460829700133", "keywords": "dietary patterns; nutrition transition; environmental sustainability; Uganda; women; rural; urban", "title": "What Can Dietary Patterns Tell Us about the Nutrition Transition and Environmental Sustainability of Diets in Uganda?", "abstract": "Uganda is undergoing dietary transition, with possible environmental sustainability and health implications, particularly for women. To explore evidence for dietary transitions and identify how environmentally sustainable women's dietary patterns are, principal component analysis was performed on dietary data collected using a 24 h recall during the Uganda Food Consumption Survey (n = 957). Four dietary patterns explained 23.6% of the variance. The traditional, high-fat, medium environmental impact pattern was characterized by high intakes of nuts/seeds, fats, oils and spreads, fish and boiled vegetables. High intakes of bread and buns, rice and pasta, tea and sugar characterized the transitioning, processed, low environmental impact' pattern. The plant-based, low environmental impact pattern was associated with high intakes of legumes, boiled roots/tubers, boiled traditional vegetables, fresh fruit and fried traditional cereals. High intakes of red/organ meats, chicken, and soups characterized the animal-based high environmental impact pattern. Urban residence was positively associated with transitioning, processed, low environmental impact ( = 1.19; 1.06, 1.32) and animal-based high environmental impact ( = 0.45; 0.28, 0.61) patterns; but negatively associated with the plant-based low environmental impact pattern (= -0.49; -0.62, -0.37). A traditional, high-fat dietary pattern with medium environmental impact persists in both contexts. These findings provide some evidence that urban women's diets are transitioning.", "journal": "NUTRIENTS", "category": "Nutrition & Dietetics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000453741300023", "keywords": "Block Toeplitz matrix; constrained estimation; reparameterization; Schur stability", "title": "CONSTRAINED ESTIMATION OF CAUSAL INVERTIBLE VARMA", "abstract": "We present a reparameterization of vector autoregressive moving average (VARMA) models that allows parameter estimation under the constraints of causality and invertibility. This reparameterization is accomplished via a bijection from the complicated causal-invertible parameter space to Euclidean space. The bijection facilitates computation of maximum likelihood estimators (MLE) via unconstrained optimization, as well as computation of Bayesian estimates via prior specification on the constrained space. The proposed parameterization is connected to the Schur-stability of polynomials and the associated Stein transformation, which are often used in dynamical systems; we establish a fundamental characterization of Schur stable polynomials via a novel characterization of positive definite block Toeplitz matrices. Our results also generalize some classical results in dynamical systems.", "journal": "STATISTICA SINICA", "category": "Statistics & Probability", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000457658800023", "keywords": "Sheep; Reproductive traits; Animal model; Bayesian method", "title": "Bayesian inference on genetic parameters for some reproductive traits in sheep using linear and threshold models", "abstract": "Genetic parameters for reproductive traits in Baluchi sheep were estimated on 3844 dams born during 1991 to 2012. The studied traits were litter size at birth (LSB), litter size at weaning (LSW), litter mean weight per lamb born (LMWLB), litter mean weight per lamb weaned (LMWLW), total litter weight at birth (TLWB) and total litter weight at weaning (TLWW). Significant effects were determined using Logistic and GLM procedure of SAS software for threshold and continuous traits, respectively. Variance components needed for breeding were estimated using Gibbs sampling methodology of Bayesian inference. The genetic parameters for LMWLB, LMWLW, TLWB TLWW were estimated by fitting a linear model. To examine the appropriateness of fitting linear or threshold model to litter size at birth and weaning, both models were used to estimate genetic parameters on the observed data. Posterior means of the heritabilities of litter size at birth and weaning by using a threshold model were estimated to be two times higher than the heritabilities based on the linear model (0.18 versus 0.09 and 0.11 versus 0.05 for LSB and LSW, respectively). Bayesian information criterion (BIC) selected the threshold model as most appropriate model for LSB and LSW given the data. The heritability estimates for LMWLB, LMWLW, TLWB TLWW resulting from linear analysis were 0.12, 0.05, 0.06 and 0.08, respectively. Genetic correlation estimates between the traits ranged from -0.67 for LMWLB-TLWW to 0.85 for LSB-TLWB. Phenotypic correlations ranged from 0.06 for LSB-LMWLW to 0.88 for LSB-TLWB and permanent environmental correlations ranged from -0.74 for LMWLB - TLWW to 0.88 for LSB-TLWB. Results of this study showed that the threshold model seems to be reliable for genetic evaluation of litter size in Baluchi sheep.", "journal": "SMALL RUMINANT RESEARCH", "category": "Agriculture, Dairy & Animal Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000460004100001", "keywords": "pancreatic cancer; adjuvant treatment; Phellinus linteus; chemotherapy; survival", "title": "Potential Impact of Phellinus linteus on Adherence to Adjuvant Treatment After Curative Resection of Pancreatic Ductal Adenocarcinoma: Outcomes of a Propensity Score-Matched Analysis", "abstract": "Background: Surgical resection followed by adjuvant chemotherapy is the only therapeutic option in pancreatic cancer. However, there is limited research evaluating methods of improving adherence to adjuvant treatment after curative resection. Methods: From January 1995 to December 2014, 323 patients with pancreatic cancer who underwent pancreatectomy at the Severance Hospital were enrolled in this study. We retrospectively analyzed clinicopathologic factors with propensity score matching method. Results: The final study population was 217, after excluding patients undergoing neoadjuvant treatment or palliative resection, those who died within 30 days after operation, and those lost to follow-up after discharge. Among them, 161 received adjuvant treatment after curative resection. Cox's proportional hazard models revealed that nodal metastasis, perioperative transfusion, and completion of adjuvant treatment were significantly correlated with cancer recurrence and cancer-related death (P < .05). Phellinus linteus (PL) medication was the only significant predictor for completion of adjuvant treatment after curative resection in logistic regression analysis (P = .039). Disease-free and overall survival of the PL medication group were significantly higher than the no PL medication group (P < .05). Conclusions: PL medication potentially contributed to long-term oncologic outcomes by increasing patients' adherence to postoperative adjuvant chemotherapy, which resulted from PL medication associated with low toxicity of chemotherapy.", "journal": "INTEGRATIVE CANCER THERAPIES", "category": "Oncology; Integrative & Complementary Medicine", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000464523200003", "keywords": "autonomous underwater vehicle (AUV); robotic exploration; view planning (VP); motion planning; frontier-based (FB) exploration; next-best-view (NBV)", "title": "Two-Dimensional Frontier-Based Viewpoint Generation for Exploring and Mapping Underwater Environments", "abstract": "To autonomously explore complex underwater environments, it is convenient to develop motion planning strategies that do not depend on prior information. In this publication, we present a robotic exploration algorithm for autonomous underwater vehicles (AUVs) that is able to guide the robot so that it explores an unknown 2-dimensional (2D) environment. The algorithm is built upon view planning (VP) and frontier-based (FB) strategies. Traditional robotic exploration algorithms seek full coverage of the scene with data from only one sensor. If data coverage is required for multiple sensors, multiple exploration missions are required. Our approach has been designed to sense the environment achieving full coverage with data from two sensors in a single exploration mission: occupancy data from the profiling sonar, from which the shape of the environment is perceived, and optical data from the camera, to capture the details of the environment. This saves time and mission costs. The algorithm has been designed to be computationally efficient, so that it can run online in the AUV's onboard computer. In our approach, the environment is represented using a labeled quadtree occupancy map which, at the same time, is used to generate the viewpoints that guide the exploration. We have tested the algorithm in different environments through numerous experiments, which include sea operations using the Sparus II AUV and its sensor suite.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000452310300006", "keywords": "multivariate; structural time series model; seasonality; tourism demand; forecasting", "title": "Forecasting Seasonal Tourism Demand Using a Multiseries Structural Time Series Method", "abstract": "Multivariate forecasting methods are intuitively appealing since they are able to capture the interseries dependencies, and therefore may forecast more accurately. This study proposes a multiseries structural time series method based on a novel data restacking technique as an alternative approach to seasonal tourism demand forecasting. The proposed approach is analogous to the multivariate method but only requires one variable. In this study, a quarterly tourism demand series is split into four component series, each component representing the demand in a particular quarter of each year; the component series are then restacked to build a multiseries structural time series model. Empirical evidence from Hong Kong inbound tourism demand forecasting shows that the newly proposed approach improves the forecast accuracy, compared with traditional univariate models.", "journal": "JOURNAL OF TRAVEL RESEARCH", "category": "Hospitality, Leisure, Sport & Tourism", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000451726600014", "keywords": "Microhaplotypes; Single nucleotide polymorphisms; Massively parallel sequencing; Biogeographic ancestry inference; Mixture deconvolution; Human identification; Missing person and relationship identification; Probabilistic genotyping; Clinical application; Non-human DNA", "title": "Microhaplotypes in forensic genetics", "abstract": "Microhaplotype loci (microhaps, MHs) are a novel type of molecular marker of less than 300 nucleotides, defined by two or more closely linked SNPs associated in multiple allelic combinations. The value of these markers is enhanced by massively parallel sequencing (MPS), which allows the sequencing of both parental haplotypes at each of the many multiplexed loci. This review describes the features of these multi-SNP markers and documents their value in forensic genetics, focusing on individualization, biogeographic ancestry inference, and mixture deconvolution. Foreseeable applications also include missing person identification, relationship testing, and medical diagnostic applications. The technique is not restricted to humans.", "journal": "FORENSIC SCIENCE INTERNATIONAL-GENETICS", "category": "Genetics & Heredity; Medicine, Legal", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000462504400111", "keywords": "trajectory planning; position/force cooperative control; hierarchical planning; object-oriented; symmetrical adaptive variable impedance", "title": "Multi-Robot Trajectory Planning and Position/Force Coordination Control in Complex Welding Tasks", "abstract": "In this paper, the trajectory planning and position/force coordination control of multi-robot systems during the welding process are discussed. Trajectory planning is the basis of the position/ force cooperative control, an object-oriented hierarchical planning control strategy is adopted firstly, which has the ability to solve the problem of complex coordinate transformation, welding process requirement and constraints, etc. Furthermore, a new symmetrical internal and external adaptive variable impedance control is proposed for position/force tracking of multi-robot cooperative manipulators. Based on this control approach, the multi-robot cooperative manipulator is able to track a dynamic desired force and compensate for the unknown trajectory deviations, which result from external disturbances and calibration errors. In the end, the developed control scheme is experimentally tested on a multi-robot setup which is composed of three ESTUN industrial manipulators by welding a pipe-contact-pipe object. The simulations and experimental results are strongly proved that the proposed approach can finish the welding task smoothly and achieve a good position/force tracking performance.", "journal": "APPLIED SCIENCES-BASEL", "category": "Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials Science, Multidisciplinary; Physics, Applied", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000467123800024", "keywords": "Bacterial community; Patinopecten yessoensis; adductor muscles; gills; gonads; intestines; high-throughput sequencing", "title": "Characterization of Bacterial Community Associated with Four Organs of the Yesso Scallop (Patinopecten yessoensis) by High-Throughput Sequencing", "abstract": "We used Illumina high-throughput sequencing of PCR-amplified V3-V4 16S rRNA gene regions to characterize bacterial communities associated with the adductor muscles, gills, gonads and intestines of the Yesso scallop (Patinopecten yessoensis) from waters around Zhangzidao, Dalian, China. Overall, 421,276 optimized reads were classified as 25 described bacterial phyla and 308 genera. Firmicutes, Proteobacteria, Tenericutes, Bacteroidetes, Chlamydiae and Spirochaetae accounted for > 97% of the total reads in the four organs. The bacterial 16S rDNA sequences assigned to Firmicutes and Proteobacteria were abundant in the adductor muscles, gills and gonads; while reads from Tenericutes were dominant in the intestines, followed by those from Firmicutes, Chlamydiae, Proteobacteria and Bacteroidetes. At the genus level, the dominant genera in the adductor muscles, gills and gonads appeared to be Bacillus, Enterococcus and Lactococcus, whereas Mycoplasma was dominant in the intestines. The relative abundances of Bacillus, Enterococcus, Lactococcus, Alkaliphilus, Raoultella, Paenibacillus and Oceanobacillus were significantly lower in the intestine than in the other three organs. Cluster analysis and principal coordinates analysis of the operational taxonomy units profile revealed significant differences in the bacterial community structure between the intestine and the other three organs. Taken together, these results suggest that scallops have intestine-specific bacterial communities and the adductor muscles, gills and gonads harbor similar communities. The difference in the bacterial community between organs may relate to unique habitats, surroundings, diet and their respective physiological functions.", "journal": "JOURNAL OF OCEAN UNIVERSITY OF CHINA", "category": "Oceanography", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000462752800033", "keywords": "Papillary thyroid carcinoma; post-traumatic stress disorder; serotonin-transporter-linked polymorphic region gene; prognosis; association", "title": "Serotonin transporter-linked polymorphic region genotypes in relation to stress conditions among patients with papillary thyroid carcinoma", "abstract": "The serotonin-transporter-linked polymorphic region (5-HTTLPR) gene has been reported to predispose individuals experiencing trauma to affective disorders such as anxiety and depression. We hypothesized that SS genotype of 5-HTTLPR gene would induce stress conditions and poor prognosis of papillary thyroid carcinoma (PTC). The study enrolled 287 patients with or without post-traumatic stress disorder (PTSD) following surgical treatment of PTC with their baseline characteristics collected. Polymerase chain reaction-restriction fragment length polymorphism (PCR-RFLP) was conducted to detect genotype frequency. Five self-rating scales, including Impact of Event Scale-Revised Edition (IES-R), Medical Coping Modes Questionnaire (MCMO), Hamilton Depression Scale (HAMD), Social Support Rating Scale (SSRS) and Stressful Life Events (SLEs), were used for depressive state assessment. Survival situations were observed through 15-year follow-up visits one time every six months. Survival rate was calculated using Life Table. Logistic regression analysis was used to analyze factors related to prognosis of PTC. Increased SS genotype and decreased LL genotypes were found in patients with PTSD. PTSD is associated with high stress, and inter-group analysis revealed that patients carrying SS genotype exhibited a high stress condition. PTSD and SS genotype correlated to large tumor size, advanced clinical stage, lymph node metastasis, and decreased 10-year and 15-year survival rate. As for patients carrying the same genotype, those suffering from PTSD showed poorer survival. Also, 5-HTTPRL, MCMQ score (confrontation/avoidance/surrender), HAMD score, SSRS total score, SLEs score, tumor size, clinical stage, and lymph node metastasis were relevant factors for prognosis of PTC. The results demonstrate SS genotype of the 5-HTTPRL gene as a contributor of high stress among patients with PTC. Thus, 5-HTTPRL and stress conditions represent potential investigative focus targets for prognosis of PTC.", "journal": "INTERNATIONAL JOURNAL OF CLINICAL AND EXPERIMENTAL PATHOLOGY", "category": "Oncology; Pathology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000451356300023", "keywords": "Thermocompressor; Stirling engine; Oscillating flow; Third order model; Thermoacoustic characteristics", "title": "Thermal analysis of Stirling thermocompressor and its prospect to drive refrigerator by using natural working fluid", "abstract": "The Stirling thermocompressor (STCP), usually working at low frequency (< 5 Hz), is a novel Stirling -type external-combustion machine. It utilizes the temperature difference between the heating temperature and ambient temperature to generate the thermoacoustic power and pressure wave for refrigerators. In order to explore the characteristics of STCP, a detailed thermal analysis based on the third order 1-D finite volume model considering the real gas effect and variable physical properties was proceeded. Three natural fluids, helium, nitrogen and hydrogen are chosen as the working fluids and compared with each other. The parameter effects of pressure fluctuation under no-load operation were studied. To understand the output capacities for refrigerator with using different working fluids, the RC load method is adopt to analysis the STCP's capacity and efficiency of the conversion from heat energy to acoustic energy, also the loss characteristics under different working gas. When the working pressure, frequency and heating temperature are 4 MPa, 4 Hz and 800 K respectively, the STCP using helium as the working fluid could output about 380 W acoustic power for the RC load and relative Carnot efficiency is about 33%.", "journal": "ENERGY CONVERSION AND MANAGEMENT", "category": "Thermodynamics; Energy & Fuels; Mechanics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000454844000012", "keywords": "older; central nervous system agents; falls", "title": "Racial/Ethnic Differences in Influenza and Pneumococcal Vaccination Rates Among Older Adults in New York City and Los Angeles and Orange Counties", "abstract": "Introduction Disparities in vaccination rates exist among racial/ethnic minority adults. This study examined factors associated with influenza (flu) and pneumococcal vaccination rates among non-Hispanic black, Hispanic, and Asian American adults aged 50 or older living in New York City or Los Angeles and Orange counties in California. Methods We used data collected by the REACH US Risk Factor Survey 2009-2012 in New York City and California. We analyzed data on 14,139 adults aged 50 or older who were categorized as non-Hispanic black (New York City [n = 1,715], California [n = 530]), Hispanic (New York City [n = 2,667], California [n = 1,099]), Chinese American (New York City [n = 1,656]), Korean American (New York City [n = 310]), Filipino American (California [n = 1,515]), or Vietnamese American (California [n = 3,435]). Bivariate analyses examined difference across race/ethnicity and location, and multivariable logistic regression models, adjusting for sociodemographic and health variables, examined flu and pneumococcal vaccination rates. Results Among adults aged 50 or older, the flu vaccination rate was lower among non-Hispanic black respondents (New York City, 53.3%; California, 40.5%) than among Hispanic (New York City, 61.0%; California, 49.4%), Chinese (New York City, 67.6%), Korean (New York City, 60.5%), Filipino (California, 66.2%), and Vietnamese (California, 68.0%) respondents. Among adults aged 65 or older, pneumococcal vaccination rates were lowest among Chinese and Korean respondents in New York City (51.7% and 49.1%, respectively), compared with non-Hispanic black (New York City, 62.0%, California, 65.6%), Hispanic (New York City, 60.0%; California 62.7%), Filipino (California, 63.4%), and Vietnamese (California, 63.8%) respondents. Older age, having had a checkup in the past year, and diabetes diagnosis were significantly associated with flu and pneumococcal vaccination in both locations. Additional variables were significant for some vaccinations and locations. Conclusion When compared with Asian American respondents, non-Hispanic black respondents were least likely to receive the flu vaccine in New York City and California. We found no racial/ethnic differences in pneumococcal vaccination rates. Our findings highlight the need for targeted efforts to increase vaccination rates among racial/ethnic minority older adults.", "journal": "PREVENTING CHRONIC DISEASE", "category": "Public, Environmental & Occupational Health", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000454078500005", "keywords": "PET-MR; dynamic AC; joint image registration; cardiac and respiratory motion correction", "title": "Joint cardiac and respiratory motion estimation for motion-corrected cardiac PET-MR", "abstract": "Respiratory and cardiac motion can strongly impair cardiac PET image quality and tracer uptake quantification. Standard gating techniques can minimize these motion artefacts but suffer from low signal-to-noise ratio because only a small percentage of the total data is utilized. Motion correction approaches have been proposed to overcome this problem but require accurate knowledge of such physiological motion. Here we present a joint PET-MR motion estimation approach which combines complimentary dynamic image information from simultaneously acquired MR and PET to ensure improved cardiac and respiratory motion estimation for motion-corrected image reconstruction (MCIR) of PET images. A 3D triple-echo Dixon MR scan is used both for calculation of MR-based attenuation correction (AC) maps and estimation of physiological motion. PET listmode data is obtained simultaneously to the MR acquisition which is used for a joint motion estimation and reconstruction of the final MCIR PET. In a first step, dynamic cardiac and respiratory motion resolved 4D MR and PET images are reconstructed. These image series are used in a joint image registration to estimate non-rigid cardiac and respiratory motion fields. In a second step, the motion fields are utilized in a MR MCIR to obtain cardiac and respiratory resolved dynamic MR-based AC maps. In the last step, the non-rigid motion fields and the dynamic AC maps are applied in a PET MCIR to obtain the final motion-corrected PET images. PET-MR data has been obtained in six patients without any known heart disease. Motion amplitudes were between 5.6 and 16 mm, with higher values in the basal compared to the mid-ventricular and apical segments. The proposed joint PET-MR motion estimation provided more accurate motion estimation than using either modality separately. The underestimation of PET uptake due to respiratory and cardiac motion artefacts in the AC maps was up to 17%. The average increase in uptake values using MCIR was 23% +/- 10% (p < 0.0001), with values of 28% +/- 11% (p < 0.0001) for basal, 21% +/- 8% (p < 0.0001) for mid-cavity and 17% +/- 7% (p < 0.0001) for apical segments. With the proposed scheme we could ensure high PET image quality and improve local PET uptake quantification by up to 30%. Attenuation correction and motion information was obtained from the same PET-MR raw data, which was obtained during free-breathing to minimize scan times and to increase patient comfort.", "journal": "PHYSICS IN MEDICINE AND BIOLOGY", "category": "Engineering, Biomedical; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000457308500025", "keywords": "side scan sonar; super resolution; sparse coding; object detection", "title": "Side Scan Sonar Image Super Resolution via Region-Selective Sparse Coding", "abstract": "Side scan sonar using low frequency can quickly search a wide range, but the images acquired are of low quality. The image super resolution (SR) method can mitigate this problem. The SR method typically uses sparse coding, but accurately estimating sparse coefficients incurs substantial computational costs. To reduce processing time, we propose a region-selective sparse coding based SR system that emphasizes object regions. In particular, the region that contains interesting objects is detected for side scan sonar based underwater images so that the subsequent sparse coding based SR process can be selectively applied. Effectiveness of the proposed method is verified by the reduced processing time required for image reconstruction yet preserving the same level of visual quality as conventional methods.", "journal": "IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS", "category": "Computer Science, Information Systems; Computer Science, Software Engineering", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000462258700033", "keywords": "cancer/malignancy/neoplasia; clinical research/practice; kidney transplantation/nephrology; surgical technique; urology", "title": "Robot-assisted laparoscopic partial nephrectomy with renal artery clamping using an endovascular balloon catheter for an allograft kidney tumor: A new perspective to manage renal vascular control?", "abstract": "Chemotherapy-induced nail toxicity is a widely spread phenomenon. Cooling the patient's hands can reduce blood flow to the fingers and consequently reducing the amount of chemical agents reaching the nails. This paper is aiming at developing a model-based controller of the finger's skin temperatures and blood flow to reduce the risk of nail toxicity during chemotherapy. Experiments were conducted to model the dynamic response of the fingers skin temperature and blood flow using an ad hoc experimental device. The device was designed to provide a localised cooling of the fingers. The experiments were performed on a homogeneous test group of 11 middle-aged women. The fingers' skin temperatures and blood flow were measured continuously. A second order discrete time transfer function model was suitable (R-2=0.91 +/- 0.18) in all the cases to model the dynamic responses of the fingers' skin temperatures. The model estimation results have shown that the a- and b-parameters were varying among different test subjects and within the same subject. The resulted models were employed in designing a model-based proportional-integral-plus controller. Simulations of the closed-loop systems were performed based on the identified models for each test subject. The simulation results have shown that the designed controller is able to regulate the finger's skin temperatures tightly about the desired level and yet, is still quite simple to implement in practice. Controlled active cooling with an online parameter estimation algorithm and continuous feedback of the patient finger temperatures is a promising solution to reduce nail toxicity during chemotherapy.", "journal": "AMERICAN JOURNAL OF TRANSPLANTATION", "category": "Surgery; Transplantation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000464384900004", "keywords": "Zea may L; floral corn; phytochemicals; genetic diversity; cluster analysis", "title": "Variability in Anthocyanins, Phenolic Compounds and Antioxidant Capacity in the Tassels of Collected Waxy Corn Germplasm", "abstract": "Corn tassel is a valuable co-product and an excellent source of phytochemicals with bioactive properties. The information on the genetic diversity in the tassel properties of waxy corn germplasm is important for creating new varieties that can have the potential for the commercial production of tassels as a co-product. Therefore, the objective of this study was to evaluate the potential of corn tassels in a set of waxy corn germplasm for the extraction of phenolic compounds with an antioxidant activity. The experiment was carried out under field conditions in the rainy season 2017 and the dry season 2017/2018. Fifty waxy corn genotypes were evaluated. Data were collected for the total anthocyanin content (TAC), total phenolic content (TPC) and the antioxidant activity was determined by the 2,2-diphenyl-1-picrylhydrazyl (DPPH) radical scavenging activity and Trolox equivalent antioxidant capacity (TEAC) assays. The season (S) had small effect on all of the parameters, accounting for 0.2-8.7% of the total variance. The genotype (G) was the largest variance component in the TAC and DPPH radical scavenging activity, accounting for 83.5-97.5% of the total variance. The G and S x G interaction contributed approximately equally to the total variance in the TPC and TEAC. Based on the TAC, TPC and antioxidant capacity variation, the genotypes were classified into seven groups. The tassels of corn genotypes belonging to three of these clusters (clusters E, F and G) had high levels of phytochemicals along with an antioxidant capacity. A significant correlation coefficient was found between the TAC and DPPH (r = 0.70 **). The TPC showed a moderate relationship with the DPPH and TEAC assays (r = 0.60 ** and 0.76 **, respectively). The information obtained from this study can be used for germplasm management and waxy corn breeding for enhancing levels of bioactive properties in waxy corn tassels.", "journal": "AGRONOMY-BASEL", "category": "Agronomy; Plant Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000462958400002", "keywords": "Computed tomography; Disc herniation; Percutaneous endoscopic lumbar discectomy; Preoperative planning; Puncture accuracy; Virtual reality", "title": "Development of a Virtual Reality Preoperative Planning System for Postlateral Endoscopic Lumbar Discectomy Surgery and Its Clinical Application", "abstract": "OBJECTIVE: Percutaneous endoscopic lumbar discectomy is an effective way to treat lumbar disc herniation. Traditional preoperative planning based on a 2-dimensional method by magnetic resonance/computed tomography may cause inaccuracy of puncture during surgery. We used virtual reality to stimulate a surgery environment and measured relevant 3-dimensional data. We then explored its applicability for increasing puncture accuracy during actual surgeries. METHODS: A prospective randomized trial of lumbar disc herniation was conducted. Both conventional and virtual reality methods were used for preoperative planning and relevant data (planned puncture point and entry angle) were measured. Data were used during surgery and adjusted to complete the operation. The final entry point and entry angle were recorded and compared with relevant planned data statistically. Fluoroscopic times and location time also were included to access the puncture accuracy during surgery. RESULTS: Thirty cases were included in our study. Both groups achieved good results after surgery, except for 1 case of postoperative dysesthesia in the traditional planning group and 1 case of residual disc in the virtual reality group. The use of virtual reality can predict a surgeryrelated angle and distance accurately except for depth. Compared with the traditional planning group, the fluoroscopic time (13.18 +/- 4.191 vs. 32.00 +/- 4.52) and location time (17.91 +/- 4.74 vs. 33.22 +/- 3.90) were statistically different, which indicates that this method can increase puncture accuracy. CONCLUSIONS: A virtual reality planning system is an accurate preoperative planning method that can significantly improve the puncture accuracy of percutaneous endoscopic lumbar discectomy and reduce fluoroscopic and location times.", "journal": "WORLD NEUROSURGERY", "category": "Clinical Neurology; Surgery", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000456271600001", "keywords": "pose and radius estimation; circular target; binocular vision", "title": "Precise pose and radius estimation of circular target based on binocular vision", "abstract": "A novel method of estimating circle pose and radius is proposed, based on binocular vision. This method uses the special vectors that are irrelevant to the unknown radius to calculate the normal vector of the circular plane, which is the foundation for calculating the vanishing line of the circular plane in the imaging plane. The projections of the circle center on the left and right image are calculated afterwards through the vanishing line. Then, the circle center is reconstructed by the triangulation technology of binocular stereo vision. Finally, the radius of the circular target is calculated through the relative positions of points in the same coordinate system. The estimates of normal vector, center position, and radius size are unique and definite, so there is no need for a subsequent screening process. When the radius is unknown, most of the conventional circle pose estimation methods are unusable. However, the proposed method has no prerequisites about the circular target in order to estimate the pose and radius of the circular target, which improves flexibility and extends the application field. Besides estimating the pose of a circular target of unknown radius, the proposed method can also be applied to the vision measurement of the radius of a circular object. Both actual measurement experiments and noise simulation experiments are performed. The results show that the proposed method is more precise and robust than conventional methods.", "journal": "MEASUREMENT SCIENCE AND TECHNOLOGY", "category": "Engineering, Multidisciplinary; Instruments & Instrumentation", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000460173100026", "keywords": "robot; surgical margin; surgical technique; locally advanced; pT3; renal neoplasm", "title": "Achieving tumour control when suspecting sinus fat involvement during robot-assisted partial nephrectomy: step-by-step", "abstract": "Objectives To report a single expert robotic surgeon's step-by-step surgical technique for achieving local cancer control during robot-assisted PN (RAPN) for T3 tumours. Patients and methods Since January 2010 to December 2016, the institutional RAPN database was queried for patients who underwent transperitoneal RAPN performed by a single surgeon for tumours <= 4 mm from the collecting system at preoperative computed tomography (three points on the 'N [Nearness]' R.E.N.A.L. nephrometry-score item) that were pT3a involving sinus fat at final pathology. Baseline characteristics, perioperative and oncological outcomes (particularly positive surgical margins, PSMs), were identified. Results Of 1497 masses that underwent RAPN, 512 scored 3 points on the 'N' item of the R.E.N.A.L. nephrometry score assessment. In all, 24 patients had pT3a tumours involving sinus fat at final pathology and represented the analysed cohort. RAPN were performed according to the here described technique. No PSMs were reported. Trifecta achievement was 54.2%. Within a median follow-up of 30 months, two and one patients had recurrence or metastasis, respectively. Two patients died unrelated to renal cancer. Retrospective analysis and limited follow-up represent study limitations. Conclusion In a selected cohort of patients with renal tumours near the sinus fat at baseline R.E.N.A.L. nephrometry score assessment and confirmed pT3a at final pathology, the described RAPN technique was able to achieve optimal local cancer control.", "journal": "BJU INTERNATIONAL", "category": "Urology & Nephrology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000466585000001", "keywords": "Constrained shortest path; must-pass nodes; multi-stage metaheuristic algorithm; routing problem; traveling salesman problem", "title": "A Multi-Stage Metaheuristic Algorithm for Shortest in Path Problem With Must-Pass Nodes", "abstract": "The shortest simple path problem with must-pass nodes (SSPP-MPN) aims to find a minimum-cost simple path in a directed graph, where some specified nodes must be visited. We call these specified nodes as must-pass nodes. The SSPP-MPN has been proven to be NP-hard when the number of specified nodes is more than one, and it is at least as difficult as the traveling salesmen problem (TSP), a well-known NP-hard problem. In this paper, we propose a multi-stage metaheuristic algorithm based on multiple strategies such as k-opt move, candidate path search, conflicting nodes promotion, and connectivity relaxation for solving the SSPP-MPN. The main idea of the proposed algorithm is to transform the problem into classical TSP by relaxing the simple path constraint and try to repair the obtained solutions in order to meet the demands of the original problem. The computational results tested on three sets of totally 863 instances and comparisons with reference algorithms show the efficacy of the proposed algorithm in terms of both solution quality and computational efficiency.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000457952700013", "keywords": "Irrigation; Southern Africa; Livelihood strategies; Household income; Gender balance in decision-making", "title": "The dynamics of the relationship between household decision-making and farm household income in small-scale irrigation schemes in southern Africa", "abstract": "Irrigation has been promoted as a strategy to reduce poverty and improve livelihoods in southern Africa. Households' livelihood strategies within small-scale irrigation schemes have become increasingly complex and diversified. Strategies consist of farm income from rain-fed and irrigated cropping as well as livestock and an increasing dependence on off-farm income. The success of these strategies depends on the household's ability to make decisions about how to utilize its' financial, labour, land and water resources. This study explores the dynamics of decision-making in households on-farm household income within six small-scale irrigation schemes, across three southern African countries. Household survey data (n = 402) was analyzed using ordered probit and ordinary least squares regression. Focus group discussions and field observations provided qualitative data on decision-making in the six schemes. We found strong support for the notion that decision-making dynamics strongly influence total household income. Households make trade-offs between irrigation, dryland, livestock and off-farm work when they allocate their labour resources to maximize household income; as opposed to maximizing the income from any individual component of their livelihood strategy, such as irrigation. Combined with the impact of the small plot size of irrigated land, this is likely to result in sub-optimal benefits from expensive investments in irrigation infrastructure. Policy-makers must consider this when developing and implementing new policies.", "journal": "AGRICULTURAL WATER MANAGEMENT", "category": "Agronomy; Water Resources", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000476867500015", "keywords": "Lodging percentage; Plant and ear height; Residue ratio; Waxy maize; Yield components; Plant growth regulator", "title": "Waxy Maize Yield in Response to a Novel Plant Growth Regulator and Plant Density", "abstract": "High crop density often weakens plants, thereby leading to an increased risk of lodging and reductions in crop yield. In this study, the effects of a novel plant growth regulator, DHEAP [N, N-diethyl-2-hexanoyl oxygen radicals-ethyl amine (2-ethyl chloride) phosphoric acid salt], on waxy maize yield and lodging was evaluated. The field experiment was conducted at two locations (Changping and Wuqiao, China), four plant densities (4.5, 6.0, 7.5 and 9.0 plants m(-2)) and three cultivars (JHN 2008, JKN 2000 and JKN 928) in 2015. After foliar spraying with DHEAP during the 8-leaf stage, fresh ear and grain yield, yield components, and lodging percentage were determined for all cultivars. Grain yield showed a significant increase (3.47%), thousand-kernel weight and kernel number per ear increased by 6.10 and 3.21%, respectively after DHEAP treatment. The optimal plant density ranged from 6.0 to 7.5 plants m(-2). DHEAP significantly decreased waxy maize lodging percentage for all cultivars. Among the maize cultivars, the yield of the compact short plant cultivar JKN 928 was higher than that of JKN 2000 and JHN 2008. It is concluded that optimized plant density might enhance waxy maize yield with a single application of DHEAP before jointing stage. (C) 2019 Friends Science Publishers", "journal": "INTERNATIONAL JOURNAL OF AGRICULTURE AND BIOLOGY", "category": "Agriculture, Multidisciplinary; Biology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000467752200066", "keywords": "renewable energy technology industry; government policies; firms' financial performance; dynamic panel approach", "title": "Exploring the Effects of Government Policies on Economic Performance: Evidence Using Panel Data for Korean Renewable Energy Technology Firms", "abstract": "Previous studies have investigated how government policies on renewable energy technology (RET) affect economic performance at the industrial level. However, each firm in the RET industry is heterogeneous in terms of their capacities, resources, and the amount of public subsidies they receive. Considering the context in which public subsidies are provided to firms, this study econometrically investigates the effects of government policies on firms' financial performance using panel data from the Korean RET industry. We consider the results of various panel framework tests; establish a panel vector autoregressive model in first differences; and test the dynamic relationships between firms' financial performance, government subsidies (R&D- and non-R&D-related), firm size and age, and organizational slack, using a bias-corrected least squares dummy variable estimator. We find that R&D- and non-R&D-related subsidies positively affect firms' financial performance in the long run. In the short run, there are bidirectional positive causal relationships between firms' financial performance and organizational slack (and non-R&D-related subsidy), and firm size and non-R&D-related subsidy. A positive short-run relationship runs from R&D-related subsidy to firms' financial performance, from firm age to non-R&D-related subsidy, and from firm size to firm age. Further, there are dynamic effects in all estimations, demonstrating that the dependent variables of the previous period enhance their values in the current period. The results provide some policy and strategic implications.", "journal": "SUSTAINABILITY", "category": "Green & Sustainable Science & Technology; Environmental Sciences; Environmental Studies", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000454965500003", "keywords": "Climate variability; Lake sedimentary archives; Oxygen isotopes; Proxy system models; Isotope-enabled GCMs", "title": "Global-scale proxy system modelling of oxygen isotopes in lacustrine carbonates: New insights from isotope-enabled-model proxy-data comparison", "abstract": "Proxy System Modelling (PSM) is now recognised as a crucial step in comparing climate model output with proxy records of past environmental change. PSMs filter the climate signal from the model, or from meteorological data, based on the physical, chemical and biological processes of the archive and proxy system under investigation. Here we use a PSM of lake carbonate delta O-18 to forward model pseudoproxy time-series for every terrestrial grid square in the SPEEDY-IER isotope enabled General Circulation Model (GCM), and compare the results with 31 records of lake delta O-18 data from the Americas in the NOAA Paleoclimate Database. The model-data comparison shows general patterns of spatial variability in the lake delta O-18 data are replicated by the combination of SPEEDY-IER and the PSM, with differences largely explained by known biases in the models. The results suggest improved spatial resolution coverage of climate models and proxy data, respectively, is required for improved data-model comparison, as are increased numbers of higher temporal resolution proxy time series (sub decadal or better) and longer GCM runs. We prove the concept of data-model comparison using isotope enabled GCMs and lake isotope PSMs and outline potential avenues for further work. (C) 2018 Elsevier Ltd. All rights reserved.", "journal": "QUATERNARY SCIENCE REVIEWS", "category": "Geography, Physical; Geosciences, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000455190200006", "keywords": "Longitudinal; Multi-centre; Magnetic resonance imaging; Study design; Data sharing; Guidelines; Big data", "title": "Longitudinal multi-centre brain imaging studies: guidelines and practical tips for accurate and reproducible imaging endpoints and data sharing", "abstract": "BackgroundResearch involving brain imaging is important for understanding common brain diseases. Study endpoints can include features and measures derived from imaging modalities, providing a benchmark against which other phenotypical data can be assessed. In trials, imaging data provide objective evidence of beneficial and adverse outcomes. Multi-centre studies increase generalisability and statistical power. However, there is a lack of practical guidelines for the set-up and conduct of large neuroimaging studies.MethodsWe address this deficit by describing aspects of study design and other essential practical considerations that will help researchers avoid common pitfalls and data loss.ResultsThe recommendations are grouped into seven categories: (1) planning, (2) defining the imaging endpoints, developing an imaging manual and managing the workflow, (3) performing a dummy run and testing the analysis methods, (4) acquiring the scans, (5) anonymising and transferring the data, (6) monitoring quality, and (7) using structured data and sharing data.ConclusionsImplementing these steps will lead to valuable and usable data and help to avoid imaging data wastage.", "journal": "TRIALS", "category": "Medicine, Research & Experimental", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000463131000001", "keywords": "k-Tridiagonal matrix; Singular value decomposition; Permutation; Block diagonalization", "title": "A fast singular value decomposition algorithm of general k-tridiagonal matrices", "abstract": "In this article we present a method to speed up the singular value decomposition (SVD) of a general k-tridiagonal matrix using its block diagonalization. We show a O(n(3)/k(3)) parallel algorithm over k threads with no synchronization required. We thoroughly analyze its complexity and show that our method can boost the performance of any general SVD algorithm when applied to k-tridiagonal matrices. We present numerical experiments confirming our results. (C) 2018 Elsevier B.V. All rights reserved.", "journal": "JOURNAL OF COMPUTATIONAL SCIENCE", "category": "Computer Science, Interdisciplinary Applications; Computer Science, Theory & Methods", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000458345500004", "keywords": "Information asymmetry; Contrarian trading; Stock returns", "title": "Informed contrarian trades and stock returns", "abstract": "We develop a novel measure for the probability of informed trading. This measure, termed PCM, captures the adverse selection component of bid-ask spreads, becomes elevated around earnings announcements, and exhibits similar time-series patterns to price impact. Returns in double-sorted portfolios increase in PCM while controlling for liquidity. A long short portfolio based on PCM generates 18.3% abnormal return annually. PCM significantly explains cross-sectional returns while controlling for other factors. This effect is robust to alternative specifications and remains significant after the decimalization in 2001. In sum, PCM is easy to implement and serves as an effective proxy for informed trading. (C) 2018 Elsevier B.V. All rights reserved.", "journal": "JOURNAL OF FINANCIAL MARKETS", "category": "Business, Finance", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000456887600009", "keywords": "Physical activity on worksite; Musculoskeletal pain; Perception; Interpersonal relations; Employee well-being", "title": "A worksite physical activity program and its association with biopsychosocial factors: An intervention study in a footwear factory", "abstract": "Objective: The objective of this study was to evaluate the effect of a worksite physical activity program on psychophysiological and social factors. Methods: A worksite physical activity program included 1113 workers from a footwear factory in northeastern Brazil. The participants were classified based on their frequency of attendance in the program. The dependent variables were psychophysiological factors, including the relief of musculoskeletal pain, the improvement of physical and psychological well-being, perceived difficulty in performing tasks, and willingness to work; and social factors related to interpersonal relationships among employees. The obtained data were processed, and the analysis of the correlations between the variables was modeled using ordinal logistic regression. Results: The frequency of physical activity was a determining factor for the effectiveness of the intended outcomes for all analyzed variables. The participants who attended more weekly exercise sessions were twice as likely to experience relief of musculoskeletal pain, 74% more likely to report psychophysiological well-being, 30% less likely to have difficulties in performing tasks, and 87% more likely to perceive improved interpersonal relations. Conclusions: Regular physical activity is associated with consistent benefits for work dynamics and the health status of employees and effectively and rapidly improves the desired outcomes. Relevance to industry: Worksite physical activity programs are used as strategies to prevent diseases and address complications caused by exposure to occupational risk factors. However, data on the effects of these interventions that consider the frequency of physical activity and work-related social factors are inconsistent.", "journal": "INTERNATIONAL JOURNAL OF INDUSTRIAL ERGONOMICS", "category": "Engineering, Industrial; Ergonomics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000461726100028", "keywords": "Chronic obstructive pulmonary disease; Cystic fibrosis; Lower urinary tract symptoms; Urinary incontinence; Women", "title": "Women with breast and uterine cancer are more likely to harbor germline mutations than women with breast or uterine cancer alone: A case for expanded gene testing", "abstract": "Objective. We explored the germline mutation spectrum and prevalence among 1650 women with breast and uterine cancer (BUC) who underwent multi-gene hereditary cancer panel testing at a single commercial laboratory. Methods. The combined frequency of mutations in 23 BC and/or UC genes was compared between BUC cases and control groups with (1) no personal cancer history; (2) BC only; and (3) UC only using logistic regression. Results. Fourteen percent (n = 231) of BUC cases tested positive for mutations in BC and/or UC genes and were significantly more likely to test positive than individuals with BC only (P < 0.001), UC only (P < 0.01), or unaffected controls (P < 0.001). Analysis of gene-specific mutation frequencies revealed that MSH6, CHEK2, BRCA1, BRCA2, ATM, PMS2, PALB2 and MSH2 were most frequently mutated among BUC cases. Compared to BC only, BRCAI, MLHI, MSH2, MSH6, PMS2 and PTEN mutations were more frequent among BUC; however, only ATM mutations were more frequent among BUC compared to UC only. All of the more commonly mutated genes have published management guidelines to guide clinical care. Of patients with a single mutation in a gene with established testing criteria (n = 152), only 81.6% met their respective criteria, and 65.8% met criteria for multiple syndromes. Conclusions. Women with BUC are more likely to carry hereditary cancer gene mutations than women with breast or uterine cancer alone, potentially warranting expanded genetic testing for these women. Most mutations found via multi-gene panel testing in women with BUC have accompanying published management guidelines and significant implications for clinical care. (C) 2018 Elsevier Inc. All rights reserved.", "journal": "GYNECOLOGIC ONCOLOGY", "category": "Oncology; Obstetrics & Gynecology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000466243900001", "keywords": "Dichotic listening; asymmetry; laterality; technical note", "title": "A primer on dichotic listening as a paradigm for the assessment of hemispheric asymmetry", "abstract": "Dichotic listening is a well-established method to non-invasively assess hemispheric specialization for processing of speech and other auditory stimuli. However, almost six decades of research also have revealed a series of experimental variables with systematic modulatory effects on task performance. These variables are a source of systematic error variance in the data and, when uncontrolled, affect the reliability and validity of the obtained laterality measures. The present review provides a comprehensive overview of these modulatory variables and offers both guiding principles as well as concrete suggestions on how to account for possible confounding effects and avoid common pitfalls. The review additionally provides guidance for the evaluation of past studies and help for resolving inconsistencies in the available literature.", "journal": "LATERALITY", "category": "Psychology, Multidisciplinary; Psychology, Experimental", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000463475300001", "keywords": "Solid-state drive; buffer management; adaptive algorithm; cross-layer aware", "title": "An Advanced Adaptive Least Recently Used Buffer Management Algorithm for SSD", "abstract": "The traditional solid-state drive buffer management algorithm generally adopts fixed structures and parameters, leading to their poor adaptability. For example, after the underlying flash translation layer (FTL) or the upper workload is changed, the traditional algorithm's performance fluctuates significantly. Focusing on this problem, based on the cross-layer aware method, we propose an Advanced Adaptive Least Recently Used buffer management algorithm (AALRU). The core idea of the AALRU is that by sensing the characteristics of the upper workload and the status of the underlying FTL, the AALRU adaptively adjusts its structure, parameters, and write-back strategy to optimize the buffer's performance. First, the AALRU divides the buffer into two parts: read buffer and write buffer, and their proportion is adjusted by sensing the read-write characteristics of the workload and the underlying read-write latency. Second, the AALRU employs different granularities to manage the buffer. On one hand, for data loading and migrating, the AALRU adopts page-level granularity, which can avoid the problem of hot and cold data page entanglement in block management, and thus improve the buffer hit ratio. On the other hand, for data writing back to the FTL, the AALRU adopts block-level granularity, which can enhance the continuity of write requests and thus reduce the underlying FTL's garbage collection overhead. Finally, when the clustered data are written back, by sensing the underlying FTL's garbage collection status, the AALRU adaptively adjusts the page-padding trigger threshold to reconstruct the continuity of the write-back data, which can mitigate the underlying FTL's garbage collection overhead. The experimental results show that the AALRU has the best adaptability to different FTLs and test workloads, and it can achieve optimal or near-optimal results.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000457939400083", "keywords": "polynya; ice production; Ross Sea; heat flux; wind forcing", "title": "Heat Flux Sources Analysis to the Ross Ice Shelf Polynya Ice Production Time Series and the Impact of Wind Forcing", "abstract": "The variation of Ross Ice Shelf Polynya (RISP) ice production is a synergistic result of several factors. This study aims to analyze the 2003-2017 RISP ice production time series with respect to the impact of wind forcing on heat flux sources. RISP ice production was estimated from passive microwave sea ice concentration images and reanalysis meteorological data using a thermodynamic model. The total ice production was divided into four components according to the amount of ice produced by different heat fluxes: solar radiation component (V-s), longwave radiation component (V-l), sensible heat flux component (V-fs), and latent heat flux component (V-fe). The results show that Vfs made the largest contribution, followed by V-l and V-fe, while Vs had a negative contribution. Our study reveals that total ice production and V-l, V-fs, and V-fe highly correlated with the RISP area size, whereas V-s negatively correlated with the RISP area size in October, and had a weak influence from April to September. Since total ice production strongly correlates with the polynya area and this significantly correlates with the wind speed of the previous day, strong wind events lead to sharply increased ice production most of the time. Strong wind events, however, may only lead to mildly increasing ice production in October, when enlarged V-s reduces the ice production. Wind speed influences ice production by two mechanisms: impact on polynya area, and impact on heat exchange and phase transformation of ice. V-fs and V-fe are influenced by both mechanisms, while V-s and V-l are only influenced by impact on polynya area. These two mechanisms show different degrees of influence on ice production during different periods. Persistent offshore winds were responsible for the large RISP area and high ice production in October 2005 and June 2007.", "journal": "REMOTE SENSING", "category": "Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000468785000007", "keywords": "Prenatal screening; Combined first trimester screening; Prenatal diagnosis; Chromosomal anomaly", "title": "Algorithm for Recovery of Reciprocity of a Wireless Communications Channel in MIMO Systems", "abstract": "A new method for correcting the Massive MIMO system for MRT precoding in a downlink channel in order to recover the reciprocity of the channel is considered. It is shown that the lack of reciprocity of the channel between the base station and the user equipment is the main factor deteriorating the system performance. It is also established that the correction is necessary only for the equipment of the base station in the downlink. An estimate of the quality of the algorithm is given.", "journal": "JOURNAL OF COMMUNICATIONS TECHNOLOGY AND ELECTRONICS", "category": "Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000455949100007", "keywords": "Sexual relations; same-sex; sex; ambivalence; attitudes; structured conflict; LGBTQ", "title": "Predicting Ambivalence: When Same-Sex Sex is Only \"Sometimes Wrong\"", "abstract": "Despite Americans' growing acceptance of LGBTQ people and their sexual behaviors over the past 40 years, approximately 10% of the population consistently expresses conflicted feelings, reporting that same-sex sex is only sometimes wrong. This research employs a theory of socially structured ambivalence to examine how individuals with ambivalence toward the morality of same-sex sex differ from those with strong moral stances. Using multinomial regression analysis of General Social Survey data, we find that socio-structural conflicts-e.g. simultaneous membership in institutions with conflicting normative messages-are predictive of ambivalent attitudes, and the presence of these structured conflicts appears to have a cumulative effect. These findings provide evidence of the predictive power of socially structured conflicts in producing ambivalent attitudes and expand the existing literature on ambivalence and attitudes about same-sex relations. We propose that scholars conceptualize ambivalence as a distinctly socio-structural and relational construct that may help to signal fertile ground for social change.", "journal": "JOURNAL OF HOMOSEXUALITY", "category": "Psychology, Multidisciplinary; Social Sciences, Interdisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000462418200001", "keywords": "Inverse radiation-conduction problem; Graded index media; Kalman filter; Recursive least squares estimator; Time-dependent heat flux", "title": "Real-time estimation of time-dependent imposed heat flux in graded index media by KF-RLSE algorithm", "abstract": "Due to the large computational time caused by complex computational process of the existing inversion algorithm, real-time reconstruction of high-magnitude time-dependent heat flux in graded index media is quite challenging. In this study, based on hybrid technology of the Kalman filter and recursive least-square estimator (KF-RLSE), the real time reconstructed high-magnitude time-varying heat flux on graded index media surface, and the measurement information comes from the opposite side of the media. The ideal participating media, which is assumed to be isotropic scattering, constant thermophysical properties, and opaque and diffuse gray boundary, is employed to verify the reliability and validity of the proposed. All the reconstruction results show that the KF-RLSE algorithm can effectively reconstruct the boundary heat flux regardless of the positive or negative gradient of the refractive index. When the refractive index of each position increases or reduces, the transient heat flux on the surface can still be predicted effectively and acceptably. Furthermore, effects of different parameters on the accuracy and stability of the estimated results are also investigated. The reconstructed results show that the time-dependent heat flux can still be effectively reconstructed even when the measurement noise does not match its covariance. Meanwhile, the accuracy of the reconstruction results improves with the decrease of measurement noise covariance when the measurement noise distribution is fixed in a curtain range.", "journal": "APPLIED THERMAL ENGINEERING", "category": "Thermodynamics; Energy & Fuels; Engineering, Mechanical; Mechanics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000458707100005", "keywords": "Replica creation; Data scheduling; Replication management; Edge computing", "title": "Cost-effective replication management and scheduling in edge computing", "abstract": "The high volumes of data are continuously generated from Internet of Things (IoT) sensors in an industrial landscape. Especially, the data-intensive workflows from IoT systems require to be processed in a real-time, reliable and low-cost way. Edge computing can provide a low-latency and cost-effective computing paradigm to deploy workflows. Therefore, data replication management and scheduling for delay-sensitive workflows in edge computing have become challenge research issues. In this work, first, we propose a replication management system which includes dynamic replication creator, a specialized cost-effective scheduler for data placement, a system watcher and some data security tools for collaborative edge and cloud computing systems. And then, considering task dependency, data reliability and sharing, the data scheduling for the workflows is modeled as an integer programming problem. And we present the faster meta-heuristic algorithm to solve it. The experimental results show that our algorithms can achieve much better system performance than comparative traditional strategies, and they can create a suitable number of data copies and search the higher quality replica placement solution while reducing the total data access costs under the deadline constraint.", "journal": "JOURNAL OF NETWORK AND COMPUTER APPLICATIONS", "category": "Computer Science, Hardware & Architecture; Computer Science, Interdisciplinary Applications; Computer Science, Software Engineering", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000460859700001", "keywords": "co-sleeping; heart rhythm; phase synchronization; causal relation; non-linear dynamics", "title": "Human Heart Rhythms Synchronize While Co-sleeping", "abstract": "Human physiological systems have a major role in maintenance of internal stability. Previous studies have found that these systems are regulated by various types of interactions associated with physiological homeostasis. However, whether there is any interaction between these systems in different individuals is not well-understood. The aim of this research was to determine whether or not there is any interaction between the physiological systems of independent individuals in an environment where they are connected with one another. We investigated the heart rhythms of co-sleeping individuals and found evidence that in co-sleepers, not only do independent heart rhythms appear in the same relative phase for prolonged periods, but also that their occurrence has a bidirectional causal relationship. Under controlled experimental conditions, this finding may be attributed to weak cardiac vibration delivered from one individual to the other via a mechanical bed connection. Our experimental approach could help in understanding how sharing behaviors or social relationships between individuals are associated with interactions of physiological systems.", "journal": "FRONTIERS IN PHYSIOLOGY", "category": "Physiology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000461865500023", "keywords": "Cardiovascular magnetic resonance tissue tracking; Strain; Cardiovascular risk factors", "title": "The role of differential DMSP production and community composition in predicting variability of global surface DMSP concentrations", "abstract": "Dimethylsulfoniopropionate (DMSP) is an important labile component of the marine dissolved organic matter pool that is produced by the majority of eukaryotic marine phytoplankton and by many prokaryotes. Despite decades of research, the contribution of different environmental drivers of DMSP production to regional and seasonal variability remains unknown. A synthesis of the current state-of-knowledge suggested that approximately half of confirmed DMSP producers are low producers (intracellular DMSP < 50 mM). Low DMSP producers (LoDPs; e.g., diatoms) were shown to strongly regulate intracellular DMSP concentrations (similar to 16-fold change) as a predictable function of nutrient stress. By comparison, high DMSP producers (HiDPs; e.g., coccolithophores) showed very little response (similar to 1.5-fold change). To assess the importance of differential DMSP production by low and high producers, DMSP concentrations were predicted for two time-series sites (a high- and low-productivity site) and for the global ocean by explicitly incorporating both community composition and mechanistic nutrient stress. Despite large, predictable intracellular DMSP changes, low producers contributed less than 5% to global DMSP. This indicates that, while variations in DMSP production by low producers could be important for predicting microbial interactions and low producer physiology, it is not necessary for predicting global DMSP concentrations. Our analysis suggests that community composition, particularly HiDP biomass, is the dominant driver of variability in in situ DMSP concentrations, even in low-productivity regions where high producers are typically the subdominant group. Accurate predictions of in situ DMSP concentrations require improved representation of subdominant community dynamics in ecosystem models and remote-sensing algorithms.", "journal": "LIMNOLOGY AND OCEANOGRAPHY", "category": "Limnology; Oceanography", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000455561200111", "keywords": "2D group III nitrides; surface engineering buckled structure; piezoelectricity; density functional theory", "title": "Piezoelectric Effects in Surface-Engineered Two-Dimensional Group III Nitrides", "abstract": "Piezoelectric effects of two-dimensional (2D) group III V compounds have received considered attention in recent years because of their wide applications in semiconductor devices. However, they face a problem that only metastable or unstable structures are noncentrosymmetric with piezoelectricity, thus leading to the difficulty in experimental observation. Motivated by the recent advances in the synthesis of 2D group III nitrides, in this paper, for the first time, we study the piezoelectric properties of the 2D group III nitrides (XN, X = Al, Ga, and In) with buckled hexagonal configurations by surface passivation, which are thermodynamically stable. Unlike the previously reported planar graphitic structure, we demonstrate that the hydro genated 2D nitrides (H XN H, X = Al, Ga, and In) exhibit both the in plane and out-of-plane piezoelectric effects in their monolayer and multilayer plane. We further elucidate the underlying mechanism of the piezoelectricity by analyzing the correlations between the piezoelectric coefficients and their structural, electronic, and chemical properties. In addition, we show that H F cofunctionalization not only enhances the stability, but also significantly improves the ionic polarization because of the charge redistribution, thus leading to large in-plane piezoelectric coefficients in F XN H. Our study advances the research in 2D piezoelectric materials and would stimulate more theoretical and experimental efforts in developing effective piezoelectric materials for device applications.", "journal": "ACS APPLIED MATERIALS & INTERFACES", "category": "Nanoscience & Nanotechnology; Materials Science, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000457755200001", "keywords": "Expectation-maximization; Fourier-based Sieve estimator; latent reference groups; local covariate shift; Monte Carlo simulation; opinion space; SCAD; selectivity bias correction; semiparametric; Vox Populi; wisdom of crowds", "title": "Semiparametric Correction for Endogenous Truncation Bias With Vox Populi-Based Participation Decision", "abstract": "We synthesize the knowledge present in various scientific disciplines for the development of the semiparametric endogenous truncation-proof algorithm, correcting for truncation bias due to endogenous self-selection. This synthesis enriches the algorithm's accuracy, efficiency, and applicability. Improving upon the covariate shift assumption, data are intrinsically affected and largely generated by their own behavior (cognition). Refining the concept of Vox Populi (Wisdom of Crowd) allows data points to sort themselves out depending on their estimated latent reference group opinion space. Monte Carlo simulations, based on 2 000 000 different distribution functions, practically generating 100 million realizations, attest to a very high accuracy of our model.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000462015700017", "keywords": "chorus waves; radiation belt electrons; ring current electrons; analytical model; wave-particle interactions; diffusion coefficients", "title": "Analytical Chorus Wave Model Derived from Van Allen Probe Observations", "abstract": "Chorus waves play an important role in the dynamic evolution of energetic electrons in the Earth's radiation belts and ring current. Using more than 5 years of Van Allen Probe data, we developed a new analytical model for upper-band chorus (UBC; 0.5f(ce) < f < f(ce)) and lower-band chorus (LBC; 0.05f(ce) < f < 0.5f(ce)) waves, where f(ce) is the equatorial electron gyrofrequency. By applying polynomial fits to chorus wave root mean square amplitudes, we developed regression models for LBC and UBC as a function of geomagnetic activity (Kp), L, magnetic latitude (lambda), and magnetic local time (MLT). Dependence on Kp is separated from the dependence on lambda, L, and MLT as Kp-scaling law to simplify the calculation of diffusion coefficients and inclusion into particle tracing codes. Frequency models for UBC and LBC are also developed, which depends on MLT and magnetic latitude. This empirical model is valid in all MLTs, magnetic latitude up to 20 degrees, Kp <= 6, L-shell range from 3.5 to 6 for LBC and from 4 to 6 for UBC. The dependence of root mean square amplitudes on L are different for different bands, which implies different energy sources for different wave bands. This analytical chorus wave model is convenient for inclusion in quasi-linear diffusion calculations of electron scattering rates and particle simulations in the inner magnetosphere, especially for the newly developed four-dimensional codes, which require significantly improved wave parameterizations.", "journal": "JOURNAL OF GEOPHYSICAL RESEARCH-SPACE PHYSICS", "category": "Astronomy & Astrophysics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000458862200003", "keywords": "MIF; HIF-1 alpha; endometriosis; immunohistochemical", "title": "Long-term effects of different planting patterns on greenhouse soil micromorphological features in the North China Plain", "abstract": "Soil structure represents a basis for soil water retention and fertiliser availability. Here, we performed a micromorphological analysis of thin soil sections to evaluate the effects of 10 years of organic planting (OPP), pollution-free planting (PFP), and conventional planting (CPP) on greenhouse soil structure in the North China Plain. We also analysed soil bulk density, soil organic matter (SOM), and wet aggregate stability. The CPP soil microstructure included weakly separated angular block or plate forms and weak development of soil pores (fissured or simply accumulated pores) with the highest bulk density (1.33 g cm(-3)) and lowest SOM (26.76 g kg(-1)). Unlike CPP, the OPP soil microstructure was characterised by highly separated granular and aggregated structures and an abundance of plant and animal remains. OPP was associated with the highest total porosity (55.4%), lowest bulk density (1.17 g cm(-3)), and highest SOM (54.81 g kg(-1)) in the soil surface layer. OPP also improved the ventilation pore content (proportion of pores > 0.1 mm, 44.09%). OPP aggregates showed different hierarchies of crumb microstructure and higher mean weight diameter and geometric mean diameter values than did CPP. These results confirm the benefits of long-term OPP for soil structure and quality in the greenhouse.", "journal": "SCIENTIFIC REPORTS", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000476810800016", "keywords": "Bridge circuits; DC-AC power converters; modular multilevel converters; pulse width modulation converters; voltage control", "title": "A Novel Multilevel DC/AC Inverter Based on Three-Level Half Bridge With Voltage Vector Selecting Algorithm", "abstract": "A novel multilevel inverter based on a three-level half bridge is proposed for the DC/AC applications. For each power cell, only one DC power source is needed and five-level output AC voltage is realized. The inverter consists of two parts, the three-level half bridge, and the voltage vector selector, and each part consists of the four MOSFETs. Both positive and negative voltage levels are generated at the output, thus, no extra H bridges are needed. The switches of the three-level half bridge are connected in series, and the output voltages are (V-o, V-o/2, and 0). The voltage vector selector is used to output minus voltages (-V-o, and -V-o/2) by different conducting states. With complementary working models, the voltages of the two input capacitors are balanced. Besides, the power cell is able to be cascaded for more voltage levels and for higher power purpose. The control algorithm and two output strategies adopted in the proposed inverter are introduced, and the effectiveness is verified by simulation and experimental results.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000456899700125", "keywords": "Magnetic; Hydrophilic molecular imprinted resin; Benzoic acid; Aqueous sample; High performance liquid chromatography", "title": "Shell thickness controlled hydrophilic magnetic molecularly imprinted resins for high-efficient extraction of benzoic acids in aqueous samples", "abstract": "Hydrophilic magnetic molecular imprinted resins (MMIRs) were firstly fabricated by one-pot copolycondensation of resorcinol, melamine, and formaldehyde in the pores of Fe3O4 @mSiO(2). The porous structure of mSiO(2) limited copolycondensation. After the removal of mSiO(2), controllable imprinting shell thickness (about 5 nm) was formed, reducing the probability of embedded sites, favoring the efficient adsorption. Moreover, the reaction time (3 h) was just 1/5-1/8 of those synthesized by conventional heating method. The transmission electron microscopy (TEM), Fourier transform infrared (FT-IR), vibrating sample magnetometer (VSM), thermo-gravimetric analysis (TGA), water contact angle analysis, and Brunauer-Emmett-Teller (BET) analysis indicated the successful preparation of MMIRs with excellent hydrophilic property, and magnetic separation characteristics. Steady-state adsorption studies showed that the resultant MMIRs had specific recognition for benzoic acids, GA, protocatechuic acid (PCA), vanillic acid (VA), 4-hydroxybenzoic acid (4-HBA), salicylic acid (SA), and benzoic acid (BA). MMIRs combination with high-performance liquid chromatography ultraviolet (HPLC-UV) were then directly used for selective extraction and determination of benzoic acids in river water, fruit juices, and human serum. Obtained limits of detection (LOD) and recoveries were in the range of 0.02-1.0 mu g/mL, and 81.8-108.7%, respectively, and the batch-to-batch relative standard deviation (RSD) was 5.9%. The proposed strategy showed excellent hydrophilicity and specificity, which was promising for detection of benzoic acids in real aqueous samples.", "journal": "TALANTA", "category": "Chemistry, Analytical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000449877100001", "keywords": "vegetation trend; NDVI3g; v1; Mann-Kendall (MK) trend test; land cover types", "title": "Detecting Global Vegetation Changes Using Mann-Kendal (MK) Trend Test for 1982-2015 Time Period", "abstract": "Vegetation is the main component of the terrestrial ecosystem and plays a key role in global climate change. Remotely sensed vegetation indices are widely used to detect vegetation trends at large scales. To understand the trends of vegetation cover, this research examined the spatial-temporal trends of global vegetation by employing the normalized difference vegetation index (NDVI) from the Advanced Very High Resolution Radiometer (AVHRR) Global Inventory Modeling and Mapping Studies (GIMMS) time series (1982-2015). Ten samples were selected to test the temporal trend of NDVI, and the results show that in arid and semi-arid regions, NDVI showed a deceasing trend, while it showed a growing trend in other regions. Mann-Kendal (MK) trend test results indicate that 83.37% of NDVI pixels exhibited positive trends and that only 16.63% showed negative trends (P < 0.05) during the period from 1982 to 2015. The increasing NDVI trends primarily occurred in tree-covered regions because of forest growth and re-growth and also because of vegetation succession after a forest disturbance. The increasing trend of the NDVI in cropland regions was primarily because of the increasing cropland area and the improvement in planting techniques. This research describes the spatial vegetation trends at a global scale over the past 30+ years, especially for different land cover types.", "journal": "CHINESE GEOGRAPHICAL SCIENCE", "category": "Environmental Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000452941100039", "keywords": "Unsafe behavior; Statistical law; Human dynamics; Association rule; Apriori algorithm", "title": "Time-statistical laws of workers' unsafe behavior in the construction industry: A case study", "abstract": "The construction industry is extremely high risk, and the unsafe behavior of workers is thought to be a critical factor in that risk level. Considering the limitation that existing studies merely analyze the relationship between unsafe behavior and the time factor, this paper uses a case study to explore two aspects of the time-statistical laws of workers' unsafe behavior: (i) the characteristics of interevent time distributions and (ii) association rules for different worker types. First, workers' unsafe acts at one metro construction site are collected and classified. Second, interevent time distributions of workers' unsafe behavior from different types are analyzed via a human dynamics approach. Finally, a rule mining database is built, from which association rules concerning unsafe behavior, worker type and construction phase are determined using the Apriori algorithm. The results indicate that the interevent time distributions are fat tailed and show that workers' unsafe behavior has the characteristics of burstiness and memory. Furthermore, the strong rules 'construction phase -> unsafe behavior' exist for different worker types. The research presented in this paper can facilitate a better understanding of workers' unsafe behavioral patterns and can accordingly be used to control frequent and widespread unsafe acts among different types of workers in different construction phases. (C) 2018 Elsevier B.V. All rights reserved.", "journal": "PHYSICA A-STATISTICAL MECHANICS AND ITS APPLICATIONS", "category": "Physics, Multidisciplinary", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000456140600001", "keywords": "Kriging; Spatial correlation; Soil nutrients; Site-specific management; Saline-sodic soil", "title": "Spatial variability analysis and mapping of soil physical and chemical attributes in a salt-affected soil", "abstract": "Knowledge of inherent spatial variability of soil physical and chemical properties is needed for more accurate site-specific management of soil nutrients. In this study we investigated the spatial variability of a wide range of soil physical and chemical properties including soil texture fractions (percentages of sand, silt, and clay denoted as Sand, Silt and Clay, respectively), soil water content (WC), bulk density (BD), gypsum, organic carbon (OC), electrical conductivity (EC), pH, Ca, Mg, Na, exchangeable sodium percentage (ESP), sodium absorption ratio (SAR), available phosphorous (AP), and available potassium (AK) in a saline-alkaline soil catena in Sistan Plain, southeast of Iran. Soil samples were collected from two depths (0-15 and 15-30cm) on a nearly regular grid at 113 sites over an 85-ha agricultural field. Statistical analysis of soil properties showed that Na, Mg, Ca, WC, EC, ESP, and SAR have a large coefficient of variation (CV) (more than 50%) and BD and pH have a low CV (less than 15%) for both layers. The correlation among soil properties varies for two layers; while Silt, WC, EC, ESP, Na, and gypsum are statistically (p<0.01 and p<0.05) correlated with most of physical and chemical properties in topsoil, Sand, EC, and OC are the most dominant properties in subsoil. Geostatistical autocorrelation analysis of soil properties were examined based on the range of spatial continuity and nugget to sill ratio. Accordingly, AP and subsoil ESP have the lowest spatial correlation while texture fractions are the most auto-correlated variables in space. The spatial structure of soil properties followed either a spherical or an exponential model with a minimum correlation distance of 70m for AP to almost 800m for soil fractions. The results indicated that spatial continuity generally increases and decreases with depth for soil physical and chemical properties, respectively. The difference in spatial variability of soil properties could be attributed to internal factors (e.g., the forming processes of soil) as well as external factors (e.g., human activities). The maps of soil physical and quality parameters were generated using either kriging or inverse distance weighting methods depending on cross-validation results. In general, topsoil layer has a greater amount of EC, ESP, SAR, pH, Na, Ca, Mg, and OC than subsoil while Silt, WC, and gypsum were often higher in subsoil. OC maps showed that the whole area is relatively low in organic carbon, mainly due to hot and dry climate and windy conditions in Sistan. The maps of soil nutrients provide useful information for adapting an efficient and precision agricultural production management.", "journal": "ARABIAN JOURNAL OF GEOSCIENCES", "category": "Geosciences, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000453667200001", "keywords": "methamphetamine; addiction; neurotoxicity; vitamin B12; homocysteine", "title": "Vitamin B12 Levels in Methamphetamine Addicts", "abstract": "Objective: It has been established that reduced vitamin B12 serum levels are associated with cognitive decline and mental illness. The chronic use of methamphetamine (MA), which is a highly addictive drug, can induce cognitive impairment and psychopathological symptoms. There are few studies addressing the association of MA with vitamin B12 serum levels. This study examined whether the serum levels of B12 are associated with MA addiction. Methods: Serum vitamin B12, homocysteine (Hcy), glucose and triglyceride concentrations were measured in 123 MA addicts and 108 controls. In addition, data were collected on their age, marital status, level of education and Body Mass Index (BMI) for all participants. In the patient group, the data for each subject were collected using the Fagerstrom Test for Nicotine Dependence (FTND), the Alcohol Use Disorders Identification Test (AUDIT), and a drug use history, which included the age of onset, total duration of MA use, the number of relapses and addiction severity. Results: Our results showed that MA addicts had lower vitamin B12 levels (p < 0.05) than those of healthy controls, but Hcy levels were not significantly different between the two groups (p > 0.05). Serum B12 levels were negatively correlated with the number of relapses in the MA group. Furthermore, binary logistics regression analysis indicated that the B12 was an influencing factor contributing to addiction severity. Conclusion: The findings of this study suggest that some MA addicts might have vitamin B12 deficiency, and serum B12 levels may be involved in the prognosis of MA addiction.", "journal": "FRONTIERS IN BEHAVIORAL NEUROSCIENCE", "category": "Behavioral Sciences; Neurosciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000463820200003", "keywords": "enriched cable modeling; uncertainty quantification; cable fatigue; vortex shedding", "title": "Numerical framework for stress cycle assessment of cables under vortex shedding excitations", "abstract": "In this paper a novel and efficient computational framework to estimate the stress range versus number of cycles curves experienced by a cable due to external excitations (e.g., seismic excitations, traffic and wind-induced vibrations, among others) is proposed. This study is limited to the wind-cable interaction governed by the Vortex Shedding mechanism which mainly rules cables vibrations at low amplitudes that may lead to their failure due to bending fatigue damage. The algorithm relies on a stochastic approach to account for the uncertainties in the cable properties, initial conditions, damping, and wind excitation which are the variables that govern the wind-induced vibration phenomena in cables. These uncertainties are propagated adopting Monte Carlo simulations and the concept of importance sampling, which is used to reduce significantly the computational costs when new scenarios with different probabilistic models for the uncertainties are evaluated. A high fidelity cable model is also proposed, capturing the effect of its internal wires distribution and helix angles on the cables stress. Simulation results on a 15 mm diameter high-strength steel strand reveal that not accounting for the initial conditions uncertainties or using a coarse wind speed discretization lead to an underestimation of the stress range experienced by the cable. In addition, parametric studies illustrate the computational efficiency of the algorithm at estimating new scenarios with new probabilistic models, running 3000 times faster than the base case.", "journal": "WIND AND STRUCTURES", "category": "Construction & Building Technology; Engineering, Civil; Mechanics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000463769500006", "keywords": "Characteristic cones; Discrete nD systems; Autonomous systems; Algebraic methods; Affine semigroups", "title": "On characteristic cones of discrete nD autonomous systems: theory and an algorithm", "abstract": "In this paper, we provide a complete answer to the question of characteristic cones for discrete autonomous nD systems, with arbitrary n2, described by linear partial difference equations with real constant coefficients. A characteristic cone is a special subset (having the structure of a cone) of the domain (here Zn) such that the knowledge of the trajectories on this set uniquely determines them over the whole domain. Despite its importance in numerous system-theoretic issues, the question of characteristic sets for multidimensional systems has not been answered in its full generality except for Valcher's seminal work for the special case of 2D systems (Valcher in IEEE Trans Circuits and Syst Part I Fundam Theory Appl 47(3):290-302, 2000). This apparent lack of progress is perhaps due to inapplicability of a crucial intermediate result by Valcher to cases with n3. We illustrate this inapplicability of the above-mentioned result in Sect. 3 with the help of an example. We then provide an answer to this open problem of characterizing characteristic cones for discrete nD autonomous systems with general n; we prove an algebraic condition that is necessary and sufficient for a given cone to be a characteristic cone for a given system of linear partial difference equations with real constant coefficients. In the second part of the paper, we convert this necessary and sufficient condition to another equivalent algebraic condition, which is more suited from algorithmic perspective. Using this result, we provide an algorithm, based on Grobner bases, that is implementable using standard computer algebra packages, for testing whether a given cone is a characteristic cone for a given discrete autonomous nD system.", "journal": "MULTIDIMENSIONAL SYSTEMS AND SIGNAL PROCESSING", "category": "Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000461205100002", "keywords": "diagnosis; DNA methylation; lung cancer; prognosis; TMEM196", "title": "TMEM196 hypermethylation as a novel diagnostic and prognostic biomarker for lung cancer", "abstract": "Emerging evidences have revealed tumor-specific gene methylation is considered to be a promising non-invasive biomarker for many different types of cancers. This study was determined whether TMEM196 gene hypermethylation and downregulation are considered to be promising biomarkers for early diagnosis and prognosis in lung cancer. Methylation status was detected with methylation-specific PCR. Kaplan-Meier survival curves and Cox regression analysis were used to determine the significance of prognosis. TMEM196 gene was hypermethylated in 68.1% (64/94) of lung cancer tissues, 52.8% (67/127) of plasma and 55.2% (79/143) of sputum samples, but unmethylated (0/50) in normal tissues. TMEM196 methylation in plasma and sputum samples was significantly correlated with that in the corresponding paired tumor tissues (r = 0.750, r = 0.880, P < 0.001). TMEM196 aberrant methylation in cancer tissues, plasma and sputum DNA was significantly associated with age and pathological type (P < 0.05). TMEM196 high methylation could robustly distinguish lung cancer patients (AUC = 0.905) from normal subjects and patients with TMEM196 high methylation have a significantly poorer survival than those with low level from The Cancer Genome Atlas (Wilcoxon P < 0.001). Multivariate models showed TMEM196 methylation is an independent prognostic marker in lung cancer. Furthermore, the overall survival of patients with low TMEM196 expression was significantly poorer than that of TMEM196-high patients (P < 0.001, log-rank test). Low TMEM196 expression in tumor tissues was found to predict poorer survival (HR = 3.007; 95%CI, 1.918-4.714). Our study provided new insights into the clinical importance and potential use of TMEM196 methylation and expression as novel early diagnostic and prognostic biomarkers for human lung cancers.", "journal": "MOLECULAR CARCINOGENESIS", "category": "Biochemistry & Molecular Biology; Oncology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000454373700009", "keywords": "Transmission ultrasound; breast imaging; breast cyst fluid; ultrasound tomography", "title": "Breast Cyst Fluid Analysis Correlations with Speed of Sound Using Transmission Ultrasound", "abstract": "Rationale and Objectives: The purpose of this work is to determine if the speed of sound value of a breast cyst can aid in the clinical management of breast masses. Breast macrocysts are defined as fluid-filled tissue masses >1 cm in diameter and are thought to be aberrations of normal development and involution, often associated with apocrine metaplasia. The benign natural history of breast cysts is well known, and it is important to obtain high specificity in breast imaging to avoid unnecessary biopsies in women who have benign diseases, particularly those with dense breast tissue. Transmission ultrasound is a tomographic imaging modality that generates high-resolution, 3D speed of sound maps that could be used to identify breast tissue types and act as a biomarker to differentiate lesions. We performed this study to investigate the microanatomy of macrocysts observed using transmission ultrasound, as well as assess the relationship of speed of sound to the physical and biochemical parameters of cyst fluids. Materials and Methods: Cyst fluid samples were obtained from 37 patients as part of a case-collection study for ultrasound imaging of the breast. The speed of sound of each sample was measured using a quantitative transmission ultrasound scanner in vivo. Electrolytes, protein, cholesterol, viscosity, and specific gravity were also measured (in the aspirated cyst fluid) to assess their relationship to the speed of sound values obtained during breast imaging. Results: We found positive correlations between viscosity and cholesterol (r = 0.71) and viscosity and total protein x cholesterol (r = 0.78). Additionally, we performed direct cell counts on cyst fluids and confirmed a positive correlation of number of cells with speed of sound (r = 0.74). The speed of sound of breast macrocysts, as observed using transmission ultrasound, correlated with the cytological features of intracystic cell clumps. Conclusion: On the basis of our work with speed as a classifier, we propose a spectrum of breast macrocysts from fluid-filled to highly cellular. Our results suggest high-speed cysts are mature macrocysts with high cell counts and many cellular clumps that correlate with cyst microanatomy as seen by transmission ultrasound. Further studies are needed to confirm our findings and to assess the clinical value of speed of sound measurements in breast imaging using transmission ultrasound.", "journal": "ACADEMIC RADIOLOGY", "category": "Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000448036300023", "keywords": "Plasmonic biosensor; Surface plasmon resonance; Multiparameter; Particle swarm optimization; Modulation mode", "title": "Optimization methodology for structural multiparameter surface plasmon resonance sensors in different modulation modes based on particle swarm optimization", "abstract": "One of the main challenges in designing plasmonic biosensors is maximizing their sensing performance. This study proposes heuristic algorithms based on surface plasmon resonance-particle swarm optimization (SPR-PSO), which were investigated for the optimization of the sensing performance of structural multiparameter SPR sensors in four modulation modes (phase, intensity, wavelength, and angle). Different fitness functions were designed for different modulation modes that comprised a variety of evaluation indicators (such as sensitivity, figure of merit, full-width-at-half-maximum, electric field intensity, and penetration depth). Four types of available experimental structures representing the various modulation schemes were compared with the corresponding optimized structure by algorithms. The results showed that the introduced algorithms have a considerable efficiency. Furthermore, the algorithms also showed some potential in aiding the parametric design of negative refractive index materials.", "journal": "OPTICS COMMUNICATIONS", "category": "Optics", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000455568300029", "keywords": "Neuropsychological test; Cognitive function; Subarachnoid hemorrhage; Intracranial aneurysm; Neurointensive care", "title": "Predictors of cognitive function in the acute phase after aneurysmal subarachnoid hemorrhage", "abstract": "BackgroundCognitive dysfunction is the most common form of neurological impairment after aneurysmal subarachnoid hemorrhage (aSAH) in the chronic phase. Cognitive deficits in the acute phase after aSAH, however, remain scarcely investigated. The aim of the present study was to test cognitive function and to identify medical predictors of cognitive deficits in the acute phase of aSAH.MethodsProspective study including 51 patients treated for aSAH. Patients were treated in accordance with a standardized institutional protocol and subjected to neuropsychological evaluation around discharge from neurosurgical care. The neuropsychological test results were transformed into a global cognitive impairment index where an index value of 0.00 is considered normal and 1.00 is considered maximally pathological. Patients with an index score of less than 0.75 were considered having good global cognitive function while those with an index score equal to or above 0.75 were considered having poor global cognitive function. Univariate and multiple regression analysis were used to identify medical predictors of cognitive function.ResultsFifty-seven percent of the patients had poor cognitive function. They showed severe cognitive deficits, with most tests falling well below two standard deviations from the expected normal mean. Poor cognitive function was not reflected in a poor modified Rankin score in almost half of the cases. Patients with good cognitive function showed only mild cognitive deficits with most tests falling only slightly below the normal mean. Delayed memory was the most affected function in both groups. Univariate analysis identified acute hydrocephalus and aSAH-acquired cerebral infarction to be predictors of poor cognitive function. Cerebrospinal fluid drainage in excess of 2000ml six-folded the risk of poor cognitive function, whereas a new cerebral infarction 11-folded the respective risk of poor cognitive function.ConclusionMore than half of aSAH patients have severe cognitive deficits in the acute phase. The modified Rankin Score should be combined with neuropsychological screening in the acute phase after aSAH to get a more accurate description of the patients' disabilities. Acute hydrocephalus and aSAH-acquired cerebral infarction are the strongest predictors of poor cognitive function in the acute phase.", "journal": "ACTA NEUROCHIRURGICA", "category": "Clinical Neurology; Surgery", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000460829200051", "keywords": "glucose; non-invasive; minimally-invasive; spectroscopy; continuous monitoring; MARD; FDA; ISO 15197; plasmon resonance; fluorescence; ultrasound; metabolic heat conformation", "title": "The Progress of Glucose MonitoringA Review of Invasive to Minimally and Non-Invasive Techniques, Devices and Sensors", "abstract": "Current glucose monitoring methods for the ever-increasing number of diabetic people around the world are invasive, painful, time-consuming, and a constant burden for the household budget. The non-invasive glucose monitoring technology overcomes these limitations, for which this topic is significantly being researched and represents an exciting and highly sought after market for many companies. This review aims to offer an up-to-date report on the leading technologies for non-invasive (NI) and minimally-invasive (MI) glucose monitoring sensors, devices currently available in the market, regulatory framework for accuracy assessment, new approaches currently under study by representative groups and developers, and algorithm types for signal enhancement and value prediction. The review also discusses the future trend of glucose detection by analyzing the usage of the different bands in the electromagnetic spectrum. The review concludes that the adoption and use of new technologies for glucose detection is unavoidable and closer to become a reality.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000457302900072", "keywords": "VANET; parked vehicle; V2V; relay routing; shadowing effect", "title": "V2V Routing in a VANET Based on the Autoregressive Integrated Moving Average Model", "abstract": "With the development of vehicle networks, the information transmission between vehicles is becoming increasingly important. Many applications, particularly regarding security, are based on communication between vehicles. These applications have strict requirements for factors such as the quality of communication between vehicles and the time delay. Many theoretical communication protocols ignore the presence of buildings or other obstacles that are present during practical use, especially in urban areas. These obstacles can cause a signal to fade or even block direct communication. Many vehicles are often parked at the roadside. Because of their location, these parked vehicles can be used as relays to effectively reduce the shadowing effect caused by obstacles and even solve communication problems. In this paper, we study the problem of parked-vehicle-assistant relay routing communication in vehicle ad hoc networks. We propose an efficient parked vehicle assistant relay routing algorithm that is composed of four parts: a periodic Hello packet exchange mechanism, candidate relay list update, communication link quality evaluation, and candidate relay list selection. Simulation results reveal obvious advantages for indexes such as the quality of communication, success rate, and time delay.", "journal": "IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY", "category": "Engineering, Electrical & Electronic; Telecommunications; Transportation Science & Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000459941200251", "keywords": "IR-UWB radar sensor; movement disorder; hyperactivity; actigraphy", "title": "Quantified Activity Measurement for Medical Use in Movement Disorders through IR-UWB Radar Sensor", "abstract": "Movement disorders, such as Parkinson's disease, dystonia, tic disorder, and attention-deficit/hyperactivity disorder (ADHD) are clinical syndromes with either an excess of movement or a paucity of voluntary and involuntary movements. As the assessment of most movement disorders depends on subjective rating scales and clinical observations, the objective quantification of activity remains a challenging area. The purpose of our study was to verify whether an impulse radio ultra-wideband (IR-UWB) radar sensor technique is useful for an objective measurement of activity. Thus, we proposed an activity measurement algorithm and quantitative activity indicators for clinical assistance, based on IR-UWB radar sensors. The received signals of the sensor are sufficiently sensitive to measure heart rate, and multiple sensors can be used together to track the positions of people. To measure activity using these two features, we divided movement into two categories. For verification, we divided these into several scenarios, depending on the amount of activity, and compared with an actigraphy sensor to confirm the clinical feasibility of the proposed indicators. The experimental environment is similar to the environment of the comprehensive attention test (CAT), but with the inclusion of the IR-UWB radar. The experiment was carried out, according to a predefined scenario. Experiments demonstrate that the proposed indicators can measure movement quantitatively, and can be used as a quantified index to clinically record and compare patient activity. Therefore, this study suggests the possibility of clinical application of radar sensors for standardized diagnosis.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000460672300003", "keywords": "Opportunistic spectrum access; sensing cost; sensing uncertainty; cascading bandits", "title": "Cost-Aware Learning and Optimization for Opportunistic Spectrum Access", "abstract": "In this paper, we investigate cost-aware joint learning and optimization for multi-channel opportunistic spectrum access in a cognitive radio system. We investigate a discrete-time model where the time axis is partitioned into frames. Each frame consists of a sensing phase, followed by a transmission phase. During the sensing phase, the user is able to sense a subset of channels sequentially before it decides to use one of them in the following transmission phase. We assume the channel states alternate between busy and idle according to independent Bernoulli random processes from frame to frame. To capture the inherent uncertainty in channel sensing, we assume the reward of each transmission when the channel is idle is a random variable. We also associate random costs with sensing and transmission actions. Our objective is to understand how the costs and reward of the actions would affect the optimal behavior of the user in both offline and online settings, and design the corresponding opportunistic spectrum access strategies to maximize the expected cumulative net reward (i.e., reward-minus-cost). We start with an offline setting where the statistics of the channel status, costs, and reward are known beforehand. We show that the optimal policy exhibits a recursive double-threshold structure, and the user needs to compare the channel statistics with those thresholds sequentially in order to decide its actions. With such insights, we then study the online setting, where the statistical information of the channels, costs and reward are unknown a priori. We judiciously balance exploration and exploitation, and show that the cumulative regret scales in O(log T). We also establish a matched lower bound, which implies that our online algorithm is order-optimal. Simulation results corroborate our theoretical analysis.", "journal": "IEEE TRANSACTIONS ON COGNITIVE COMMUNICATIONS AND NETWORKING", "category": "Telecommunications", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000467252800018", "keywords": "Battery electric vehicle; in-wheel motor electric vehicle; power management strategy; stochastic programming; Markov processes; dynamic programming", "title": "A stochastic power management strategy with skid avoidance for improving energy efficiency of in-wheel motor electric vehicles", "abstract": "In this study, a stochastic power management strategy for in-wheel motor electric vehicles is proposed to reduce the energy consumption and increase the driving range, considering the unpredictable nature of the driving power demand. A stochastic dynamic programming approach, policy iteration algorithm, is used to create an infinite horizon problem formulation to calculate optimal power distribution policies for the vehicle. The developed stochastic dynamic programming strategy distributes the demanded power, P-dem between the front and rear in-wheel motors by considering states of the vehicle, including the vehicle speed and the front and the rear wheels' slip ratios. In addition, a skid avoidance rule is added to the power management strategy to maintain the wheels' slip ratios within the desired values. Undesirable slip ratios cause poor brake and traction control performances and therefore should be avoided. The resulting strategy consists of a time-invariant, rule-based controller which is fast enough for real time implementations, and additionally, it is not expensive to be launched since the future power demand is approximated without a need to vehicle communication systems or telemetric capability. A high-fidelity model of an in-wheel motor electric vehicle is developed in the Autonomie/Simulink environment for evaluating the proposed strategy. The simulation results show that the proposed stochastic dynamic programming strategy is more efficient in comparison to some benchmark strategies, such as an equal power distribution and generalized rule-based dynamic programming. The simulation results of different driving scenarios for the considered in-wheel motor electric vehicle show the proposed power management strategy leads to 3% energy consumption reduction in average, at no additional cost. If the resulting energy savings is considered for the total annual trips for the vehicle and also the total number of electric vehicles in the country, the proposed power management strategy has a significant impact.", "journal": "PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART D-JOURNAL OF AUTOMOBILE ENGINEERING", "category": "Engineering, Mechanical; Transportation Science & Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000462114200002", "keywords": "pyramid wavefront sensor; wavefront reconstruction; adaptive optics; iterative methods", "title": "Real-time adaptive optics with pyramid wavefront sensors: part II. Accurate wavefront reconstruction using iterative methods", "abstract": "In this paper, we address the inverse problem of fast, stable, and high-quality wavefront reconstruction from pyramid wavefront sensor data for adaptive optics systems on extremely large telescopes. For solving the indicated problem we apply well-known iterative mathematical algorithms, namely conjugate gradient, steepest descent, Landweber, Landweber-Kaczmarz and steepest descent-Kaczmarz iteration based on theoretical studies of the pyramid wavefront sensor. We compare the performance (in terms of correction quality and speed) of these algorithms in end-to-end numerical simulations of a closed adaptive loop. The comparison is performed in the context of a high-order SCAO system for METIS, one of the first-light instruments currently under design for the extremely large telescope. We show that, though being iterative, the analyzed algorithms, when applied in the studied context, can be implemented in a very efficient manner, which reduces the related computational effort significantly. We demonstrate that the suggested analytically developed approaches involving iterative algorithms provide comparable quality to standard matrix-vector-multiplication methods while being computationally cheaper.", "journal": "INVERSE PROBLEMS", "category": "Mathematics, Applied; Physics, Mathematical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000458469200010", "keywords": "Emotion labeling; Emotion labeling ability; Middle childhood; Poverty; Chronic poverty exposure; Intensity", "title": "Intensity, not emotion: The role of poverty in emotion labeling ability in middle childhood", "abstract": "Poverty exposure has been linked to difficulties in emotion expression recognition, which further increases risks for negative emotional outcomes among children. The current study aimed to investigate whether the difficulties in emotion expression recognition among children experiencing poverty may be emotion specific or expression intensity specific. Thus, the current study investigated the relationship between poverty exposure and emotion labeling ability in an ethnically and economically diverse sample of children (N = 46) in middle childhood. A novel experimental design measured emotion labeling ability at different valences of emotion (fearful, angry, and happy) and at varying intensities (0-100%) of emotion presentation. Using a hierarchical logistic regression, we found a significant interaction between the percentage of time since birth a child has lived in poverty and the intensity of the emotional stimulus in affecting correct emotion identification. Children who lived longer in poverty gained less accuracy for equivalent increases in intensity compared with children who had not lived in poverty. On average, children who chronically lived in poverty required emotional intensity set at 60% in order to reach levels of accuracy observed at 30% intensity among children who were never exposed to poverty. We found no significant emotion-specific effect. These findings demonstrate that children who experience chronic poverty require more intense expressions to recognize emotions across valences. This further elaborates the existing understanding of a relationship between poverty exposure and emotion recognition, informing future studies examining expression recognition as a mechanism involved in developing psychopathology. (C) 2019 Elsevier Inc. All rights reserved.", "journal": "JOURNAL OF EXPERIMENTAL CHILD PSYCHOLOGY", "category": "Psychology, Developmental; Psychology, Experimental", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000461427900004", "keywords": "Community children's nursing; complex needs; out-of-hours care; WellChild", "title": "Caring for children - '24-7': The experience of WellChild Nurses and the families for whom they are providing care and support", "abstract": "There is a growing population of children with complex health needs and disabilities who are being cared for at home by their parents 24 hours per day, 7 days per week. Community Children's Nursing Teams are a major source of support to these children. In 2006, the charity WellChild introduced the first WellChild Nurse (WCN) post specifically focused upon this group of children. In order to gain insight into how the WCN model was supporting this group of children throughout the 24-hour day, semi-structured interviews were undertaken with 12 WCNs and 10 parents of children with a range of long-term clinical care needs. Analysis of the interviews from both groups of study participants revealed complex patterns of decision-making by parents when seeking support and advice particularly 'out of hours'. This related to four key questions: 'Why call?', 'When to call?', 'Who to call?' and 'How to call?'. Parents identified how, as a result of the support provided by the WCNs, they are able to draw upon a range of decision-making skills and algorithms that enhance their ability to troubleshoot both clinical and non-clinical problems throughout the 24-hour day.", "journal": "JOURNAL OF CHILD HEALTH CARE", "category": "Nursing; Pediatrics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000454146000006", "keywords": "External labeling; boundary labeling; user study; readability", "title": "On the readability of leaders in boundary labeling", "abstract": "External labeling deals with annotating features in images with labels that are placed outside of the image and are connected by curves (so-called leaders) to the corresponding features. While external labeling has been extensively investigated from a perspective of automatization, the research on its readability has been neglected. In this article, we present the first formal user study on the readability of leader types in boundary labeling, a special variant of external labeling that considers rectangular image contours. We consider the four most studied leader types (straight, L-shaped, diagonal, and S-shaped) with respect to their performance, that is, whether and how fast a viewer can assign a feature to its label and vice versa. We give a detailed analysis of the results regarding the readability of the four models and discuss their aesthetic qualities based on the users' preference judgments and interviews. As a consequence of our experiment, we can generally recommend L-shaped leaders as the best compromise between measured task performance and subjective preference ratings, while straight and diagonal leaders received mixed ratings in the two measures. S-shaped leaders are generally not recommended from a practical point of view.", "journal": "INFORMATION VISUALIZATION", "category": "Computer Science, Software Engineering", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000457492600006", "keywords": "Petascale computing; Life time safety; Ultra high dams; Hydraulic engineering; Dual mortar method; Thermo-mechanical-contact coupling; Extended finite element method; Preconditioning; Linear solver", "title": "A challenging dam structural analysis: large-scale implicit thermo-mechanical coupled contact simulation on Tianhe-II", "abstract": "Due to huge bulk volume and extremely complex geometrical and geological features, it is forbiddingly difficult to perform a dam structural analysis with even moderate geometry fidelity in engineering practices. We present a high resolution of engineering structural analysis of the first ultra-high concrete-faced rockfill dam in China. Mesh resolution is taken to be 20cm along slab thickness for the bulk volume of 20M m3 of the whole dam. The engineering problem is solved by considering nonlinear behaviors such as joints' contact nonlinearity, creep deformation, and strong thermo-mechanical coupling, as well as blended continuous-discontinuous approximation, on a mesh model of 1.1 billion dofs using 16K CPU cores of Tianhe-II. The problem to be solved is a challenging non-positive definite, non-symmetric and ill-conditioned matrix problem. The simulation confirms in the first time that the sunlight temperature effect can contribute up to a contact stress increment of maximum 10.9 MPa and explains frequent extrusion damage observed for the dam. As model tests are difficult to perform for high dams, with this first success, we envision that extreme-scale simulation would pose broad impact on the safety evaluation of high dams in future.", "journal": "COMPUTATIONAL MECHANICS", "category": "Mathematics, Interdisciplinary Applications; Mechanics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000691062700565", "keywords": "Whitematter hyperintensities; Automated segmentation; Brain MRI; Aging; Vascular pathology; Small vessel disease", "title": "A new CT based tissue automatic segmentation method for absorbed dose calculation in preclinical radiation therapy", "abstract": "White matter hyperintensities (WMH) are commonly seen in the brain of healthy elderly subjects and patients with several neurological and vascular disorders. A truly reliable and fully automated method for quantitative assessment of WMH on magnetic resonance imaging (MRI) has not yet been identified. In this paper, we review and compare the large number of automated approaches proposed for segmentation of WMH in the elderly and in patients with vascular risk factors. We conclude that, in order to avoid artifacts and exclude the several sources of bias that may influence the analysis, an optimal method should comprise a careful preprocessing of the images, be based on multimodal, complementary data, take into account spatial information about the lesions and correct for false positives. All these features should not exclude computational leanness and adaptability to available data.", "journal": "RADIOTHERAPY AND ONCOLOGY", "category": "Oncology; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000356567500041", "keywords": "Minimum volume estimator; Outlier detection; Robust regression", "title": "Multi-Organ Contribution to the Metabolic Plasma Profile Using Hierarchical Modelling", "abstract": "Hierarchical modelling was applied in order to identify the organs that contribute to the levels of metabolites in plasma. Plasma and organ samples from gut, kidney, liver, muscle and pancreas were obtained from mice. The samples were analysed using gas chromatography time-of-flight mass spectrometry (GC TOF-MS) at the Swedish Metabolomics centre, Umea University, Sweden. The multivariate analysis was performed by means of principal component analysis (PCA) and orthogonal projections to latent structures (OPLS). The main goal of this study was to investigate how each organ contributes to the metabolic plasma profile. This was performed using hierarchical modelling. Each organ was found to have a unique metabolic profile. The hierarchical modelling showed that the gut, kidney and liver demonstrated the greatest contribution to the metabolic pattern of plasma. For example, we found that metabolites were absorbed in the gut and transported to the plasma. The kidneys excrete branched chain amino acids (BCAAs) and fatty acids are transported in the plasma to the muscles and liver. Lactic acid was also found to be transported from the pancreas to plasma. The results indicated that hierarchical modelling can be utilized to identify the organ contribution of unknown metabolites to the metabolic profile of plasma.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000348214500093", "keywords": "catalogs; cosmology: observations; galaxies: clusters: general; large-scale structure of universe", "title": "STRUCTURE IN THE 3D GALAXY DISTRIBUTION. II. VOIDS AND WATERSHEDS OF LOCAL MAXIMA AND MINIMA", "abstract": "The major uncertainties in studies of the multi-scale structure of the universe arise not from observational errors but from the variety of legitimate definitions and detection methods for individual structures. To facilitate the study of these methodological dependencies, we have carried out 12 different analyses defining structures in various ways. This has been done in a purely geometrical way by utilizing the HOP algorithm as a unique parameter-free method of assigning groups of galaxies to local density maxima or minima. From three density estimation techniques (smoothing kernels, Bayesian blocks, and self-organizing maps) applied to three data sets (the Sloan Digital Sky Survey Data Release 7, the Millennium simulation, and randomly distributed points) we tabulate information that can be used to construct catalogs of structures connected to local density maxima and minima. We also introduce a void finder that utilizes a method to assemble Delaunay tetrahedra into connected structures and characterizes regions empty of galaxies in the source catalog.", "journal": "ASTROPHYSICAL JOURNAL", "category": "Astronomy & Astrophysics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000357586600012", "keywords": "Input estimation; Convolution; Deconvolution; Stability; Conditioning; Posedness", "title": "Mitigating Observation Perturbation Sampling Errors in the Stochastic EnKF", "abstract": "The stochastic ensemble Kalman filter (EnKF) updates its ensemble members with observations perturbed with noise sampled from the distribution of the observational errors. This was shown to introduce noise into the system and may become pronounced when the ensemble size is smaller than the rank of the observational error covariance, which is often the case in real oceanic and atmospheric data assimilation applications. This work introduces an efficient serial scheme to mitigate the impact of observations' perturbations sampling in the analysis step of the EnKF, which should provide more accurate ensemble estimates of the analysis error covariance matrices. The new scheme is simple to implement within the serial EnKF algorithm, requiring only the approximation of the EnKF sample forecast error covariance matrix by a matrix with one rank less. The new EnKF scheme is implemented and tested with the Lorenz-96 model. Results from numerical experiments are conducted to compare its performance with the EnKF and two standard deterministic EnKFs. This study shows that the new scheme enhances the behavior of the EnKF and may lead to better performance than the deterministic EnKFs even when implemented with relatively small ensembles.", "journal": "MONTHLY WEATHER REVIEW", "category": "Meteorology & Atmospheric Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000356616900022", "keywords": "complex mode indication function; damping estimate; experimental modal analysis; frequency response function matrix; singular values decomposition", "title": "THE ROLE OF SINGULAR VALUES OF MEASURED FREQUENCY RESPONSE FUNCTION MATRIX IN MODAL DAMPING ESTIMATION (PART I: THEORY)", "abstract": "The singular value decomposition of the measured frequency response function matrix, as a very effective tool of experimental modal analysis is used over the last twenty-five years. The complex mode indication function has become a common numerical tool in processing experimental data. There are many references on the development of complex mode indication function including the enhanced mode indication function and its use together with the enhanced frequency response function to form spatial domain modal parameter estimation methods. Another amendment of the enhanced mode indicator function method is the extension of the single degree-of-freedom aspects of the complex mode indication function method to include a limited number of modes. In the paper, methods for estimation of damped eigen frequencies, modal damping and mode shapes are presented that are based on singular value decomposition of frequency response function matrix. It is shown how to obtain phase information for the complex mode indication function, in order to use the standard single degree-of-freedom modal parameter estimation methods. New aggregated frequency response function is introduced. A least-squares approximation will be presented for eliminating the error caused by frequency discretization. Analytical models and examples taken from vehicle industry are also used to demonstrate applications of the aggregated frequency response function method and estimation of modal damping.", "journal": "TEHNICKI VJESNIK-TECHNICAL GAZETTE", "category": "Engineering, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000358150400077", "keywords": "Mass concrete; Thermal field; Heat-fluid coupling model; Cooling pipe system; Monitoring temperature", "title": "A Study on Serum Antithyroglobulin Antibodies Interference in Thyroglobulin Measurement in Fine-Needle Aspiration for Diagnosing Lymph Node Metastasis in Postoperative Patients", "abstract": "Purpose Thyroglobulin measurement in fine-needle aspiration washout fluid (FNA-Tg) is widely used for detection of lymph node metastasis (LNM) in patients with papillary thyroid cancer (PTC). Recent studies suggested that serum anti-thyroglobulin antibodies (TgAbs) could interfere with FNA-Tg. We evaluated whether TgAbs can affect FNA-Tg when diagnosing LNM in postoperative patients with PTC. Methods From November 2006 to June 2011, a total of 239 LNs from 201 patients who underwent bilateral thyroidectomy and radioactive iodine ablation therapy were included. The interactions between FNA-Tgs and serum TgAbs, and diagnostic performances between FNA with additional FNA-Tg and FNA alone according to the presence of serum TgAbs were evaluated using the generalized linear mixed model and the bootstrap method. Results From 106 (44.4%) malignant and 133 (55.6%) benign LNs, there were 32 (13.4%) LNs with detectable serum TgAb levels and 207 (86.6%) LNs with undetectable serum TgAb levels. In logistic regression analysis, a significant negative interaction was observed between FNA-Tgs and serum TgAbs (p = 0.031). In the absence of serum TgAbs, the diagnostic performances were superior in the FNA with FNA-Tg than in the FNA only. However, in the presence of serum TgAbs, the diagnostic performances of the FNA with FNA-Tg were not significantly different from the FNA only, even with a different cutoff value of FNA-Tg. Conclusions Serum TgAbs may interfere with FNA-Tg studies and caution is advised while analyzing FNA-Tg for detection of LNM in patients with PTC.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000357390400006", "keywords": "Project management; Evaluation; Big Data; Ex post", "title": "Use of big data in project evaluations", "abstract": "Purpose - The purpose of this paper is to investigate how Big Data can be used in project evaluations. Design/methodology/approach - The study is based on literature research and interviews with 15 professionals in IT, project and asset management and government agencies. The authors discuss and illustrate what data that can be used for project evaluations and discuss potential obstacles. Findings - New data is creating new opportunities to analyse a phenomenon based on different types of data. Interesting data categories include: internet traffic, movement-related data, physical environment data and data in organisational internal systems. The authors show how these data categories can be applied in project evaluations. Research limitations/implications - Big Data gives an opportunity to add quantitative data in ex post evaluations. Use of Big Data can serve as a step towards a stronger technology focus in evaluations of projects. Practical implications - There are major advantages in using Big Data, increasing the opportunities to find indicators that are relevant when a project is evaluated. Social implications - Possible problematic issues related to use of Big Data that are addressed in the study include: availability, applicability, relevance, privacy policy, ownership, cost and competence. The study indicates that none of the challenges need to hinder use of Big Data when evaluating projects, provided that the issues are properly managed. Originality/value - The study illustrates how Big Data can be applied in project management research.", "journal": "INTERNATIONAL JOURNAL OF MANAGING PROJECTS IN BUSINESS", "category": "Business; Management", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000351663600005", "keywords": "CMOS image sensor; high frame rate; HDR; relative acquisition; contrast imaging; machine vision", "title": "Dynamics of excited instantons in the system of forced Gursey nonlinear differential equations", "abstract": "The Gursey model is a 4D conformally invariant pure fermionic model with a nonlinear spinor self-coupled term. Gursey proposed his model as a possible basis for a unitary description of elementary particles following the \"Heisenberg dream.\" In this paper, we consider the system of Gursey nonlinear differential equations (GNDEs) formed by using the Heisenberg ansatz. We use it to understand how the behavior of spinor-type Gursey instantons can be affected by excitations. For this, the regular and chaotic numerical solutions of forced GNDEs are investigated by constructing their Poincar, sections in phase space. A hierarchical cluster analysis method for investigating the forced GNDEs is also presented.", "journal": "JOURNAL OF EXPERIMENTAL AND THEORETICAL PHYSICS", "category": "Physics, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000356585300003", "keywords": "gallium compounds; III-V semiconductors; p-n junctions; avalanche diodes; gain control; gain measurement; radio frequency avalanche gain; GaN-based PN junction diodes; University of Notre Dame; Avogy Inc; vertical p plus -n structure; GaN substrate; GaN", "title": "gaN gains", "abstract": "This study studies the online adaptive optimal control problems for a class of continuous-time Markov jump linear systems (MJLSs) based on a novel policy iteration algorithm. By utilising a new decoupling technique named subsystems transformation, the authors re-construct the MJLSs and a set of new coupled systems composed of N subsystems are obtained. The online policy iteration algorithm was used to solve the coupled algebraic matrix Riccati equations with partial knowledge regarding to the system dynamics, and the relevant optimal controllers equivalent to the investigated MJLSs are designed. Moreover, the convergence of the novel policy iteration algorithm is also established. Finally, a simulation example is given to illustrate the effectiveness and applicability of the proposed approach.", "journal": "ELECTRONICS LETTERS", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000351972100006", "keywords": "falls risk; elderly; women; stance", "title": "Identifying Fallers and Nonfallers With the Maximal Base of Support Width (BSW): A One-year Prospective Study", "abstract": "The purpose of this prospective cohort study was to determine whether the maximal width of the base of support (BSW) measure is able to predict the risk of multiple falls in community-dwelling women. Thirty-eight community-dwelling women (mean age of 72 +/- 8 years old) participated. Falls were prospectively recorded during the following year. Overall, 29 falls were recorded; six (16%) women were multiple fallers and 32 (84%) were nonfallers. There was a significant difference in the BSW between the fallers and nonfallers (F[1, 37] = 5.134 [p = .030]). A logistic regression analysis indicated a significant contribution of the BSW test to the model (odds ratio = 0.637; 95% CI [0.407, 0.993]; p = .046 per 1 cm). The cut-off score was determined to be 27.8 cm (67% sensitivity and 84% specificity). These results indicate that women with a smaller BSW at baseline had a significantly higher risk of sustaining a fall.", "journal": "JOURNAL OF AGING AND PHYSICAL ACTIVITY", "category": "Geriatrics & Gerontology; Gerontology; Sport Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000362421500130", "keywords": "Maize (Zea mays L.); Quantitative trait locus; Recombinant inbred line; Kernel thickness", "title": "Genetic analysis of maize kernel thickness by quantitative trait locus identification", "abstract": "Kernel thickness is one of the most important traits in kernel structure, and is related to yield. To ascertain its genetic information more clearly, an immortal recombinant inbred line segregation population was used to map the quantitative trait loci (QTLs) for kernel thickness. As a result, two QTLs were identified on chromosome 9; both of them had negative additive effects, and could decrease kernel thickness to some extent. The QTLs explained 25.8% of the total phenotypic variation. These results advance our understanding of the genetic basis of kernel thickness in maize-breeding programs.", "journal": "GENETICS AND MOLECULAR RESEARCH", "category": "Biochemistry & Molecular Biology; Genetics & Heredity", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000351848600014", "keywords": "agricultural machinery operation; computing processes; unit operating costs; Bayesian networks", "title": "IMPLEMENTATION OF COMPUTATION PROCESS IN A BAYESIAN NETWORK ON THE EXAMPLE OF UNIT OPERATING COSTS DETERMINATION", "abstract": "In technical systems understood in terms of Agile Systems, the important elements are information flows between all phases of an object existence. Among these information streams computation processes play an important role and can be done automatically and also in a natural way should include consideration of uncertainty. This article presents a model of such a process implemented in a Bayesian network technology. The model allows the prediction of the unit costs of operation of a combine harvester based on the monitoring of dependent variables. The values of the decision variables representing the parameters of the machine's operation and the intensity and the conditions for its operation, are known to an accuracy, which is defined by a probability distribution. The study shows, using inference mechanisms built into the network, how cost simulation studies of various situational options can be carried out.", "journal": "EKSPLOATACJA I NIEZAWODNOSC-MAINTENANCE AND RELIABILITY", "category": "Engineering, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000357045400002", "keywords": "Practical tracking; Uncertain nonlinear systems; Sampled-data control; Output feedback", "title": "Zero-temperature configurations of short odd-numbered classical spin chains with bilinear and biquadratic exchange interactions", "abstract": "The lowest energy configurations of short odd open chains with classical spins are determined for antiferromagnetic bilinear and biquadratic nearest-neighbor exchange interactions. The zero field residual magnetization generates differences with the magnetic behavior of even chains, as the odd chain is like a small magnet for weak magnetic fields. The lowest energy configuration is calculated as a function of the total magnetization M, even for M less than the zero field residual magnetization. Analytic expressions and their proofs are provided for the threshold magnetic field needed to drive the system away from the antiferromagnetic configuration and the spin polar angles in its vicinity, when the biquadratic interaction is relatively weak. They are also given for the saturation magnetic field and the spin polar angles close to it. Finally, an analytic expression along with its proof is given for the maximum magnetization in zero magnetic field for stronger biquadratic interaction, where the lowest energy configuration is highly degenerate.", "journal": "EUROPEAN PHYSICAL JOURNAL B", "category": "Physics, Condensed Matter", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000349493000011", "keywords": "Single nucleotide polymorphisms; gastrointestinal bleeding; SLCO1B1*1b haplotype; and CHST2", "title": "Plasma Biomarker of Dietary Phytosterol Intake", "abstract": "Background Dietary phytosterols, plant sterols structurally similar to cholesterol, reduce intestinal cholesterol absorption and have many other potentially beneficial biological effects in humans. Due to limited information on phytosterol levels in foods, however, it is difficult to quantify habitual dietary phytosterol intake (DPI). Therefore, we sought to identify a plasma biomarker of DPI. Methods and Findings Data were analyzed from two feeding studies with a total of 38 subjects during 94 dietary periods. DPI was carefully controlled at low, intermediate, and high levels. Plasma levels of phytosterols and cholesterol metabolites were assessed at the end of each diet period. Based on simple ordinary least squares regression analysis, the best biomarker for DPI was the ratio of plasma campesterol to the endogenous cholesterol metabolite 5-alpha-cholestanol (R-2 = 0.785, P < 0.0001). Plasma campesterol and 5-alpha-cholestanol levels varied greatly among subjects at the same DPI level, but were positively correlated at each DPI level in both studies (r > 0.600; P < 0.01). Conclusion The ratio of plasma campesterol to the coordinately regulated endogenous cholesterol metabolite 5-a-cholestanol is a biomarker of dietary phytosterol intake. Conversely, plasma phytosterol levels alone are not ideal biomarkers of DPI because they are confounded by large inter-individual variation in absorption and turnover of non-cholesterol sterols. Further work is needed to assess the relation between non-cholesterol sterol metabolism and associated cholesterol transport in the genesis of coronary heart disease.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000354859500006", "keywords": "philosophy of science; big data; theoretical progress; organizational research", "title": "Editorial Essay: What Is Organizational Research For?", "abstract": "Organizational research is guided by standards of what journals will publish and what gets rewarded in scholarly careers. This system can promote novelty rather than truth and impact rather than coherence. The advent of big data, combined with our current system of scholarly career incentives, is likely to yield a high volume of novel papers with sophisticated econometrics and no obvious prospect of cumulative knowledge development. Moreover, changes in the world of organizations are not being met with changes in how and for whom organizational research is done. It is time for a dialogue on who and what organizational research is for and how that should shape our practice.", "journal": "ADMINISTRATIVE SCIENCE QUARTERLY", "category": "Business; Management", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000359390400012", "keywords": "Physical activity; Prevalence of obesity; Body Mass Index; Korea", "title": "ASSOCIATION BETWEEN LEISURE-TIME PHYSICAL ACTIVITIES AND OBESITY IN A SELECTED SAMPLE OF KOREAN ADULTS", "abstract": "The aim of this study was to determine whether leisure-time physical activities (PAs) affect obesity in Korean adults. The participants included 505 men and 1,061 women (>20 years) who visited a public health centre in Seoul during 2010-2011. They completed the International Physical Activity Questionnaire and their Body Mass Index was calculated. Obesity was defined according to the current World Health Organization criteria. The association was assessed using multivariate logistic regression analysis after adjustment for sex, age, smoking/drinking, sleep duration, mental stress, education and economic status. Odds ratios (95% confidence interval) for the association between obesity and vigorous, moderate, and light PA compared to those who do not participate in any PA were: 0.931 (p=0.691), 0.893 (p=0.531) and 0.815 (p=0.302) for once/week, respectively; 0.940 (p=0.789), 0.690 (p=0.129), and 0.787 (p=0.342) for twice/week, respectively; 1.031 (p=0.897), 1.375 (p=0.137) and 1.180 (p=0.473) for three times/week, respectively; 1.109 (p=0.759), 0.804 (p=0.491), and 0.907 (p=0.763) for four times/week, respectively; and 0.357 = 0.006), 0.509 (p=0.034), and 0.641 (p=0.038) for >five times/week, respectively. Vigorous, moderate, and light PA >five times/week may reduce or prevent obesity in Korean adults.", "journal": "SOUTH AFRICAN JOURNAL FOR RESEARCH IN SPORT PHYSICAL EDUCATION AND RECREATION", "category": "Social Sciences, Interdisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000352158200012", "keywords": "clustering; requirements engineering; survey; tools", "title": "Commonalities and Differences between Requirements Engineering Tools: A Quantitative Approach", "abstract": "System and software developers are concerned to gain insight into how current requirements engineering (RE) tools support processes. There is an important number of RE tools currently available on the market but, unfortunately, existing RE tool lists do not usually provide detailed and precise information about the tools they catalogue. In this paper, we study and compare current RE tools in the quest to answer the following research question: What level of variation, in terms of functionality, is observable in state-of-practice RE tools? A 188-item survey was designed, aimed at major tool vendors worldwide and based principally on the features covered by the ISO/IEC TR 24766:2009. Extensive data obtained from 29 participants was used to classify and group the RE tools, based on their capabilities. First of all, an inter-rater reliability analysis was performed to ensure the trustworthiness of the data. Descriptive statistics, hierarchical cluster analysis and statistical hypothesis testing were then applied. The tool scores for each candidate were calculated. A total of three clusters were identified. Statistically significant differences in coverage of features among these groups came to light. Our findings can help practitioners to decide which tool is the most suitable among several alternatives, according to their particular needs.", "journal": "COMPUTER SCIENCE AND INFORMATION SYSTEMS", "category": "Computer Science, Information Systems; Computer Science, Software Engineering", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000356964000006", "keywords": "complex regional pain syndrome; CRPS; injury; immobilization; fracture; risk factors; population-based case-control study", "title": "Hormonal factors and incident asthma and allergic rhinitis during puberty in girls", "abstract": "Background: Accumulating evidence is indicating that hormonal factors play a role in new-onset allergic rhinitis and asthma after puberty. Objective: To determine whether age at menarche and use of hormonal contraceptives predict new-onset allergic rhinitis and asthma after puberty in young German women. Methods: A prospective community-based cohort study followed 1,191 girls 9 to 11 years old to early adulthood (19-24 years old). Self-administrated questionnaires concerning age at menarche, use of hormonal contraceptives, and status and age at onset of physician-diagnosed allergic rhinitis and asthma were collected at 16 to 18 and 19 to 24 years of age. Logistic regression models were used to analyze the incidence of asthma and allergic rhinitis after puberty and pooled estimates were obtained from the final model. Results: Eleven percent of girls developed allergic rhinitis after menarche and 3% reported new-onset asthma. Late menarche (>13 years of age) was statistically significantly inversely related to allergic rhinitis (adjusted odds ratio [OR] 0.32, 95% confidence interval [CI] 0.14-0.74) but did not reach the level of statistical significance for asthma (OR 0.32, 95% CI 0.07-1.42). Use of hormonal contraceptives was inversely associated with new-onset allergic rhinitis (OR 0.14, 95% CI 0.08-0.23) and asthma (OR 0.27, 95% CI 0.12-0.58) after puberty. Conclusion: This study shows that girls with late onset of menarche are less likely to develop allergic rhinitis after puberty compared with those who have menarche at an average age. These findings also suggest that, in addition to endogenous hormones, hormonal contraceptives play a role and might protect young women from allergies and asthma. (C) 2015 American College of Allergy, Asthma & Immunology. Published by Elsevier Inc. All rights reserved.", "journal": "ANNALS OF ALLERGY ASTHMA & IMMUNOLOGY", "category": "Allergy; Immunology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000357816400005", "keywords": "VAR model; compositional data; isometric log-ratio transformation; Granger causality", "title": "Modeling Compositional Time Series with Vector Autoregressive Models", "abstract": "Multivariate time series describing relative contributions to a total (like proportional data) are called compositional time series. They need to be transformed first to the usual Euclidean geometry before a time series model is fitted. It is shown how an appropriate transformation can be chosen, resulting in coordinates with respect to the Aitchison geometry of compositional data. Using vector autoregressive models, the standard approach based on raw data is compared with the compositional approach based on transformed data. The results from the compositional approach are consistent with the relative nature of the observations, while the analysis of the raw data leads to several inconsistencies and artifacts. The compositional approach is extended to the case when also the total of the compositional parts is of interest. Moreover, a concise methodology for an interpretation of the coordinates in the transformed space together with the corresponding statistical inference (like hypotheses testing) is provided. Copyright (C) 2015 John Wiley & Sons, Ltd.", "journal": "JOURNAL OF FORECASTING", "category": "Economics; Management", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000350710900003", "keywords": "Electromyography; rehabilitation; robot; torque control; angle tracking", "title": "Torque Control by EMG Feedback in Normal and Stroke Subjects Performing Ankle Angle Tracking with a Rehabilitation Robot", "abstract": "The goal of this study was to investigate the eligibility of using surface electromyography (EMG) as a feedback signal for ankle torque control in both normal and stroke subjects performing ankle joint angle tracking with a rehabilitation robot. The potential advantage of using EMG as an estimator of active torque is direct facilitation of torque control of individual muscle. A fuzzy PD+I controller was implemented to control the robot. A static EMG-torque map was constructed at 5 ankle positions experimentally for each subject. The map was interpolated to estimate the active dorsiflexion torque exerted on the ankle from the EMG of tibialis anterior muscle. Both EMG and the output of the torque sensor were acquired and a weighting factor was used to adjust the relative contribution from these two signals for controlling the robot. Six normal subjects and seven stroke patients were recruited. The angle trajectory to be tracked was alternating ramps (3 degrees/s) of dorsiflexion and plantarflexion. The results showed that all the tested combinations of EMG and the torque sensor signal could be used as the feedback signal for torque control during angle tracking with the robot. The normal subjects had a better performance than the stroke patients and the tracking performance was better when only the signal from the torque sensor was used.", "journal": "JOURNAL OF THE CHINESE SOCIETY OF MECHANICAL ENGINEERS", "category": "Engineering, Mechanical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000351190700010", "keywords": "Withdrawal; Coitus interruptus; Young adults; Pregnancy ambivalence; Sexual pleasure", "title": "Which young adults are most likely to use withdrawal? The importance of pregnancy attitudes and sexual pleasure", "abstract": "Objectives: Use of withdrawal (coitus intenuptus) has consequences for reproductive health, but few nationally representative studies exist. We (1) examined patterns of withdrawal among 15- to 24-year-old women and men, and (2) explored withdrawal's associations with sociodemographic, psychological, and sexual factors. Study design: Using data from the 2006-2010 National Survey of Family Growth, we assessed reports of any and only withdrawal use at last sexual episode in the last month from 3517 sexually active 15 to 24 year-old women and men at risk of unintended pregnancy. Logistic regression documented associations with withdrawal. Results: Fourteen percent of young women and 17% of young men reported any use of withdrawal at last sex; 7% and 6%, respectively, reported only use of withdrawal. Though associated with few sociodemographic factors, withdrawal was significantly linked with pregnancy-and condom attitudes. In regression models, compared to those who said they would be upset if they discovered they were pregnant, young women who said they would be pleased about a pregnancy were 2.2-2.6 times as likely to have used any/only withdrawal (p<.01). For both women and men, those who felt that condoms were likely to diminish sexual pleasure were more likely to have used any/only withdrawal (odds ratio=1.8-2.6, p<.05). Conclusions: At their last sexual episode, a greater proportion of young adults used withdrawal in conjunction with other methods than by itself. The psychological and sexual variables of orientation toward pregnancy and attitudes about condoms and pleasure were more strongly linked with withdrawal practices than most sociodemographic variables. Implications statement: Since a substantial minority of young adults use withdrawal, providers may wish to speak directly to contraceptive clients about this method, though they should distinguish between only versus any withdrawal use. Practitioners may also be well served by assessing and responding to pregnancy orientation and pleasure attitudes in contraceptive counseling. (C) 2015 Elsevier Inc. All rights reserved.", "journal": "CONTRACEPTION", "category": "Obstetrics & Gynecology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000345743100013", "keywords": "blowdown wind tunnel; flow control; extended Kalman filter", "title": "Advanced Flow Control for Supersonic Blowdown Wind Tunnel Using Extended Kalman Filter", "abstract": "Supersonic blowdown wind tunnels provide controlled test environments for aerodynamic research on scaled models. During the experiments, the stagnation pressure in the test section is required to remain constant. Due to nonlinearity and distributed characteristics of the controlled system, a robust controller with effective flow control algorithms is required for this type of wind tunnels. In this paper, an extended Kalman filter (EKF) based flow control strategy is proposed and implemented. The control strategy is designed based on state estimation of the blowdown process under the EKF structure. One of the distinctive advantages of the proposed approach is its adaptability to a wide range of operating conditions for blowdown wind tunnels. Furthermore, it provides a systematic approach to tune the control parameters to ensure the stability of the controlled air flow. Experiments with different initial conditions and control targets have been conducted to test the applicability and performance of the designed controller. The results demonstrate that the controller and its strategies can effectively control the stagnation pressure in the test section and maintain the target pressure during the stable stage of the blowdown process.", "journal": "JOURNAL OF DYNAMIC SYSTEMS MEASUREMENT AND CONTROL-TRANSACTIONS OF THE ASME", "category": "Automation & Control Systems; Instruments & Instrumentation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000355093400012", "keywords": "Breast reconstruction; Tachycardia; TRAM; Autologous; Microvascular; IMA", "title": "Tachycardia in breast reconstructive microsurgery: Affirmation of the IMA tachycardia syndrome", "abstract": "Introduction: The internal mammary vessels are frequently chosen as recipient vessels for breast free flap reconstruction. We have noticed that when using the internal mammary recipients that these patients have a propensity for tachycardia that was not previously observed. Our aim was to investigate the factors related to perioperative tachycardia in the microsurgical breast reconstruction population and to address whether use of the internal mammary system is a causative factor in tachycardia. Methods: A retrospective chart review was conducted to identify patients who underwent abdominal-based microvascular breast reconstruction at the Washington University School of Medicine between 2002 and 2012 to identify the presence of tachycardia. After application of exclusion criteria, 76 microvascular abdominal-based free flap reconstructions were identified. The internal mammary (IM) TRAM group (n=24) and the thoracodorsal (TD) TRAM group (n=52) were compared. A binomial logistic regression was performed with the presence of tachycardia as the dependent variable. Results: There was a higher incidence of tachycardia in the IM TRAM group when compared to the TD TRAM group (p=0.004). The variables predictive of tachycardia in our logistic regression model were IMA recipient (p=0.04), need for transfusion (p=0.03), and presence of fever (p=0.01). Conclusion: Our study reaffirms that there are several factors that are predictive of tachycardia in the setting of microvascular breast reconstruction. The IMA syndrome should be a recognized cause of tachycardia as using these recipient vessels are shown to be predictive of postoperative tachycardia as shown in our study. (C) 2015 British Association of Plastic, Reconstructive and Aesthetic Surgeons. Published by Elsevier Ltd. All rights reserved.", "journal": "JOURNAL OF PLASTIC RECONSTRUCTIVE AND AESTHETIC SURGERY", "category": "Surgery", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000351118600002", "keywords": "Functional Data Analysis; Cue Trading; Dynamic trajectories; Diphthong and hiatus; European Spanish", "title": "Using Functional Data Analysis for investigating multidimensional dynamic phonetic contrasts", "abstract": "The study of phonetic contrasts and related phenomena, e.g. inter- and intra-speaker variability, often requires to analyse data in the form of measured time series, like f(0) contours and formant trajectories. As a consequence, the investigator has to find suitable ways to reduce the raw and abundant numerical information contained in a bundle of time series into a small but sufficient set of numerical descriptors of their shape. This approach requires one to decide in advance which dynamic traits to include in the analysis and which not. For example, a rising pitch gesture may be represented by its duration and slope, hence reducing it to a straight segment, or by a richer coding specifying also whether (and how much) the rising contour is concave or convex, the latter being irrelevant in some context but crucial in others. Decisions become even more complex when a phenomenon is described by a multidimensional time series, e.g. by the first two formants. In this paper we introduce a methodology based on Functional Data Analysis (FDA) that allows the investigator to delegate most of the decisions involved in the quantitative description of multidimensional time series to the data themselves. FDA produces a data-driven parametrisation of the main shape traits present in the data that is visually interpretable, in the same way as slopes or peak heights are. These output parameters are numbers that are amenable to ordinary statistical analysis, e.g. linear (mixed effects) models. FDA is also able to capture correlations among different dimensions of a time series, e.g. between formants F-1 and F-2. We present FDA by means of an extended case study on diphthong hiatus distinction in Spanish, a contrast that involves duration, formant trajectories and pitch contours. (C) 2014 Elsevier Ltd. All rights reserved.", "journal": "JOURNAL OF PHONETICS", "category": "Linguistics; Language & Linguistics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000351930100008", "keywords": "impact echo; accelerometers; mixture of Gaussians; semi-supervised Bayes classification", "title": "Cell cycle staging of individual cells by fluorescence microscopy", "abstract": "Progression through the cell cycle is one of the most fundamental features of cells. Studies of the cell cycle have traditionally relied on the analysis of populations, and they often require specific markers or the use of genetically modified systems, making it difficult to determine the cell cycle stage of individual, unperturbed cells. We describe a protocol, suitable for use in high-resolution imaging approaches, for determining cell cycle staging of individual cells by measuring their DNA content by fluorescence microscopy. The approach is based on the accurate quantification by image analysis of the integrated nuclear intensity of cells stained with a DNA dye, and it can be used in combination with several histochemical methods. We describe and provide the algorithms for two automated image analysis pipelines and the derivation of cell cycle profiles with both commercial and open-source software. This 1-2-d protocol is applicable to adherent cells, and it is adaptable for use with several DNA dyes.", "journal": "NATURE PROTOCOLS", "category": "Biochemical Research Methods", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000366021300019", "keywords": "Global Positioning System/Inertial Navigation System; loosely coupled; extended Kalman filtering; inertial navigation", "title": "Synthesis of novel polysubstituted N-benzyl-1H-pyrroles via a cascade reaction of alkynyl Fischer carbenes with alpha-imino glycine methyl esters", "abstract": "An efficient and simple synthesis of novel and densely substituted N-benzyl-1H-pyrroles 6a-r is described by a 1,4-addition/isomerization/ring closure/demetalation cascade process of alkynyl Fischer carbene complexes 1a-f and 2a and a-imino glycine methyl esters 3a, b, d, g, h, and k promoted with LDA.", "journal": "ORGANIC & BIOMOLECULAR CHEMISTRY", "category": "Chemistry, Organic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000352955900016", "keywords": "medical terminology; derivative morphology; lexicology; Japanese; English", "title": "Locus Number Estimation of MHC Class II B in Stone Flounder and Japanese Flounder", "abstract": "Members of major histocompatibility complex (MHC) family are important in immune systems. Great efforts have been made to reveal their complicated gene structures. But many existing studies focus on partial sequences of MHC genes. In this study, by gene cloning and sequencing, we identified cDNA sequences and DNA sequences of the MHC class II B in two flatfishes, stone flounder (Kareius bicoloratus) and homozygous diploid Japanese flounder (Paralichthys olivaceus). Eleven cDNA sequences were acquired from eight stone flounder individuals, and most of the polymorphic sites distributed in exons 2 and 3. Twenty-eight alleles were identified from the DNA fragments in these eight individuals. It could be deduced from their Bayesian inference phylogenetic tree that at least four loci of MHC class II B exist in stone flounder. The detailed whole-length DNA sequences in one individual were analyzed, revealing that the intron length varied among different loci. Four different cDNA sequences were identified from one homozygous diploid Japanese flounder individual, implying the existence of at least four loci. Comparison of the cDNA sequences to the DNA sequence confirmed that six exons existed in this gene of Japanese flounder, which was a common feature shared by Pleuronectiformes fishes. Our results proved the multi-locus feature of MHC class II B. The sequences we obtained would provide detailed and systematic data for further research.", "journal": "INTERNATIONAL JOURNAL OF MOLECULAR SCIENCES", "category": "Biochemistry & Molecular Biology; Chemistry, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000356864800008", "keywords": "Kinetic models; ODE; Differential equations; Model validation; Model selection; Cross validation; Hold-out validation", "title": "Safety and tolerability of ledipasvir/sofosbuvir with and without ribavirin in patients with chronic hepatitis C virus genotype 1 infection: Analysis of phase III ION trials", "abstract": "In phase III studies, treatment with the once-daily fixed-dose combination tablet of ledipasvir/sofosbuvir (LDV/SOF) with and without ribavirin (RBV) resulted in high rates of sustained virological response (SVR) in patients chronically infected with genotype 1 hepatitis C virus, including those with compensated cirrhosis. We conducted an analysis of data from these trials to compare the safety and tolerability profile of LDV-SOF with and without RBV. We analyzed treatment-emergent adverse events (AEs) and laboratory abnormalities in patients who were randomized to 8, 12, and 24 weeks of LDV/SOF with or without RBV. In total, data from 1,952 patients (of whom 872 received LDV/SOF with RBV and 1,080 received LDV/SOF alone) were analyzed. Overall, 308 patients (16%) were African American, 224 (11%) had compensated cirrhosis, 501 (26%) had a body mass index 30 kg/m(2), and 440 (23%) were treatment experienced. Treatment-related AEs occurred in 71% and 45% of patients treated with and without RBV, respectively, including fatigue, insomnia, irritability, and rash/pruritus. Patients receiving RBV with LDV/SOF were more likely to require dose modification, interruptions of treatment resulting from AEs, or require the use of concomitant medications than those receiving LDV/SOF alone. Rates of treatment-related serious AEs and discontinuations resulting from AEs were similarly low (<1%) in both groups. The rate of SVR in those receiving RBV and those not receiving RBV was the same (97%). Conclusion: LDV/SOF plus RBV was associated with a greater incidence of AEs as well as concomitant medication use than LDV/SOF alone. Use of RBV did not impact the efficacy of LDV/SOF regimens in the ION phase III studies. (Hepatology 2015;62:25-30)", "journal": "HEPATOLOGY", "category": "Gastroenterology & Hepatology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000350843300002", "keywords": "E-learning; student model; domain ontology; human resource management; ontology; personalization; semantic web", "title": "AN ONTOLOGY BASED APPROACH FOR MODELING E-LEARNING IN HEALTHCARE HUMAN RESOURCE MANAGEMENT", "abstract": "The paper proposes to use ontologies for modeling e-learning process in organizing the educational information in Healthcare Human Resource Management in Romania (HHRM), in order to use existing health workforce data and information systems for decision making and human resource management and support. One of the main objectives of this e-learning system is related to the need for training the managers in charge with the health care system management in order to increase the system's quality and safety. The main benefit of the proposed e-learning method for the Romanian health care system is a tailored training system adapted to the needs of the professionals working in different areas of the management in a high degree hospital. This will be achieved by implementation of a modern e-learning technologies and specific ontologies. The proposed model particularity consists in implementation of domain specific ontologies using Protege environment using a personal methodology according to the student's knowledge profile. The settling of the students' profile is based on processing their entry data to allow the training process personalization, automatically generated by the intelligent system. The student's profile is identified by integrating a static and a dynamic model. Due to this methodology, students will be able to receive the learning material by an e-learning system, according to their level of knowledge, preferences and interests: a personalized model driven approach.", "journal": "ECONOMIC COMPUTATION AND ECONOMIC CYBERNETICS STUDIES AND RESEARCH", "category": "Economics; Mathematics, Interdisciplinary Applications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000352188200002", "keywords": "Graphical model determination; Local updates; Markov chain Monte Carlo; Mixture priors; Perfect elimination order", "title": "First-principles study of a sodium borosilicate glass-former. II. The glass state", "abstract": "We use ab initio simulations to investigate the properties of a sodium borosilicate glass of composition 3Na(2)O-B2O3-6SiO(2). We find that the broadening of the first peak in the radial distribution functions g(BO)(r) and g(BNa)(r) is due to the presence of trigonal and tetrahedral boron units as well as to nonbridging oxygen atoms connected to BO3 units. In agreement with experimental results, we find that the ([3]) B units involve a significant number of nonbridging oxygens, whereas the vast majority of ([4]) B have only bridging oxygens. We determine the three-dimensional distribution of the Na atoms around the ([3]) B and ([4]) B units and use this information to explain why the sodium atoms associated with the latter share more oxygen atoms with the central boron atoms than the former units. From the distribution of the electrons we calculate the total electronic density of states, as well its decomposition into angular momentum contributions. The vibrational density of states shows at high frequencies a band that originates from the motion of the boron atoms. We find that the ([3]) B and ([4]) B units give rise to well-defined features in the spectrum, which thus can be used to estimate the concentration of these structural entities. The contribution of ([3]) B can be decomposed further into symmetric and asymmetric parts that can also be easily identified in the spectrum. Furthermore, it is found that certain features in the spectrum can be used to obtain information on the type of atom that is the second-nearest neighbor of a boron in the ([4]) B unit. We calculate the average Born charges on the bridging and nonbridging oxygen atoms and show that these depend linearly on the angle between the two bonds and the distance from the connected cation, respectively. Finally, we have determined the frequency dependence of the dielectric function, as well as the absorption spectra. The latter is in good quantitative agreement with the experimental data.", "journal": "PHYSICAL REVIEW B", "category": "Materials Science, Multidisciplinary; Physics, Applied; Physics, Condensed Matter", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000352021400011", "keywords": "Rotavirus; G3P[4]; Equine; Reassortment; Sendai", "title": "IBM POWER8 performance and energy modeling", "abstract": "The IBM POWER8 (TM) architecture introduces many novel features that improve the overall performance and energy management of systems based on this new platform. To cover a wide range of workloads, the design team focused on not just the traditional server workloads like transaction processing and enterprise resource planning, but also on emerging workloads such as big data, analytics, and cloud. Designed for openness, the POWER8 platform enables other technology providers to innovate freely with OpenPower (TM). In designing the most critical POWER8 processor features, the development team relied on state-of-the-art performance and energy models for design optimization and trade-off studies, and for projecting the system-level performance for the most important workloads. In this paper, we describe these models in detail and spotlight how the models were used to influence the design of specific POWER8 features.", "journal": "IBM JOURNAL OF RESEARCH AND DEVELOPMENT", "category": "Computer Science, Hardware & Architecture; Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000351584100008", "keywords": "memory; visual search; scene perception; eye movements", "title": "The role of memory for visual search in scenes", "abstract": "Many daily activities involve looking for something. The ease with which these searches are performed often allows one to forget that searching represents complex interactions between visual attention and memory. Although a clear understanding exists of how search efficiency will be influenced by visual features of targets and their surrounding distractors or by the number of items in the display, the role of memory in search is less well understood. Contextual cueing studies have shown that implicit memory for repeated item configurations can facilitate search in artificial displays. When searching more naturalistic environments, other forms of memory come into play. For instance, semantic memory provides useful information about which objects are typically found where within a scene, and episodic scene memory provides information about where a particular object was seen the last time a particular scene was viewed. In this paper, we will review work on these topics, with special emphasis on the role of memory in guiding search in organized, real-world scenes.", "journal": "COMPETITIVE VISUAL PROCESSING ACROSS SPACE AND TIME: ATTENTION, MEMORY, AND PREDICTION", "category": "Neurosciences; Psychology; Psychology, Experimental", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000348577500007", "keywords": "photovoltaic; maximum power point tracking; partial shading; particle swarm optimization", "title": "A PSO-based maximum power point tracking for photovoltaic systems under environmental and partially shaded conditions", "abstract": "To increase the efficiency of photovoltaic (PV) systems, maximum power point (MPP) tracking of the solar arrays is needed. Solar arrays output power depends on the solar irradiance and temperature. Also the mismatch phenomenon caused by partial shade will affect the output power of solar systems and lead to the incorrect operation of conventional MPP tracker. Under partially shaded conditions, the solar array power-current characteristic has multiple maximum. This paper presents a maximum power point tracking (MPPT) with particle swarm optimization method for PV systems under partially shaded condition. The performance of the proposed method is compared with perturb and observe (P&O), improved P&O, voltage-based maximum power point tracking and current-based maximum power point tracking algorithms, especially, under partially shaded condition. Simulation results confirm that proposed MPPT algorithm with high accuracy can track the peak power point under different insolation, temperature and partially shaded conditions, and it has the best performance in comparison with four mentioned MPPT algorithms. Also under rapidly changing atmospheric conditions, the P&O algorithm is diverged. Copyright (C) 2013 John Wiley & Sons, Ltd.", "journal": "PROGRESS IN PHOTOVOLTAICS", "category": "Energy & Fuels; Materials Science, Multidisciplinary; Physics, Applied", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000357502900006", "keywords": "AIC; bias-corrected AIC; BIC; consistent AIC; high-dimensional asymptotic framework; multivariate linear model; selection probability; variable selection", "title": "Variation in Fractal Symmetry of Annual Growth in Aspen as an Indicator of Developmental Stability in Trees", "abstract": "Fractal symmetry is symmetry across scale. If one looks at a branch of a tree its branching pattern is reminiscent of the tree as a whole. Plants exhibit a number of different symmetries, including bilateral, rotational, translational, and fractal; deviations from each of these types has been associated with organisms developing in stressful environments. Here, we explore the utilization and meaning of fractal analysis on annual growth ring production in woody plants. Early detection of stress in plants is difficult and the compounding effects of multiple or severe stressors can lead to irreversible damage or death. Annual wood production was used to produce a time series for individuals from stands classified as either high vigor or low vigor (a general measure of health). As a measure of symmetry over time, the fractal dimension of each time series was determined and compared among vigor classes. We found that individuals obtained from low vigor sites had a significantly lower fractal dimension than those from high vigor sites. These results agree with patterns found in a variety of other organisms, and we argue that the reduced fractal dimension is related to a loss in system complexity of stressed individuals.", "journal": "SYMMETRY-BASEL", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000364716600001", "keywords": "Pancreatic neuroendocrine tumor; Lymph node; Survival surgery", "title": "Cyclic AMP-Responsive Element Modulator alpha Polymorphisms Are Potential Genetic Risks for Systemic Lupus Erythematosus", "abstract": "To investigate whether the cyclic AMP-responsive element modulator alpha (CREM alpha) polymorphisms are novel susceptibility factors for systemic lupus erythematosus (SLE), four tag SNPs, rs1057108, rs2295415, rs11592925, and rs1148247, were genotyped in 889 SLE cases and 825 healthy controls. Association analyses were performed on whole dataset or clinical/serologic subsets. Association statistics were calculated by age and sex adjusted logistic regression. The G allele frequencies of rs2295415 and rs1057108 were increased in SLE patients, compared with healthy controls (rs2295415: 21.2% versus 17.8%, OR 1.244, P = 0.019; rs1057108: 30.8% versus 27.7%, OR 1.165, P = 0.049). The haplotype constituted by the two risk alleles \"G-G\" from rs1057108 and rs2295415 displayed strong association with SLE susceptibility (OR 1.454, P = 0.00056). Following stratification by clinical/serologic features, a suggestive association was observed between rs2295415 and anti-Sm antibodies-positive SLE (OR 1.382, P = 0.044). Interestingly, a potential protective effect of rs2295415 was observed for SLE patients with renal disorder (OR 0.745, P = 0.032). Our data provide first evidence that CREM alpha SNPs rs2295415 and rs1057108 maybe novel genetic susceptibility factors for SLE. SNP rs2295415 appears to confer higher risk to develop anti-Sm antibodies-positive SLE and may play a protective role against lupus nephritis.", "journal": "JOURNAL OF IMMUNOLOGY RESEARCH", "category": "Immunology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000349635400001", "keywords": "Isogeometric analysis; Hierarchical splines; Adaptive mesh refinement; T-splines", "title": "Hierarchical T-splines: Analysis-suitability, Bezier extraction, and application as an adaptive basis for isogeometric analysis", "abstract": "In this paper hierarchical analysis-suitable T-splines (HASTS) are developed. The resulting spaces are a superset of both analysis-suitable T-splines and hierarchical B-splines. The additional flexibility provided by the hierarchy of T-spline spaces results in simple, highly localized refinement algorithms which can be utilized in a design or analysis context. A detailed theoretical formulation is presented. Bezier extraction is extended to HASTS simplifying the implementation of HASTS in existing finite element codes. The behavior of a simple HASTS refinement algorithm is compared to the local refinement algorithm for analysis-suitable T-splines demonstrating the superior efficiency and locality of the HASTS algorithm. Finally, HASTS are utilized as a basis for adaptive isogeometric analysis. (C) 2014 Elsevier B.V. All rights reserved.", "journal": "COMPUTER METHODS IN APPLIED MECHANICS AND ENGINEERING", "category": "Engineering, Multidisciplinary; Mathematics, Interdisciplinary Applications; Mechanics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000379906800007", "keywords": "OASI; Risk factors; Episiotomy; Instrumental delivery", "title": "Emergency Department Use: A Reflection of Poor Primary Care Access?", "abstract": "Objectives To determine whether the use of the emergency department (ED) for nonurgent care reflects poor access to community-based primary care providers (PCPs). Study Design Using a survey of ED patients, insurance claims data, and administrative records identifying demographic factors, we analyzed the use of the ED in an impoverished area of Brooklyn, New York. Methods We examined original survey data to investigate the extent to which residents of northern and central Brooklyn use EDs for non-emergencies and whether these patients have access to PCPs. We used data from health insurers operating in northern and central Brooklyn, and New York state hospital ED visit data to investigate the factors influencing ED visits for ambulatory care-sensitive conditions (ACSCs). Logistic regression was used to identify characteristics that predict ED visits not resulting in admission for ACSCs. Results Of 11,546 patients that completed our survey, the presenting complaint was self-described as emergent by 57%, 30% had no PCP, and 19% reported no health insurance coverage. Using health insurance plan encounter data, only 15 % of patients had seen any provider within 1 week of the ED visit. Insurance type, age, gender, race/ethnicity, and socioeconomic status of area of residence influence the likelihood of these ED visits. Conclusions Correlating data from 3 sources, we suggest that the expansion of insurance under the Affordable Care Act may not be sufficient to reduce ED use for nonurgent conditions.", "journal": "AMERICAN JOURNAL OF MANAGED CARE", "category": "Health Care Sciences & Services; Health Policy & Services; Medicine, General & Internal", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000349934400011", "keywords": "Fuzzy disturbance observer; Sliding mode control; Switching gain; Chattering reduction; Uncertainty; External disturbance", "title": "Diagonalization of Large Matrices: A New Parallel Algorithm", "abstract": "On the basis of a dressed matrices formalism, a new algorithm has been devised for obtaining the lowest eigenvalue and the corresponding eigenvector of large real symmetric matrices. Given an N X N matrix, the proposed algorithm consists in the diagonalization of (N - 1)2 X 2 dressed matrices. Both sequential and parallel versions of the proposed algorithm have been implemented. Tests have been performed on a Hilbert matrix, and the results show that this algorithm is up 340 times faster than the corresponding LAPACK routine for N = 10(4) and about 10% faster than the Davidson method. The parallel MPI version has been tested using up to 512 nodes. The speed-up for a N = 10(6) matrix is fairly lineal until 64 cores. The time necessary to obtain the lowest eigenvalue and eigenvector is nearly 5.5 min with 512 cores. For an N = 10(7) matrix, the speed-up is nearly linear to 256 cores and the calculation time is 5.2 h with 512 nodes. Finally, in order to test the new algorithm on MRCI matrices, we have calculated the ground state and the pi -> pi excited state of the butadiene molecule, starting from both SCF and CASSCF wave functions. In all the cases considered, correlation energies and wave functions are the same as obtained with the Davidson algorithm.", "journal": "JOURNAL OF CHEMICAL THEORY AND COMPUTATION", "category": "Chemistry, Physical; Physics, Atomic, Molecular & Chemical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000347761600012", "keywords": "amputation; lower limb; balance and falls; balance measurement; prosthesis", "title": "BALANCE ABILITY MEASURED WITH THE BERG BALANCE SCALE: A DETERMINANT OF FALL HISTORY IN COMMUNITY-DWELLING ADULTS WITH LEG AMPUTATION", "abstract": "Objective: Falls are common among adults with leg amputations and associated with balance confidence. But subjective confidence is not equivalent with physical ability. This multivariate analyses of community-dwelling adults with leg amputations examined relationships among individual characteristics, falls, balance ability and balance confidence. Design: Cross-sectional study. Subjects/Patients: Community-dwelling adults with leg amputations recruited from a support group and prosthetic clinic. Methods: Subjects provided self-reported medical/fall history, prosthetic functional use, and Activities-specific Balance Confidence (ABC) questionnaire data. Balance ability was assessed with the Berg Balance Scale (BBS). Fall incidence was categorized as any fall (one or more) and recurrent falls (more than one). Multivariate logistic regression analyzed relationships within the two fall categories. Cross tabulations and ANOVA analyzed differences among subcategories. Results: Fifty-four subjects (mean age 56.8) with various etiologies, amputation levels, and balance abilities participated. 53.7% had any fall; 25.9% had recurrent falls. Models for both fall categories correctly classified fall history in >70% of subjects with combinations of the variables ABC, BBS, body-mass-index, and amputation level. Conclusion: Falls occurred regardless of clinical characteristics. Total BBS and select item scores were independent determinants of fall history. Unlike other balance-impaired populations, adults with leg amputation and better balance ability had greater odds of falling.", "journal": "JOURNAL OF REHABILITATION MEDICINE", "category": "Rehabilitation; Sport Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000371013200151", "keywords": "Percutaneous coronary intervention (PCI); in stent restenosis (ISR); stromal interaction molecular 1 (STIM1); osteoprotegerin (OPG); high-sensitivity C-reactive protein (Hs-CRP)", "title": "Higher plasma level of STIM1, OPG are correlated with stent restenosis after PCI", "abstract": "Object: Percutaneous Coronary Intervention (PCI) is one of the most effective treatments for Coronary Heart Disease (CHD), but the high rate of In Stent Restenosis (ISR) has plagued clinicians after PCI. We aim to investigate the correlation of plasma Stromal Interaction Molecular 1 (STIM1) and Osteoprotegerin (OPG) level with stent restenosis after PCI. Methods: A total of 100 consecutive patients with Coronary Heart Disease (CHD) received PCI procedure were recruited. Coronary angiography was performed 8 months after their PCI. Then patients were divided into 2 groups: observation group was composed by patients who existing postoperative stenosis after intervention; Control group was composed by patients with no postoperative stenosis. The plasma levels of STIM, OPG in all patients were tested before and after intervention. Pearson correlation and multiple linear regression analysis were performed to analysis the correlation between STIM, OPG level and postoperative stenosis. Results: 35 cases were divided into observation group and other 65 were divided into control group. The plasma levels of STIM, OPG have no statistical difference before their PCI procedure, but we observed higher level of High-sensitivity C-reactive protein (Hs-CRP) existed in observation group. We observed higher level of plasma STIM, OPG in observation group when compared with control group after PCI procedure (P < 0.05). Regression analysis demonstrated that Hs-CRP, STIM1, OPG are independent risk factors for ISR. Conclusion: Elevated levels of plasma STIM1, OPG are independent risk factors for ISR in patients received PCI, which could provide useful information for the restenosis control after PCI.", "journal": "INTERNATIONAL JOURNAL OF CLINICAL AND EXPERIMENTAL MEDICINE", "category": "Medicine, Research & Experimental", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000348521900007", "keywords": "antidepressant treatment; clinical practice variation; depressive disorder; prescribing; primary health care", "title": "Explaining the variation between practices in the duration of new antidepressant treatment: a database cohort study in primary care", "abstract": "Background Practices vary in the duration of newly initiated antidepressant treatment, even after adjusting for patient characteristics. It was hypothesised that this may be because of differences between practices in demographic (practice deprivation and antidepressant prescribing rates), organisational (practice size and proportion of female GPs), and clinical factors (proportion of new episodes of depression coded). Aim To examine the effect of practice characteristics on the duration of new selective serotonin reuptake inhibitor antidepressant treatment in primary care. Design and setting Database cohort study of 28 027 patients from 237 GP practices in Scotland. Method Prescription data were used to estimate duration of treatment for individual patients beyond three time points: 30, 90, and 180 days. Data at patient and practice level were analysed by multilevel logistic regression to quantify the variation between practices. Results The mean rate of diagnostic coding for depression in patients beginning a course of treatment was 29% (range 0-80%). Practice-level deprivation and rate of new antidepressant prescribing were not associated with duration of treatment. The practice level factor most strongly associated with duration of treatment at practice level was the proportion of patients coded as having depression: odds ratio for continuing beyond 30 days was 1.54 (95% confidence interval [CI] = 1.22 to 1.94); beyond 90 days, 1.37 (95% CI = 1.09 to 1.71); and beyond 180 days 1.41 (95% CI = 1.10 to 1.82). Conclusion Encouraging coding and structured follow-up at the onset of treatment of depression is likely to reduce early discontinuation of antidepressant treatment and improve outcomes.", "journal": "BRITISH JOURNAL OF GENERAL PRACTICE", "category": "Primary Health Care; Medicine, General & Internal", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000356306800001", "keywords": "QoS; Uplink; Scheduling; LTE; Delay estimation", "title": "Adaptive Decision Fusion with a Guidance Sensor in Wireless Sensor Networks", "abstract": "In wireless sensor networks, the fusion center collects the dates from the sensor nodes and makes the optimal decision fusion, while the optimal decision fusion rules need the performance parameters of each sensor node. However, sensors, particularly low-cost and low-precision sensors, are usually displaced in harsh environment and their performance parameters can be easily affected by the environment and hardly be known in advance. In order to resolve this issue, we take a heterogeneous wireless sensor network system, which is composed of both low-quality and high-quality sensors. Low-quality sensors are inexpensive and consume less energy while high-quality sensors are expensive and consume much more energy but provide high accuracy. Our approach uses one high-quality sensor as the guidance sensor, which enables the fusion center to estimate the performance parameters of the low-quality sensors online during the whole sampling process, and optimal decision fusion rule can be used in practice. Through using the low-quality sensors rather than the high-quality sensor most of the time, the system can efficiently reduce the system-level energy cost and prolong the network lifetime.", "journal": "INTERNATIONAL JOURNAL OF DISTRIBUTED SENSOR NETWORKS", "category": "Computer Science, Information Systems; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000348263200004", "keywords": "Gaussian estimation; Induced smoothing method; Pseudolikelihood; Repeated measurements; Working covariance matrix", "title": "A Gaussian pseudolikelihood approach for quantile regression with repeated measurements", "abstract": "To enhance the efficiency of regression parameter estimation by modeling the correlation structure of correlated binary error terms in quantile regression with repeated measurements, we propose a Gaussian pseudolikelihood approach for estimating correlation parameters and selecting the most appropriate working correlation matrix simultaneously. The induced smoothing method is applied to estimate the covariance of the regression parameter estimates, which can bypass density estimation of the errors. Extensive numerical studies indicate that the proposed method performs well in selecting an accurate correlation structure and improving regression parameter estimation efficiency. The proposed method is further illustrated by analyzing a dental dataset. (C) 2014 Elsevier B.V. All rights reserved.", "journal": "COMPUTATIONAL STATISTICS & DATA ANALYSIS", "category": "Computer Science, Interdisciplinary Applications; Statistics & Probability", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000350933800006", "keywords": "Grand fir; Concolor fir; Discriminant analysis; Hybrids; Niche divergence", "title": "Volume and landmark analysis: comparison of MRI measurements obtained with an endorectal coil and with a phased-array coil", "abstract": "AIM: To compare prostate volumes and distances between anatomical landmarks on MRI images obtained with a phased-array coil (PAC) only and with a PAC and an endorectal coil (ERC). MATERIALS AND METHODS: Informed consent was waived for this Health Insurance Portability and Accountability Act-compliant study. Fifty-nine men underwent PAC-MRI and ERC-MRI at 1.5 (n = 3) or 3 T (n = 56). On MRI images, two radiologists independently measured prostate volume and distances between the anterior rectal wall (ARW) and symphysis pubis at the level of the verumontanum; ARW and symphysis pubis at the level of the mid-symphysis pubis; and bladder neck and mid-symphysis pubis. Differences between measurements from PAC-MRI and ERC-MRI were assessed with the Wilcoxon RANK SUM test. Inter-reader agreement was assessed using the concordance correlation coefficient (CCC). RESULTS: Differences in prostate volume between PAC-MRI and ERC-MRI [median: -0.75 mm(3) (p = 0.10) and median: -0.84 mm(3) (p = 0.06) for readers 1 and 2, respectively] were not significant. For readers 1 and 2, median differences between distances were as follows: -10.20 and -12.75 mm, respectively, ARW to symphysis pubis at the level of the verumontanum; -6.60 and -6.08 mm, respectively, ARW to symphysis pubis at the level of the mid-symphysis pubis; -3 and -3 mm respectively, bladder neck to mid-symphysis pubis. All differences in distance were significant for both readers (p <= 0.0005). Distances were larger on PAC-MRI (p <= 0.0005). Inter-reader agreement regarding prostate volume was almost perfect on PAC-MRI (CCC: 0.99; 95% CI: 0.98-1.00) and ERC-MRI (CCC: 0.99; 95% CI: 0.99-1.00); inter-reader agreement for distance measurements varied (CCCs: 0.54-0.86). CONCLUSION: Measurements of distances between anatomical landmarks differed significantly between ERC-MRI and PAC-MRI, although prostate volume measurements did not. (C) 2014 The Royal College of Radiologists. Published by Elsevier Ltd. All rights reserved.", "journal": "CLINICAL RADIOLOGY", "category": "Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000377809600016", "keywords": "Handwritten manuscripts; layout analysis; text height estimation; text line segmentation; frequency-based analysis; Document Layout Analysis; Cultural Heritage", "title": "Cup products, the Johnson homomorphism and surface bundles over surfaces with multiple fiberings", "abstract": "Let Sigma(g) -> E -> Sigma(h) be a surface bundle over a surface with monodromy representation rho:pi(1)Sigma(h) -> Mod(Sigma(g)) contained in the Torelli group I-g. We express the cup product structure in H*(E, Z) in terms of the Johnson homomorphism tau:I-g -> Lambda(3)(H-1(Sigma(g), Z)) / H-1(Sigma(g), Z). This is applied to the question of obtaining an upper bound on the maximal n such that p(1) : E -> Sigma(h1), ..., p(n) : E ->Sigma h(n) are fibering maps realizing E as the total space of a surface bundle over a surface in n distinct ways. We prove that any nontrivial surface bundle over a surface with monodromy contained in the Johnson kernel K-g fibers in a unique way.", "journal": "ALGEBRAIC AND GEOMETRIC TOPOLOGY", "category": "Mathematics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000388606800001", "keywords": "inflammation; immune homeostasis; mathematical modeling; NF-kappa B; TNF-alpha; IL-10", "title": "Mathematical Modeling of Pro- and Anti-Inflammatory Signaling in Macrophages", "abstract": "Inflammation is a beneficial mechanism that is usually triggered by injury or infection and is designed to return the body to homeostasis. However, uncontrolled or sustained inflammation can be deleterious and has been shown to be involved in the etiology of several diseases, including inflammatory bowel disorder and asthma. Therefore, effective anti-inflammatory signaling is important in the maintenance of homeostasis in the body. However, the inter-play between pro- and anti-inflammatory signaling is not fully understood. In the present study, we develop a mathematical model to describe integrated pro- and anti-inflammatory signaling in macrophages. The model incorporates the feedback effects of de novo synthesized pro-inflammatory (tumor necrosis factor alpha; TNF-alpha) and anti-inflammatory (interleukin-10; IL-10) cytokines on the activation of the transcription factor nuclear factor kappa B (NF-kappa B) under continuous lipopolysaccharide (LPS) stimulation (mimicking bacterial infection). In the model, IL-10 upregulates its own production (positive feedback) and also downregulates TNF-alpha production through NF-kappa B (negative feedback). In addition, TNF-alpha upregulates its own production through NF-kappa B (positive feedback). Eight model parameters are selected for estimation involving sensitivity analysis and clustering techniques. We validate the mathematical model predictions by measuring phosphorylated NF-kappa B, de novo synthesized TNF-alpha and IL-10 in RAW 264.7 macrophages exposed to LPS. This integrated model represents a first step towards modeling the interaction between pro- and anti-inflammatory signaling.", "journal": "PROCESSES", "category": "Engineering, Chemical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000346241000017", "keywords": "Parametric splines; Real roots; Classification; Discriminant sequence; Delta-sequence", "title": "Real root classification of parametric spline functions", "abstract": "The real root classification of a given parametric spline function is a collection of possible cases of its real root distribution on every interval, together with the conditions of its coefficients must be satisfied for each case. This paper presents an algorithm to deal with the real root classification of a given parametric spline function. Two examples are provided to illustrate the proposed algorithm is flexible. (C) 2014 Elsevier Inc. All rights reserved.", "journal": "APPLIED MATHEMATICS AND COMPUTATION", "category": "Mathematics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000350304500008", "keywords": "heart transplantation; pediatrics; risk assessment; risk factors", "title": "Goserelin for Ovarian Protection during Breast-Cancer Adjuvant Chemotherapy", "abstract": "BACKGROUND Ovarian failure is a common toxic effect of chemotherapy. Studies of the use of gonadotropin-releasing hormone (GnRH) agonists to protect ovarian function have shown mixed results and lack data on pregnancy outcomes. METHODS We randomly assigned 257 premenopausal women with operable hormone-receptor-negative breast cancer to receive standard chemotherapy with the GnRH agonist goserelin (goserelin group) or standard chemotherapy without goserelin (chemotherapy-alone group). The primary study end point was the rate of ovarian failure at 2 years, with ovarian failure defined as the absence of menses in the preceding 6 months and levels of follicle-stimulating hormone (FSH) in the postmenopausal range. Rates were compared with the use of conditional logistic regression. Secondary end points included pregnancy outcomes and disease-free and overall survival. RESULTS At baseline, 218 patients were eligible and could be evaluated. Among 135 with complete primary end-point data, the ovarian failure rate was 8% in the goserelin group and 22% in the chemotherapy-alone group (odds ratio, 0.30; 95% confidence interval, 0.09 to 0.97; two-sided P = 0.04). Owing to missing primary end-point data, sensitivity analyses were performed, and the results were consistent with the main findings. Missing data did not differ according to treatment group or according to the stratification factors of age and planned chemotherapy regimen. Among the 218 patients who could be evaluated, pregnancy occurred in more women in the goserelin group than in the chemotherapy-alone group (21% vs. 11%, P = 0.03); women in the goserelin group also had improved disease-free survival (P = 0.04) and overall survival (P = 0.05). CONCLUSIONS Although missing data weaken interpretation of the findings, administration of goserelin with chemotherapy appeared to protect against ovarian failure, reducing the risk of early menopause and improving prospects for fertility. (Funded by the National Cancer Institute and others; POEMS/S0230 ClinicalTrials.gov number, NCT00068601.)", "journal": "NEW ENGLAND JOURNAL OF MEDICINE", "category": "Medicine, General & Internal", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000351788700090", "keywords": "Shutdown; Grid code; Wind power plant; Wind mast; Ramp rate; Braking system", "title": "Shutdown of an offshore wind power plant without using a brake to meet the required ramp rate in various storm-driven conditions", "abstract": "This paper proposes an offshore WPP (wind power plant) shutdown algorithm that does not use a braking system and meets the required ramp rate in the grid code in various storm-driven conditions. The proposed algorithm determines the number of WGs (wind generators) to shut down simultaneously to achieve this requirement without using brakes. Based on the storm speed and direction measured at a WM (wind mast) installed several kilometers away from the WPP, the storm-arrival time from the WM to each WG is calculated. Then, an arrival-ordered sequence is generated for the WGs based on these storm-arrival times. The WGs are grouped in a predetermined number to shut down simultaneously. The shutdown start- and end-times of the WGs are determined by considering the storm-arrival time and the shutdown duration time. The algorithm re-calculates the storm-arrival times and the shutdown start-and end-times of the WGs if the storm speed and/or direction change. The various test results demonstrate that the algorithm successfully shuts down the WPP without using a brake by meeting the required ramp rate even when the storm speed and direction change. (C) 2015 Elsevier Ltd. All rights reserved.", "journal": "ENERGY", "category": "Thermodynamics; Energy & Fuels", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000351643100002", "keywords": "Urban parking; Transportation planning; Agent-based modeling; Scenario assessment", "title": "Spatially explicit modeling of parking search as a tool for urban parking facilities and policy assessment", "abstract": "The engineering view of a measurable, supply-independent, demand for parking that can be expressed by \"minimum parking codes\" has been generally rejected during the last two decades and is gradually being replaced by \"maximum provision\" codes, limited parking development, and demand pricing. To assess new planning practices one has to estimate the drivers' reaction to proposed spatial-temporal parking limitations. The paper applies a high-resolution spatially explicit agent-based model termed \"PARICAGENT\" as a tool for this assessment. The model is used for evaluation of parking demand in the Diamond Exchange area in Ramat Gan, a city in the Tel Aviv metropolitan area, for estimating the effectiveness of planned parking facilities for different development scenarios in the area and assessing electronic signage system that directs drivers to vacant parking lots. The results strongly indicate the advantages of agent-based modeling over the current dominant engineering approach and show the potential benefits of using an intelligent parking guidance system. (C) 2015 Elsevier Ltd. All rights reserved.", "journal": "TRANSPORT POLICY", "category": "Economics; Transportation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000351826400001", "keywords": "Hyperlipidemia; Stroke; Outcome", "title": "Influences of hyperlipidemia history on stroke outcome; a retrospective cohort study based on the Kyoto Stroke Registry", "abstract": "Background: Although hyperlipidemia is known as a risk factor of stroke, its effects on the outcome are unknown. The aim of the study is to clarify the influences of hyperlipidemia on the stroke early outcome by estimating odds ratio (OR) for sequelae requiring care and hazard ratio (HR) for death. Methods: A total of 12617 stroke patients registered in the Kyoto Stroke Registry with information on a hyperlipidemia history. We compared patients who had hyperlipidemia history and patients who hadn't. The OR for remaining sequelae requiring certain care on 30 day after stroke was calculated using a logistic regression in stroke as a whole and in each stroke subtype; cerebral infarction (CI), cerebral hemorrhage (CH) and subarachnoid hemorrhage (SAH). The HR for death within 30 day after stroke was estimated by the Cox regression. Results: The OR (95% confidence interval) for remaining sequelae 30 days after stroke was 0.66 (0.60-0.73, p < 0.001) in patients with hyperlipidemia history compared with patients without hyperlipidemia history. After stratified by stroke subtypes, it was 0.75 (0.67-0.85, p < 0.001) in CI, 0.59 (0.45-0.77, p < 0.001) in CH and 0.77 (0.43-1.38, p = 0.767) in SAH. The HR (95% confidence interval) for death was 0.39 (0.31-0.48, p < 0.001) in patients with hyperlipidemia history comparing patients without hyperlipidemia history. After stratified by stroke subtypes, it was 0.45 (0.32-0.63, p < 0.001) in CI, 0.64 (0.44-0.93, p = 0.018) in CH and 0.76 (0.47-1.23, p = 0.264) in SAH. Each value was adjusted for age and sex. Conclusions: This study suggests that the outcome is favorable for patients with hyperlipidemia history in terms of both remaining sequelae and HR for death. A factor which increases the incidence of the disease could influence on the severity of the disease in a favorable way.", "journal": "BMC NEUROLOGY", "category": "Clinical Neurology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000361288800005", "keywords": "Algorithm; head impulse test; Jitter; saccades analysis; vestibular physiology; vHIT; VOR", "title": "HITCal: a software tool for analysis of video head impulse test responses", "abstract": "Conclusion: The developed software (HITCal) may be a useful tool in the analysis and measurement of the saccadic video head impulse test (vHIT) responses and with the experience obtained during its use the authors suggest that HITCal is an excellent method for enhanced exploration of vHIT outputs. Objective: To develop a (software) method to analyze and explore the vHIT responses, mainly saccades. Methods: HITCal was written using a computational development program; the function to access a vHIT file was programmed; extended head impulse exploration and measurement tools were created and an automated saccade analysis was developed using an experimental algorithm. For pre-release HITCal laboratory tests, a database of head impulse tests (HITs) was created with the data collected retrospectively in three reference centers. This HITs database was evaluated by humans and was also computed with HITCal. Results: The authors have successfully built HITCal and it has been released as open source software; the developed software was fully operative and all the proposed characteristics were incorporated in the released version. The automated saccades algorithm implemented in HITCal has good concordance with the assessment by human observers (Cohen's kappa coefficient = 0.7).", "journal": "ACTA OTO-LARYNGOLOGICA", "category": "Otorhinolaryngology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000351072300004", "keywords": "Neurogenic bladder; Neurogenic lower urinary tract dysfunction; Detrusor sphincter dyssynergia; Sacral nerve stimulation; Pudendal nerve stimulation; Spinal cord injuries", "title": "Electrical stimulation for the treatment of lower urinary tract dysfunction after spinal cord injury", "abstract": "Electrical stimulation for bladder control is an alternative to traditional methods of treating neurogenic lower urinary tract dysfunction (NLUTD) resulting from spinal cord injury (SCI). In this review, we systematically discuss the neurophysiology of bladder dysfunction following SCI and the applications of electrical stimulation for bladder control following SCI, spanning from historic clinical approaches to recent pre-clinical studies that offer promising new strategies that may improve the feasibility and success of electrical stimulation therapy in patients with SCI. Electrical stimulation provides a unique opportunity to control bladder function by exploiting neural control mechanisms. Our understanding of the applications and limitations of electrical stimulation for bladder control has improved due to many pre-clinical studies performed in animals and translational clinical studies. Techniques that have emerged as possible opportunities to control bladder function include pudendal nerve stimulation and novel methods of stimulation, such as high frequency nerve block. Further development of novel applications of electrical stimulation will drive progress towards effective therapy for SCI. The optimal solution for restoration of bladder control may encompass a combination of efficient, targeted electrical stimulation, possibly at multiple locations, and pharmacological treatment to enhance symptom control.", "journal": "JOURNAL OF SPINAL CORD MEDICINE", "category": "Clinical Neurology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000349426100005", "keywords": "Image registration; Non-stationary intensity distortion; Sparseness; Outlier; Huber norm; Non-linear regression", "title": "Robust Huber similarity measure for image registration in the presence of spatially-varying intensity distortion", "abstract": "Similarity measure is an important part of image registration. The main challenge of similarity measure is lack of robustness to different distortions. A well-known distortion is spatially-varying intensity distortion. Its main characteristic is correlation among pixels. Most traditional intensity based similarity measures (e.g., SSD, MI) assume stationary image and pixel to pixel independence. Hence, these similarity measures are not robust against spatially-varying intensity distortion. Here, we suppose that non-stationary intensity distortion has a sparse representation in transform domain, i.e. its distribution has high peak at origin and a long tail. We use two viewpoints of Maximum Likelihood (ML) and Robust M-estimator. First, using the ML view, we propose robust Huber similarity measure (RHSM) in spatial transform domain as a new similarity measure in a mono-modal setting. In fact, RHSM, is a combination of l(2) and l(1) norms. To demonstrate robustness of the proposed similarity measure, image registration is treated as a nonlinear regression problem. In this view, covariance matrix of estimated parameters is obtained based on the one-step M-estimator. Then with minimizing Fisher information function, robust similarity measure of RHSM is introduced. This measure produces accurate registration results on both artificial as well as real-world problems that we have examined. Crown Copyright (C) 2014 Published by Elsevier B.V. All rights reserved.", "journal": "SIGNAL PROCESSING", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000356982400011", "keywords": "Non-fatal overdose; Opioids; Injection drug use; Malaysia; Harm reduction", "title": "High prevalence of non-fatal overdose among people who inject drugs in Malaysia: Correlates of overdose and implications for overdose prevention from a cross-sectional study", "abstract": "Background: Overdose is the leading cause of death among opioid users, but no data are available on overdose among people who inject drugs in Malaysia. We present the first estimates of the prevalence and correlates of recent non-fatal overdose among people who inject drugs in Malaysia. Methods: In 2010, 460 people who inject drugs were recruited using respondent-driven sampling (RDS) in Kiang Valley to assess health outcomes associated with injection drug use. Self-reported history of non-fatal overdose in the previous 6 months was the primary outcome. Sociodemographic, behavioral and structural correlates of non-fatal overdose were assessed using multivariable logistic regression. Results: All 460 participants used opioids and nearly all (99.1%) met criteria for opioid dependence. Most injected daily (91.3%) and were male (96.3%) and ethnically Malay (90.4%). Overall, 20% of participants had overdosed in the prior 6 months, and 43.3% had ever overdosed. The RDS-adjusted estimate of the 6-month period prevalence of overdose was 12.3% (95% confidence interval [CI] 7.9-16.6%). Having injected for more years was associated with lower odds of overdose (adjusted odds ratio [AOR] 0.6 per 5 years of injection, CI: 0.5-0.7). Rushing an injection from fear of the police nearly doubled the odds of overdose (AOR 1.9, CI: 1.9-3.6). Alcohol use was associated with recent non-fatal overdose (AOR 2.1, CI: 1.1-4.2), as was methamphetamine use (AOR 2.3, CI: 1.3-4.6). When adjusting for past-month drug use, intermittent but not daily methadone use was associated with overdose (AOR 2.8, CI: 1.5-5.9). Conclusion: This study reveals a large, previously undocumented burden of non-fatal overdose among people who inject drugs in Malaysia and highlights the need for interventions that might reduce the risk of overdose, such as continuous opioid substitution therapy, provision of naloxone to prevent fatal overdose, treatment of polysubstance use, and working with police to improve the risk environment. (C) 2014 Elsevier B.V. All rights reserved.", "journal": "INTERNATIONAL JOURNAL OF DRUG POLICY", "category": "Substance Abuse", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000347601600002", "keywords": "Multicore; Parallelism; Functional programming; Presburger Arithmetic; Decision procedure", "title": "An approach to multicore parallelism using functional programming: A case study based on Presburger Arithmetic", "abstract": "In this paper we investigate multicore parallelism in the context of functional programming by means of two quantifier-elimination procedures for Presburger Arithmetic: one is based on Cooper's algorithm and the other is based on the Omega Test. We first develop correct-by-construction prototype implementations in a functional programming language. Thereafter, the parallelism inherent in the decision procedures is analyzed using the Directed Acyclic Graph (DAG) model of multicore parallelism. In the step from a DAG model to a parallel implementation, the parallel implementation is optimized taking into account negative factors such as cache misses, garbage collection and overhead due to task creations, because such factors may introduce sequential bottlenecks with severe consequences for the parallel efficiency. The experiments were conducted using the functional programming language F# and .NET platform executing on an 8-core machine. A speedup of approximately 4 was obtained for Cooper's algorithm and a speedup of approximately 6 was obtained for the exact-shadow part of the Omega Test. The considered procedures are complex, memory-intense algorithms on huge formula trees and the case study reveals more general applicable techniques and guideline for deriving parallel algorithms from sequential ones in the context of data-intensive tree algorithms. The obtained insights should apply for any strict and impure functional programming language. Furthermore, the results obtained for the exact-shadow elimination procedure have a wider applicability because they can directly be transferred to the Fourier-Motzkin elimination method. (C) 2014 Elsevier Inc. All rights reserved.", "journal": "JOURNAL OF LOGICAL AND ALGEBRAIC METHODS IN PROGRAMMING", "category": "Computer Science, Theory & Methods; Logic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000355403100091", "keywords": "UHPLC; Q-TOF; MS/MS; fast polarity switching; metabolomics", "title": "Optimization of Data Acquisition and Sample Preparation Methods for LC-MS Urine Metabolomic Analysis", "abstract": "Nowadays, chromatographic methods coupled with mass spectrometry are the most commonly used tools in metabolomics studies. These methods are currently being developed and various techniques and strategies are proposed for the profiling analysis of biological samples. However, the most important thing used to maximize the number of entities in the recorded profiles is the optimization of sample preparation procedure and the data acquisition method. Therefore, ultra high performance liquid chromatography coupled with accurate quadrupole-time-of-flight (Q-TOF) mass spectrometry was used for the comparison of urine metabolomic profiles obtained by the use of various spectral data acquisition methods. The most often used method of registration of metabolomics data acquisition - TOF (MS) was compared with the fast polarity switching MS and auto MS/MS methods with the use of multivariate chemometric analysis (PCA). In all the cases both ionization mode (positive and negative) were studied and the number of the identified compounds was compared. Additionally, various urine sample preparation procedures were tested and it was found that the addition of organic solvents to the sample noticeably reduces the number of entities in the registered profiles. It was also noticed that the auto MS/MS method is the least efficient way to register metabolomic profiles.", "journal": "OPEN CHEMISTRY", "category": "Chemistry, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000350322700052", "keywords": "epidemiology; environmental lung disease; air pollution; traffic; prenatal exposures; respiratory infection", "title": "Iron Status is Associated with Asthma and Lung Function in US Women", "abstract": "Background Asthma and iron deficiency are common conditions. Whether iron status affects the risk of asthma is unclear. Objective To determine the relationship between iron status and asthma, lung function, and pulmonary inflammation. Methods Relationships between measures of iron status (serum ferritin, serum soluble transferrin receptor (sTfR), and sTfR/log10ferritin (sTfR-F Index)) and asthma, lung function, and pulmonary inflammation were examined in women 20-49 years in the National Health and Nutrition Examination Survey. Logistic, linear, and quadratic regression models accounting for the survey design of NHANES were used to evaluate associations between iron status and asthma-related outcomes and were adjusted for race/ethnicity, age, smoking status, income, and BMI. Results Approximately 16% reported a lifetime history of asthma, 9% reported current asthma, and 5% reported a recent asthma episode/attack (n = 2906). Increased ferritin (iron stores) was associated with decreased odds of lifetime asthma, current asthma, and asthma attacks/episodes in the range of ferritin linearly correlated with iron stores (20-300ng/ml). The highest quintile of ferritin (>76 ng/ml) was also associated with a decreased odds of asthma. Ferritin levels were not associated with FEV1. Increased values of the sTfR-F Index and sTfR, indicating lower body iron and higher tissue iron need, respectively, were associated with decreased FEV1, but neither was associated with asthma. None of the iron indices were associated with FeNO. Conclusion In US women, higher iron stores were inversely associated with asthma and lower body iron and higher tissue iron need were associated with lower lung function. Together, these findings suggest that iron status may play a role in asthma and lung function in US women.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000356247100012", "keywords": "Helianthus annuus; cut flowers; humic substances", "title": "Depression and Risk of Venous Thromboembolism: A Population-Based Retrospective Cohort Study", "abstract": "Objective This study investigated the relationship between depression and the risk of subsequent venous thromboembolism (VTE) development. Methods We conducted a population-based retrospective cohort analysis by using data for the period of 2000 to 2011 from the Longitudinal Health Insurance Database 2000 of Taiwan. A depression cohort comprising 35,274 patients and a nondepression cohort comprising 70,548 patients matched according to sex, age, and index year with no history of VTE were evaluated. Cox proportional hazard regression analysis was used to assess the effects of depression and comorbidities, and the Kaplan-Meier method was applied to estimate the cumulative VTE incidence curves. Results Compared with individuals without depression, depressed patients had a 1.38-fold greater risk (95% confidence interval = 1.09-1.73) of developing VTE. This risk was significantly higher in male and younger (<= 49 years) patients. In addition, patients with comorbidities such as hypertension, diabetes, heart failure, and cancer had a higher risk of depression-associated VTE that was attenuated, although nonsignificantly, by antidepressant use. Conclusions The incidence of VTE in Taiwan is higher in depressed patients than in nondepressed patients. Moreover, men, people 49 years or younger, and patients with comorbidities have a significantly greater risk of VTE after depression.", "journal": "PSYCHOSOMATIC MEDICINE", "category": "Psychiatry; Psychology; Psychology, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000353111900004", "keywords": "Calibration; heavy-tailed noise distribution; inertial sensors; sensor fusion; ultrawideband (UWB)", "title": "Indoor Positioning Using Ultrawideband and Inertial Measurements", "abstract": "In this paper, we present an approach to combine measurements from inertial sensors (accelerometers and gyroscopes) with time-of-arrival measurements from an ultrawideband (UWB) system for indoor positioning. Our algorithm uses a tightly coupled sensor fusion approach, where we formulate the problem as a maximum a posteriori (MAP) problem that is solved using an optimization approach. It is shown to lead to accurate 6-D position and orientation estimates when compared to reference data from an independent optical tracking system. To be able to obtain position information from the UWB measurements, it is imperative that accurate estimates of the UWB receivers' positions and their clock offsets are available. Hence, we also present an easy-to-use algorithm to calibrate the UWB system using a maximum-likelihood (ML) formulation. Throughout this work, the UWB measurements are modeled by a tailored heavy-tailed asymmetric distribution to account for measurement outliers. The heavy-tailed asymmetric distribution works well on experimental data, as shown by analyzing the position estimates obtained using the UWB measurements via a novel multilateration approach.", "journal": "IEEE TRANSACTIONS ON VEHICULAR TECHNOLOGY", "category": "Engineering, Electrical & Electronic; Telecommunications; Transportation Science & Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000355933700006", "keywords": "Design; Algorithms; Performance; Image haze removal; dehaze; visibility restoration", "title": "An Advanced Visibility Restoration Algorithm for Single Hazy Images", "abstract": "Haze removal is the process by which horizontal obscuration is eliminated from hazy images captured during inclement weather. Images captured in natural environments with varied weather conditions frequently exhibit localized light sources or color-shift effects. The occurrence of these effects presents a difficult challenge for hazy image restoration, with which many traditional restoration methods cannot adequately contend. In this article, we present a new image haze removal approach based on Fisher's linear discriminant-based dual dark channel prior scheme in order to solve the problems associated with the presence of localized light sources and color shifts, and thereby achieve effective restoration. Experimental restoration results via qualitative and quantitative evaluations show that our proposed approach can provide higher haze-removal efficacy for images captured in varied weather conditions than can the other state-of-the-art approaches.", "journal": "ACM TRANSACTIONS ON MULTIMEDIA COMPUTING COMMUNICATIONS AND APPLICATIONS", "category": "Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000350603600001", "keywords": "gamma H2AX; DNA double strand breaks; Rectal cancer; Radiotherapy; Individual radiosensitivity", "title": "Distinct increased outliers among 136 rectal cancer patients assessed by gamma H2AX", "abstract": "Background: In recent years attention has focused on gamma H2AX as a very sensitive double strand break indicator. It has been suggested that gamma H2AX might be able to predict individual radiosensitivity. Our aim was to study the induction and repair of DNA double strand breaks labelled by gamma H2AX in a large cohort. Methods: In a prospective study lymphocytes of 136 rectal cancer (RC) patients and 59 healthy individuals were ex vivo irradiated (IR) and initial DNA damage was compared to remaining DNA damage after 2 Gy and 24 hours repair time and preexisting DNA damage in unirradiated lymphocytes. Lymphocytes were immunostained with anti-gamma H2AX antibodies and microscopic images with an extended depth of field were acquired. gamma H2AX foci counting was performed using a semi-automatic image analysis software. Results: Distinct increased values of preexisting and remaining gamma H2AX foci in the group of RC patients were found compared to the healthy individuals. Additionally there are clear differences within the groups and there are outliers in about 12% of the RC patients after ex vivo IR. Conclusions: The gamma H2AX assay has the capability to identify a group of outliers which are most probably patients with increased radiosensitivity having the highest risk of suffering radiotherapy-related late sequelae.", "journal": "RADIATION ONCOLOGY", "category": "Oncology; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000350990400003", "keywords": "alternating sign matrix (ASM); 05B20; 05C22; 05C50; 15B35; 15B36", "title": "A Generalization of Alternating Sign Matrices", "abstract": "In alternating sign matrices, the first and last nonzero entry in each row and column is specified to be +1. Such matrices always exist. We investigate a generalization by specifying independently the sign of the first and last nonzero entry in each row and column to be either a +1 or a -1. We determine necessary and sufficient conditions for such matrices to exist whose proof contains an algorithm for their construction.", "journal": "JOURNAL OF COMBINATORIAL DESIGNS", "category": "Mathematics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000354609900011", "keywords": "Anti-inflammation; design of experiment; diclofenac; ethosomes; permeation; quality by design", "title": "Quality by design approach for formulation, evaluation and statistical optimization of diclofenac-loaded ethosomes via transdermal route", "abstract": "The objective of this study was to fabricate and understand ethosomal formulations of diclofenac (DF) for enhanced anti-inflammatory activity using quality by design approach. DF-loaded ethosomal formulations were prepared using 4 x 5 full-factorial design with phosphatidylcholine:cholesterol (PC:CH) ratios ranging between 50:50 and 90:10, and ethanol concentration ranging between 0% and 30% as formulation variables. These formulations were characterized in terms of physicochemical properties and skin permeation kinetics. The interaction of formulation variables had a significant effect on both physicochemical properties and permeation kinetics. The results of multivariate regression analysis illustrated that vesicle size and elasticity of ethosomes were the dominating physicochemical properties affecting skin permeation, and could be suitably controlled by manipulation of formulation variables to optimize the formulation and enhance the skin permeation of DF-loaded ethosomes. The optimized formulation had ethanol concentration of 22.9% and PC:CH ratio of 88.4:11.6, with vesicle size of 144 +/- 5 nm, zeta potential of -23.0 +/- 3.76 mV, elasticity of 2.48 +/- 0.75 and entrapment efficiency of 71 +/- 4%. Permeation flux for the optimized formulation was 12.9 +/- 1.0 mu g/h cm(2), which was significantly higher than the drug-loaded conventional liposome, ethanolic or aqueous solution. The in vivo study indicated that optimized ethosomal hydrogel exhibited enhanced anti-inflammatory activity compared with liposomal and plain drug hydrogel formulations.", "journal": "PHARMACEUTICAL DEVELOPMENT AND TECHNOLOGY", "category": "Pharmacology & Pharmacy", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000348206700010", "keywords": "Channel assignment; regret matching; correlated equilibrium; wireless sensor network", "title": "Dynamic Channel Assignment for Wireless Sensor Networks: A Regret Matching Based Approach", "abstract": "Multiple channels in Wireless Sensor Networks (WSNs) are often exploited to support parallel transmission and to reduce interference. However, the extra overhead posed by the multi-channel usage coordination dramatically challenges the energy-constrained WSNs. In this paper, we propose a Regret Matching based Channel Assignment algorithm (RMCA) to address this challenge, in which each sensor node updates its choice of channels according to the historical record of these channels' performance to reduce interference. The advantage of RMCA is that it is highly distributed and requires very limited information exchange among sensor nodes. It is proved that RMCA converges almost surely to the set of correlated equilibrium. Moreover, RMCA can adapt the channel assignment among sensor nodes to the time-variant flows and network topology. Simulations show that RMCA achieves better network performance in terms of both delivery ratio and packet latency than CONTROL [1], MMSN [2] and randomized CSMA. In addition, real hardware experiments are conducted to demonstrate that RMCA is easy to be implemented and performs better.", "journal": "IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS", "category": "Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000366292600003", "keywords": "sleep disordered breathing; hypoxemia; echocardiography; elderly; left cardiac function; obesity", "title": "Echocardiographic Findings in Healthy Elderly People with Unrecognized Sleep Disordered Breathing", "abstract": "Objective: Sleep disordered breathing (SDB) is associated with cardiovascular disease such as hypertension and left ventricular hypertrophy in middle-aged patients; however, this association is not well described in the elderly. The aim of this study was to evaluate the impact of unrecognized SDB on cardiac function and remodeling in a population-based sample of healthy elderly without cardiac disease. Methodology: A total of 405 healthy elderly (age >= 65 years) were examined by echocardiography and respiratory polygraphy. According to the apnea-hypopnea index (AHI), subjects were stratified in four categories: snorers (AHI <5), mild (AHI: 5-15), moderate (AHI: 15-30), and severe (AHI > 30) cases. Results: Comparative analysis between snorers and SDB cases revealed that left atrial (LA) diameter and surface increased according to SDB severity (p < 0.05) without differences in LA mass index. In subjects with an AHI > 30, an increase was found for LV end-diastolic and end-systolic dimension (p < 0.001), as well as for LV mass (p < 0.03) and LV index (p < 0.05). The current study showed a weak but significant correlation between altered LA and LV measurements versus AHI and hypoxemia indices (p < 0.001). In the regression analysis, AHI and hypoxemia had a minimal effect, body mass index and male gender being the most significant predictors. Conclusions: In a population of healthy elderly with SDB, slight changes in left atrial and ventricular measurements occur in severe cases (AHI > 30). Irrespective of the lack of a strong association between SDB and cardiac dysfunction, the presence of slight cardiac pathology in severe SDB cases might be considered.", "journal": "JOURNAL OF CLINICAL SLEEP MEDICINE", "category": "Clinical Neurology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000357765000015", "keywords": "Rectal bleeding; Endometrial cancer; Vaginal brachytherapy", "title": "Rectal bleeding after radiation therapy for endometrial cancer", "abstract": "Background and purpose: The goals of this study were to determine the rate and risk factors of rectal bleeding (RB) after external beam radiotherapy and vaginal brachytherapy (EBRT + VB), and to compare these data to previously unreported RB rates from PORTEC-2 patients receiving EBRT or VB alone. Materials and methods: Retrospective chart review identified 212 endometrial cancer patients receiving adjuvant EBRT + VB between 2006 and 2013. Patient-reported RB data were also obtained from PORTEC-2 patients randomized to EBRT (n = 166) or VB (n = 182). The two populations were compared using an RB scale of symptom severity. Results: After a median 35 months, 17.9% of EBRT + VB patients (n = 38) experienced any RB with 1.9% (n = 4) having bleeding requiring intervention. Age <= 70 years was the only predictor of RB (OR 2.8; 95% CI 1.1-8.7; p = 0.027). Rates of patient-reported RB after EBRT were similar with 15.0% (n = 25) having any RB and 0.6% (n = 1) having \"very much\" bleeding. On regression analysis, any EBRT (either EBRT alone or EBRT + VB) increased the risk of RB compared to those who received VB alone (OR 3.0; p = 0.0028; 95% CI 1.4-6.7). The rates of more severe RB were low and did not significantly differ between treatments. Conclusions: Significant RB is rare after radiation. EBRT has higher rates of rectal bleeding than VB. The addition of VB to EBRT does not significantly alter bleeding rates. (C) 2015 Elsevier Ireland Ltd. All rights reserved.", "journal": "RADIOTHERAPY AND ONCOLOGY", "category": "Oncology; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000348037800023", "keywords": "Two-stage ignition; NTC phenomenon; Diesel spray; Dual-fuel engine", "title": "Two-stage ignition and NTC phenomenon in diesel engines", "abstract": "Two-stage ignition and NTC phenomenon in diesel sprays is investigated by performing 3-D two-phase reacting flow simulations in a dual-fuel engine. Spray processes modeled include fuel atomization, droplet distortion, droplet drag, turbulent dispersion, droplet interactions in terms of collision and coalescence, vaporization, and spray-wall interaction. A validated reaction mechanism is implemented in the CFD solver, which has previously been validated for both evaporating and reacting sprays. For single-fuel cases, the effect of temperature on two-stage ignition is examined by varying the start of injection (SOI). While results indicate global similarities between the two-stage ignition processes in diesel sprays and spatially homogeneous mixtures, there are also noticeable differences between them due to temporally and spatially evolving temperature and species fields in the spray case. For instance, both the first-and second-stage ignition delays are higher for the spray cases compared to homogeneous mixtures. Second, while ignition delay for homogeneous mixtures exhibits a NTC region, that for sprays indicate a ZTC region. Moreover, the first- and second-stage ignitions for the spray occur over a wide phi range and at multiple locations in the spray, implying a spatially wide ignition kernel. Additionally, while the chemical ignition delays are strongly influenced by the injection timing, the physical delays are essentially independent of this parameter. Results with dual fuel indicate that the two-stage ignition behavior remains intact even at high molar fractions of methane. The addition of methane increases ignition delays for both sprays and homogeneous mixtures, and can be attributed to the reduction in O-2 and the chemical effect of methane. The sensitivity analysis indicated that the chemical effect is primarily due to reaction CH4 + OH = CH3 + H2O. (c) 2015 Elsevier Ltd. All rights reserved.", "journal": "FUEL", "category": "Energy & Fuels; Engineering, Chemical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000355668000002", "keywords": "RASI; Cardiac surgery; CABG; Mortality; Meta-analysis", "title": "Meta-analysis of the effects of preoperative renin-angiotensin system inhibitor therapy on major adverse cardiac events in patients undergoing cardiac surgery", "abstract": "The purpose of this meta-analysis was to assess the role of preoperative renin-angiotensin system inhibitor (RASI) therapy on major adverse cardiac events (MACE) in patients undergoing cardiac surgery. The Medline, Cochrane Library and Embase databases were searched for clinical studies published up to May 2014. Studies that evaluated the effects of preoperative RASI therapy in cardiac surgery were included. Odds ratio (OR) estimates were generated under a random-effects model. After a literature search in the major databases, 18 studies were identified [three randomized prospective clinical trials (RCTs) and 15 observational trials] that reported outcomes of 54 528 cardiac surgery patients with (n = 22 661; 42%) or without (n = 31 867; 58%) preoperative RASI therapy. Pool analysis indicated that preoperative RASI therapy was not associated with a significant reduction of early all-cause mortality [OR: 1.01; 95% confidence interval (CI) 0.88-1.15, P = 0.93; I-2 = 25%], myocardial infarction (OR: 1.04; 95% CI 0.91-1.19, P = 0.60; I-2 = 16%), or stroke (OR: 0.93; 95% CI 0.75-1.14, P = 0.46; I-2 = 38%). Meta-regression analysis confirmed that there was a strong negative correlation between the percentage of diabetics and early all-cause mortality (P = 0.03). Furthermore, preoperative RASI therapy significantly reduced mortality in studies containing a high proportion of diabetic patients (OR: 0.84; 95% CI 0.71-0.99, P = 0.04; I-2 = 0%). In conclusion, our meta-analysis indicated that although preoperative RASI therapy was not associated with a lower risk of MACE in cardiac surgery patients, it might provide benefits for diabetic patients.", "journal": "EUROPEAN JOURNAL OF CARDIO-THORACIC SURGERY", "category": "Cardiac & Cardiovascular Systems; Respiratory System; Surgery", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000358804700003", "keywords": "Lorenz knots; Hyperbolic knots; Symbolic dynamics", "title": "Partial classification of Lorenz knots: Syllable permutations of torus knots words", "abstract": "We define families of aperiodic words associated to Lorenz knots that arise naturally as syllable permutations of symbolic words corresponding to torus knots. An algorithm to construct symbolic words of satellite Lorenz knots is defined. We prove, subject to the validity of a previous conjecture, that Lorenz knots coded by some of these families of words are hyperbolic, by showing that they are neither satellites nor torus knots and making use of Thurston's theorem. Infinite families of hyperbolic Lorenz knots are generated in this way, to our knowledge, for the first time. The techniques used can be generalized to study other families of Lorenz knots. (C) 2015 Elsevier B.V. All rights reserved.", "journal": "PHYSICA D-NONLINEAR PHENOMENA", "category": "Mathematics, Applied; Physics, Fluids & Plasmas; Physics, Multidisciplinary; Physics, Mathematical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000349296400010", "keywords": "Simulation; Technical skills; Laparoscopy training; Fundamentals of Laparoscopic Surgery", "title": "Camera navigation and cannulation: validity evidence for new educational tasks to complement the Fundamentals of Laparoscopic Surgery Program", "abstract": "Experts identified camera navigation and cannulation as important skills that are not assessed by the Fundamentals of Laparoscopic Surgery (FLS) hands-on examination. The purpose of this study was to create metrics for and evaluate the validity for two new tasks: camera navigation (N) and cannulation (C), and to explore the potential value of adding these tasks to the FLS program. Participants were assessed by two raters during performance of N and C in addition to the five standard FLS tasks. They also completed a questionnaire regarding the educational value of the new tasks. Validity evidence was assessed by comparing performance between Novice (PGY 1 and 2) and Experienced (PGY 3 and higher) participants, and by correlating new task scores with standard FLS scores. The ability to predict level of training using scores was evaluated by regression analysis. Sixty subjects participated from five North American centers. Inter-rater reliabilities for both tasks were 0.99. Novice and Experienced participants scored 74 +/- A 17.8 versus 85 +/- A 8.3 (p < 0.01) and 21 +/- A 17.3 versus 39 +/- A 20.1 (p < 0.01) on N and C tasks, respectively. Correlations with total FLS scores for N and C were 0.39 and 0.53, respectively. Prediction of training level using the combination of all seven tasks was 52.6 % (R (2) = 0.526, p < 0.01), adding an additional 2.2 % to the five FLS tasks. Of 55 participants with laparoscopic experience, 51 % reported N to be similar in difficulty to reality. Of 28 participants who perform intraoperative cholangiograms, 43 % found C to be more difficult than reality. Most (70 %) participants thought the new tasks added value to FLS. This study provides preliminary validity evidence for the metrics of these new tasks. The value of adding these tasks to the FLS manual skills assessment is marginal in terms of predicting level of training.", "journal": "SURGICAL ENDOSCOPY AND OTHER INTERVENTIONAL TECHNIQUES", "category": "Surgery", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000357000900007", "keywords": "Microscopic hematuria; Diagnosis; Evaluation; ESRD; Genitourinary cancer", "title": "Evaluation of Microscopic Hematuria: A Critical Review and Proposed Algorithm", "abstract": "Microscopic hematuria (MH), often discovered incidentally, has many causes, including benign processes, kidney disease, and genitourinary malignancy. The clinician, therefore, must decide how intensively to investigate the source of MH and select which tests to order and referrals to make, aiming not to overlook serious conditions while simultaneously avoiding unnecessary tests. Existing professional guidelines for the evaluation of MH are largely based on expert opinion and have weak evidence bases. Existing data demonstrate associations between isolated MH and various diseases in certain populations, and these associations serve as the basis for our proposed approach to the evaluation of MH. Various areas of ongoing uncertainty regarding the appropriate evaluation should be the basis for ongoing research. (C) 2015 by the National Kidney Foundation, Inc. All rights reserved.", "journal": "ADVANCES IN CHRONIC KIDNEY DISEASE", "category": "Urology & Nephrology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000345492500005", "keywords": "Optical extensometer; Out-of-plane motions; Strain measurement; Correction sheet; Digital image correlation", "title": "Optical extensometer and elimination of the effect of out-of-plane motions", "abstract": "A novel optical extensometer is developed to estimate the local uniform strain on planar surface accurately. The proposed system consists of a shared large format lens and two image sensors, which acquire pairs of images of two isolated small regions on the object surface simultaneously. Digital image correlation (DIC) algorithm is applied to determine the relative displacement between gauge points designated on recorded pairs of images. Then local strain can be extracted after dividing the relative displacement by the scale distance. Moreover, a special experimental setup called \"correction sheet\" is used to eliminate the virtual strain induced by out-of-plane motions. Uni-axial tensile experiments are performed to validate the reliability and resolution of the optical extensometer, and the measurement results demonstrate that the resolution of the optical extensometer achieves 2-3 mu epsilon. (C) 2014 Elsevier Ltd. All rights reserved.", "journal": "OPTICS AND LASERS IN ENGINEERING", "category": "Optics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000356237500007", "keywords": "Gestational weight gain; IOM-guidelines; BMI; Diet; Physical activity; Health promotion", "title": "Weight gain in healthy pregnant women in relation to pre-pregnancy BMI, diet and physical activity", "abstract": "Objective: to explore gestational weight gain in healthy women in relation to pre-pregnancy Body Mass Index, diet and physical activity. Design: a cross-sectional survey was conducted among 455 healthy pregnant women of all gestational ages receiving antenatal care from an independent midwife in the Netherlands. Weight gain was assessed using the Institute of Medicine (IOM) guidelines and classified as below, within, or above the guidelines. A multinomial regression analysis was performed with weight gain classifications as the dependent variable (within IOM-guidelines as reference). Independent variables were pre-pregnancy Body Mass Index, diet (broken clown into consumption of vegetables, fruit and fish) and physical activity (motivation to engage in physical activity, pre-pregnancy physical activity and decline in physical activity during pregnancy). Covariates were age, gestational age, parity, ethnicity, family income, education, perceived sleep deprivation, satisfaction with pre-pregnancy weight, estimated prepregnancy body mass index, smoking, having a weight gain goal and having received weight gain advice from the midwife. Findings: forty-two per cent of the women surveyed gained weight within the guidelines. Fourteen per cent of the women gained weight below the guidelines and 44 per cent gained weight above the guidelines. Weight gain within the guidelines, compared to both above and below the guidelines, was not associated with pre-pregnancy Body Mass Index nor with diet. A decline in physical activity was associated with weight gain above the guidelines (OR 0.54, 95 per cent CI 0.33-0.89). Weight gain below the guidelines was seen more often in women who perceived a greater sleep deprivation (OR 1.20, 95 per cent CI 1.02-1.41). Weight gain above the guidelines was seen less often in Caucasian women in comparison to non Caucasian women (OR 0.22, 95 per cent CI 0.08-0.58) and with women who did not stop smoking during pregnancy (OR 0.49, 95 per cent CI 0.25-0.95). Key conclusions awl implications for practice: a decline in physical activity was the only modifiable factor in our population associated with weight gain above the gain recommended by the guidelines. Prevention of reduced physical activity during pregnancy seems a promising approach to promoting healthy weight gain. Interventions to promote healthy weight gain should focus on all women, regardless of pre-pregnancy body mass index. (C) 2015 Elsevier Ltd. All rights reserved.", "journal": "MIDWIFERY", "category": "Nursing", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000371388200146", "keywords": "Tracking filter; Kalman filter; parameter setting; steady-state performance; process noise", "title": "Automatic Parameter Setting Method for an Accurate Kalman Filter Tracker Using an Analytical Steady-State Performance Index", "abstract": "We present an automatic parameter setting method to achieve an accurate second-order Kalman filter tracker based on a steady-state performance index. First, we propose an efficient steady-state performance index that corresponds to the root-mean-square (rms) prediction error in tracking. We then derive an analytical relationship between the proposed performance index and the generalized error covariance matrix of the process noise, for which the automatic determination using the derived relationship is presented. The model calculated by the proposed method achieves better accuracy than the conventional empirical model of process noise. Numerical analysis and simulations demonstrate the effectiveness of the proposed method for targets with accelerating motion. The rms prediction error of the tracker designed by the proposed method is 63.8% of that with the conventional empirically selected model for a target accelerating at 10 m/s(2).", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000357854700004", "keywords": "Air traffic flow management; Dynamic adjustment and distribution; En-route traffic load balance", "title": "Statistical physics of nonlinear wave interaction", "abstract": "The thermodynamic properties of vector [O(2) and complex spherical] models with four-body interactions are analyzed. When defined in dense topologies, these are effective models for the nonlinear interaction of scalar fields in the presence of a stochastic noise, as has been well established for the case of the mode-locking laser formation in a closed cavity. With the help of an efficient Monte Carlo algorithm we show how, beyond the fully connected case, rich phenomenology emerges. Below a certain dilution threshold, the spherical model condenses in a nonequipartite way, while in the XY model the transition becomes continuous and the O(2) symmetry remains unbroken. We attribute this fact to the invariance under gauge transformations. The introduction of topological inhomogeneities in the network of quadruplets induces some features: again symmetry conservation; the vanishing of two-point correlators; and a dynamical correlation function presenting two time scales, the large one being related to the transition between different degenerated configurations, connected by particular gauge transformations. We discuss possible experimental implications of these results in the context of nonlinear optics.", "journal": "PHYSICAL REVIEW B", "category": "Materials Science, Multidisciplinary; Physics, Applied; Physics, Condensed Matter", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000346616800030", "keywords": "Ovarian cancer; Biomarker; Biological factors; Biological variability", "title": "Human epididymis protein 4: Factors of variation", "abstract": "Background: Amongst the newly proposed biomarkers for ovarian cancer, serum human epididymis protein 4 (HE4) shows the greatest potential for clinical use. However, systematic appraisals of its biological characteristics are not available. This study sought to critically revise the available literature on biological and lifestyle factors affecting HE4 concentrations in serum to understand their possible influence on the marker interpretation. Methods: A literature search was undertaken on electronic databases and references from retrieved articles. Article results were analyzed by evaluating study design, sample size, statistical approach, employed assay and, when available, by collecting similar information for carbohydrate antigen 125 (CA-125). Results: Several factors may influence serum HE4 concentrations. In contrast to CA-125, higher HE4 concentrations are reported in the elderly. Although no variations in HE4 concentrations can be clearly associated to menopausal status, a strong difference in biomarker biological intra-individual variation according to the fertility status is reported. Smoking and renal function can also significantly influence HE4 results. Conclusion: The knowledge of factors influencing HE4 concentrations is relevant to promote more adequate interpretative criteria for use of this biomarker in the clinical setting. (C) 2014 Elsevier B.V. All rights reserved.", "journal": "CLINICA CHIMICA ACTA", "category": "Medical Laboratory Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000353833800011", "keywords": "urban impervious surfaces; land surface temperature; spatial pattern; Shanghai city", "title": "Understanding the effects of the impervious surfaces pattern on land surface temperature in an urban area", "abstract": "It is well known that urban impervious surface (IS) has a warming effect on urban land surface temperature (LST). However, the influence of an IS's structure, components, and spatial distribution on LST has rarely been quantitatively studied within strictly urban areas. Using ETM+ remote sensing images from the downtown area of Shanghai, China in 2010, this study characterized and quantified the influence of the IS spatial pattern on LST by selecting the percent cover of each IS cover feature and ten configuration metrics. The IS fraction was estimated by linear spectral mixture analysis (LSMA), and LST was retrieved using a mono-window algorithm. The results indicate that high fraction IS cover features account for the majority of the study area. The high fraction IS cover features are widely distributed and concentrated in groups, which is similar with that of high temperature zones. Both the percent composition and the configuration of IS cover features greatly affect the magnitude of LST, but the percent composition is a more important factor in determining LST than the configuration of those features. The significances and effects of the given configuration variables on LST vary greatly among IS cover features.", "journal": "FRONTIERS OF EARTH SCIENCE", "category": "Geosciences, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000354772400002", "keywords": "diffuse parenchymal lung disease; digital signature; lung fibrosis; pulmonary fibrosis; X-ray", "title": "Developing and Evaluating Ice Cloud Parameterizations for Forward Modeling of Radar Moments Using in situ Aircraft Observations", "abstract": "Observing ice clouds using zenith pointing millimeter cloud radars is challenging because the transfer functions relating the observables to meteorological quantities are not uniquely defined. Here, the authors use a spectral radar simulator to develop a consistent dataset containing particle mass, area, and size distribution as functions of size. This is an essential prerequisite for radar sensitivity studies and retrieval development. The data are obtained from aircraft in situ and ground-based radar observations during the Indirect and Semi-Direct Aerosol Campaign (ISDAC) campaign in Alaska. The two main results of this study are as follows: 1) An improved method to estimate the particle mass-size relation as a function of temperature is developed and successfully evaluated by combining aircraft in situ and radar observations. The method relies on a functional relation between reflectivity and Doppler velocity. 2) The impact on the Doppler spectrum by replacing measurements of particle area and size distribution by recent analytical expressions is investigated. For this, higher-order moments such as skewness and kurtosis as well as the slopes of the Doppler spectrum are also used as a proxy for the Doppler spectrum. For the area-size relation, it is found that a power law is not sufficient to describe particle area and small deviations from a power law are essential for obtaining consistent higher moments. For particle size distributions, the normalization approach for the gamma distribution of Testud et al., adapted to maximum diameter as size descriptor, is preferred.", "journal": "JOURNAL OF ATMOSPHERIC AND OCEANIC TECHNOLOGY", "category": "Engineering, Ocean; Meteorology & Atmospheric Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000356108400003", "keywords": "Community structure; Evolution; Hamming distance; Hasse diagram; Payoff matrix; Tanimoto coefficient", "title": "Evolutionary growth of certain metabolic pathways involved in the functioning of GAD and INS genes in Type 1 Diabetes Mellitus: Their architecture and stability", "abstract": "Background: Studying biochemical pathway evolution for diseases is a flourishing area of Systems Biology. Here, we study Type 1 Diabetes Mellitus (T1D), focusing on growth of glutamate, beta-alanine, taurine and hypotaurine, and butanoate metabolisms involved in onset of GAD and INS genes in Homo sapiens with comparative analysis in non-obese diabetic Mus musculus, biobreeding Diabetes-prone Rattus norvegicus, Pan troglodytes, Oryctolagus cuniculus, Danio rerio and Drosophila melanogaster respectively. Methods: We propose an algorithm for growth analysis for four metabolic pathways involved in T1D. It has three modules, pattern finding, interaction identification and growth detection. The first module identifies patterns using Community structures using Hamming distances and the Tanimoto coefficient. We have performed functional analysis by representing patterns using ODEs, and identified Stoichiometric, Gradient and Jacobian matrices. The second module identifies interactions among patterns using cut-sets and network-partitioning by 'Divide-and-conquer'. The third module identifies functions of patterns using interactions, thereby highlighting their nature of growth. Results: We observed that metabolites that are genetically robust and resist alterations against stable state during evolution, account for emergence of a scale-free network. Discussion: New modules get acquired to the fundamental cluster in a preferential manner, an instance of micro-evolution theory. For instance, (S)-3-hydroxy butanoyl-CoA, acetoacetyl-CoA, acetoacetate, acetyl-CoA, (S)-3-hydroxy-3-methyl glutatyl-CoA acts as a fundamental cluster in butanoate metabolism. Moreover, the interactions among metabolites are divergent in nature. (C) 2015 Elsevier Ltd. All rights reserved.", "journal": "COMPUTERS IN BIOLOGY AND MEDICINE", "category": "Biology; Computer Science, Interdisciplinary Applications; Engineering, Biomedical; Mathematical & Computational Biology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000360933800001", "keywords": "Optical coherence tomography angiography; Amplitude decorrelation angiography; Choroidal neovascularization; Exudative age-related macular degeneration; Multimodal imaging", "title": "Optical Coherence Tomography Angiography during Follow-Up: Qualitative and Quantitative Analysis of Mixed Type I and II Choroidal Neovascularization after Vascular Endothelial Growth Factor Trap Therapy", "abstract": "Purpose: To report the optical coherence tomography angiography (OCT-A) findings in an exudative age-related macular degeneration (AMD) patient presenting mixed type I and II choroidal neovascularization (CNV) during follow-up after intravitreal vascular endothelial growth factor (VEGF) trap treatment. Methods: The clinical assessment included both traditional multimodal imaging, based on fluorescein angiography (FA), indocyanine green angiography (ICGA) and B-scan OCT, and OCT-A at baseline and follow-up. OCT-A images were obtained using a Spectralis OCT-A prototype able to acquire 70,000 A-scans per second, with a resolution of 7 mu m axially and 14 mu m laterally. An amplitude decorrelation algorithm developed by Heidelberg Engineering was applied to a volume scan, on a 15 x 5 degrees area, which was com- posed of 131 B-scans (35 frames per scan) at a distance of 11 mu m each. The borders of type I and type II CNV were manually outlined and then the areas were analyzed using the provided automated software before and after treatment. Results: The qualitative approach revealed a substantial decrease in the visibility of tiny branching vessels and anastomoses both in type I and type II components of the neovascular complex, associated with persistence of a clear hyperintense signal coming from the larger trunks, which remained well-perfused. Quantitative analysis confirmed a reduction of the lesion area after VEGF trap treatment: the type II component decreased from 0.25 to 0.19 mm(2), while the type I component decreased from 2.03 to 1.80 mm(2). Conclusions: Our study qualitatively and quantitatively demonstrated the response of a mixed type I-II CNV to intravitreal VEGF trap therapy. Although FA remains the gold standard for determining the presence of leakage and OCT easily shows fluid accumulation and its variations, OCT-A offers noninvasive monitoring of the retinal and choriocapillaris microvasculature in patients with CNV, aiding in diagnosis and treatment decisions during follow-up. (C) 2015 S. Karger AG, Basel", "journal": "OPHTHALMIC RESEARCH", "category": "Ophthalmology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000354412400003", "keywords": "Image mosaicking; Parallel computing; Review", "title": "A review of parallel computing for large-scale remote sensing image mosaicking", "abstract": "Interest in image mosaicking has been spurred by a wide variety of research and management needs. However, for large-scale applications, remote sensing image mosaicking usually requires significant computational capabilities. Several studies have attempted to apply parallel computing to improve image mosaicking algorithms and to speed up calculation process. The state of the art of this field has not yet been summarized, which is, however, essential for a better understanding and for further research of image mosaicking parallelism on a large scale. This paper provides a perspective on the current state of image mosaicking parallelization for large scale applications. We firstly introduce the motivation of image mosaicking parallel for large scale application, and analyze the difficulty and problem of parallel image mosaicking at large scale such as scheduling with huge number of dependent tasks, programming with multiple-step procedure, dealing with frequent I/O operation. Then we summarize the existing studies of parallel computing in image mosaicking for large scale applications with respect to problem decomposition and parallel strategy, parallel architecture, task schedule strategy and implementation of image mosaicking parallelization. Finally, the key problems and future potential research directions for image mosaicking are addressed.", "journal": "CLUSTER COMPUTING-THE JOURNAL OF NETWORKS SOFTWARE TOOLS AND APPLICATIONS", "category": "Computer Science, Information Systems; Computer Science, Theory & Methods", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000347856300017", "keywords": "Rolling element bearing; Non-linear dynamics; Hertzian contact; Health monitoring; Fault simulation; Duplex bearing", "title": "Toward a 3D dynamic model of a faulty duplex ball bearing", "abstract": "Bearings are vital components for safe and proper operation of machinery. Increasing efficiency of bearing diagnostics usually requires training of health and usage monitoring systems via expensive and time-consuming ground calibration tests. The main goal of this research, therefore, is to improve bearing dynamics modeling tools in order to reduce the time and budget needed to implement the health and usage monitoring approach. The proposed three-dimensional ball bearing dynamic model is based on the classic dynamic and kinematic equations. Interactions between the bodies are simulated using non-linear springs combined with dampers described by Hertz-type contact relation. The force friction is simulated using the hyperbolic-tangent function. The model allows simulation of a wide range of mechanical faults. It is validated by comparison to known bearing behavior and to experimental results. The model results are verified by demonstrating numerical convergence. The model results for the two cases of single and duplex angular ball bearings with axial deformation in the outer ring are presented. The qualitative investigation provides insight into bearing dynamics, the sensitivity study generalizes the qualitative findings for similar cases, and the comparison to the test results validates model reliability. The article demonstrates the variety of the cases that the 3D bearing model can simulate and the findings to which it may lead. The research allowed the identification of new patterns generated by single and duplex bearings with axially deformed outer race. It also enlightened the difference between single and duplex bearing manifestation. In the current research the dynamic model enabled better understanding of the physical behavior of the faulted bearings. Therefore, it is expected that the modeling approach has the potential to simplify and improve the development process of diagnostic algorithms. (C) 2014 Elsevier Ltd. All rights reserved.", "journal": "MECHANICAL SYSTEMS AND SIGNAL PROCESSING", "category": "Engineering, Mechanical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000347602000025", "keywords": "Approximate GCD; Overdetermined systems; Nearest consistent system; Weierstrass-Durand-Kerner method", "title": "Overdetermined Weierstrass iteration and the nearest consistent system", "abstract": "We propose a generalization of the Weierstrass iteration for overdetermined systems of equations and we prove that the proposed method is the Gauss-Newton iteration to find the nearest system which has at least k common roots and which is obtained via a perturbation of prescribed structure. In the univariate case we show the connection of our method to the optimization problem formulated by Karmarkar and Lakshman for the nearest GCD. In the multivariate case we generalize the expressions of Karmarkar and Lakshman, and give explicitly several iteration functions to compute the optimum. The arithmetic complexity of the iterations is detailed. (C) 2014 Elsevier B.V. All rights reserved.", "journal": "THEORETICAL COMPUTER SCIENCE", "category": "Computer Science, Theory & Methods", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000354422400005", "keywords": "Brain damage chronic; Solvents; Occupational exposure", "title": "Organic psychosyndrome among workers exposed to organic solvents", "abstract": "Aim of the study. To determine occupational factors of organic psychosyndrome (PSO) among workers exposed to moderate to low concentrations of organic solvents. Method. A descriptive cross-sectional study was conducted including 306 persons working in factories using organic solvents. PSO evaluation was performed using a questionnaire adapted from the Swedish Q16 questionnaire. Acute and cumulative solvent exposures were measured and calculated. Results. PSO prevalence was 10.9%. Air concentration measurements showed organic solvent concentrations under exposure limits values. Prevalence of PSO was not correlated with acute exposure but a significant relation was found with cumulative exposure (P < 10(-3); OR = 13.9 IC: 3.2-59.7). Multivariate analysis in logistic regression showed significant correlations of PSO with marital status (P = 0.036; OR = 2.86 IC: 1.07-7.64) and exposure group (P = 0.008; OR = 15.27 IC: 2.04-114.42). Conclusion. Workers chronically exposed to organic solvents even to low concentrations require vigilance. It is interesting to develop a work-exposure matrix to objectively evaluate solvent exposure and implement appropriate prevention measures. (C) 2014 Elsevier Masson SAS. All rights reserved.", "journal": "ARCHIVES DES MALADIES PROFESSIONNELLES ET DE L ENVIRONNEMENT", "category": "Public, Environmental & Occupational Health", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000365233100004", "keywords": "Cognitive radio; channel uncertainty; noise uncertainty; spectrum sensing; transceiver imperfections; underlay", "title": "Cognitive Radio Techniques Under Practical Imperfections: A Survey", "abstract": "Cognitive radio (CR) has been considered as a potential candidate for addressing the spectrum scarcity problem of future wireless networks. Since its conception, several researchers, academic institutions, industries, and regulatory and standardization bodies have put their significant efforts toward the realization of CR technology. However, as this technology adapts its transmission based on the surrounding radio environment, several practical issues may need to be considered. In practice, several imperfections, such as noise uncertainty, channel/interference uncertainty, transceiver hardware imperfections, signal uncertainty, and synchronization issues, may severely deteriorate the performance of a CR system. To this end, the investigation of realistic solutions toward combating various practical imperfections is very important for the successful implementation of cognitive technology. In this direction, first, this survey paper provides an overview of the enabling techniques for CR communications. Subsequently, it discusses the main imperfections that may occur in the most widely used CR paradigms and then reviews the existing approaches toward addressing these imperfections. Finally, it provides some interesting open research issues.", "journal": "IEEE COMMUNICATIONS SURVEYS AND TUTORIALS", "category": "Computer Science, Information Systems; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000378799300096", "keywords": "nonlinear gene-environment interaction; sparse principal component analysis; varying-coefficient model", "title": "A Nonlinear Model for Gene-Based Gene-Environment Interaction", "abstract": "A vast amount of literature has confirmed the role of gene-environment (G x E) interaction in the etiology of complex human diseases. Traditional methods are predominantly focused on the analysis of interaction between a single nucleotide polymorphism (SNP) and an environmental variable. Given that genes are the functional units, it is crucial to understand how gene effects (rather than single SNP effects) are influenced by an environmental variable to affect disease risk. Motivated by the increasing awareness of the power of gene-based association analysis over single variant based approach, in this work, we proposed a sparse principle component regression (sPCR) model to understand the gene-based G x E interaction effect on complex disease. We first extracted the sparse principal components for SNPs in a gene, then the effect of each principal component was modeled by a varying-coefficient (VC) model. The model can jointly model variants in a gene in which their effects are nonlinearly influenced by an environmental variable. In addition, the varying-coefficient sPCR (VC-sPCR) model has nice interpretation property since the sparsity on the principal component loadings can tell the relative importance of the corresponding SNPs in each component. We applied our method to a human birth weight dataset in Thai population. We analyzed 12,005 genes across 22 chromosomes and found one significant interaction effect using the Bonferroni correction method and one suggestive interaction. The model performance was further evaluated through simulation studies. Our model provides a system approach to evaluate gene-based G x E interaction.", "journal": "INTERNATIONAL JOURNAL OF MOLECULAR SCIENCES", "category": "Biochemistry & Molecular Biology; Chemistry, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000379761500031", "keywords": "Medical IoT device; Personal healthcare; Medical diagnosis; Trajectory-based analytics", "title": "Branch: an interactive, web-based tool for testing hypotheses and developing predictive models", "abstract": "A Summary: Branch is a web application that provides users with the ability to interact directly with large biomedical datasets. The interaction is mediated through a collaborative graphical user interface for building and evaluating decision trees. These trees can be used to compose and test sophisticated hypotheses and to develop predictive models. Decision trees are built and evaluated based on a library of imported datasets and can be stored in a collective area for sharing and re-use.", "journal": "BIOINFORMATICS", "category": "Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000372457601737", "keywords": "sensor; opportunistic network; communication; big data; energy supply", "title": "Variation in large feeding biomechanics datasets visualized using different dimensionality reduction methods", "abstract": "Energy consumption is an important index in mobile ad hoc networks. Data packet transmission increases among nodes, particularly in big data communication. However, nodes may be unable to transmit data packets because of energy over-consumption. Consequently, information may be lost and network communication may block. While opportunistic network is a kind of mobile ad hoc networks, researchers do not focus on energy consumption in opportunistic network communication. This study proposed an effective sensor energy supply scheme that can be applied in opportunistic networks. This scheme considers nodes sensor requests and communication model. In this scheme, nodes do not only accomplish energy supply in communication, but also extend communication time in opportunistic networks. Compared with the Spray and Wait algorithm and the Binary Spray and Wait algorithm in simulations, the proposed scheme extends communication time, increases data packet transmission, and accomplishes energy supply among nodes.", "journal": "INTEGRATIVE AND COMPARATIVE BIOLOGY", "category": "Zoology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000372583700008", "keywords": "Autoregressive conditional duration model; Bootstrap; Cramer-von Mises statistic; Goodness-of-fit test; Kolmogorov-Smirnov statistic; C32; C41; C52", "title": "A Goodness-of-Fit Test for a Class of Autoregressive Conditional Duration Models", "abstract": "This article develops a method for testing the goodness-of-fit of a given parametric autoregressive conditional duration model against unspecified nonparametric alternatives. The test statistics are functions of the residuals corresponding to the quasi maximum likelihood estimate of the given parametric model, and are easy to compute. The limiting distributions of the test statistics are not free from nuisance parameters. Hence, critical values cannot be tabulated for general use. A bootstrap procedure is proposed to implement the tests, and its asymptotic validity is established. The finite sample performances of the proposed tests and several other competing ones in the literature, were compared using a simulation study. The tests proposed in this article performed well consistently throughout, and they were either the best or close to the best. None of the tests performed uniformly the best. The tests are illustrated using an empirical example.", "journal": "ECONOMETRIC REVIEWS", "category": "Economics; Mathematics, Interdisciplinary Applications; Social Sciences, Mathematical Methods; Statistics & Probability", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000381021600002", "keywords": "Learning experience; online; quality; students' perceptions; teachers' perceptions", "title": "Measuring teachers and learners' perceptions of the quality of their online learning experience", "abstract": "This article explores the quality of the online learning experience based on the Sloan-C framework and the Online Learning Consortium's (OLC) quality scorecard. The OLC index has been implemented to evaluate quality in online programs from different perspectives. Despite this, the opinions of learners are ignored, and it is built using feedback from experts and panelists while ignoring the factors that teachers consider important during their lectures. We propose an alternative way of measuring the quality of online learning programs by analyzing the satisfaction of the learning experience and using teachers and students' perceptions. The 11 categories composing the index were weighted in the teacher and student indices using principal component analysis, and finally these two indices were linearly combined with a parameter that defines the importance of each body. Findings show that while teachers perceive collaborative learning variables as crucial, learners are more concerned with their own learning benefits.", "journal": "DISTANCE EDUCATION", "category": "Education & Educational Research", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000378974500041", "keywords": "Oryza sativa; rice; specialized metabolite; anti-inflammatory; anti-oxidant", "title": "Metabolome Analysis of Oryza sativa (Rice) Using Liquid Chromatography-Mass Spectrometry for Characterizing Organ Specificity of Flavonoids with Anti-inflammatory and Anti-oxidant Activity", "abstract": "Oryza sativa L. (rice) is an important staple crop across the world. In the previous study, we identified 36 specialized (secondary) metabolites including 28 flavonoids. In the present study, a metabolome analysis using liquid chromatography-mass spectrometry was conducted on the leaf, bran, and brown and polished rice grains to better understand the distribution of these metabolites. Principal component analysis using the metabolome data clearly characterized the accumulation patterns of the metabolites. Flavonoids, e.g., tricin, tricin 7-O-rutinoside, and tricin 7-O-beta-D-glucopyranoside, were mainly present in the leaf and bran but not in the polished grain. In addition, anti-inflammatory and anti-oxidant activity of the metabolites were assayed in vitro. Tricin 4'-O-(erythro-beta-guaiacylglyceryl)ether and isoscoparin 2 ''-O-(6 ''-(E)-feruloyl)glucopyranoside showed the strongest activity for inhibiting nitric oxide (NO) production and 1,1-diphenyl-2-picrylhydrazyl (DPPH) radical-scavenging, respectively.", "journal": "CHEMICAL & PHARMACEUTICAL BULLETIN", "category": "Chemistry, Medicinal; Chemistry, Multidisciplinary; Pharmacology & Pharmacy", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000382571100004", "keywords": "Condom use; early sexual debut; reproductive tract infections (RTIs); sexual partners; sexually transmitted infections (STIs); Malawi; youth population", "title": "Sexual Activity of the Youth Population in Malawi: The Emerging Health Care Scenario", "abstract": "The sexual behaviour of youths in Malawi is believed to play an important role in the spread of sexually transmitted infections (STIs). Relevant data from the Malawi Demographic and Health Survey 2010 and a sample of 16,217 youths aged 15-24 have been studied and subjected to bivariate and logistic regression analysis. The results show that married youths were not interested in using condoms (94.2%, p < 0.05) and that those who were living together were 69 times (OR = 1.69, 95% CI, 1.26-2.26) more likely to be involved in early sexual activity than those who were not living together. It is argued that the results should help other researchers, policy makers and planners to create strategies to encourage these youths make use of contraception.", "journal": "JOURNAL OF ASIAN AND AFRICAN STUDIES", "category": "Area Studies", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000374602000018", "keywords": "Inverse modelling; Sparse optimization; Integer optimization; Least squares; European tracer experiment; Free Matlab codes", "title": "Sparse optimization for inverse problems in atmospheric modelling", "abstract": "We consider inverse problems in atmospheric modelling represented by a linear system which is based on a source-receptor sensitivity matrix and measurements. Instead of using the ordinary least squares, we add a weighting matrix based on the topology of measurement points and show the connection with Bayesian modelling. Since the source-receptor sensitivity matrix is usually ill-conditioned, the problem is often regularized, either by perturbing the objective function or by modifying the sensitivity matrix. However, both these approaches may be heavily dependent on specified parameters. To ease this burden, we propose to use techniques looking for a sparse solution with a small number of positive elements. Finally, we compare all these methods on the European Tracer Experiment (ETEX) data where there is no apriori information apart from the release position and some measurements. (C) 2016 Elsevier Ltd. All rights reserved.", "journal": "ENVIRONMENTAL MODELLING & SOFTWARE", "category": "Computer Science, Interdisciplinary Applications; Engineering, Environmental; Environmental Sciences; Water Resources", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000384455900001", "keywords": "Mobile robot coverage; adversarial coverage; path planning; robotics in hazardous fields; demining", "title": "Robotic adversarial coverage of known environments", "abstract": "Coverage is a fundamental problem in robotics, where one or more robots are required to visit each point in a target area at least once. Most previous work has concentrated on finding a coverage path that would minimize the coverage time. In this paper, we consider a new and more general version of the problem: adversarial coverage. Here, the robot operates in an environment that contains threats that might stop the robot. The objective is to cover the target area as quickly as possible, while minimizing the probability that the robot will be stopped before completing the coverage. This version of the problem has many real-world applications, from performing coverage missions in hazardous fields such as nuclear power plants, to surveillance of enemy forces in the battlefield and field demining. In this paper, we discuss the offline version of adversarial coverage, in which a map of the threats is given to the robot in advance. First, we formally define the adversarial coverage problem and present different optimization criteria used to evaluate coverage algorithms in adversarial environments. We show that finding an optimal solution to the adversarial coverage problem is -hard. We therefore suggest two heuristic algorithms: STAC, a spanning-tree-based coverage algorithm, and GAC, which follows a greedy approach. We establish theoretical bounds on the total risk involved in the coverage paths created by these algorithms and on their lengths. Lastly, we compare the effectiveness of these two algorithms in various environments and settings.", "journal": "INTERNATIONAL JOURNAL OF ROBOTICS RESEARCH", "category": "Robotics", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000378979200013", "keywords": "thermal ablation; PET/CT; interventional radiology; imaging biomarkers; treatment failure", "title": "F-18-FDG PET/CT Is an Immediate Imaging Biomarker of Treatment Success After Liver Metastasis Ablation", "abstract": "The rationale of this study was to examine whether F-18-FDG PET/CT and contrast-enhanced CT performed immediately after percutaneous ablation of liver metastases are predictors of local treatment failure at 1 y. Methods: This Health Insurance Portability and Accountability Act compliant, Institutional Review Board-approved retrospective study reviewed 25 PET/CT-guided thermal, ablations performed from September 2011 to March 2013 on 21 patients (11 women and 10 men; mean age, 56.8 y; range, 35-79 y) for the treatment of liver metastases (colorectal, n = 23; breast, n = 1; and sarcoma, n = 1). One to 3 tumors (mean size, 2.3 cm; range, 0.7-4.6 cm; mean SUVmax, 22.7; range, 9.5-77.1) were ablated using radiofrequency (n = 16) or microwave (n = 9) energy in a single session. Immediate-postablation enhanced CT and PET/CT scans were qualitatively evaluated by 2 reviewers independently, and the results were compared with clinical and imaging outcome at 1 y. The PET/CT scans were also analyzed to determine tissue radioactivity concentration (TRC) from 3-dimensional regions of interest in the ablation zone, the margin, and the surrounding normal liver to calculate a TRC ratio, which was then compared with outcome at 1 y. Receiver operating characteristics (ROC) were used, and the maximal accuracy threshold in predicting recurrence was calculated. Results: Eleven (44%) of the 25 tumors recurred within 1 y. Enhanced CT did not significantly correlate with recurrence (P = 0.288). Accuracy was 64% (16/25), and the area under the ROC curve was 0.601 (95% confidence interval [95% CI], 0.387-0.789). The accuracy of the qualitative analysis of F-18-FDG PET was 92% (23/25) (P < 0.001), and the area under the ROC curve was 0.929 (95% CI, 0.740-0.990). The mean TRC ratio was 40.6 in the recurrence group (SD, 9.2; range, 29.3-53.9) and 15.9 in the group without recurrence (SD, 7.3; range, 3-27.3). A TRC ratio of 28.3 predicted recurrence at 1 y with 100% accuracy (25/25) (P < 0.001), and the area under the ROC curve was 1 (95% CI, 0.863-1). Conclusion: Immediate PET/CT accurately predicts the success of liver metastasis ablation at 1 y and is superior to immediate enhanced CT.", "journal": "JOURNAL OF NUCLEAR MEDICINE", "category": "Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000378864300036", "keywords": "Soil; Environmental magnetism; PTEs; Anthropogenic sources", "title": "Integration of magnetic measurements, chemical and statistical analysis in characterizing agricultural soils (central Portugal)", "abstract": "The agricultural soils near the city of Coimbra (central Portugal) have high levels of potentially toxic trace elements (PTEs). This is a pioneering study that allows the characterization of agricultural soils in central Portugal, using the environmental magnetism techniques and further to establish the main sources of pollution. On completion of magnetic analyses, a scanning electron microscope observation was carried out on magnetic extracts to provide information on the possible origin of particles in general. Chemical studies show a high concentration of some of PTEs in the topsoil. Statistical studies were used for examination of the relationships between the magnetic properties of soil samples and the concentrations of chemical elements. A positive and significant correlation was observed between magnetic measurements and PTEs concentrations. Principal component analysis allowed us to conclude that the main sources of the analysed elements are mostly related to the burning of fossil fuels. The results confirm that (1) simple, rapid and non-destructive magnetic measurements can provide useful information about PTEs enrichment in agricultural soils near the urban centre, (2) the environmental magnetism techniques can be used as a proxy for spatial distribution of pollution in this area and (3) the combination of magnetic methods with chemical and statistical analysis is an efficient tool for the identification of the main pollution sources in poorly industrialized areas and to assess environmental implications.", "journal": "ENVIRONMENTAL EARTH SCIENCES", "category": "Environmental Sciences; Geosciences, Multidisciplinary; Water Resources", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000384057600002", "keywords": "electrocorticography; spatial; resolution; cortical surface; optimal spacing; ECoG array; human", "title": "Spatial resolution dependence on spectral frequency in human speech cortex electrocorticography", "abstract": "Objective. Electrocorticography (ECoG) has become an important tool in human neuroscience and has tremendous potential for emerging applications in neural interface technology. Electrode array design parameters are outstanding issues for both research and clinical applications, and these parameters depend critically on the nature of the neural signals to be recorded. Here, we investigate the functional spatial resolution of neural signals recorded at the human cortical surface. We empirically derive spatial spread functions to quantify the shared neural activity for each frequency band of the electrocorticogram. Approach. Five subjects with high-density (4 mm center-to-center spacing) ECoG grid implants participated in speech perception and production tasks while neural activity was recorded from the speech cortex, including superior temporal gyrus, precentral gyrus, and postcentral gyrus. The cortical surface field potential was decomposed into traditional EEG frequency bands. Signal similarity between electrode pairs for each frequency band was quantified using a Pearson correlation coefficient. Main results. The correlation of neural activity between electrode pairs was inversely related to the distance between the electrodes; this relationship was used to quantify spatial falloff functions for cortical subdomains. As expected, lower frequencies remained correlated over larger distances than higher frequencies. However, both the envelope and phase of gamma and high gamma frequencies (30-150 Hz) are largely uncorrelated (<90%) at 4 mm, the smallest spacing of the high-density arrays. Thus, ECoG arrays smaller than 4 mm have significant promise for increasing signal resolution at high frequencies, whereas less additional gain is achieved for lower frequencies. Significance. Our findings quantitatively demonstrate the dependence of ECoG spatial resolution on the neural frequency of interest. We demonstrate that this relationship is consistent across patients and across cortical areas during activity.", "journal": "JOURNAL OF NEURAL ENGINEERING", "category": "Engineering, Biomedical; Neurosciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000379661300012", "keywords": "liver transplantation; new-onset diabetes; population-based study", "title": "Prevalence, predictive factors, and survival outcome of new-onset diabetes after liver transplantation: A population-based cohort study", "abstract": "The aim of the present nationwide population-based cohort study was to explore the prevalence, risk factors, and survival outcome of new-onset diabetes (NOD) in recipients after liver transplantation.The National Health Insurance Research Database of Taiwan was searched for ICD-9-codes, 2248 patients who had received liver transplant without pretransplant diabetes from July 1, 1998 to December 31, 2012 were included in the study. The preoperative risks factors were considered and analyzed using logistic regression analysis, following adjustments for age and sex. All patients were followed up until the end of the study or death.The final dataset included 189 patients with NOD and 2059 without diabetes after liver transplantation. The prevalence of NOD was 8.4% and in 64% NOD appeared in the first year after liver transplantation. Preoperative clinical events, alcoholic liver cirrhosis, and hepatic encephalopathy were the most important risk factors for NOD after liver transplantation. The mortality rate was lower in NOD recipients than in non-NOD recipients within 5 years.In this study, we provide evidence that NOD recipients had better 5-year survival outcomes in this clinical population. The most important identifiable predictive factors for NOD after liver transplantation were alcoholic hepatitis, ascites, hepatic coma, and esophageal varices.", "journal": "MEDICINE", "category": "Medicine, General & Internal", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000384438900012", "keywords": "Systemic Sclerosis; Outcomes research; Epidemiology", "title": "Prediction of improvement in skin fibrosis in diffuse cutaneous systemic sclerosis: a EUSTAR analysis", "abstract": "Objectives Improvement of skin fibrosis is part of the natural course of diffuse cutaneous systemic sclerosis (dcSSc). Recognising those patients most likely to improve could help tailoring clinical management and cohort enrichment for clinical trials. In this study, we aimed to identify predictors for improvement of skin fibrosis in patients with dcSSc. Methods We performed a longitudinal analysis of the European Scleroderma Trials And Research (EUSTAR) registry including patients with dcSSc, fulfilling American College of Rheumatology criteria, baseline modified Rodnan skin score (mRSS) 7 and follow-up mRSS at 122months. The primary outcome was skin improvement (decrease in mRSS of >5 points and 25%) at 1year follow-up. A respective increase in mRSS was considered progression. Candidate predictors for skin improvement were selected by expert opinion and logistic regression with bootstrap validation was applied. Results From the 919 patients included, 218 (24%) improved and 95 (10%) progressed. Eleven candidate predictors for skin improvement were analysed. The final model identified high baseline mRSS and absence of tendon friction rubs as independent predictors of skin improvement. The baseline mRSS was the strongest predictor of skin improvement, independent of disease duration. An upper threshold between 18 and 25 performed best in enriching for progressors over regressors. Conclusions Patients with advanced skin fibrosis at baseline and absence of tendon friction rubs are more likely to regress in the next year than patients with milder skin fibrosis. These evidence-based data can be implemented in clinical trial design to minimise the inclusion of patients who would regress under standard of care.", "journal": "ANNALS OF THE RHEUMATIC DISEASES", "category": "Rheumatology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000388568700011", "keywords": "Lipids; Genetics; Intracerebral hemorrhages; Cerebral microbleeds; Epidemiology", "title": "Cerebral White Matter Lesions and Post-Thrombolytic Remote Parenchymal Hemorrhage", "abstract": "Objective: Parenchymal hematoma (PH) following intravenous thrombolysis (IVT) in ischemic stroke can occur either within the ischemic area (iPH) or as a remote PH (rPH). The latter could be, at least partly, related to cerebral amyloid angiopathy, which belongs to the continuum of cerebral small vessel disease. We hypothesized that cerebral white matter lesions (WMLs)-an imaging surrogate of small vessel disease-are associated with a higher rate of rPH. Methods: We analyzed 2,485 consecutive patients treated with IVT at the Helsinki University Hospital. Blennow rating scale of 5 to 6 points on baseline computed tomographic head scans was considered as severe WMLs. An rPH was defined as hemorrhage that-contrary to iPH-appears in brain regions without visible ischemic damage and is clinically not related to the symptomatic acute lesion site. The associations between severe WMLs and pure rPH versus no PH, pure iPH versus no PH, and pure rPH versus pure iPH were studied in multivariate logistic regression models. Results: rPHs were mostly (74%) located in lobar regions. After adjustments, the presence of severe WMLs was associated with pure rPH (odds ratio [OR] 56.79, 95% confidence interval [CI] = 2.57-17.94) but not with pure iPH (OR=1.45, 95% CI=0.83-2.53) when compared to patients with no PH. In direct comparison of pure rPH with pure iPH, severe cerebral WMLs were further associated with higher iPH rates (OR=3.60, 95% CI=1.06-12.19). Interpretation: Severe cerebral WMLs were associated with post-thrombolytic rPH but not with iPH within the ischemic area.", "journal": "ANNALS OF NEUROLOGY", "category": "Clinical Neurology; Neurosciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000377642700045", "keywords": "Nanowires; GaN; nucleation; coalescence", "title": "Nucleation, Growth, and Bundling of GaN Nanowires in Molecular Beam Epitaxy: Disentangling the Origin of Nanowire Coalescence", "abstract": "We investigate the nucleation, growth, and coalescence of spontaneously formed GaN nanowires In, molecular beam epitaxy combining the statistical analysis of scanning electron micrographs with Monte Carlo growth models. We find that (i) the nanowire density is limited by the shadowing of the substrate from the impinging fluxes by already, existing nanowires, (ii) shortly after the nucleation stage, nanowire radial growth becomes negligible, and (iii) coalescence is caused by, bundling of nanowires. The latter phenomenon is driven by the gain of surface energy at the expense of the elastic energy of bending and becomes energetically favorable once the nanowires, exceed a certain critical length.", "journal": "NANO LETTERS", "category": "Chemistry, Multidisciplinary; Chemistry, Physical; Nanoscience & Nanotechnology; Materials Science, Multidisciplinary; Physics, Applied; Physics, Condensed Matter", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000385745800016", "keywords": "dystocia; fetal occiput position; labor; malposition; subpubic arch angle; three-dimensional ultrasound; ultrasound in labor", "title": "Narrow subpubic arch angle is associated with higher risk of persistent occiput posterior position at delivery", "abstract": "Objective To determine whether the subpubic arch angle (SPA) measured by three-dimensional ultrasound is associated with the fetal occiput position at delivery and the mode of delivery. Methods Nulliparous women with an uncomplicated singleton pregnancy at >= 37 weeks' gestation were recruited from two tertiary centers between September 2013 and August 2015. All women underwent a three-dimensional transperineal ultrasound examination and the SPA was measured using the previously validated Oblique View Extended Imaging software. Data on the outcome of labor were obtained prospectively in all cases and the correlations between SPA and the fetal occiput position at delivery and the incidence of operative delivery were investigated. Results Overall, 368 women were included in the study. Fetal position at delivery was occiput anterior in 339 (92.1%) cases and occiput posterior (OP) in 29 (7.9%) cases. A significantly narrower SPA was found in the OP group compared with the occiput anterior group (104.4 +/- 16.8 degrees vs 116.4 +/- 11.9 degrees; P<0.0001). The SPA was significantly narrower in women requiring obstetric intervention compared with in women with a spontaneous vaginal delivery. From multivariable logistic regression analysis, SPA and maternal height appeared to be significant predictors of both the fetal occiput position at delivery and the risk of operative delivery. The best cut-off value of SPA for predicting an OP position at delivery was 90.5 degrees. Conclusion A narrow SPA is associated with a higher risk of persistent OP position at delivery and of operative delivery. Copyright (C) 2015 ISUOG. Published by John Wiley & Sons Ltd.", "journal": "ULTRASOUND IN OBSTETRICS & GYNECOLOGY", "category": "Acoustics; Obstetrics & Gynecology; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000374323000003", "keywords": "XRD; in-plane diffraction; lateral block size; tilt and twist mosaicity", "title": "Determination of lateral block size and mosaicity of crystals using X-ray diffraction from the edge of the sample", "abstract": "In this paper a novel lateral block size characterization method based on X-ray diffractometry (XRD) is presented. The developed method based on scans of plans perpendicular to the surface visible from the edge of the sample and subconsequent numerical analysis based on Scherrer equation. In presented investigations as a reference sample bulk GaN substrate fabricated by AMMONO Ltd. was used. The GaN layers deposited on sapphire substrate were investigated as a case of study. Moreover, a method of analyzing the magnitude of the tilt and twist mosaicity based on the scans of rocking curves and reciprocal lattice maps (RLM) from the edge of sample is presented. As an example for this methodology highly mosaicited SiC crystal was used. Presented approach allows to directly determine the value of the angular deviation of mosaicity in a direction perpendicular and parallel to the sample surface.", "journal": "CRYSTAL RESEARCH AND TECHNOLOGY", "category": "Crystallography", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000382527800009", "keywords": "Electro-hydraulic control loading system; Flight simulator; Feed-forward inverse model; Force tracking control; System identification", "title": "A survey of Canadian urologists' opinions and prescribing patterns of testosterone replacement therapy in men on active surveillance for low-risk prostate cancer", "abstract": "Introduction: Attitudes regarding the safety of testosterone replacement therapy (TRT) in hypogonadal men with prostate cancer (PCa) have changed over the past few years with the emergence of case studies suggesting a low risk of cancer progression and a better understanding of the interaction of different levels of androgen with prostate cellular metabolism. This new view has the potential to change clinical practice. Methods: Active members of the Canadian Urological Association were surveyed about their opinions on the safety of TRT in men with low-risk PCa, as well as their current prescribing habits. Results: Of 57 responding urologists, 86% actively prescribe TRT in men with testosterone deficiency syndrome (TDS), 93% are involved in the treatment of men with PCa, and 95% offer active surveillance as a management option for low-grade/low-stage disease. Furthermore, 65% stated that they would offer TRT to men with TDS who were on active surveillance for PCa and 63% believed that TRT did not increase the risk of progression of PCa in these men. In terms of treatment methods, 96% believed TRT was safe for men who have undergone radical prostatectomy, while a smaller number felt it was safe for patients who have undergone brachytherapy (86%) or external beam radiation (84%). Despite these figures, only 35% of the surveyed physicians had ever offered TRT for men on active surveillance and only 42% actually had men in their practice who were taking testosterone while on active surveillance. Conclusions: The discrepancy between urologists' beliefs about the safety of TRT and their clinical practice patterns may be due to multiple factors, such as hesitation in recommending treatment in real-life practice, low numbers of eligible patients, absence of screening for testosterone deficiency in patients on active surveillance, and patient preference or fears. Furthermore, the difference in perceived safety in men treated by radical prostatectomy vs. radiation therapy suggests that some urologists are concerned that the radiated gland remaining in-situ may be \"reactivated\" by TRT. The results from this survey will be used as the basis of developing a national Canadian registry of men with low-grade/stage PCa who are receiving TRT while on active surveillance.", "journal": "CUAJ-CANADIAN UROLOGICAL ASSOCIATION JOURNAL", "category": "Urology & Nephrology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000389236700030", "keywords": "Acrodonta; biogeography; Cretaceous; Gondwana; phylogeny; Squamata", "title": "The first iguanian lizard from the Mesozoic of Africa", "abstract": "The fossil record shows that iguanian lizards were widely distributed during the Late Cretaceous. However, the biogeographic history and early evolution of one of its most diverse and peculiar clades (acrodontans) remain poorly known. Here, we present the first Mesozoic acrodontan from Africa, which also represents the oldest iguanian lizard from that continent. The new taxon comes from the Kem Kem Beds in Morocco (Cenomanian, Late Cretaceous) and is based on a partial lower jaw. The new taxon presents a number of features that are found only among acrodontan lizards and shares greatest similarities with uromastycines, specifically. In a combined evidence phylogenetic dataset comprehensive of all major acrodontan lineages using multiple tree inference methods (traditional and implied weighting maximumparsimony, and Bayesian inference), we found support for the placement of the new species within uromastycines, along with Gueragama sulamericana (Late Cretaceous of Brazil). The new fossil supports the previously hypothesized widespread geographical distribution of acrodontans in Gondwana during the Mesozoic. Additionally, it provides the first fossil evidence of uromastycines in the Cretaceous, and the ancestry of acrodontan iguanians in Africa.", "journal": "ROYAL SOCIETY OPEN SCIENCE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000377602900001", "keywords": "Hadron-Hadron scattering (experiments)", "title": "Search for the Standard Model Higgs boson decaying into b(b)over-bar produced in association with top quarks decaying hadronically in pp collisions at root s=8 TeV with the ATLAS detector", "abstract": "A search for Higgs boson production in association with a pair of top quarks (t (t) over barH) is performed, where the Higgs boson decays to b (b) over bar, and both top quarks decay hadronically. The data used correspond to an integrated luminosity of 20.3 fb(-1) of pp collisions at root s = 8 TeV collected with the ATLAS detector at the Large Hadron Collider. The search selects events with at least six energetic jets and uses a boosted decision tree algorithm to discriminate between signal and Standard Model background. The dominant multijet background is estimated using a dedicated data-driven technique. For a Higgs boson mass of 125 GeV, an upper limit of 6.4 (5.4) times the Standard Model cross section is observed (expected) at 95% confidence level. The best-fit value for the signal strength is mu = 1.6 +/- 2.6 times the Standard Model expectation for m(H) = 125 GeV. Combining all t (t) over barH searches carried out by ATLAS at root s = 8 and 7 TeV, an observed (expected) upper limit of 3.1 (1.4) times the Standard Model expectation is obtained at 95% confidence level, with a signal strength mu = 1.7 +/- 0.8.", "journal": "JOURNAL OF HIGH ENERGY PHYSICS", "category": "Physics, Particles & Fields", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000379193400001", "keywords": "birdsong; cultural evolution; vocal communication; computational acoustics; zebra finch", "title": "Zebra Finch Song Phonology and Syntactical Structure across Populations and Continents-A Computational Comparison", "abstract": "Learned bird songs are often characterized by a high degree of variation between individuals and sometimes between populations, while at the same time maintaining species specificity. The evolution of such songs depends on the balance between plasticity and constraints. Captive populations provide an opportunity to examine signal variation and differentiation in detail, so we analyzed adult male zebra finch (Taeniopygia guttata) songs recorded from 13 populations across the world, including one sample of songs from wild-caught males in their native Australia. Cluster analysis suggested some, albeit limited, evidence that zebra finch song units belonged to universal, species-wide categories, linked to restrictions in vocal production and non-song parts of the vocal repertoire. Across populations, songs also showed some syntactical structure, although any song unit could be placed anywhere within the song. On the other hand, there was a statistically significant differentiation between populations, but the effect size was very small, and its communicative significance dubious. Our results suggest that variation in zebra finch songs within a population is largely determined by species-wide constraints rather than population-specific features. Although captive zebra finch populations have been sufficiently isolated to allow them to genetically diverge, there does not appear to have been any divergence in the genetically determined constraints that underlie song learning. Perhaps more surprising is the lack of locally diverged cultural traditions. Zebra finches serve as an example of a system where frequent learning errors may rapidly create within-population diversity, within broad phonological and syntactical constraints, and prevent the formation of long-term cultural traditions that allow populations to diverge.", "journal": "FRONTIERS IN PSYCHOLOGY", "category": "Psychology, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000379787100048", "keywords": "Conflict detection and resolution; intelligent systems; nonlinear optimization; unmanned aerial vehicles", "title": "Cooperative conflict detection and resolution of civil unmanned aerial vehicles in metropolis", "abstract": "Unmanned air vehicles have recently attracted attention of many researchers because of their potential civil applications. A systematic integration of unmanned air vehicles in non-segregated airspace is required that allows safe operation of unmanned air vehicles along with other manned aircrafts. One of the critical issues is conflict detection and resolution. This article proposes to solve unmanned air vehicles' conflict detection and resolution problem in metropolis airspace. First, the structure of metropolis airspace in the coming future is studied, and the airspace conflict problem between different unmanned air vehicles is analyzed by velocity obstacle theory. Second, a conflict detection and resolution framework in metropolis is proposed, and factors that have influences on conflict-free solutions are discussed. Third, the multi-unmanned air vehicle conflict resolution problem is formalized as a nonlinear optimization problem with the aim of minimizing overall conflict resolution consumption. The safe separation constraint is further discussed to improve the computation efficiency. When the speeds of conflict-involved unmanned air vehicles are equal, the nonlinear safe separation constraint is transformed into linear constraints. The problem is solved by mixed integer convex programming. When unmanned air vehicles are with unequal speeds, we propose to solve the nonlinear optimization problem by stochastic parallel gradient descent-based method. Our approaches are demonstrated in computational examples.", "journal": "ADVANCES IN MECHANICAL ENGINEERING", "category": "Thermodynamics; Engineering, Mechanical", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000372631500021", "keywords": "Electronic patient diary data; statistical software; time series analysis; vector autoregression (VAR)", "title": "Automating Vector Autoregression on Electronic Patient Diary Data", "abstract": "Finding the best vector autoregression model for any dataset, medical or otherwise, is a process that, to this day, is frequently performed manually in an iterative manner requiring a statistical expertize and time. Very few software solutions for automating this process exist, and they still require statistical expertize to operate. We propose a new application called Autovar, for the automation of finding vector autoregression models for time series data. The approach closely resembles the way in which experts work manually. Our proposal offers improvements over the manual approach by leveraging computing power, e.g., by considering multiple alternatives instead of choosing just one. In this paper, we describe the design and implementation of Autovar, we compare its performance against experts working manually, and we compare its features to those of the most used commercial solution available today. The main contribution of Autovar is to show that vector autoregression on a large scale is feasible. We show that an exhaustive approach for model selection can be relatively safe to use. This study forms an important step toward making adaptive, personalized treatment available and affordable for all branches of healthcare.", "journal": "IEEE JOURNAL OF BIOMEDICAL AND HEALTH INFORMATICS", "category": "Computer Science, Information Systems; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Medical Informatics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000383611900004", "keywords": "Gan-Lu-Yin; traditional Chinese medicine; cytokines; TNF-; NF-B; AKT; ERK-dependent pathways", "title": "Suppression of the TNF-alpha level is mediated by Gan-Lu-Yin (traditional Chinese medicine) in human oral cancer cells through the NF-kappa B, AKT, and ERK-dependent pathways", "abstract": "Oral cancer is one of the major causes of deaths in the male population of Taiwan. Gan-Lu-Yin (GLY) is used for an adjuvant treatment of Traditional Chinese Medicine in clinical patients. In this study, we investigated the molecular mechanisms in oral cancer cell lines after exposure to GLY. The cytometric bead-based array (CBA) method was used for the examining and analyzing of tumor necrosis factor-alpha (TNF-) secretion level. TNF- mRNA expression was determined by real-time PCR analysis. Nuclear factor kappa B (NF-B) activity and other relative proteins were determined by NF-B promoter assay, Western blotting, electrophoretic mobility shift assay (EMSA), and immuno-staining analyses. GLY decreased the secretion of TNF- from the oral cancer CAL 27 cells. Furthermore, 2000 g/mL of GLY significantly suppressed TNF- mRNA expression of CAL 27 cells in a time-dependent manner. GLY reduced the levels of proteins, including nuclear NF-B (p65 and p50), p-IKK (ser176), p-IB, p-AKT, p-ERK, and nuclear Egr-1 in a time and dose-dependent manner. GLY also suppressed the NF-B activity and translocation in CAL 27 cells. We suggest that GLY might promote the cure of oral cancer through decreasing the level of TNF- cytokine, and these actions were mediated partially through the NF-B, AKT, and ERK-dependent pathways in vitro. (c) 2015 Wiley Periodicals, Inc. Environ Toxicol 31: 1196-1205, 2016.", "journal": "ENVIRONMENTAL TOXICOLOGY", "category": "Environmental Sciences; Toxicology; Water Resources", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000380869500025", "keywords": "Diffuse large B-cell lymphoma; C-reactive protein; Prognosis; Rituximab", "title": "Pretreatment C-reactive protein was an independent prognostic factor for patients with diffuse large B-cell lymphoma treated with RCHOP", "abstract": "Background: We assessed the prognostic significance of pretreatment C-reactive protein (CRP) concentration in diffuse large B-cell lymphoma (DLBCL) patients. Methods: We retrospectively reviewed 156 patients with newly diagnosed DLBCL Receiver operating characteristic (ROC) curve analysis was used to generate a cutoff value for CRP. Both log-rank test and multivariable analysis by Cox regression model were used to assess the impact of pretreatment CRP concentrations on the overall survival (OS) and progression-free survival (PFS). Results: Among those 156 patients enrolled, increased CRP concentration was seen in 51 patients before treatment (>= 20 mg/l), while the other 105 patients were considered with low CRP concentration. There was no obvious difference in the baseline characteristics between two groups. Increased CRP concentration was significantly associated with poorer OS and PFS than those patients with low CRP concentration (P = 0.001 and P = 0.000, respectively). Multivariable analysis further showed that CRP concentration was an independent predictor of OS (hazard ratio [HR] = 0.47; 95% confidence interval [CI] = 0.24-0.92, P = 0.028) and PFS (HR = 0.37; 95% Cl = 0.30-0.87, P = 0.013). Conclusion: CRP concentration before treatment was an independent prognostic factor in patients with DLBCL (C) 2016 Elsevier B.V. All rights reserved.", "journal": "CLINICA CHIMICA ACTA", "category": "Medical Laboratory Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000385836000010", "keywords": "inner-core structure; TC track; radar data assimilation", "title": "An investigation on how inner-core structures obtained through radar data assimilation affect track forecasting of typhoon Jangmi (2008) near Taiwan Island", "abstract": "The impacts of radar data assimilation (DA) on the westward track deflection of Typhoon Jangmi (2008) near Taiwan Island and the deflection mechanism are investigated. Initial conditions from two data assimilation experiments with significant track forecast differences are analyzed and compared. The environmental, axisymmetric, wave number 1 to 3 asymmetric fields of the typhoon are decomposed by using vortex separation and Fourier decomposition methods. The components are selectively recomposed into new initial conditions that include different vortex-scale components to examine the impact of individual components on the track prediction. The wave number 1 asymmetric structure is found to play a dominant role in the westward deflection of Typhoon Jangmi, and the accurate analysis of this component with radar DA helps to improve the track forecast. The wave number 1 asymmetric circulation is manifested as a pair of cyclonic and anticyclonic gyres with well-defined ventilation flows through the inner-core region, which provides additional steering of the typhoon vortex. The layer-mean environmental steering flow and ventilation flow associated with the wave number 1 gyres are further calculated to quantitatively evaluate the impact of ventilation flow. The ventilation flow is shown to be responsible for most of the westward motion component, suggesting again its role in causing the westward track deflection of Typhoon Jangmi. The results also suggest the importance of analyzing vortex-scale asymmetric structures for accurate tropical cyclone track forecasting, especially when there is a significant track deflection.", "journal": "JOURNAL OF GEOPHYSICAL RESEARCH-ATMOSPHERES", "category": "Meteorology & Atmospheric Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000387768300056", "keywords": "p-coumalic acid; caffeic acid; gallic acid; chlorogenic acid; beta-Conglycinin; 7S; interaction", "title": "Interactions of beta-Conglycinin (7S) with Different Phenolic Acids-Impact on Structural Characteristics and Proteolytic Degradation of Proteins", "abstract": "p-Coumalic acid (PCA), caffeic acid (CA), gallic acid (GA) and chlorogenic acid (CGA) are the major phenolic acids that co-exist with soy protein components in foodstuffs. Surprisingly, there are only a handful of reports that describe their interaction with beta-Conglycinin (7S), a major soy protein. In this report, we investigated the interaction between phenolic acids and soy protein 7S and observed an interaction between each of these phenolic acids and soy protein 7S, which was carried out by binding. Further analysis revealed that the binding activity of the phenolic acids was structure dependent. Here, the binding affinity of CA and GA towards 7S was found to be stronger than that of PCA, because CA and GA have one more hydroxyl group. Interestingly, the binding of phenolic acids with soy protein 7S did not affect protein digestion by pepsin and trypsin. These findings aid our understanding of the relationship between different phenolic acids and proteins in complex food systems.", "journal": "INTERNATIONAL JOURNAL OF MOLECULAR SCIENCES", "category": "Biochemistry & Molecular Biology; Chemistry, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000379976200013", "keywords": "Lyapunov methods; costing; continuous time systems; distributed algorithms; optimisation; convergence; finite-time convergent distributed consensus optimisation; finite-time convergent distributed continuous-time algorithm; network optimisation problem; global cost function; strictly convex local cost function; undirected network; fixed topologies; finite-time consensus protocols; continuous-time zero-gradient-sum algorithm; Lyapunov method", "title": "Finite-time convergent distributed consensus optimisation over networks", "abstract": "In this study, a finite-time convergent distributed continuous-time algorithm is proposed to solve a network optimisation problem where the global cost function is the sum of strictly convex local cost functions under an undirected network with fixed topologies. The algorithm is inspired by finite-time consensus protocols and continuous-time zero-gradient-sum algorithms. Instead of the exponential convergence in existing works, the finite-time convergence is guaranteed based on the Lyapunov method. A numerical simulation example is provided to illustrate the effectiveness of the developed algorithm.", "journal": "IET CONTROL THEORY AND APPLICATIONS", "category": "Automation & Control Systems; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000384792700003", "keywords": "ALSPAC; Endocrine disrupting compounds; Organochlorine pesticides; Puberty; Menarche", "title": "Early Outcomes of Coronary Endarterectomy in Patients Undergoing Coronary Artery Bypass Surgery", "abstract": "Background: Coronary endarterectomy (CE) is performed as an adjunct to coronary artery bypass surgery (CABG); however, the efficacy of this technique is still controversial. We aimed to evaluate the impact of CE combined with CABG when compared with isolated CABG. Methods: Patients who underwent CABG between July 2007 and June 2014 were included. 70 of 2452 patients (2.8%) underwent CE in addition to CABG. Early results were compared with isolated CABG and predictors of adverse outcome were measured in stepwise multivariate logistic regression analyses. Results: The incidence of comorbidities including prior myocardial infarction, diabetes mellitus, and three-vessel coronary disease in CE patients was higher; however, mortality (4.3% versus control 3.6%; P = .762) and postoperative complications were not significantly increased in this group of patients (except supraventricular arrhythmia). Although age greater than 70 years, impaired ejection fraction, intraoperative intraaortic balloon pump, and prolonged cardiopulmonary bypass time were important predictors of adverse outcomes, CE was not associated with increased mortality or postoperative morbidities. Conclusion: Despite the higher risk profile of patients who underwent CE, this technique was not identified as an independent risk factor for adverse postoperative outcomes.", "journal": "HEART SURGERY FORUM", "category": "Cardiac & Cardiovascular Systems; Surgery", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000382666000003", "keywords": "Design; Algorithms; Performance; Lyapunov equation; Bartels-Stewart algorithm; Sylvester equation; blocked algorithm; BLAS level-3", "title": "On BLAS Level-3 Implementations of Common Solvers for (Quasi-) Triangular Generalized Lyapunov Equations", "abstract": "The solutions of Lyapunov and generalized Lyapunov equations are a key player in many applications in systems and control theory. Their stable numerical computation, when the full solution is sought, is considered solved since the seminal work of Bartels and Stewart [1972] and its generalization by Penzl [1997]. Those variants do not go completely beyond BLAS level-2 style implementation. On modern computers, however, the formulation of level-3 BLAS type implementations is crucial to enable optimal usage of cache hierarchies and modern block scheduling methods based on directed acyclic graphs describing the interdependence of single block computations. Although there exists a recursive blocked solution scheme for (quasi-) triangular generalized Lyapunov equations [Jonsson and Kagstrom 2002b], we focus on standard blocking techniques. Using the standard blocking approach our contribution lifts the aforementioned level-2 algorithm by Penzl to BLAS level-3 for (quasi-) triangular equations. Especially, we consider the solution of the appearing Sylvester equations and provide a hybrid algorithm merging our strategy with the recursive blocking method above.", "journal": "ACM TRANSACTIONS ON MATHEMATICAL SOFTWARE", "category": "Computer Science, Software Engineering; Mathematics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000385527700188", "keywords": "wireless sensor networks; data gathering; compressive sensing; matrix completion; graph based transform; ADMM; A2DM2", "title": "Efficient Data Gathering Methods in Wireless Sensor Networks Using GBTR Matrix Completion", "abstract": "To obtain efficient data gathering methods for wireless sensor networks (WSNs), a novel graph based transform regularized (GBTR) matrix completion algorithm is proposed. The graph based transform sparsity of the sensed data is explored, which is also considered as a penalty term in the matrix completion problem. The proposed GBTR-ADMM algorithm utilizes the alternating direction method of multipliers (ADMM) in an iterative procedure to solve the constrained optimization problem. Since the performance of the ADMM method is sensitive to the number of constraints, the GBTR-A2DM2 algorithm obtained to accelerate the convergence of GBTR-ADMM. GBTR-A2DM2 benefits from merging two constraint conditions into one as well as using a restart rule. The theoretical analysis shows the proposed algorithms obtain satisfactory time complexity. Extensive simulation results verify that our proposed algorithms outperform the state of the art algorithms for data collection problems in WSNs in respect to recovery accuracy, convergence rate, and energy consumption.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000373516000002", "keywords": "Epigenetics; heart defect detection", "title": "Epigenetic markers for newborn congenital heart defect (CHD)", "abstract": "Objective: Our objective was to determine whether there were significant differences in genome-wide DNA methylation in newborns with major congenital heart defect (CHD) compared to controls. We also evaluated methylation of cytosines in CpG motifs for the detection of these CHDs.Methods: Genome-wide DNA methylation analysis was performed on DNA from 60 newborns with various CHDs, including hypoplastic left heart syndrome, ventricular septal deficit, atrial septal defect, pulmonary stenosis, coarctation of the aorta and Tetralogy of Fallot, and 32 controls.Results: Highly significant differences in cytosine methylation were seen in a large number of genes throughout the genome for all CHD categories. Gene ontology analysis of CHD overall indicated over-represented biological processes involving cell development and differentiation, and anatomical structure morphogenesis. Methylation of individual cytosines in CpG motifs had high diagnostic accuracy for the detection of CHD. For example, for coarctation one predictive model based on levels of particular cytosine nucleotides achieved a sensitivity of 100% and specificity of 93.8% (AUC=0.974, p<0.00001).Conclusion: Profound differences in cytosine methylation were observed in hundreds of genes in newborns with different types of CHD. There appears to be the potential for development of accurate genetic biomarkers for CHD detection in newborns.", "journal": "JOURNAL OF MATERNAL-FETAL & NEONATAL MEDICINE", "category": "Obstetrics & Gynecology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000385180600061", "keywords": "dystocia; failure to progress; fetal attitude; first stage of labor; intrapartum ultrasound; labor; malpresentation; reproducibility", "title": "Direct observation of magnetic vortex behavior in an ordered La0.7Sr0.3MnO3 dot arraysd", "abstract": "Directly observing the magnetic domain behavior in patterned nanostructures is crucial to the investigation into advanced spin-based devices. Herein, we show that the magnetic vortex behavior can be deterministically observed and controlled in highly spin polarized La0.7Sr0.3MnO3 (LSMO) triangular dots by successive in-field magnetic force microscopy (MFM). Imaging the magnetic domains with MFM shows that most of the LSMO dots exhibit magnetic vortex states with a clockwise or anticlockwise \"pinwheel'' structure for decreasing the demagnetization energy. Probing the vortex chirality using in-field MFM indicates that the selective spin circulation of the triangular dots depends on the magnetic orientation of the bias nanomagnet with specially designed geometries. Comparison between measurement and simulation reveals that the vortex behavior should be governed by an interface involved pinning strength at the boundaries, as well as a geometrically induced shape anisotropy of the triangular dot, both of which result in shape-dominated magnetic domain reversals.", "journal": "PHYSICAL CHEMISTRY CHEMICAL PHYSICS", "category": "Chemistry, Physical; Physics, Atomic, Molecular & Chemical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000370127300034", "keywords": "Vibration measurement; Multiple Self-Mixing Interferometry; Power spectrum; Nanometer measurements", "title": "Vibration measurement based on Multiple Self-Mixing Interferometry", "abstract": "We propose a novel algorithm for Multiple Self-Mixing Interferometry (MSMI). The algorithm is able to measure nanometer scale vibration by the power spectrum analysis. In the paper, the principles of the method are introduced in detail. The experimental setup has been built. The validity of the proposed algorithm was confirmed by conducting a series of experimental measurements at different reflection times, feedback factors, and vibrational frequencies using PZT as a reference. Experimental results showed that the method can quickly demodulate parameters of vibration and good correspondence between theory and experiment. The proposed algorithm, thus, furnishes nanometer measurements with a very high resolution using a self-aligned, cost effective and compact experimental setup. (C) 2016 Elsevier B.V. All rights reserved.", "journal": "OPTICS COMMUNICATIONS", "category": "Optics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000371724200095", "keywords": "Cocoa bean; Dark chocolate; ICP-MS; Elemental profile; Geographic origin", "title": "Can Urine Metabolomics Be Helpful in Differentiating Neuropathic and Nociceptive Pain? A Proof-of-Concept Study", "abstract": "The diagnosis of pain nature is a troublesome task and a wrong attribution often leads to an increase of costs and to avoidable pharmaceutical adverse reactions. An objective and specific approach to achieve this diagnosis is highly desirable. The aim of this work was to investigate urine samples collected from patients suffering from pain of different nature by a metabolomics approach based on H-1 NMR spectroscopy and multivariate statistical analysis. We performed a prospective study on 74 subjects: 37 suffering from pain (12 with nociceptive and 25 with neuropathic pain), and 37 controls not suffering from any kind of chronic pain. The application of discriminant analysis on the urine spectral profiles allowed us to classify these two types of pain with high sensibility and specificity. Although the classification relies on the global urine metabolic profile, the individual contribution in discriminating neuropathic pain patients of metabolites such as choline and phosphocholine, taurine and alanine, suggests potential lesions to the nervous system. To the best of our knowledge, this is the first time that a urine metabolomics profile is used to classify these two kinds of pain. This methodology, although based on a limited sample, may constitute the basis for a new helpful tool in the clinical diagnosis.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000384612800001", "keywords": "dilated cardiomyopathy; Troponin T; myosin heavy chain; contractile dynamics; thin filament function; myofilament cooperativity", "title": "Dilated Cardiomyopathy Mutation (R134W) in Mouse Cardiac Troponin T Induces Greater Contractile Deficits against alpha-Myosin Heavy Chain than against beta-Myosin Heavy Chain", "abstract": "Many studies have demonstrated that depressed myofilament Ca2+ sensitivity is common to dilated cardiomyopathy (DCM) in humans. However, it remains unclear whether a single determinant-such as myofilament Ca2+ sensitivity-is sufficient to characterize all cases of DCM because the severity of disease varies widely with a given mutation. Because dynamic features dominate in the heart muscle, alterations in dynamic contractile parameters may offer better insight on the molecular mechanisms that underlie disparate effects of DCM mutations on cardiac phenotypes. Dynamic features are dominated by myofilament cooperativity that stem from different sources. One such source is the strong tropomyosin binding region in troponin T (TnT), which is known to modulate crossbridge (XB) recruitment dynamics in a myosin heavy chain (MHC)-dependent manner. Therefore, we hypothesized that the effects of DCM linked mutations in TnT on contractile dynamics would be differently modulated by alpha- and beta-MHC. After reconstitution with the mouse TnT equivalent (TnT(R134W)) of the human DCM mutation (R131VV), we measured dynamic contractile parameters in detergent-skinned cardiac muscle fiber bundles from normal (alpha-MHC) and transgenic mice (beta-MHC). Tn(TR134W) significantly attenuated the rate constants of tension redevelopment, XB recruitment dynamics, XB distortion dynamics, and the magnitude of length-mediated XB recruitment only in alpha-MHC fiber bundles. Tn(TR134W) decreased myofilament Ca2+ sensitivity to a greater extent in alpha-MHC (0.14 pCa units) than in beta-MHC fiber bundles (0.08 pCa units). Thus, our data demonstrate that Tn(TR134W) induces a more severe DCM-like contractile phenotype against alpha-MHC than against beta-MHC background.", "journal": "FRONTIERS IN PHYSIOLOGY", "category": "Physiology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000382741100010", "keywords": "Algorithm; Dehydration; Gastroenteritis; Infant diarrhoea; Prolonged diarrhoea", "title": "An international consensus report on a new algorithm for the management of infant diarrhoea", "abstract": "Aim: Implementing international guidelines guarantees high standards of clinical care. A group of experts developed an algorithm to drive the management of common gastrointestinal symptoms in infancy by paediatricians and general practitioners. Methods: The algorithm started from the evidence-based recommendations of the European Society of Gastroenterology, Hepatology and Nutrition and the European Society of Pediatric Infectious Diseases and an updated review of the literature. We used the structured quantitative method of nominal group technique to reach a consensus. Results: A practical algorithm for the management of infants with acute diarrhoea was designed based on the consensus reached for each statement. The management of an infant with acute diarrhoea should include a sequence of actions: (i) a semiquantitative estimate of infant dehydration through validated clinical scores, (ii) rehydration therapy and early refeeding with breast milk or regular formula and (iii) effective agents to reduce the severity and duration of the diarrhoea. Finally, in children with prolonged diarrhoea, the search for aetiology should include persistent infections or reinfections, cows' milk protein allergy and coeliac diseases. Lactose should always be withdrawn. Conclusion: This algorithm provides an evidence-based sequence of interventions to optimise the management of infants with acute diarrhoea.", "journal": "ACTA PAEDIATRICA", "category": "Pediatrics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000379420500017", "keywords": "Rolling bearings; Compound faults; Fault diagnosis; Empirical wavelet transform; Duffing oscillator", "title": "Neuropathologic Associations of Learning and Memory in Primary Progressive Aphasia", "abstract": "IMPORTANCE The dementia syndrome of primary progressive aphasia (PPA) can be caused by 1 of several neuropathologic entities, including forms of frontotemporal lobar degeneration (FTLD) or Alzheimer disease (AD). Although episodic memory is initially spared in this syndrome, the subtle learning and memory features of PPA and their neuropathologic associations have not been characterized. OBJECTIVE To detect subtle memory differences on the basis of autopsy-confirmed neuropathologic diagnoses in PPA. DESIGN, SETTING, AND PARTICIPANTS Retrospective analysis was conducted at the Northwestern Cognitive Neurology and Alzheimer's Disease Center in August 2015 using clinical and postmortem autopsy data that had been collected between August 1983 and June 2012. Thirteen patients who had the primary clinical diagnosis of PPA and an autopsy-confirmed diagnosis of either AD (PPA-AD) or a tau variant of FTLD (PPA-FTLD) and 6 patients who had the clinical diagnosis of amnestic dementia and autopsy-confirmed AD (AMN-AD) were included. MAIN OUTCOMES AND MEASURES Scores on the effortless learning, delayed retrieval, and retention conditions of the Three Words Three Shapes test, a specialized measure of verbal and nonverbal episodic memory. RESULTS The PPA-FTLD (n = 6), PPA-AD (n = 7), and AMN-AD (n = 6) groups did not differ by demographic composition (all P > .05). The sample mean (SD) age was 64.1 (10.3) years at symptom onset and 67.9 (9.9) years at Three Words Three Shapes test administration. The PPA-FTLD group had normal (ie, near-ceiling) scores on all verbal and nonverbal test conditions. Both the PPA-AD and AMN-AD groups had deficits in verbal effortless learning (mean [SD] number of errors, 9.9 [4.6] and 14.2 [2.0], respectively) and verbal delayed retrieval (mean [SD] number of errors, 6.1 [5.9] and 12.0 [4.4], respectively). The AMN-AD group had additional deficits in nonverbal effortless learning (mean [SD] number of errors, 10.3 [4.0]) and verbal retention (mean [SD] number of errors, 8.33 [5.2]), which were not observed in the PPA-FTLD or PPA-AD groups (all P < .005). CONCLUSIONS AND RELEVANCE This study identified neuropathologic associations of learning and memory in autopsy-confirmed cases of PPA. Among patients with clinical PPA syndrome, AD neuropathology appeared to interfere with effortless learning and delayed retrieval of verbal information, whereas FTLD-tau pathology did not. The results provide directions for future research on the interactions between limbic and language networks.", "journal": "JAMA NEUROLOGY", "category": "Clinical Neurology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000373197500055", "keywords": "HIV; Alcohol screening; Education; Confidence; Knowledge", "title": "Current Workforce of General Pediatricians in the United States", "abstract": "BACKGROUND AND OBJECTIVES: A near vacuum exists for credible information regarding specialty-specific demography, including gender, years since training completion, current employer, academic affiliation, and hours worked in specific tasks. Understanding the current status and changes to the medical workforce and its work patterns is essential to assessing whether the supply and distribution meets the needs of patients, institutions, society, and educational programs. METHODS: A self-administered electronic survey sent to all pediatricians at the time of their enrollment in the Maintenance of Certification program in 2013-2014. The survey focused on exploring trends associated with career choice, career paths, time spent in professional activities, and current practice characteristics. Logistic regression and chi(2) analyses were conducted. RESULTS: The response rate was 87.2% (N = 15 351). Of those who completed the survey, 9253 (64%) self-identified as general pediatricians. An increased likelihood of working part-time was seen among women (odds ratio [ OR]: 12.21), those without an academic appointment (OR: 1.32), and those not working in a private/independent practice (OR: 1.15). Overall, 89% (n = 8214) of respondents stated that their current allocation of professional time was approximately what they wanted. Those more likely to be involved in quality improvement in the past year did not work in independent/private practices (OR: 1.78) and worked fulltime (OR: 1.16). CONCLUSIONS: Understanding the current nature of the pediatric workforce is a first step in providing data to guide future workforce planning and the training experiences required to maintain and shape the workforce to meet the current and future needs of children.", "journal": "PEDIATRICS", "category": "Pediatrics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000372589900005", "keywords": "Indoor navigation; Grid line patterns; Particle filter; Mobile robot", "title": "Mobile robot navigation using grid line patterns via probabilistic measurement modeling", "abstract": "Mobile robots are generally equipped with proprioceptive motion sensors such as odometers and inertial sensors. These sensors are used for dead-reckoning navigation in an indoor environment where GPS is not available. However, this dead-reckoning scheme is susceptible to drift error in position and heading. This study proposes using grid line patterns which are often found on the surface of floors or ceilings in an indoor environment to obtain pose (i.e., position and orientation) fix information without additional external position information by artificial beacons or landmarks. The grid lines can provide relative pose information of a robot with respect to the grid structure and thus can be used to correct the pose estimation errors. However, grid line patterns are repetitive in nature, which leads to difficulties in estimating its configuration and structure using conventional Gaussian filtering that represent the system uncertainty using a unimodal function (e.g., Kalman filter). In this study, a probabilistic sensor model to deal with multiple hypotheses is employed and an online navigation filter is designed in the framework of particle filtering. To demonstrate the performance of the proposed approach, an experiment was performed in an indoor environment using a wheeled mobile robot, and the results are presented.", "journal": "INTELLIGENT SERVICE ROBOTICS", "category": "Robotics", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000373628900008", "keywords": "Prelacteal; Factors; Ethiopia", "title": "Aesthetic dissatisfaction in patients with hand osteoarthritis and its impact on daily life", "abstract": "Objectives: To evaluate the nature and extent of aesthetic dissatisfaction in patients with hand osteoarthritis (OA), and to investigate its impact on daily life and its determinants.Method: Patients with primary hand OA, consulting secondary care, underwent physical examination for the number of joints with bony joint enlargements, soft tissue swelling and deformities, and radiographs. Questionnaires were filled in to measure pain and function (Functional Index for Hand Osteoarthritis, FIHOA), dissatisfaction with the appearance of the hands and its impact (aesthetic scales from the Michigan Hand Outcomes Questionnaire, MHQ), anxiety and depression (the Hospital Anxiety and Depression Scale, HADS), and illness perceptions (the revised Illness Perception Questionnaire, IPQ-R). Odds ratios (ORs) with 95% confidence intervals (CIs) were calculated using multivariate logistic regression as measures of relative risk for dissatisfaction with appearance or its impact, adjusted for age, sex, body mass index (BMI), and joint-specific abnormalities (bony joint enlargements, deformities, or radiographic severity), self-reported pain and function.Results: Of 247 patients (mean age 61.6years, 88% women), 63 (26%) were aesthetically dissatisfied and 33 (13%) reported impact on daily life due to dissatisfaction. Patients with joint-specific abnormalities were at higher risk for reporting dissatisfaction. Patients who reported impact also reported more depression and negative illness perceptions, independently from joint-specific abnormalities.Conclusions: Hand OA patients report aesthetic dissatisfaction with their hands regularly, especially in those with joint abnormalities. This dissatisfaction has a negative impact in a small group of patients who also reported more depression and negative illness perceptions. These results indicate the influence of psychosocial factors on outcome measures in patients with hand OA.", "journal": "SCANDINAVIAN JOURNAL OF RHEUMATOLOGY", "category": "Rheumatology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000382591000031", "keywords": "MG scheduling; UT (Unscented transform); Modification approach; Renewable energy sources; Uncertainty", "title": "Stochastic energy management of renewable micro-grids in the correlated environment using unscented transformation", "abstract": "This paper addresses the optimal stochastic scheduling of the distributed generation units in a micro-grid. In this way, it introduces a new sufficient stochastic framework to model the correlated uncertainties in the micro-grid that includes different types of RESs such as photovoltaics, wind turbines, micro-turbine, fuel cell as well as battery as the storage device. The proposed stochastic method makes use of unscented transforms to model correlated uncertain parameters. The ability of the unscented transform method to model correlated uncertain variables is particularly appealing in the context of power systems, wherein noticeable inherent correlation exists. Due to the highly complex nature of the problem, a new optimization method based on the harmony search algorithm along with an intelligent modification method is devised to solve the proposed optimization problem, efficiently. The proposed optimization algorithm is equipped with powerful search mechanisms that make it suitable for solving both discrete and continuous problems. In comparison with the original harmony search algorithm, the proposed modified optimization algorithm has few setting parameters. The new modified harmony search algorithm provides proper balance between the local and global searches. The feasibility and satisfactory performance of performance of the proposed method are examined on two typical grid-connected MGs. (C) 2016 Elsevier Ltd. All rights reserved.", "journal": "ENERGY", "category": "Thermodynamics; Energy & Fuels", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000381959400007", "keywords": "Douglas-Rachford algorithm; Global convergence; Feasibility problem; Half-space; Non-convex", "title": "Multiple Xanthomonas euvesicatoria Type III Effectors Inhibit flg22-Triggered Immunity", "abstract": "Xanthomonas euvesicatoria is the causal agent of bacterial spot disease in pepper and tomato. X. euvesicatoria bacteria interfere with plant cellular processes by injecting effector proteins into host cells through the type III secretion (T3S) system. About 35 T3S effectors have been identified in X. euvesicatoria 85-10, and a few of them were implicated in suppression of pattern-triggered immunity (PTI). We used anArabidopsis thaliana pathogen-free protoplast based assay to identify X. euvesicatoria 85-10 effectors that interfere with PTI signaling induced by the bacterial peptide flg22. Of 33 tested effectors, 17 inhibited activation of a PTI-inducible promoter. Among them, nine effectors also interfered with activation of an abscisic acid inducible promoter. However, effectors that inhibited flg22-induced signaling did not affect phosphorylation of mitogen-activated protein (MAP) kinases acting downstream of flg22 perception. Further investigation of selected effectors revealed that XopAJ, XopE2, and XopF2 inhibited activation of a PTI-inducible promoter by the bacterial peptide elf18 in Arabidopsis protoplasts and by flg22 in tomato protoplasts. The effectors XopF2, XopE2, XopAP, XopAE, XopH, and XopAJ inhibited flg22-induced callose deposition in planta and enhanced disease symptoms caused by attenuated Pseudomonas syringae bacteria. Finally, selected effectors were found to localize to various plant subcellular compartments. These results indicate that X. euvesicatoria bacteria utilize multiple T3S effectors to suppress flg22-induced signaling acting downstream or in parallel to MAP kinase cascades and suggest they act through different molecular mechanisms.", "journal": "MOLECULAR PLANT-MICROBE INTERACTIONS", "category": "Biochemistry & Molecular Biology; Biotechnology & Applied Microbiology; Plant Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000375466700003", "keywords": "EPA; phosphatidylcholine; phosphatidylserine; lipid metabolism", "title": "Evaluating the stationarity assumption in statistically downscaled climate projections: is past performance an indicator of future results?", "abstract": "Empirical statistical downscaling (ESD) methods seek to refine global climate model (GCM) outputs via processes that glean information from a combination of observations and GCM simulations. They aim to create value-added climate projections by reducing biases and adding finer spatial detail. Analysis techniques, such as cross-validation, allow assessments of how well ESD methods meet these goals during observational periods. However, the extent to which an ESD method's skill might differ when applied to future climate projections cannot be assessed readily in the same manner. Here we present a \"perfect model\" experimental design that quantifies aspects of ESD method performance for both historical and late 21st century time periods. The experimental design tests a key stationarity assumption inherent to ESD methods - namely, that ESD performance when applied to future projections is similar to that during the observational training period. Case study results employing a single ESD method (an Asynchronous Regional Regression Model variant) and climate variable (daily maximum temperature) demonstrate that violations of the stationarity assumption can vary geographically, seasonally, and with the amount of projected climate change. For the ESD method tested, the greatest challenges in downscaling daily maximum temperature projections are revealed to occur along coasts, in summer, and under conditions of greater projected warming. We conclude with a discussion of the potential use and expansion of the perfect model experimental design, both to inform the development of improved ESD methods and to provide guidance on the use of ESD products in climate impacts analyses and decision-support applications.", "journal": "CLIMATIC CHANGE", "category": "Environmental Sciences; Meteorology & Atmospheric Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000374102700012", "keywords": "First trimester pregnancy; Gestational diabetes mellitus; Insulin resistance; Body mass index; Waist/hip ratio; Weight gain", "title": "Predicting gestational diabetes mellitus during the first trimester using anthropometric measurements and HOMA-IR", "abstract": "Purpose To determine the predictability of gestational diabetes mellitus (GDM) during the first trimester using the degree of insulin resistance and anthropometric measurements and to assign the risk of developing GDM by weight gained during pregnancy (WGDP). Methods A total of 250 singleton pregnancies at 7-12 gestational weeks were studied. Body mass index (BMI), waist/hip ratio (WHR), quantitative insulin sensitivity check index (QUICKI), homeostasis model assessment-insulin resistance (HOMA-IR) scores and WGDP were determined. The backward stepwise method was applied to estimate possible associations with GDM. Cutoff points were estimated using receiver operating characteristic curve analysis. Results GDM was found in 20 of 227 singleton pregnancies (8.8 %). The calculated HOMA-IR, QUICKI, BMI, WHR, WGDP, and parity were significantly associated with GDM. Logistic regression analyses showed that three covariates (HOMA-IR, BMI, WGDP) remained independently associated with GDM. It was calculated as OR 1.254 (95 % CI 1.006-1.563), AUC 0.809, sensitivity 90 %, specificity 61 % with cutoff = 2.08 for HOMA-IR; OR 1.157 (CI 1.045-1.281), AUC 0.723, sensitivity 80 %, specificity 58 % with cutoff = 25.95 for BMI; OR 1.221, (CI 1.085-1.374), AUC 0.654, sensitivity 80 %, specificity 46 % with cutoff = 4.7 for WGDP. Despite a HOMA-IR score of > 3.1 in pregnant women, GDM was detected in only three of 29 patients (10.3 %) if WGDP was < 4.7 kg at weeks 24-28. Conclusions First trimester screening for GDM can be achieved based on maternal anthropometric measurements and HOMA-IR. In particular, if BMI is > 25.95 kg/m(2) and the HOMA-IR score > 2.08, controlling weight gain may protect against GDM.", "journal": "JOURNAL OF ENDOCRINOLOGICAL INVESTIGATION", "category": "Endocrinology & Metabolism", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000380945600005", "keywords": "expenditures; local products; regression analysis; rural tourism; sustainable destination development; tourist experience", "title": "Purchase of local products within the rural tourist experience context", "abstract": "Rural tourism is frequently considered a development tool for rural areas. In particular, demand for local products may stimulate local economy and is simultaneously an important part of the tourist experience. Little is known, however, of the role of local product purchase within and as a result of the rural tourist experience. The present article addresses this gap by analysing the impact of the tourist experience on (a) the decision to purchase local products and (b) the amount of respective expenditures made, based on survey data on rural tourists in Portugal. Results reveal a positive impact of the knowledge, sensorial and interaction dimensions of the tourist experience in both models, and length of stay, age, place attachment and nationality also play a role.", "journal": "TOURISM ECONOMICS", "category": "Economics; Hospitality, Leisure, Sport & Tourism", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000383291800051", "keywords": "Monte Carlo simulation; Genetic Algorithm; Renewable energy; NZEBs; Sustainable design", "title": "A hybrid Genetic Algorithm and Monte Carlo simulation approach to predict hourly energy consumption and generation by a cluster of Net Zero Energy Buildings", "abstract": "Employing a hybrid Genetic Algorithm (GA) and Monte Carlo (MC) simulation approach, energy consumption and renewable energy generation in a cluster of Net Zero Energy Buildings (NZEBs) was thoroughly investigated with hourly simulation. Moreover, the cumulative energy consumption and generation of the whole cluster and each individual building within the simulation space were accurately monitored and reported. The results indicate that the developed simulation algorithm is able to predict the total instantaneous and cumulative amount of energy taken from and supplied to the central energy grid over any time period. During the course of simulation, about 60-100% of total daily generated renewable energy was consumed by NZEBs and up to 40% of that was fed back into the central energy grid as surplus energy. The minimum grid dependency of the cluster was observed in June and July where 11.2% and 9.9% of the required electricity was supplied from the central energy grid, respectively. On the other hand, the NZEB cluster was strongly grid dependant in January and December by importing 70.7% and 76.1% of its required energy demand via the central energy grid, in the order given. Simulation results revealed that the cluster was 63.5% grid dependant on annual bases. In general, this stochastic algorithm is a self-learning one, i.e., at the end of each year, it utilizes the instantaneous energy consumption and generation data of each building to predict its energy balance in subsequent years. Hence, the accuracy and validity of the predictions increase over time. The simulation results are capable of modifying and readjusting the energy consumption patterns of buildings via appropriate predefined policies and well designed monitoring systems. (C) 2016 Elsevier Ltd. All rights reserved.", "journal": "APPLIED ENERGY", "category": "Energy & Fuels; Engineering, Chemical", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000371324500052", "keywords": "Slicing orientation; Volumetric error; Area weighted normal; Least absolute deviation; Prominent components analysis", "title": "Fast slicing orientation determining and optimizing algorithm for least volumetric error in rapid prototyping", "abstract": "Rapid prototyping fabricates physical prototypes from three-dimensional designing models using the additive process with layers. Aims at reducing the inevitable volumetric error induced in phrase of model slicing which impacts the shape accuracy of fabricated entity, a fast determining scheme of optimal slicing orientation for least volumetric error is proposed. The work analyses the staircase effect between two consecutive layers, then infers a direct computing formula of volume deviation of a whole model. Introduces the term of area weighted normal to express the significant effect of facet area on volumetric error and converts the optimal orientation determining problem to the least absolute deviation linear regression issue. Employs prominent components analysis on weighted normal set to obtain an approximate orientation efficiently, then optimizes the solution through few searchings in neighboring orientation space. The validity and efficiency of the algorithm are evaluated on several examples. Results demonstrate that proposed algorithm consumes less than 32 % of computation load and adaptively obtains the optimal slicing orientation.", "journal": "INTERNATIONAL JOURNAL OF ADVANCED MANUFACTURING TECHNOLOGY", "category": "Automation & Control Systems; Engineering, Manufacturing", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000378793700007", "keywords": "cancer-associated pain; celiac plexus neurolysis; endoscopic ultrasound; EUS-guided neurolysis; pancreatic cancer; predictor", "title": "Predictors of pain response in patients undergoing endoscopic ultrasound-guided neurolysis for abdominal pain caused by pancreatic cancer", "abstract": "Background: Interventional endoscopic ultrasound (EUS)-guided procedures such as EUS-guided celiac ganglia neurolysis (EUS-CGN) and EUS-guided broad plexus neurolysis (EUS-BPN) were developed to treat abdominal cancer-associated pain; however, these procedures are not always effective. The aim of this study was to explore predictors of pain response in EUS-guided neurolysis for pancreatic cancer-associated pain. Methods: This was a retrospective analysis of prospectively collected data of 112 consecutive patients who underwent EUS-BPN in our institution. EUS-CGN was added in cases of visible celiac ganglia. The neurolytic-spread area was divided into six sections and evaluated by post-procedural computed tomography scanning. Pain intensity was assessed using a visual analog scale (VAS), and a decrease in VAS scores by 3 points after neurolysis was considered a good pain response. Univariable and multivariable logistic regression analyses were performed to explore predictors of pain response at 1 and 4 weeks, and complications. Results: A good pain response was obtained in 77.7% and 67.9% of patients at 1 and 4 weeks, respectively. In the multivariable analysis of these patients, the combination method (EUS-BPN plus CGN) was a significant positive predictive factor at 1 week (odds ratio = 3.69, p = 0.017) and 4 weeks (odds ratio = 6.37, p = 0.043). The numbers of neurolytic/contrast spread areas (mean SD) were 4.98 +/- 1.08 and 4.15 +/- 1.12 in patients treated with the combination method and single method, respectively (p < 0.001). There was no significant predictor of complications. Conclusions: EUS-BPN in combination with EUS-CGN was a predictor of a good pain response in EUS-guided neurolysis for pancreatic cancer-related pain. The larger number of neurolytic/contrast spread areas may lead to better outcomes in patients receiving combination treatment.", "journal": "THERAPEUTIC ADVANCES IN GASTROENTEROLOGY", "category": "Gastroenterology & Hepatology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000378448600003", "keywords": "Air pollution; China; Health benefit; Intervention; Outdoor", "title": "Health benefits from improved outdoor air quality and intervention in China", "abstract": "China is at its most critical stage of outdoor air quality management. In order to prevent further deterioration of air quality and to protect human health, the Chinese government has made a series of attempts to reduce ambient air pollution. Unlike previous literature reviews on the widespread hazards of air pollution on health, this review article firstly summarized the existing evidence of human health benefits from intermittently improved outdoor air quality and intervention in China. Contents of this paper provide concrete and direct clue that improvement in outdoor air quality generates various health benefits in China, and confirm from a new perspective that it is worthwhile for China to shift its development strategy from economic growth to environmental economic sustainability. Greater emphasis on sustainable environment design, consistently strict regulatory enforcement, and specific monitoring actions should be regarded in China to decrease the health risks and to avoid long-term environmental threats. (C) 2016 Elsevier Ltd. All rights reserved.", "journal": "ENVIRONMENTAL POLLUTION", "category": "Environmental Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000367754400044", "keywords": "Variational mode decomposition; Filter banks; Detrending; Fractional Gaussian noise; Impacts", "title": "Filter bank property of variational mode decomposition and its applications", "abstract": "The variational mode decomposition (VMD) was proposed recently as an alternative to the empirical mode decomposition (EMD). To shed further light on its performance, we analyze the behavior of VMD in the presence of irregular samples, impulsive response, fractional Gaussian noise as well as tones separation. Extensive numerical simulations are conducted to investigate the parameters mentioned in VMD on these filter bank properties. It is found that, unlike EMD, the statistical characterization of the obtained modes reveals a different equivalent filter bank structure, robustness with respect to the non-uniformly sampling and good resolution in spectrum analysis. Moreover, we illustrate the influences of the main parameters on these properties, which provides a guidance on tuning them. Based on these findings, three potential applications in extracting time-varying oscillations, detrending as well as detecting impacts using VMD are presented. (C) 2015 Elsevier B.V. All rights reserved.", "journal": "SIGNAL PROCESSING", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000388369200010", "keywords": "body mass index; cardiac self-efficacy; general self-efficacy; health-promoting behaviors", "title": "Predictors of health-promoting behaviors in patients with coronary artery disease in the Iranian population", "abstract": "This study was carried out to determine the predictors of health-promoting behaviors (HPBs) in patients with coronary artery diseases (CAD) in the Iranian population. In this cross-sectional descriptive study, 250 eligible patients ages 42-80 years with a body mass index (BMI) between 18 and 45.7 kg/m(2) with CAD who were admitted to a cardiac hospital in Urmia, Iran participated. Valid questionnaires used the Health Promoting Lifestyle Profile-II (HPLP-II), Cardiac Self-Efficacy (CSE) and General Self-Efficacy (GSE) scales to assess HPBs, CSE and GSE, respectively. Fifty five percent of participants were men and mean (SD) age was 59 (12.1) years. Hierarchical multiple regression analysis indicated that CSE, GSE, education and BMI were the best predictors of HPBs, respectively. This model predicted 31% of HPB change (adjusted R-2=0.31). Responsibility for health and spiritual growth motivates patients to apply HPBs. However, physical activity and stress management are least applied.", "journal": "INTERNATIONAL JOURNAL OF NURSING PRACTICE", "category": "Nursing", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000371754700018", "keywords": "Joint inversion; Neighborhood Algorithm; Rayleigh waves; Phase velocity dispersion; ZH ratio; Crustal structure", "title": "Joint inversion of Rayleigh wave vertical-horizontal amplitude ratios and dispersion based on the Neighborhood Algorithm and its application", "abstract": "Rayleigh wave ellipticity (or ZH ratio) is a function of frequency and is particularly sensitive to shallow crustal structure beneath the seismograph station. Since depth sensitivity kernels of ZH ratios are different from those of dispersion data, the ZH ratio provides good complementary information for the dispersion-based inversion method. Therefore, we can combine the ZH ratio and dispersion data of Rayleigh wave fundamental mode to better invert for the velocity structure under a specific seismograph station. In this paper, we propose a joint inversion method using the dispersion and ZH ratio data based on the Neighborhood Algorithm. We conduct synthetic tests based on a theoretical model and prove the robustness of the joint inversion method, which can better constrain the shallow crustal structure. Compared to traditional inversion methods that only use dispersion data, the joint inversion can provide a more accurate crustal Vs model as well as V-p/V-s ratios for the layered crust. Finally, we apply the joint inversion technique to real measurements and obtain a more accurate crust shear velocity and V-p/V-s model beneath the station at Kunming (KMI) in southwest China.", "journal": "CHINESE JOURNAL OF GEOPHYSICS-CHINESE EDITION", "category": "Geochemistry & Geophysics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000382256100004", "keywords": "panel data models; panel smooth transition regression model; transition economies; environmental Kuznets curve", "title": "TESTING ENVIRONMENTAL KUZNETS CURVE IN THE SELECTED TRANSITION ECONOMIES WITH PANEL SMOOTH TRANSITION REGRESSION ANALYSIS", "abstract": "The Environmental Kuznets Curve (EKC) introduces an inverted U-shaped relationship between environmental pollution and economic development. The inverted U-shaped curve is seen as complete pattern for developed economies. However, our study tests the EKC for developing transition economies of European Union, therefore, our results could make a significant contribution to the literature. In this paper, the relationship between carbon dioxide (CO2) emissions, gross domestic product (GDP), energy use and urban population is investigated in the Transition Economies (Bulgaria, Croatia, Czech Republic, Estonia, Hungary, Latvia, Lithuania, Poland, Romania, Slovakia and Slovenia). Environmental Kuznets Curve is tested by panel smooth transition regression for these economies for 1993 - 2010 periods. As a result of study, the null hypothesis of linearity was rejected and no-remaining nonlinearity test showed that there is a smooth transition exists between two regimes (below $5176 GDP per capita is first one and above $5176 GDP per capita is second one) in the related period for these economies.", "journal": "AMFITEATRU ECONOMIC", "category": "Business; Economics; Management", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000367759500078", "keywords": "Marine renewable energy; Combined platforms; Geographical information systems; Site selection; Europe", "title": "Multi-criteria site selection for offshore renewable energy platforms", "abstract": "Geographical Information Systems (GIS) are commonly used in renewable energy resource analysis to establish optimal locations for development. Previous work focuses either on a single technology with fixed site-selection criteria, or on small, localised areas. The potential for combining or co-locating different offshore energy technologies, particularly over a large region, has been explored previously but at a relatively low level of detail. Here, bespoke resource data from high resolution co-located, co-temporal wind and wave models are presented in a GIS with a range of additional environmental and physical parameters. Dedicated decision-support tools have been developed to facilitate flexible, multicriteria site selections specifically for combined wind-wave energy platforms, focusing on the energy resources available. Time-series tools highlight some of the more detailed factors impacting on a site-selection decision. The results show that the main potential for combined technologies in Europe is focused to the north and west due to strong resources and acceptable depth conditions, but that there are still obstacles to be overcome in terms of constructability and accessibility. The most extreme conditions generally coincide with the maximum energy output, and access to these sites is more limited. (C) 2015 Elsevier Ltd. All rights reserved.", "journal": "RENEWABLE ENERGY", "category": "Green & Sustainable Science & Technology; Energy & Fuels", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000374805000032", "keywords": "Auditory processing disorder; Diagnostic criteria; Listening difficulties; Psychoacoustic tests; Screening; Children's auditory performance scale", "title": "Setting appropriate pass or fail cut-off criteria for tests to reflect real life listening difficulties in children with suspected auditory processing disorder", "abstract": "Objective: This paper explores the pass or fail cut-off criteria, the number of test fails, and the nature of tests that are most appropriate in predicting listening difficulties (LiD) in children with suspected APD (SusAPD). Methods: One hundred and nine English-speaking children (67 males, 42 females) aged between 6 and 11 years with SusAPD were assessed. The Children's Auditory Performance Scale (CHAPS) scores 2 SD below the mean were taken as markers of LiD in different listening conditions. Binary logistic regression analyses were carried out to evaluate the cut-off criterion (2 SD or 1.5 SD or 1 SD below the mean) of failing at least two tests, from the SCAN-C and IMAP test batteries, which significantly predicted LiD. Analyses were also carried out to assess if the group of auditory processing (AP) or cognitive or combination of AP plus cognitive tests were significant in predicting LiD. Receiver Operative Characteristic (ROC) curves were also explored to evaluate how the sensitivity and specificity in confirming LiD varied with the number of test fails. Results: Filtered Words, Competing Words, Competing Sentences, VCV in ICRA noise, Digit Span, Sight Word Reading and the Cued Auditory Attention tests correlated with one or more of the CHAPS domains. Failing at least two of these tests 1.5 SD below the mean significantly predicted (p<.05) CHAPS Ideal scores 2 SD below the mean, and failing at least two of the tests 1 SD below the mean significantly predicted (p<.05) CHAPS Memory and CHAPS Attention scores 2 SD below the mean. The combination of AP plus cognitive tests had significantly higher ability to predict CHAPS Ideal, Memory and Attention scores, compared to the group of AP or cognitive tests separately. ROC curves showed that failing at least two of the tests was associated with the best sensitivity and specificity in predicting LiD. Conclusion: Of the different CHAPS domains only the CHAPS Ideal, Memory and Attention correlated with the APD tests. Failing at least two APD tests from a combination of AP and cognitive tests 1 SD and 1.5 SD below the mean, but not 2 SD, is more appropriate in confirming LiD. (C) 2016 Elsevier Ireland Ltd. All rights reserved.", "journal": "INTERNATIONAL JOURNAL OF PEDIATRIC OTORHINOLARYNGOLOGY", "category": "Otorhinolaryngology; Pediatrics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000380831600003", "keywords": "kala-azar; environmental health; urbanisation; spatial analysis", "title": "Human and canine visceral leishmaniasis in an emerging focus in Aracuai, Minas Gerais: spatial distribution and socio-environmental factors", "abstract": "This study aimed to analyse the spatial distribution of human (2007-2013) and canine (2013) visceral leishmaniasis (VL) in the city of Aracuai, Minas Gerais, Brazil, and identify the socio-environmental factors related to their occurrence. The spatial distribution of human and canine cases was analysed by kernel density estimation (KDE) and the K function. The KDE values were analysed for correlation between human and canine LV and for normalised difference vegetation index (NDVI). Socio-environmental aspects of household structures and surroundings were evaluated. The spatial distribution of human and canine VL cases exhibited a significant aggregated pattern in distances greater than 350 and 75 m, respectively. The higher occurrence of human and canine infection occurred in the central area of the city. A positive correlation between the densities of human and canine cases was observed, as well as a negative correlation between NDVI and densities of human and canine cases. Socio-environmental analysis revealed that the large amount of animals, organic material from trees and deficiencies in environmental sanitation are possibly contributing to the continuation of the transmission cycle of Leishmania infantum in Aracuai. These results can contribute to the planning by competent agencies to reduce the incidence of infection in the city.", "journal": "MEMORIAS DO INSTITUTO OSWALDO CRUZ", "category": "Parasitology; Tropical Medicine", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000378774100020", "keywords": "Sparsity regularization; Diffusion coefficient identification problem; Nonlinear inverse problems; Well-posedness; Convergence rate", "title": "Sparsity Regularization of the Diffusion Coefficient Identification Problem: Well-Posedness and Convergence Rates", "abstract": "In this paper, we investigate sparsity regularization for the diffusion coefficient identification problem. Here, the regularization method is incorporated with the energy functional approach. The advantages of our approach are to deal with convex minimization problems. Therefore, the well-posedness of the problem is obtained without requiring regularity property of the parameter. The convexity of regularized problems also allows use the fast algorithms developed recently. Furthermore, the convergence rates of the method are obtained under a simple source condition. The main results of this paper are the well-posedness and convergence rates of sparsity regularization. We also obtain some new results of the continuity and the differentiability of related operators.", "journal": "BULLETIN OF THE MALAYSIAN MATHEMATICAL SCIENCES SOCIETY", "category": "Mathematics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000384808200021", "keywords": "magnetic anomalies; rock and mineral magnetism; instruments and techniques; paleomagnetism; magnetic inversion", "title": "Estimating the magnetization distribution within rectangular rock samples", "abstract": "Over the last decades, scanning magnetic microscopy techniques have been increasingly used in paleomagnetism and rock magnetism. Different from standard paleomagnetic magnetometers, scanning magnetic microscopes produce high-resolution maps of the vertical component of the magnetic induction field (flux density) on a plane located over the sample. These high-resolution magnetic maps can be used for estimating the magnetization distribution within a rock sample by inversion. Previous studies have estimated the magnetization distribution within rock samples by inverting the magnetic data measured on a single plane above the sample. Here we present a new spatial domain method for inverting the magnetic induction measured on four planes around the sample in order to retrieve its internal magnetization distribution. We have presumed that the internal magnetization distribution of the sample varies along one of its axes. Our method approximates the sample geometry by an interpretation model composed of a one-dimensional array of juxtaposed rectangular prisms with uniform magnetization. The Cartesian components of the magnetization vector within each rectangular prism are the parameters to be estimated by solving a linear inverse problem. Our method automatically deals with the averaging of the measured magnetic data due to the finite size of the magnetic sensor, preventing the application of a deconvolution before the inversion. Tests with synthetic data show the performance of our method in retrieving complex magnetization distributions even in the presence of magnetization heterogeneities. Moreover, they show the advantage of inverting the magnetic data on four planes around the sample and how this new acquisition scheme improves the estimated magnetization distribution within the rock sample. We have also applied our method to invert experimentally measured magnetic data produced by a highly magnetized synthetic sample that was manufactured in the laboratory. The results show that even in the presence of apparent position noise, our method was able to retrieve the magnetization distribution consistent with the isothermal remanent magnetization induced in the sample.", "journal": "GEOCHEMISTRY GEOPHYSICS GEOSYSTEMS", "category": "Geochemistry & Geophysics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000381507400023", "keywords": "Diffusion tensor imaging; smoking; tract-based spatial statistics", "title": "Quiet T1-Weighted Pointwise Encoding Time Reduction with Radial Acquisition for Assessing Myelination in the Pediatric Brain", "abstract": "BACKGROUND AND PURPOSE: T1-weighted pointwise encoding time reduction with radial acquisition (PETRA) sequences require limited gradient activity and allow quiet scanning. We aimed to assess the usefulness of PETRA in pediatric brain imaging. MATERIALS AND METHODS: We included consecutive pediatric patients who underwent both MPRAGE and PETRA. The contrast-to-noise and contrast ratios between WM and GM were compared in the cerebellar WM, internal capsule, and corpus callosum. The degree of myelination was rated by using 4-point scales at each of these locations plus the subcortical WM in the anterior frontal, anterior temporal, and posterior occipital lobes. Two radiologists made all assessments, and the intra- and interrater agreement was calculated by using intraclass correlation coefficients. Acoustic noise on MPRAGE and PETRA was measured. RESULTS: We included 56 patients 5 days to 14 years of age (mean age, 36.6 months) who underwent both MPRAGE and PETRA. The contrast-to-noise and contrast ratios for PETRA were significantly higher than those for MPRAGE (P < .05), excluding the signal ratio for cerebellar WM. Excellent intra- and interrater agreement were obtained for myelination at all locations except the cerebellar WM. The acoustic noise on PETRA (58.2 dB[A]) was much lower than that on MPRAGE (87.4 dB[A]). CONCLUSIONS: PETRA generally showed better objective imaging quality without a difference in subjective image-quality evaluation and produced much less acoustic noise compared with MPRAGE. We conclude that PETRA can substitute for MPRAGE in pediatric brain imaging.", "journal": "AMERICAN JOURNAL OF NEURORADIOLOGY", "category": "Clinical Neurology; Neuroimaging; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000383291800042", "keywords": "Photovoltaic system; Incremental Conductance; Maximum power point tracker; Step-up converter; Modeling; Simulation", "title": "Photovoltaic maximum power point tracking under fast varying of solar radiation", "abstract": "Perturb and Observe (P&O) and Incremental Conductance (INC) are widely used as Maximum Power Point Tracking (MPPT) techniques in Photovoltaic (PV) systems. But, they fail under rapidly varying of sunlight levels. This paper proposes a new MPPT technique, which can make a distinction between perturbation in the reference voltage and sudden-changing of sunlight and thus optimize the PV system efficiency. This method consists on a modified INC algorithm, which is used to fine-tune the duty cycle of the DC/DC converter in order to avoid divergences of the maximum power point (MPP) when using basic INC under fast varying of luminosity levels. The proposed PV-MPPT system, which is composed by a step-up converter as the interface to feed the load, is tested by simulation within the Matlab/Simulink software by taking into account the luminosity, the temperature-and the load variation. The simulation results are satisfactory and demonstrate that the improved INC technique can track the PV maximum power at diverse operating conditions with the most excellent performance, the energy conversion efficiency is increased by approximately 5%. (C) 2016 Elsevier Ltd. All rights reserved.", "journal": "APPLIED ENERGY", "category": "Energy & Fuels; Engineering, Chemical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000381101800003", "keywords": "Sampled-data systems; Switched systems; Robust exponential stabilization; Variable sampling; Packet dropouts", "title": "Robust Exponential Stabilization for Sampled-Data Systems with Variable Sampling and Packet Dropouts", "abstract": "This paper investigates the problem of robust exponential stabilization for sampled-data systems with variable sampling and packet dropouts. It is assumed that the system parameter uncertainties are norm-bounded and appear in both the state and input matrices. An input delay approach is adopted to model the sample-and-hold behavior with a time-varying delayed control input, and a switched system approach is proposed to model the data-missing phenomenon. On this basis, the sampled-data control system with variable sampling and packet dropouts is modeled as a switched system with time-varying delay. The objective is to design a sampled-data controller to guarantee the robust exponential stability of the resulting closed-loop system. Based on a new piecewise time-dependent Lyapunov functional, novel sufficient conditions are derived for the existence of robustly exponentially stabilizing sampled-data controllers. It is shown that the controller gains can be obtained by solving a set of linear matrix inequalities. Two simulation examples are given to demonstrate the effectiveness of the proposed method.", "journal": "CIRCUITS SYSTEMS AND SIGNAL PROCESSING", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000374171800025", "keywords": "Neonatal EEG; Normal neonatal sleep-wake cycle; Immediate postnatal period; Healthy term newborn infant; Factors influencing neonatal sleep", "title": "Sleep-wake cycle of the healthy term newborn infant in the immediate postnatal period", "abstract": "Objective: To examine sleep-wake cycle (SWC) composition of healthy term infants in the immediate postnatal period using EEG, and investigate factors that might influence it. Methods: Multichannel video-EEG was recorded for a median of 61.9 min (IQR: 60.0-69.3). The absolute and relative scores of sleep states were calculated for each infant's recording. Parametric/non-parametric statistical tests and multiple linear regression analysis were used to investigate the influence of perinatal factors on SWC composition. Results: Eighty healthy term infants aged 1-36 h were studied. A well-developed SWC was evident as early as within the first 6 h after birth. The mean (SD) percentage of active sleep (AS) was 52.1% (12.9) and quiet sleep (QS) was 38.6% (12.5). AS was longer and QS shorter in infants delivered by elective caesarean section (CS) compared to infants delivered by vaginal delivery or emergency CS. Conclusions: This is the first large cohort EEG study that has quantified neonatal sleep. SWC is clearly present immediately after birth, it is dominated by AS, and is influenced by mode of delivery. Significance: This knowledge of the early neonatal EEG/SWC can be used as reference data for EEG studies of neurologically compromised infants. (C) 2015 International Federation of Clinical Neurophysiology. Published by Elsevier Ireland Ltd. All rights reserved.", "journal": "CLINICAL NEUROPHYSIOLOGY", "category": "Clinical Neurology; Neurosciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000372977600007", "keywords": "random process; resource discovery; social network; gossip-based algorithm; distributed algorithm; probabilistic analysis", "title": "Discovery Through Gossip", "abstract": "We study randomized gossip-based processes in dynamic networks that are motivated by information discovery in large-scale distributed networks such as peer-to-peer and social networks. Awell-studied problem in peer-to-peer networks is resource discovery, where the goal for nodes (hosts with IP addresses) is to discover the IP addresses of all other hosts. Also, some of the recent work on self-stabilization algorithms for P2P/overlay networks proceed via discovery of the complete network. In social networks, nodes (people) discover new nodes through exchanging contacts with their neighbors (friends). In both cases the discovery of new nodes changes the underlying network new edges are added to the network - and the process continues in the changed network. Rigorously analyzing such dynamic (stochastic) processes in a continuously changing topology remains a challenging problem with obvious applications. This paper studies and analyzes two natural gossip-based discovery processes. In the push discovery or triangulation process, each node repeatedly chooses two random neighbors and connects them (i. e., \"pushes\" their mutual information to each other). In the pull discovery process or the two-hop walk, each node repeatedly requests or \"pulls\" a random contact from a random neighbor and connects itself to this two-hop neighbor. Both processes are lightweight in the sense that the amortized work done per node is constant per round, local, and naturally robust due to the inherent randomized nature of gossip. Our main result is an almost-tight analysis of the time taken for these two randomized processes to converge. We show that in any undirected n-node graph both processes take O(n log(2) n) rounds to connect every node to all other nodes with high probability, whereas Omega(n log n) is a lower bound. We also study the two-hop walk in directed graphs, and show that it takes O(n(2) log n) time with high probability, and that the worst-case bound is tight for arbitrary directed graphs, whereas Omega(n(2)) is a lower bound for strongly connected directed graphs. A key technical challenge that we overcome in our work is the analysis of a randomized process that itself results in a constantly changing network leading to complicated dependencies in every round. We discuss implications of our results and their analysis to discovery problems in P2P networks as well as to evolution in social networks. (C) 2016 Wiley Periodicals, Inc.", "journal": "RANDOM STRUCTURES & ALGORITHMS", "category": "Computer Science, Software Engineering; Mathematics, Applied; Mathematics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000381105400005", "keywords": "Nimaviridae; Whispovirus; Crustacean; Aquaculture; LD50", "title": "Experimental inoculation of Louisiana red swamp crayfish Procambarus clarkii with white spot syndrome virus (WSSV)", "abstract": "The red swamp crayfish Procambarus clarkii represents an important aquaculture species responsible for over half of all commercial aquaculture profits in Louisiana, USA. White spot syndrome virus (WSSV) is highly pathogenic in crustacean species and induces mass mortality in aquaculture operations worldwide. Natural outbreaks of WSSV occur yearly in cultured populations of crayfish in Louisiana. The goal of this study was to better understand the infectivity of WSSV in P. clarkii, by determining the minimum lethal dose necessary to initiate infection and to measure the resulting cumulative mortality following infection with different doses. A real time quantitative PCR (qPCR) method was used to detect WSSV in DNA extracted from gill tissue to ensure P. clarkii study populations were WSSV-free before the start of trials. Viable viral particles were isolated from naturally infected P. clarkii gill tissue and quantified using a novel digital PCR approach. Three infectivity trials were performed, and WSSV inocula were created by serial dilution, generating 5 treatments per trial. Five crayfish (weighing similar to 25 g) per dilution per trial received viral inoculations. Mortality was monitored daily for the duration of the trial in order to construct a median lethal dose (LD50) curve, and probit regression analysis was used to determine LD50 concentrations of viral particles. Knowledge of the infectivity of WSSV in native crayfish populations is of critical importance to the management of the commercial crayfish aquaculture industry in Louisiana. This is the first study to investigate the infectivity and to determine the LD50 of the Louisiana strain of WSSV in native crayfish.", "journal": "DISEASES OF AQUATIC ORGANISMS", "category": "Fisheries; Veterinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000371333200013", "keywords": "Anemia; cardiovascular; inflammation", "title": "Residual Mammographic Microcalcifications and Enhancing Lesions on MRI After Neoadjuvant Systemic Chemotherapy for Locally Advanced Breast Cancer: Correlation with Histopathologic Residual Tumor Size", "abstract": "To evaluate the accuracy of residual microcalcifications on mammogram (MG) in predicting the extent of the residual tumor after neoadjuvant systemic treatment (NST) in patients with locally advanced breast cancer and to evaluate factors affecting the accuracy of MG microcalcifications using magnetic resonance imaging (MRI) as a reference. The patients who underwent NST and showed suspicious microcalcifications on MG comprised our study population. Clinicopathologic and imaging (MG, MRI) findings were investigated. Agreement between image findings and pathology was assessed and factors affecting the discrepancy were analyzed. Among 207 patients, 196 had residual invasive ductal carcinoma or ductal carcinoma-in-situ (mean size, 3.78 cm). The overall agreement of residual microcalcifications on MG predicting residual tumor extents was lower than MRI in all tumor subtypes (intraclass correlation coefficient [ICC] = 0.368 and 0.723, p < 0.0001). The agreement of residual MG microcalcifications and pathology was highest in HR+/HER2(+) tumors and lowest in the triple-negative tumors (ICC = 0.417 and 0.205, respectively). Multivariate linear regression analysis revealed that a size discrepancy between microcalcifications and histopathology was correlated with molecular subtype (p = 0.005). In HR+/HER2(-) and triple-negative subtypes, the mean extents of residual microcalcification were smaller than residual cancer, and overestimation of tumor extent was more frequent in HR+/HER2(+) and HR-/HER2(+) tumors. The extent of microcalcifications on MG after NST showed an overall lower correlation with the extent of the pathologic residual tumor than enhancing lesions on MRI. The accuracy of residual tumor evaluation after NST with MG and MRI is affected by their molecular subtype.", "journal": "ANNALS OF SURGICAL ONCOLOGY", "category": "Oncology; Surgery", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000380736700022", "keywords": "Freeform surfaces; Non-Euclidean geometry; Triangular meshes; Surface characterisation; Texture representation; Surface metrology; Geometrical metrology", "title": "Freeform texture representation and characterisation based on triangular mesh projection techniques", "abstract": "Texture characterisation for freeform non-Euclidean surfaces is becoming increasingly important due to the widespread of the use of such surfaces in different applications, e.g. the additive manufacturing. Four main steps are required to analyse and characterise those surfaces which include new surface representation, surface filtration and decomposition, texture representation methods and finally the calculation of the surface parameters. Recently, the representation, as well as the filtration and decomposition, of freeform surfaces have been investigated and some algorithms have been proposed. This paper, however, shed the light on how to represent the texture of freeform non-Euclidean surfaces before calculating the parameters. A novel model for freeform surface parameterisation is introduced; this new model proposes the use of a projection algorithm before the actual calculation of the parameters. Different projection algorithms have been adopted from the mesh projection techniques found in the field of computer graphics. The results of applying those algorithms to represent the texture of both simulated and bioengineering surfaces are shown, also a comparison between those algorithms has been carried out. Furthermore, examples of calculating some of the surface parameters for freeform surfaces are given. (C) 2016 Elsevier Ltd. All rights reserved.", "journal": "MEASUREMENT", "category": "Engineering, Multidisciplinary; Instruments & Instrumentation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000378187500016", "keywords": "Discrete-time switched system; Fault estimation; Reduced-order observer; Output feedback controller", "title": "Actuator fault estimation and accommodation for switched systems with time delay: Discrete-time case", "abstract": "This paper investigates the problems of actuator fault estimation and accommodation for discrete-time switched systems with state delay. By using reduced-order observer method and switched Lyapunov function technique, a fault estimation algorithm is achieved for the discrete-time switched system with actuator fault and state delay. Then based on the obtained online fault estimation information, a switched dynamic output feedback controller is employed to compensate for the effect of faults by stabilizing the closed-loop systems. Finally, an example is proposed to illustrate the obtained results. (C) 2016 ISA. Published by Elsevier Ltd. All rights reserved.", "journal": "ISA TRANSACTIONS", "category": "Automation & Control Systems; Engineering, Multidisciplinary; Instruments & Instrumentation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000378551500007", "keywords": "wind turbine; aeroelasticity; fatigue damage; spectral method; wind turbine control; multidisciplinary design optimization; linear model", "title": "Wind turbine fatigue damage evaluation based on a linear model and a spectral method", "abstract": "Wind turbine multidisciplinary design optimization is currently the focus of several investigations because it has showed potential in reducing the cost of energy. This design approach requires fast methods to evaluate wind turbine loads with a sufficiently high level of fidelity. This paper presents a method to estimate wind turbine fatigue damage suited for optimization design applications. The method utilizes a high-order linear wind turbine model. The model comprehends a detailed description of the wind turbine and the controller. The fatigue is computed with a spectral method applied to power spectral densities of wind turbine sensor responses to turbulent wind. In this paper, the model is validated both in time domain and frequency domain with a nonlinear aeroservoelastic model. The approach is compared quantitatively against fatigue damage obtained from the power spectra of time series evaluated with nonlinear aeroservoelastic simulations and qualitatively against rainflow counting. Results are presented for three cases: load evaluation at normal operation in the full wind speed range, load change evaluation due to two different controller tunings at normal operation at three different wind speeds above rated and load dependency on the number of turbulence seeds used for their evaluation. For the full-range normal operation, the maximum difference between the two frequency domain-based estimates of the tower base lateral fatigue moments is 36%, whereas the differences for the other sensors are less than 15%. For the load variation evaluation, the maximum difference of the tower base longitudinal bending moment variation is 22%. Such large difference occurs only when the change in controller tuning has a low effect on the loads. Furthermore, results show that loads evaluated with the presented method are less dependent on the turbulent wind realization; therefore, less turbulence seeds are required compared with time-domain simulations to remove the dependency on the wind realization used to estimate loads. Copyright (c) 2015 John Wiley & Sons, Ltd.", "journal": "WIND ENERGY", "category": "Energy & Fuels; Engineering, Mechanical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000385170400007", "keywords": "Diagnostic algorithms; Low dose CT; Lung neoplasms; Early detection of cancer; Image processing", "title": "Diagnostic work-up of pulmonary nodules. Management of pulmonary nodules detected with low-dose CT screening", "abstract": "Pulmonary nodules are the most frequent pathological finding in low-dose computed tomography (CT) scanning for early detection of lung cancer. Early stages of lung cancer are often manifested as pulmonary nodules; however, the very commonly occurring small nodules are predominantly benign. These benign nodules are responsible for the high percentage of false positive test results in screening studies. Appropriate diagnostic algorithms are necessary to reduce false positive screening results and to improve the specificity of lung cancer screening. Such algorithms are based on some of the basic principles comprehensively described in this article. Firstly, the diameter of nodules allows a differentiation between large (> 8 mm) probably malignant and small (< 8 mm) probably benign nodules. Secondly, some morphological features of pulmonary nodules in CT can prove their benign nature. Thirdly, growth of small nodules is the best noninvasive predictor of malignancy and is utilized as a trigger for further diagnostic work-up. Non-invasive testing using positron emission tomography (PET) and contrast enhancement as well as invasive diagnostic tests (e.g. various procedures for cytological and histological diagnostics) are briefly described in this article. Different nodule morphology using CT (e.g. solid and semisolid nodules) is associated with different biological behavior and different algorithms for followup are required. Currently, no obligatory algorithm is available in German-speaking countries for the management of pulmonary nodules, which reflects the current state of knowledge. The main features of some international and American recommendations are briefly presented in this article from which conclusions for the daily clinical use are derived.", "journal": "RADIOLOGE", "category": "Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000383242900014", "keywords": "endometriosis; infertility; prospective cohort studies; bias; epidemiology", "title": "A prospective cohort study of endometriosis and subsequent risk of infertility", "abstract": "STUDY QUESTION: Is there a temporal relationship between endometriosis and infertility? SUMMARY ANSWER: Endometriosis is associated with a higher risk of subsequent infertility, but only among women age <35 years. WHAT IS KNOWN ALREADY: Endometriosis is the most commonly observed gynecologic pathology among infertile women undergoing laparoscopic examination. Whether endometriosis is a cause of infertility or an incidental discovery during the infertility examination is unknown. STUDY DESIGN, SIZE, DURATION: This study included data collected from 58 427 married premenopausal female nurses <40 years of age from 1989 to 2005, who are participants of the Nurses' Health Study II prospective cohort. PARTICIPANTS/MATERIALS, SETTING, METHODS: Our exposure was laparoscopically confirmed endometriosis. Multivariate Cox proportional hazards regression models were used to calculate hazard ratios (HRs) and 95% confidence intervals (CIs) for infertility risk (defined as attempting to conceive for >12 months) among women with and without endometriosis. MAIN RESULTS AND THE ROLE OF CHANCE: We identified 4612 incident cases of infertility due to any cause over 362 219 personyears of follow-up. Compared with women without a history of endometriosis, women with endometriosis had an age-adjusted 2-fold increased risk of incident infertility (HR = 2.12, 95% CI = 1.76-2.56) that attenuated slightly after accounting for parity. The relationship with endometriosis was only observed among women <35 years of age (multivariate HR <35 years = 1.77, 95% CI = 1.46-2.14; multivariate HR 35-39 years = 1.20, 95% CI = 0.94-1.53; P-interaction = 0.008). Risk of primary versus secondary infertility was similar subsequent to endometriosis diagnosis. Among women with primary infertility, 50% became parous after the endometriosis diagnosis, and among all women with endometriosis, 83% were parous by age 40 years. LIMITATIONS, REASONS FOR CAUTION: We did not have information on participants' intentions to conceive, but by restricting the analytic population to married women we increased the likelihood that pregnancies were planned (and therefore infertility would be recognized). Women in our cohort with undiagnosed asymptomatic endometriosis will be misclassified as unexposed. However, the small proportion of these women are diluted among the <50 000 women accurately classified as endometriosis-free, minimizing the impact of exposure misclassification on the effect estimates. WIDER IMPLICATIONS OF THE FINDINGS: This study supports a temporal association between endometriosis and infertility risk. Our prospective analysis indicates a possible detection bias in previous studies, with our findings suggesting that the infertility risk posed by endometriosis is about half the estimates observed in cross-sectional analyses. study funding/competing interests: This work was supported by the National Institutes of Health (grant numbers: UM1 CA176726, HD52473, HD57210, T32DK007703, T32HD060454, K01DK103720). We have no competing interests to declare.", "journal": "HUMAN REPRODUCTION", "category": "Obstetrics & Gynecology; Reproductive Biology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000377457200017", "keywords": "Data processing; graph theory; time series analysis; travel time prediction", "title": "Managing Spatial Graph Dependencies in Large Volumes of Traffic Data for Travel-Time Prediction", "abstract": "The exploration of the potential correlations of traffic conditions between roads in large urban networks, which is of profound importance for achieving accurate traffic prediction, often implies high computational complexity due to the implicated network topology. Hence, focal methods are required for dealing with the urban network complexity, reducing the performance requirements that are associated to the classical network search techniques (e. g., Breadth First Search). This paper introduces a graph-theory-based technique for managing spatial dependence between roads of the same network. In particular, after representing the traffic network as a graph, the local neighbors of each road are extracted using Breadth First Search graph traversal algorithm and a lower complexity variant of it. A Pearson product-moment correlation-coefficient-based metric is applied on the selected graph nodes for a prescribed number of level sets of neighbors. In order to evaluate the impact of the new method to the traffic prediction accuracy achieved, the most correlated roads are used to build a STARIMA model, taking also into account the possible time delays of traffic conditions between the interrelated roads. The proposed technique is benchmarked using traffic data from two different cities: Berlin, Germany, and Thessaloniki, Greece. Benchmark results not only indicate significant improvement on the computational time required for calculating traffic correlation metric values but also reveal that a different variant works better in different network topologies, after comparison to third-party approaches.", "journal": "IEEE TRANSACTIONS ON INTELLIGENT TRANSPORTATION SYSTEMS", "category": "Engineering, Civil; Engineering, Electrical & Electronic; Transportation Science & Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000382802900001", "keywords": "MIMO systems; Model predictive control; Controller tuning; Time-domain specifications; Parametric uncertainty; Paper machines", "title": "Robust tuning for machine-directional predictive control of MIMO paper-making processes", "abstract": "This paper solves the controller tuning problem of machine-directional predictive control for multipleinput multiple-output (MIMO) paper-making processes represented as superposition of first-order-plus dead-time (FOPDT) components with uncertain model parameters. A user-friendly multi-variable tuning problem is formulated based on user-specified time domain specifications and then simplified based on the structure of the closed-loop system. Based on the simplified tuning problem and a proposed performance evaluation technique, a fast multi-variable tuning technique is developed by ignoring the constraints of the MPC. In addition, a technique to predict the computation time of the tuning algorithm is proposed. The efficiency of the proposed method is verified through Honeywell real time simulator platform with a MIMO paper-making process obtained from real data from an industrial site. (C) 2016 Elsevier Ltd. All rights reserved.", "journal": "CONTROL ENGINEERING PRACTICE", "category": "Automation & Control Systems; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000379772500032", "keywords": "Numerical solutions; Inverse theory; Tomography", "title": "Self-Potential data inversion through the integration of spectral analysis and tomographic approaches", "abstract": "An integrated approach to interpret Self-Potential (SP) anomalies based on spectral analysis and tomographic methods is presented. The Maximum Entropy Method (MEM) is used for providing accurate estimates of the depth of the anomaly source. The 2-D tomographic inversion technique, based on the underground charge occurrence probability (COP) function, is, then, used to fully characterize the anomalous body, as the MEM is not helpful in delineating the shape of the anomaly source. The proposed integrated approach is applied for the inversion of synthetic SP data generated by geometrically simple anomalous bodies, such as cylinders and inclined sheets. This numerical study has allowed the determination of mathematical relationships between zero lines of the COP distributions, the polarization angles and the positions along the profile of the causative sources, which have been of great help for interpreting the related SP anomalies. Finally, the analysis of field examples shows the high potential applicability of the proposed integrated approach for SP data inversion.", "journal": "GEOPHYSICAL JOURNAL INTERNATIONAL", "category": "Geochemistry & Geophysics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000375492600014", "keywords": "overlapping particle; moving particle semi-implicit (MPS); generating particles; free surface flow; dam breaking", "title": "Numerical simulation of 3-D free surface flows by overlapping MPS", "abstract": "An overlapping moving particle semi-implicit (MPS) method is applied for 3-D free surface flows based on our in-house particle solver MLParticle-SJTU. In this method, the coarse particles are distributed in the whole domain and the fine particles are distributed in the local region of interest at the same time. With the fine particles being generated and removed dynamically, an algorithm of generating particles based on the 3-D overlapping volume is developed. Then, a 3-D dam break flow with an obstacle is simulated to validate the overlapping MPS. The qualitative comparison among experimental data and the results obtained by the VOF and the MPS shows that the shape of the free surface obtained by the overlapping MPS is more accurate than that obtained by the UNI-coarse and close to that obtained by the UNI-fine in the overlapping domain. In addition, the water height and the impact pressure at P1 are also in an overall agreement with experimental data. Finally, the CPU time required by the overlapping MPS is about half of that required by the UNI-fine.", "journal": "JOURNAL OF HYDRODYNAMICS", "category": "Mechanics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000372808200025", "keywords": "Speech enhancement; microphone array; noise reduction; multichannel; pseudo-coherence vector; ad hoc array", "title": "Buckling behavior of carbon nanotubes under bending: From ripple to kink", "abstract": "This paper elucidates the buckling behavior of carbon nanotubes (CNTs) under bending. CNTs are modeled as continuous thin-wall circular tubes, and their buckling is governed by equations that take into account of the sectional Brazier effect and non-uniform structural deformation. The CNT governing equations (fourth-order ordinary differential nonlinear equations with integral conditions) are solved by introducing a continuation algorithm. In addition, the buckling behavior of CNTs under bending is simulated with objective molecular dynamics (OMD). The atomistic simulations are used to verify the continuum results. We show that there exist low- and high-strain phases during the bending process of CNTs, and the transition in between may divide the whole bending process into three stages: low-curvature stage, mixed-curvature stage and high-curvature stage. Ripples are generated on the CNT surfaces before the formation of kinks. Compared to single-walled CNTs (SWCNTs), hydrogen-filled CNTs have a longer mixed-strain stage owing to the presence of internal pressure, and are therefore more inclined to exhibit a ripple morphology. Our results offer better understanding of the buckling behavior of CNTs, and may open up new opportunities for the design and applications of novel CNT-based nanoelectronics. (C) 2016 Elsevier Ltd. All rights reserved.", "journal": "CARBON", "category": "Chemistry, Physical; Materials Science, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000379960200083", "keywords": "hepatocellular cancer; Sp1; HIF1 alpha; The Cancer Genome Atlas; prognosis", "title": "The impact of high co-expression of Sp1 and HIF1 alpha on prognosis of patients with hepatocellular cancer", "abstract": "Transcription factor specificity protein 1 (Sp1) and hypoxia-inducible factor 1 alpha (HIF1 alpha) serve vital roles in tumor growth and metastasis. The present study aimed to evaluate the impact of co-expression of Sp1 and HIF1a on the prognosis of patients with hepatocellular cancer (HCC) using The Cancer Genome Atlas (TCGA) database and to validate the association between the expression levels of Sp1/HIF1 alpha in HCC specimens and patient survival using immunohistochemical analysis. A total of 214 eligible patients with HCC from TCGA database were collected for the study. The expression profile of Sp1 and HIF1a were obtained from the TCGA RNAseq database. Clinicopathological characteristics, including age, height, weight, gender, race, ethnicity, family cancer history, serum a-fetoprotein (AFP), surgical procedures and TNM stage were collected. The Cox proportional hazards regression model and Kaplan-Meier curves were used to assess the relative factors. Receiver operating characteristic (ROC) curves for cancer-specific survival (CSS) prediction were plotted to compare the prediction ability of expression of Sp1 and HIF1a and their co-expression. The location and expression of Sp1 and HIF1a in the HCC tissues were detected by immunohistochemistry (IHC) to verify the association between these two genes and CSS. The results demonstrated that the expressions of Sp1 and HIF1a were significantly increased in the succumbed group (P=0.001), compared with the surviving group. The CSS rates were 60.1% at 3 years (1,067 days), 35.8% at 5 years (1,823 days) and 9.5% at 10 years (3,528 days). Multivariate Cox regression analysis demonstrated that only the high expression levels of Sp1 and HIF1 alpha (>= 2x10(3)) were independent predictors for cancer mortality, with P=0.001 and P=0.029, respectively. The area under the curve for the ROC was found to be higher using the combination testing for two genes (0.751) in predicting cancer mortality, compared to a single gene (0.632 for Sp1 and 0.717 for HIF1a). Based on the cutoff points for gene expression, patients were divided into 3 groups: G1 (both genes <2x10(3)), G2 (either gene >= 2x10(3)) and G3 (both genes >= 2x10(3)). The risk of cancer mortality increased with high expression of genes, and G3 exhibited a greater risk than G2 when compared with the G1 group (HR=5.420, 95% CI 2.767-10.616, P=0.001; HR=3.270, 95% CI 1.843-5.803, P=0.001). The IHC staining results indicated that patients who died of cancer presented with significantly higher expression levels of these genes compared with those that did not (P=0.001). In summary, high expression levels of Sp1 and HIF1a in HCC tissues were associated with poor prognosis; in particular, the co-expression of these two genes increased the risk of cancer mortality.", "journal": "ONCOLOGY LETTERS", "category": "Oncology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000376774400014", "keywords": "Dynamic prosody modification; Instants of significant excitation; Objective measure; Pitch markers; Jitter", "title": "Improving the Flexibility of Dynamic Prosody Modification Using Instants of Significant Excitation", "abstract": "Modification of suprasegmental features such as pitch and duration of original speech by fixed scaling factors is referred to as static prosody modification. In dynamic prosody modification, the prosodic scaling factors (time-varying modification factors) are defined for all the pitch cycles present in the original speech. The present work is focused on improving the naturalness of the prosody modified speech by reducing the generation of piecewise constant segments in the modified pitch contour. The prosody modification is performed by anchoring around the accurate instants of significant excitation estimated from the original speech. The division of longer pitch intervals into many equal intervals over long speech segments introduces step-like discontinuities in the form of piecewise constant segments in the modified pitch contours. The effectiveness of proposed dynamic modification method is initially confirmed from the smooth modified pitch contour plot obtained for finer static prosody scaling factors, waveforms, spectrogram plots and comparison subjective evaluations. Also, the average jitter computed from the pitch segments of each glottal activity region in the modified speech is proposed as an objective measure for the prosody modification. The naturalness of the prosody modified speech using the proposed method is objectively and subjectively compared with that of the existing zero frequency filtered signal-based dynamic prosody modification. Also, the proposed algorithm effectively preserves the dynamics of the prosodic patterns in singing voices where in the parameters rapidly and continuously fluctuate within a higher range.", "journal": "CIRCUITS SYSTEMS AND SIGNAL PROCESSING", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000381102200011", "keywords": "Magneto-electro-thermo elastic nanoplates; magneto-electro-thermal loading; geometrically nonlinear free vibration; nonlocal elasticity theory; size effect", "title": "Size-Dependent Nonlinear Vibrations of First-Order Shear Deformable Magneto-Electro-Thermo Elastic Nanoplates Based on the Nonlocal Elasticity Theory", "abstract": "This paper deals with the size-dependent geometrically nonlinear free vibration of magneto-electro-thermo elastic (METE) nanoplates using the nonlocal elasticity theory. The mathematical formulation is developed based on the first-order shear deformation plate theory, von Karman-type of kinematic nonlinearity and nonlocal elasticity theory. The influences of geometric nonlinearity, rotary inertia, transverse shear deformation, magneto-electro-thermal loading and nonlocal parameter are considered. First, the generalized differential quadrature (GDQ) method is utilized to reduce the nonlinear partial differential equations to a system of time-dependent nonlinear ordinary differential equations. Afterwards, the numerical Galerkin method, periodic time differential operators and pseudo-arc length continuation algorithm are employed to compute the nonlinear frequency versus the amplitude for the METE nanoplates. The presented methodology enables one to describe the large-amplitude vibration characteristics of METE nanoplates with various sets of boundary conditions. A detailed parametric study is carried out to analyze the important parameters such as the nondimensional nonlocal parameter, external electric potential, external magnetic potential, temperature change, length-to-thickness ratio, aspect ratio and various edge conditions on the nonlinear free vibration characteristics of METE nanoplates. The results demonstrate that considering the size effect on the vibration response of METE nanoplate results in decreasing the natural frequency, a remarkable increasing effect on the hardening behavior and subsequently increasing the nonlinear-to-linear frequency ratio.", "journal": "INTERNATIONAL JOURNAL OF APPLIED MECHANICS", "category": "Mechanics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000373615500036", "keywords": "Buoyancy convection; High temperature; Projection method; Non-Boussinesq effects; Molecular degree of freedom excitation", "title": "A mathematical and numerical framework for the analysis of compressible thermal convection in gases at very high temperatures", "abstract": "The relevance of non-equilibrium phenomena, nonlinear behavior, gravitational effects and fluid compressibility in a wide range of problems related to high-temperature gas-dynamics, especially in thermal, mechanical and nuclear engineering, calls for a concerted approach using the tools of the kinetic theory of gases, statistical physics, quantum mechanics, thermodynamics and mathematical modeling in synergy with advanced numerical strategies for the solution of the Navier-Stokes equations. The reason behind such a need is that in many instances of relevance in this field one witnesses a departure from canonical models and the resulting inadequacy of standard CFD approaches, especially those traditionally used to deal with thermal (buoyancy) convection problems. Starting from microscopic considerations and typical concepts of molecular dynamics, passing through the Boltzmann equation and its known solutions, we show how it is possible to remove past assumptions and elaborate an algorithm capable of targeting the broadest range of applications. Moving beyond the Boussinesq approximation, the Sutherland law and the principle of energy equipartition, the resulting method allows most of the fluid properties (density, viscosity, thermal conductivity, heat capacity and diffusivity, etc.) to be derived in a rational and natural way while keeping empirical contamination to the minimum. Special attention is deserved as well to the well-known pressure issue. With the application of the socalled multiple pressure variables concept and a projection-like numerical approach, difficulties with such a term in the momentum equation are circumvented by allowing the hydrodynamic pressure to decouple from its thermodynamic counterpart. The final result is a flexible and modular framework that on the one hand is able to account for all the molecule (translational, rotational and vibrational) degrees of freedom and their effective excitation, and on the other hand can guarantee adequate interplay between molecular and macroscopic-level entities and processes. Performances are demonstrated by computing some incompressible and compressible benchmark test cases for thermal (gravitational) convection, which are then extended to the high-temperature regime taking advantage of the newly developed features. (C) 2016 Elsevier Inc. All rights reserved.", "journal": "JOURNAL OF COMPUTATIONAL PHYSICS", "category": "Computer Science, Interdisciplinary Applications; Physics, Mathematical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000383140000003", "keywords": "Clostridium difficile; laboratory diagnosis; diagnostic algorithms; nucleic acid amplification tests; toxigenic culture; cell cytotoxicity assay; enzyme immune assay", "title": "Optimizing the diagnostic testing of Clostridium difficile infection", "abstract": "Introduction: Clostridium difficile infection (CDI) is the leading cause of hospital-acquired diarrhea and is associated with a considerable health and cost burden. However, there is still not a clear consensus on the best laboratory diagnosis approach and a wide variation of testing methods and strategies can be encountered.Areas covered: We aim to review the most practical aspects of CDI diagnosis providing our own view on how to optimize CDI diagnosis.Expert commentary: Laboratory diagnosis in search of C. difficile toxins should be applied to all fecal diarrheic samples reaching the microbiology laboratory in patients > 2years old, with or without classic risk factors for CDI. Detection of toxins either directly in the fecal sample or in the bacteria isolated in culture confirm CDI in the proper clinical setting. Nuclear Acid Assay techniques (NAAT) allow to speed up the process with epidemiological and therapeutic consequences.", "journal": "EXPERT REVIEW OF ANTI-INFECTIVE THERAPY", "category": "Infectious Diseases; Microbiology; Pharmacology & Pharmacy", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000388212300013", "keywords": "melanoma; microvasculature; optical coherence tomography; microcirculation; optical clearing; speckle variance", "title": "Optical clearing of melanoma in vivo: characterization by diffuse reflectance spectroscopy and optical coherence tomography", "abstract": "Melanoma is the most aggressive type of skin cancer, with significant risk of fatality. Due to its pigmentation, light-based imaging and treatment techniques are limited to near the tumor surface, which is inadequate, for example, to evaluate the microvascular density that is associated with prognosis. White-light diffuse reflectance spectroscopy (DRS) and near-infrared optical coherence tomography (OCT) were used to evaluate the effect of a topically applied optical clearing agent (OCA) in melanoma in vivo and to image the microvascular network. DRS was performed using a contact fiber optic probe in the range from 450 to 650 nm. OCT imaging was performed using a swept-source system at 1310 nm. The OCT image data were processed using speckle variance and depth-encoded algorithms. Diffuse reflectance signals decreased with clearing, dropping by similar to 90% after 45 min. OCT was able to image the microvasculature in the pigmented melanoma tissue with good spatial resolution up to a depth of similar to 300 mu m without the use of OCA; improved contrast resolution was achieved with optical clearing to a depth of similar to 750 mu m in tumor. These findings are relevant to potential clinical applications in melanoma, such as assessing prognosis and treatment responses. Optical clearing may also facilitate the use of light-based treatments such as photodynamic therapy. (C) 2016 Society of Photo-Optical Instrumentation Engineers (SPIE)", "journal": "JOURNAL OF BIOMEDICAL OPTICS", "category": "Biochemical Research Methods; Optics; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000382352200020", "keywords": "DNA methylation; White blood cell; Hepatocellular carcinoma; Epigenetics", "title": "Blood DNA methylation markers in potentially identified Chinese patients with hepatocellular carcinoma", "abstract": "To determine whether blood DNA methylation is associated with hepatocellular carcinoma (HCC) for Chinese patients, we used genome-wide DNA methylation detection to access the blood samples of Chinese patients by Illumina Human methylation 450K arrays. Sixty potentially gene locis which had different methylated levels significantly among tumor and adjacent normal tissues would be tested in this study. A previous study was conducted in China communities and followed with 7 years. The DNA from white blood cells (WBC) from 192 patients with HCC and 215 matched controls were assayed in this study. The chi 2 test was used to measure data to categorize variables and t-test was uesd to evaluate the different characteristics among groups. Besides, odds ratios (OR) and 95%CI was calculated for matching factors by conditional logistic regression models. We found that high methylation in WNK2 was related to increased risk of HCC, and high methylation in TPO were related to decreased risk of HCC. In our multivariable conditional logistic regression models, these results all exist. Those findings support the methylated changes of WNK2 and TPO may become a new detection index for HCC patients in clinical laboratory. However, the results should be replicated in additional prospective studies with lager samples.", "journal": "PAKISTAN JOURNAL OF PHARMACEUTICAL SCIENCES", "category": "Pharmacology & Pharmacy", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000382357400011", "keywords": "Dimensioning; internet service provider; pricing; tiering", "title": "ISP Service Tier Design", "abstract": "Internet Service Provider design of service tiers are modeled and analyzed, based on demand for web browsing and video streaming. A basic model that considers user willingness to pay, network capacity, and application performance is formulated to determine when multiple tiers maximize profit. An extended model that also considers the time that users devote to each application is formulated to determine the optimal network capacity, tier rates, and tier prices. We show that an Internet Service Provider may simplify tier and capacity design by allowing its engineering department to set network capacity, its marketing department to set tier prices, and both to jointly set tier rates. Numerical results are presented to illustrate the magnitude of the decrease in profit compared to the optimal profit resulting from such a simplified design.", "journal": "IEEE-ACM TRANSACTIONS ON NETWORKING", "category": "Computer Science, Hardware & Architecture; Computer Science, Theory & Methods; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000373961000001", "keywords": "Unstructured grids; Geostatistics; Direct sequential simulation; Change of support; Discrete Gaussian model", "title": "Change-of-Support Models on Irregular Grids for Geostatistical Simulation", "abstract": "In many domains, numerical models are initialized with inputs defined on irregular grids. In petroleum reservoir engineering, they consist of a great variety of grid cells of different size and shape to enable fine-scale modeling in the vicinity of the wells and coarse modeling in less important regions. Geostatistical simulation algorithms, which are used to populate the cells of unstructured grids, often have to address the problem of transition from the small-scale statistical data stemming from laboratory cores analysis and seismic processing to the multiple larger scale geological supports. The reasonable generalization of the above-mentioned problem is integrating the point-support data to simulations on irregular supports. Classical geostatistical simulation methods for generating realizations of a stationary Gaussian random function cannot be applied to unstructured grids directly, because of the uneven supports. This article provides a critical review of existing geostatistical simulation methodologies for unstructured grids, including fine-scale simulations with upscaling and direct sequential simulation algorithms, and presents two different generalizations of the discrete Gaussian model for this purpose, thereby discussing the theoretical assumptions and the accuracy when implementing these models.", "journal": "MATHEMATICAL GEOSCIENCES", "category": "Geosciences, Multidisciplinary; Mathematics, Interdisciplinary Applications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000372986600016", "keywords": "Bad data identification (BDI); distributed state estimation; domain decomposition; large-scale systems; parallel programming; phasor measurement units (PMUs); weighted least square (WLS)", "title": "Parallel Domain-Decomposition-Based Distributed State Estimation for Large-Scale Power Systems", "abstract": "Growing system sizes and complexity, along with the large amount of data provided by phasor measurement units (PMUs), are the drivers to accurate state estimation algorithms for online monitoring and operation of power systems. In this paper, a distributed weighted-least-square state estimation method using an additive Schwarz domain decomposition technique is proposed to reduce the computational execution time. The proposed approach divides a data set into several subsets to be processed in parallel using a multiprocessor architecture considering data exchange among distributed areas. The slow coherency method and balanced partitioning are utilized to reduce the communication overhead and increase accuracy. Moreover, bad data analysis is also investigated in a distributed manner. The performance of the proposed distributed state estimator, along with the speed-up for several test systems, was compared with the traditional centralized state estimator. The simulation results show a speed-up of 6.5 for a 4992-bus system.", "journal": "IEEE TRANSACTIONS ON INDUSTRY APPLICATIONS", "category": "Engineering, Multidisciplinary; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000611822500007", "keywords": "Optimization; Genetic algorithms; Distribution networks; Simulation; Standards; Minimization; Linear programming; Distribution system; efficient genetic algorithm; loss minimization; network reconfiguration", "title": "Ant Colony Optimization Using Common Social Information and Self-Memory", "abstract": "Ant colony optimization (ACO), which is one of the metaheuristics imitating real ant foraging behavior, is an effective method to ?nd a solution for the traveling salesman problem (TSP). The rank-based ant system (AS(rank)) has been proposed as a developed version of the fundamental model AS of ACO. In the AS(rank), since only ant agents that have found one of some excellent solutions are let to regulate the pheromone, the pheromone concentrates on a specific route. As a result, although the AS(rank) can find a relatively good solution in a short time, it has the disadvantage of being prone falling into a local solution because the pheromone concentrates on a specific route. This problem seems to come from the loss of diversity in route selection according to the rapid accumulation of pheromones to the specific routes. Some ACO models, not just the AS(rank), also suffer from this problem of loss of diversity in route selection. It can be considered that the diversity of solutions as well as the selection of solutions is an important factor in the solution system by swarm intelligence such as ACO. In this paper, to solve this problem, we introduce the ant system using individual memories (ASIM) aiming to improve the ability to solve TSP while maintaining the diversity of the behavior of each ant. We apply the existing ACO algorithms and ASIM to some TSP benchmarks and compare the ability to solve TSP.", "journal": "COMPLEXITY", "category": "Mathematics, Interdisciplinary Applications; Multidisciplinary Sciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000624804100001", "keywords": "blank optimization design; workstep element; process route; low-carbon emission; combinatorial optimization algorithm", "title": "Research on Blank Optimization Design Based on Low-Carbon and Low-Cost Blank Process Route Optimization Model", "abstract": "The optimization of blank design is the key to the implementation of a green innovation strategy. The process of blank design determines more than 80% of resource consumption and environmental emissions during the blank processing. Unfortunately, the traditional blank design method based on function and quality is not suitable for today's sustainable development concept. In order to solve this problem, a research method of blank design optimization based on a low-carbon and low-cost process route optimization is proposed. Aiming at the processing characteristics of complex box type blank parts, the concept of the workstep element is proposed to represent the characteristics of machining parts, a low-carbon and low-cost multi-objective optimization model is established, and relevant constraints are set up. In addition, an intelligent generation algorithm of a working step chain is proposed, and combined with a particle swarm optimization algorithm to solve the optimization model. Finally, the feasibility and practicability of the method are verified by taking the processing of the blank of an emulsion box as an example. The data comparison shows that the comprehensive performance of the low-carbon and low-cost multi-objective optimization is the best, which meets the requirements of low-carbon processing, low-cost, and sustainable production.", "journal": "SUSTAINABILITY", "category": "Green & Sustainable Science & Technology; Environmental Sciences; Environmental Studies", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000613300300009", "keywords": "Network design; Sustainability; COVID-19; Simulation?optimization model; Hybrid meta-heuristic", "title": "A Fault Analysis Method for Three-Phase Induction Motors Based on Spiking Neural P Systems", "abstract": "The fault prediction and abductive fault diagnosis of three-phase induction motors are of great importance for improving their working safety, reliability, and economy; however, it is difficult to succeed in solving these issues. This paper proposes a fault analysis method of motors based on modified fuzzy reasoning spiking neural P systems with real numbers (rMFRSNPSs) for fault prediction and abductive fault diagnosis. To achieve this goal, fault fuzzy production rules of three-phase induction motors are first proposed. Then, the rMFRSNPS is presented to model the rules, which provides an intuitive way for modelling the motors. Moreover, to realize the parallel data computing and information reasoning in the fault prediction and diagnosis process, three reasoning algorithms for the rMFRSNPS are proposed: the pulse value reasoning algorithm, the forward fault prediction reasoning algorithm, and the backward abductive fault diagnosis reasoning algorithm. Finally, some case studies are given, in order to verify the feasibility and effectiveness of the proposed method.", "journal": "COMPLEXITY", "category": "Mathematics, Interdisciplinary Applications; Multidisciplinary Sciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000414485500009", "keywords": "APINAC; synthetic cannabinoids; metabolites; urine; rats; LC; QTOF; exact masses; GC-MS", "title": "Procedure for setting control for the turnover of new, potentially hazardous psychoactive substances. Detection of metabolites of a new APINAC psychoactive compound in rat urine by gas and liquid chromatography with mass spectrometry detection", "abstract": "The appearance of a new APINAC compound (AKB-57, ACBL(N)-018, adamantan-1-yl-1-pentyl-1H-indazol-3-carboxylate) in the Russian market of psychoactive drugs led to the need in setting measures of state control over its turnover and in solving the problem of categorizing this compound as a potentially hazardous psychoactive substance. To establish these control measures, it was necessary to determine the appropriate chromatographic-mass spectrometric characteristics and to search for its metabolites for their subsequent automated detection. The structure of an APINAC molecule has significant similarity with the molecules of other synthetic cannabinoids. In this paper, primary information on the metabolism of APINAC in the body of rats is presented. A number of putative metabolites, which are the products of hydrolysis of the initial structure and additional monohydroxylation of these products, carbonylation and carboxylation of the lateral N-pentyl chain of indazole-containing metabolites, were detected in rat urine by liquid chromatography-mass spectrometry in the mode of measurement of exact masses and gas chromatography-mass spectrometry. It was found that the formation of glucuronides is characteristic for 1-adamantol and its monohydroxylated metabolite and for the indazole-containing product of hydrolysis of APINAC. The presented mass spectra and retention characteristics of the detected metabolites can help in the detection of these (or similar) compounds in human urine.", "journal": "JOURNAL OF ANALYTICAL CHEMISTRY", "category": "Chemistry, Analytical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000405361900008", "keywords": "Electronic health records; Data mining; Collaborative filtering; Practice variability; Prediction models", "title": "Decaying relevance of clinical data towards future decisions in data-driven inpatient clinical order sets", "abstract": "Objective: Determine how varying longitudinal historical training data can impact prediction of future clinical decisions. Estimate the \"decay rate\" of clinical data source relevance. Materials and methods: We trained a clinical order recommender system, analogous to Netflix or Amazon's\" Customers who bought A also bought B...\" product recommenders, based on a tertiary academic hospital's structured electronic health record data. We used this system to predict future (2013) admission orders based on different subsets of historical training data (2009 through 2012), relative to existing human-authored order sets. Results: Predicting future (2013) inpatient orders is more accurate with models trained on just one month of recent (2012) data than with 12 months of older (2009) data (ROC AUC 0.91 vs. 0.88, precision 27% vs. 22%, recall 52% vs. 43%, all P < 10(-10)). Algorithmically learned models from even the older (2009) data was still more effective than existing human-authored order sets (ROC AUC 0.81, precision 16% recall 35%). Training with more longitudinal data (2009-2012) was no better than using only the most recent (2012) data, unless applying a decaying weighting scheme with a \"half-life\" of data relevance about 4 months. Discussion: Clinical practice patterns (automatically) learned from electronic health record data canvary substantially across years. Gold standards for clinical decision support are elusive moving targets, reinforcing the need for automated methods that can adapt to evolving information. Conclusions and relevancm: Prioritizing small amounts of recent data is more effective than using larger amounts of older data towards future clinical predictions. (C) 2017 The Authors. Published by Elsevier Ireland Ltd.", "journal": "INTERNATIONAL JOURNAL OF MEDICAL INFORMATICS", "category": "Computer Science, Information Systems; Health Care Sciences & Services; Medical Informatics", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000399896300006", "keywords": "community predictions; ecological indicator values; global warming; monitoring; plant community; species distribution models; Switzerland", "title": "Assessing and predicting shifts in mountain forest composition across 25years of climate change", "abstract": "AimSpatial predictions of future communities under climate change can be obtained by stacking species distribution models (S-SDM), but proper evaluation of community S-SDM predictions across time with fully independent data has rarely been carried out. The aim of this study was to evaluate the predictive abilities of S-SDMs for whole forest communities across the last 25years in a mountain region. LocationThe western Swiss Alps. MethodsWe used past vegetation surveys (2,984 plots) and environmental data from the 1990s to calibrate SDMs for 364 plant species and predict changes in forest composition under contemporary conditions. These projections were then evaluated by resurveying a random subset of 92 forest plots in summer 2014. ResultsSpecies distribution models showed the same accuracy in the past (calibration data) and present (evaluation data). The S-SDMs correctly predicted the general trends in species richness and shift of ecological conditions (i.e., temperature, moisture) at the regional level. However, it proved more difficult to identify precisely which forest communities or areas are most or least affected by climate change. Main conclusionOur results show that, across a period of a few decades, S-SDMs can usefully predict trends in macroecological properties such as richness or average ecological conditions, but fail to accurately predict changes in composition. This is likely due to the combined effects of the stochasticity of local colonization and extinction events, dispersal limitations, community assembly rules (e.g., competition), observer bias, model and location errors and interannual variation. Furthermore, these models cannot account for potential species adaptations leading to persistence in sites predicted unsuitable. This highlights the need for developing more accurate forest community predictions as support to help prioritizing conservation actions.", "journal": "DIVERSITY AND DISTRIBUTIONS", "category": "Biodiversity Conservation; Ecology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000403517000002", "keywords": "Wavelets; Multiscale principal components; Factor models; Forecasting", "title": "A wavelet-based multivariate multiscale approach for forecasting", "abstract": "In our increasingly data-rich environment, factor models have become the workhorse approach for modelling and forecasting purposes. However, factors are not observable and have to be estimated. In particular, the space spanned by the unknown factors is typically estimated via principal components. This paper proposes a novel procedure for estimating the factor space, resorting to a wavelet-based multiscale principal component analysis. A Monte Carlo simulation study is used to demonstrate that such an approach may improve both the estimation and the forecasting performances of factor models. The empirical application then illustrates its usefulness for forecasting GDP growth and inflation in the United States. (C) 2017 International Institute of Forecasters. Published by Elsevier B.V. All rights reserved.", "journal": "INTERNATIONAL JOURNAL OF FORECASTING", "category": "Economics; Management", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000403517700003", "keywords": "Nonlinear inverse problem; Bayesian inference; diffusion model", "title": "Active structural control for load mitigation of wind turbines via adaptive sliding-mode approach", "abstract": "This paper studies the load mitigation problem for wind turbines by using active tuned mass dampers. A state space model for the tower/nacelle system is established with the consideration of tower/blade interaction. The uncertainties that appear in the damping matrix and natural frequencies are also considered in the controller design. External loads acting on the tower including the drag force induced by winds and the absolute base shear induced by the rotating blades are involved, and shaping filters for online generating these loads are proposed which can be easily implemented in numerical simulations. An adaptive sliding-mode controller is proposed to handle the system uncertainties, external disturbances and hard constraint, and also to improve the overall performance of the wind turbine system. Numerical simulations are performed to demonstrate the effectiveness of the proposed control law. (C) 2017 The Franklin Institute. Published by Elsevier Ltd. All rights reserved.", "journal": "JOURNAL OF THE FRANKLIN INSTITUTE-ENGINEERING AND APPLIED MATHEMATICS", "category": "Automation & Control Systems; Engineering, Multidisciplinary; Engineering, Electrical & Electronic; Mathematics, Interdisciplinary Applications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000418566300013", "keywords": "volatile organic compounds; fungi; diagnosis; mycotoxins; multivariate class-modeling", "title": "IDENTIFICATION OF DIFFERENT FUSARIUM spp. THROUGH mVOCS PROFILING BY MEANS OF PROTON-TRANSFER-REACTION TIME-OF-FLIGHT (PTR-ToF-MS) ANALYSIS", "abstract": "The microbial volatile organic compounds (mVOCs) profiling of different Fusarium spp. affecting small grain cereals has been analyzed by means of Proton-Transfer-Reaction Time-of-Flight (PTR-ToF-MS). Morphological and sequence analysis of the elongation factor conserved domain allow identifying unequivocally different Fusarium species. Several mVOCs, ranging from 27.022 to 205.195 m/z were revealed. Partial Least Square Discriminant Analysis (PLS-DA) on the whole mVOCs dataset was used to classify the different species of Fusarium. Results showed that PTR-ToF-MS profiling of mVOCs is able to distinguish among different Fusarium spp. with percentages of correct classification of 97.8 and 92.6%, in the model/validation dataset and in the independent test set, respectively.", "journal": "JOURNAL OF PLANT PATHOLOGY", "category": "Plant Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000413930000015", "keywords": "Cognitive impairment; Kidney transplantation; MoCA", "title": "An improved SSA forecasting result based on a filtered recurrent forecasting algorithm", "abstract": "The Singular Spectrum Analysis (SSA) technique is a non-parametric powerful method in the field of time series analysis whose popularity has increased in recent years owing to its widespread applications. Recurrent forecasting is one of the important forecasting methods in SSA. In this paper, the forecasting accuracy of recurrent forecasts is improved via the introduction of a new recurrent forecasting algorithm. In the novel approach, the recurrent coefficients are generated from the filtered series which has less noise and thus enables one to achieve the better forecasts. The performance of the new method has been compared with the established recurrent forecasting method. The comparison involves applications to various real and simulated time series. The obtained results confirm that the new approach can provide more accurate forecasts. (C) 2017 Academie des sciences. Published by Elsevier Masson SAS. All rights reserved.", "journal": "COMPTES RENDUS MATHEMATIQUE", "category": "Mathematics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000401083600203", "keywords": "ICH score; APACHE II score; intracerebral hemorrhage; receiver operating characteristic; mortality", "title": "Activation of DNA Pattern Recognition Receptors After Plasmid Electrotransfer in Multiple Tumor Cell Types", "abstract": "Background: The intracerebral hemorrhage (ICH) score is well established as a reliable prognostic score in ICH, whereas recently, Acute Physiology and Chronic Health Evaluation II (APACHE II) has been observed to have a better discrimination in predicting mortality in primary pontine hemorrhage. Further, physiological parameters of APACHE II have been associated with outcome in ICH. This study is the first to observe a direct comparison between APACHE II and ICH scores in predicting 30-day mortality in spontaneous intracerebral hemorrhage (SICH). Materials and Methods: This study was a prospective observational study where we compared the receiver operating characteristic (ROCs) of baseline ICH and APACHE II scores in patients with SICH for predicting 30-day mortality outcome. Results: We observed that both APACHE II and ICH scores were good for predicting 30-day mortality with both having an area under the ROC curve of more than.8 (.831 [95% confidence interval {CI},.740-. 922; P <.001] and.892 [95% CI,.757-. 932; P <.001], respectively). However, the ICH score was better discriminative (area under the curve AUC,.892 versus.831; P =.040) and better calibrated (P =.037 versus P =.089, Hosmer-Lemeshow goodness-of-fit test for logistic regression) for the same. Both APACHE II and ICH scores had a sensitivity of 87% at cutoff values of 19 and 3, respectively; however, the ICH score had a better specificity (90% versus 76.5%). Conclusion: The ICH score was observed to have a better discrimination and calibration for predicting 30-day mortality in SICH. (C) 2017 National Stroke Association. Published by Elsevier Inc. All rights reserved.", "journal": "MOLECULAR THERAPY", "category": "Biotechnology & Applied Microbiology; Genetics & Heredity; Medicine, Research & Experimental", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000415330900042", "keywords": "Trimethylamine-N-oxide; Nuclear magnetic resonance spectroscopy; Cardiovascular disease; TMAO", "title": "Enhancing the resolution of a near-eye display with a Pancharatnam-Berry phase deflector", "abstract": "We report an electro-optic image shifter to enhance the resolution of display devices with a Pancharatnam-Berry phase deflector (PBD). The switching time of our PBD is less than 1 ms. Through synchronizing and computational factorization, we are able to double the display resolution for reducing the screen door effect. Such a thin and lightweight PBD image shifter can be easily integrated into wearable display devices. Its potential application for virtual reality and augmented reality is emphasized. (C) 2017 Optical Society of America", "journal": "OPTICS LETTERS", "category": "Optics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000412916200037", "keywords": "modal parameters; operational modal analysis; stochastic subspace identification; frequency domain decomposition; extended frequency domain decomposition", "title": "Dual fluorescence-absorption deconvolution applied to extended-depth-of-field microscopy", "abstract": "Fast imaging over large volumes can be obtained in a simple manner with extended-depth-of-field (EDOF) microscopy. A standard technique of Wiener deconvolution can correct for the blurring inherent in EDOF images. We compare Wiener deconvolution with an alternative, parameter-free technique based on the dual reconstruction of fluorescence and absorption layers in a sample. This alternative technique provides significantly enhanced reconstruction contrast owing to a quadratic positivity constraint that intrinsically favors sparse solutions. We demonstrate the advantages of this technique with mouse neuronal images acquired in vivo. (C) 2017 Optical Society of America", "journal": "OPTICS LETTERS", "category": "Optics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000399129400003", "keywords": "state and parameter estimation; particle filter; measurement test; nonlinear process system", "title": "Particle filter-based robust state and parameter estimation for nonlinear process systems with variable parameters", "abstract": "State and parameter estimation (SPE) plays an important role in process monitoring, online optimization, and process control. The estimation of states and parameters is generally solved simultaneously in the SPE problem, where the parameters to be estimated are specified as augmented states. When state and/or measurement equations are highly nonlinear and the posterior probability of the state is non-Gaussian, particle filter (PF) is commonly used for SPE. However, when the parameters switch with the operating conditions, the change of parameters cannot be detected and tracked by the conventional SPE method. This paper proposes a PF-based robust SPE method for a nonlinear process system with variable parameters. The measurement test criterion based on observation error is introduced to indirectly identify whether the parameters are changed. Based on the result of identification, the variances of the particles are modified adaptively for the tracking of the changed parameters. Finally, reliable SPE can be derived through iterative particles. The proposed PF-based robust SPE method is applied to two nonlinear process systems. The results demonstrate the effectiveness and robustness of the proposed method.", "journal": "MEASUREMENT SCIENCE AND TECHNOLOGY", "category": "Engineering, Multidisciplinary; Instruments & Instrumentation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000413042700032", "keywords": "Bearing; disk motors; force/torque (F/T) motors; load compensation; speed control", "title": "Design and Decoupled Compensation Methods of a PM Motor Capable of 6-D Force/Torque Actuation for Minimum Bearing Reaction", "abstract": "This paper presents a methodology to design and control a permanent magnet (PM) motor capable of 6-D force/torque actuation for real-time compensation of external loads to achieve minimum bearing reaction (MBR). Unlike conventional multiphase designs, the current inputs to the stator-electromagnets (EMs) can be flexibly configured to enable 6-D force and/or torque actuation in one motor; two common motor structures (radial and axial types) are illustrated. Both the forward and inverse force/torque models are presented in terms of coordinate-independent kernel functions that characterize the force between an EM and a PM pole pair. Two closed-form solutions to the inverse model that solves for the current-input vector minimizing the total input energy to generate a desired force/torque vector, which can be computed within 1 ms, are derived and verified numerically. A feedforward MBR compensator designed to argument the proportional-integral-derivative (PID) speed regulator has been experimentally evaluated on a structurally smart spindle system to minimize bearing reactions. Experiments show that the MBR compensation effectively reduces vibrations and improves cutting quality.", "journal": "IEEE-ASME TRANSACTIONS ON MECHATRONICS", "category": "Automation & Control Systems; Engineering, Manufacturing; Engineering, Electrical & Electronic; Engineering, Mechanical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000412980000015", "keywords": "Pharmacy; practice; rural; self-medication", "title": "Level and Correlates of Self-medication among Adults in a Rural Setting of Mainland Tanzania", "abstract": "A descriptive cross-sectional study was conducted to estimate the proportion of adults practicing self-medication in a rural setting of Tanzania. Using a structured interview schedule, we interviewed adults (aged at least 18 years) living in two selected wards. Data analysis involved descriptive statistics and logistic regression analysis. Among 474 study participants, 364 (76.8%) reported having experienced illnesses during the past three months before the survey. Among these, 236 (64.8%) reported having practiced self-medication. The main reported reason behind this practice was lack of money to consult a medical doctor. The major reported self-medication drugs include antipyretics 167 (58.2%), analgesics 162 (56.4%) and antimalarials 118 (41.1%). Use of antibiotics was reported by 51 (17.8%) participants. Self-medication was significantly associated with sex, education status and occupation of the respondent. Males had almost twice odds to practice self-medication as compared to females, Adjusted odds ratio was 1.9 (95% confidence interval, 1.1-3.4). Also, respondents with incomplete primary education had about four odds to practice self-medication than those without formal education. Although there were few participants reporting use of antibiotics, we recommend health educators to enhance understanding of benefits and possible risks associated with self-medication. Also, we recommend a study to examine the attitudes associated with self-medication practices.", "journal": "INDIAN JOURNAL OF PHARMACEUTICAL SCIENCES", "category": "Pharmacology & Pharmacy", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000413968600011", "keywords": "point vortex; Riemann surface; Bolza surface; hyperelliptic surface; vortex crystals", "title": "The motion of a vortex on a closed surface of constant negative curvature", "abstract": "The purpose of this work is to present an algorithm to determine the motion of a single hydrodynamic vortex on a closed surface of constant curvature and of genus greater than one. The algorithm is based on a relation between the Laplace-Beltrami Green function and the heat kernel. The algorithm is used to compute the motion of a vortex on the Bolza surface. This is the first determination of the orbits of a vortex on a closed surface of genus greater than one. The numerical results show that all the 46 vortex equilibria can be explicitly computed using the symmetries of the Bolza surface. Some of these equilibria allow for the construction of the first two examples of infinite vortex crystals on the hyperbolic disc. The following theorem is proved: 'a Weierstrass point of a hyperellitic surface of constant curvature is always a vortex equilibrium'.", "journal": "PROCEEDINGS OF THE ROYAL SOCIETY A-MATHEMATICAL PHYSICAL AND ENGINEERING SCIENCES", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000407078000009", "keywords": "allergy; birth cohort; cow's milk; egg; food introduction; latent class analysis; peanut; sensitization; skin prick test", "title": "Timing of food introduction and development of food sensitization in a prospective birth cohort", "abstract": "BackgroundThe effect of infant feeding practices on the development of food allergy remains controversial. We examined the relationship between timing and patterns of food introduction and sensitization to foods at age 1year in the Canadian Healthy Infant Longitudinal Development (CHILD) birth cohort study. MethodsNutrition questionnaire data prospectively collected at age 3, 6, 12, 18, and 24months were used to determine timing of introduction of cow's milk products, egg, and peanut. At age 1year, infants underwent skin prick testing to cow's milk, egg white, and peanut. Logistic regression models were fitted to assess the impact of timing of food exposures on sensitization outcomes, and latent class analysis was used to study patterns of food introduction within the cohort. ResultsAmong 2124 children with sufficient data, delaying introduction of cow's milk products, egg, and peanut beyond the first year of life significantly increased the odds of sensitization to that food (cow's milk adjOR 3.69, 95% CI 1.37-9.08; egg adjOR 1.89, 95% CI 1.25-2.80; peanut adjOR 1.76, 95% CI 1.07-3.01). Latent class analysis produced a three-class model: early, usual, and delayed introduction. A pattern of delayed introduction, characterized by avoidance of egg and peanut during the first year of life, increased the odds of sensitization to any of the three tested foods (adjOR 1.78, 95% CI 1.26-2.49). ConclusionsAvoidance of potentially allergenic foods during the first year of life significantly increased the odds of sensitization to the corresponding foods.", "journal": "PEDIATRIC ALLERGY AND IMMUNOLOGY", "category": "Allergy; Immunology; Pediatrics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000417029500012", "keywords": "Keratin associated protein 1.3; PCR-RFLP; Polymorphisms; Sheep; Wool traits", "title": "Association of polymorphic variants of KAP 1.3 gene with wool traits in Rambouillet sheep", "abstract": "The objective of the present study was to investigate the association of polymorphic variants of keratin-associated protein (KAP) 1.3 gene with wool traits in Rambouillet sheep. Genomic DNA was isolated from blood samples of 100 Rambouillet sheep. A 598 bp KAP 1.3 gene segment was amplified by PCR using ovine specific primers. One SNP was confirmed by sequencing, which was C264T nucleotide transition mutation. Restriction Fragment Length Polymorphism (RFLP) using Bsr I restriction enzyme revealed two alleles A and B with 0.645 and 0.355 allele frequencies, respectively. Genotypic frequencies were 0.41 for AA, 0.47 for AB and 0.12 for BB genotypes. The. chi(2)-test value for KAP 1.3 gene was non-significant revealing that the population under study was in Hardy Weinberg Equilibrium (HWE). Significant differences for CWY and highly significant differences for FD, SL, WC and GFW were observed between the least squares means of sex. Males showed higher level of production for CWY (56.91+/-0.26%) and GFW (2.53+/-0.09 kg); whereas, females were found to be superior in FD (21.74+/-0.11 mu), SL (6.07+/-0.15 cm) and WC (62.80+/-0.29). The least squares means of various genotypes for GFW differed significantly. Highest GFW production was recorded for BB genotype (2.47+/-0.13 kg) followed by AB genotype (2.05+/-0.07 kg) and least by AA genotypes (1.62+/-0.08 kg). These results suggested that polymorphisms in the KAP 1.3 gene might be a potential molecular marker for genetic selection for GFW in Rambouillet sheep.", "journal": "INDIAN JOURNAL OF ANIMAL SCIENCES", "category": "Agriculture, Dairy & Animal Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000406380900004", "keywords": "Testicular neoplasms; Germ cell and embryonal neoplasms; Drug therapy; Thrombocytosis", "title": "Can we predict chemotherapy response in patients with metastatic testicular cancer? Role of thrombocytosis as a novel marker", "abstract": "BACKGROUND: Primary treatment of choice for advanced germ cell tumors is 3 to 4 cycles of combination bleomycin-etoposide-cisplatin (BEP) chemotherapy. Although most patients treated for advanced germ cell tumors (GCT) will be cured, approximately 30% will fail to achieve a durable complete response (CR). Thrombocytosis has been found to be related with significantly shorter survival in many cancers. However, its role in testicular cancer patients has not been studied previously. The objective of this study was to investigate the relationship between thrombocytosis and chemotherapy response in patients with metastatic testicular cancer. METHODS: Records of 113 patients with advanced stage testicular cancer were reviewed. Treatment outcomes were classified as complete clinical response (cCR), partial clinical response (pCR), complete pathological response (cPR) and treatment failure and the relationship with thrombocytosis was investigated. Logistic regression analysis was performed to identify factors associated with treatment failure. RESULTS: Totally 103 patients met the eligibility criteria. Thrombocytosis was detected in 26 (25.2%) patients. Treatment failure was observed in 14 (53.8%) and 28 (36.4%) of the patients in the thrombocytosis and non-thrombocytosis groups respectively (P=0.037). Thrombocytosis and IGCCCG high-risk group are found as independent prognostic factors for treatment failure in multivariate analysis. CONCLUSIONS: Thrombocytosis is seen in 25% of patients with testicular GCT and it is found to be associated with poorer chemotherapy response in metastatic patients. It can be used to predict the response to chemotherapy.", "journal": "MINERVA UROLOGICA E NEFROLOGICA", "category": "Urology & Nephrology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000409274400043", "keywords": "Cord blood transplant; HHV-6; EBV; CMV; Reduced intensity; PBSC", "title": "Use of a Digital Health Application for Influenza Surveillance in China", "abstract": "Objectives. To examine whether a commercial digital health application could support influenza surveillance in China. Methods. We retrieved data from the Thermia online and mobile educational tool, which allows parents to monitor their children's fever and infectious febrile illnesses including influenza. We modeled monthly aggregated influenza-like illness case counts from Thermia users over time and compared them against influenza monthly case counts obtained from the National Health and Family Planning Commission of the People's Republic of China by using time series regression analysis. We retrieved 44 999 observations from January 2014 through July 2016 from Thermia China. Results. Thermia appeared to predict influenza outbreaks 1 month earlier than the National Health and Family Planning Commission influenza surveillance system (P = .046). Being younger, not having up-to-date immunizations, and having an underlying health condition were associated with participant-reported influenza-like illness. Conclusions. Digital health applications could supplement traditional influenza surveillance systems in China by providing access to consumers' symptom reporting. Growing popularity and use of commercial digital health applications in China potentially affords opportunities to support disease detection and monitoring and rapid treatment mobilization.", "journal": "AMERICAN JOURNAL OF PUBLIC HEALTH", "category": "Public, Environmental & Occupational Health", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000402351000005", "keywords": "Recursive signal reconstruction; Multivariate kernel-based regression and estimation; Sparse linear prediction; Minimum mean square error estimation; Image/video error concealment; Filling ordering", "title": "A statistical analysis of the kernel-based MMSE estimator with application to image reconstruction", "abstract": "In this paper we carry out a statistical analysis of a multivariate minimum mean square error (MMSE) estimator developed from a nonparametric kernel-based probability density function. This kernel-based MMSE (KMMSE) estimation has been recently proposed by the authors and successfully applied to image and video reconstruction. The statistical analysis that we present here allows us to understand the utility and limitations of this estimator. Thus, we propose a couple of estimation error measures intended for locally linear signals and show how KMMSE can approximate a sparse estimator. Also, we study how the estimation error propagates when signals are reconstructed recursively, that is, when already-reconstructed samples are used for estimating new samples. As an application, we focus on the problem of the filling ordering (FO) associated to the reconstruction of heterogeneous image blocks. Thus, borrowing the concept of soft data from missing data theory, the error measures established in the first part of the paper can be transformed into reliability measures from which a novel FO procedure is developed. We show that the proposed FO outperforms other state-of-the-art procedures.", "journal": "SIGNAL PROCESSING-IMAGE COMMUNICATION", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000404049000008", "keywords": "pancreatic neuroendocrine tumors; chromogranin A; chromogranin B; pancreatic diseases", "title": "Utility of chromogranin B compared with chromogranin A as a biomarker in Japanese patients with pancreatic neuroendocrine tumors", "abstract": "Objective: Currently, serum chromogranin A is a well-established biomarker for pancreatic neuroendocrine tumors; however, other pancreatic diseases, oral use of a proton pump inhibitor and renal impairment can affect chromogranin A. Meanwhile, chromogranin B, belonging to the same granin family as chromogranin A, is not fully examined in these conditions. The present study aimed to evaluate the utility of chromogranin B as a pancreatic neuroendocrine tumor biomarker. Methods: Serum chromogranin B levels were determined by radioimmunoassay and serum chromogranin A levels by enzyme-linked immunosorbent assay in pancreatic neuroendocrine tumor (n = 91) and other pancreatic conditions, and in healthy people (n = 104), to assess the relationships with clinical features. Results: The diagnostic ability of chromogranin B was as good as chromogranin A. The area under the curve was 0.79 for chromogranin B (sensitivity/specificity: 72%/77%), and 0.78 for chromogranin A (sensitivity/specificity: 79%/64%). Chromogranin B was not affected by proton pump inhibitor use and age, which affected chromogranin A. The number of cases without liver metastases was larger in pancreatic neuroendocrine tumor patients with positive chromogranin B and negative chromogranin A. Though chromogranin A significantly elevated cases with proton pump inhibitor treatment and had positive correlation with age, chromogranin B did not have the tendencies. However, both chromogranin B and chromogranin A elevated in the case with renal impairment. In addition, the logistic regression analysis showed that chromogranin B was superior to chromogranin A in differentiation of pancreatic neuroendocrine tumor from other pancreatic diseases. Conclusions: Compared with chromogranin A, chromogranin B may be more useful during proton pump inhibitor treatment and can detect tumors without liver metastases. In addition, chromogranin B may be an excellent biomarker when differentiation of pancreatic neuroendocrine tumor from other pancreatic diseases is required.", "journal": "JAPANESE JOURNAL OF CLINICAL ONCOLOGY", "category": "Oncology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000411653600003", "keywords": "Burns; QOL; Vulnerability; Resilience; Trauma; Patients' perspective", "title": "Probing the Surfaces of Biomacromolecules with Polymer-Scaffolded Dynamic Combinatorial Libraries", "abstract": "Methods to analyze and compare biomacromolecular surfaces are still in their relative infancy on account of the challenges involved in comparing surfaces computationally. We describe a systems chemistry approach that utilizes polymer-scaffolded dynamic combinatorial libraries to experimentally probe biomacromolecular surfaces in aqueous solution which provides feedback as to the nature of the surfaces, allowing the comparison of three globular proteins and a nucleic acid.", "journal": "ACS MACRO LETTERS", "category": "Polymer Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000407226400005", "keywords": "Pelteoobagrus fulvidraco; Fucoidan; Ultra-performance liquid chromatography-quadrupole-time-of-flight-mass spectrometry; Lipidomic", "title": "Morphometric covariation between palatal shape and skeletal pattern in Class II growing subjects", "abstract": "Objectives: To evaluate the patterns of covariation between palatal and craniofacial morphology in Class II subjects in the early mixed dentition by means of geometric morphometrics. Methods: A cross-sectional sample of 85 Class II subjects (44 females, 41 males; mean age 8.7 years +/- 0.8) was collected retrospectively according to the following inclusion criteria: European ancestry (white), Class II skeletal relationship, Class II division 1 dental relationship, early mixed dentition, and prepubertal skeletal maturation. Pre-treatment digital 3D maxillary dental casts and lateral cephalograms were available. Landmarks and semilandmarks were digitized (239 on the palate and 121 on the cephalogram) and geometric morphometric methods (GMM) were applied. Procrustes analysis and principal component analysis (PCA) were performed to reveal the main patterns of palatal shape and craniofacial skeletal shape variation. Two-block partial least squares analysis (PLS) assessed patterns of covariation between palatal morphology and craniofacial morphology. Results: For the morphology of the palate, the first principal component (PC1) described variation in all three dimensions. For the morphology of the craniofacial complex, PC1 showed shape variation mainly in the vertical direction. Palatal shape and craniofacial shape covaried significantly (RV coefficient: 0.199). PLS1 accounted for more than 64 per cent of total covariation and related divergence of the craniofacial complex to palatal height and width. The more a Class II subject tended towards high-angle divergence, the narrower and higher was the palate. Conclusions: Class II high-angle patients tended to have narrower and higher palates, while Class II low-angle patients were related to wider and more shallow palates.", "journal": "EUROPEAN JOURNAL OF ORTHODONTICS", "category": "Dentistry, Oral Surgery & Medicine", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000419690700001", "keywords": "benchmarking; reconciliation; Bayes estimator; normal-gamma family", "title": "Benchmarking and Reconciliation of Time Series An Applied Bayesian Method", "abstract": "The present article features a hierarchical Bayes method applied to solving problems of benchmarking and contemporaneous reconciliation across time series. This method enables the use of high frequency series to be either approximations or one or several related indicators. This method may be applied when facing flow or index disaggregation problems. The authors compare their results to classical procedures (viz. Denton univariate and Rossi multivariate methods) through the use of indicators. This article concludes that the suggested method bestows greater importance on the low frequency series profile, consequently providing smoother solutions than its counterparts.", "journal": "METHODOLOGY-EUROPEAN JOURNAL OF RESEARCH METHODS FOR THE BEHAVIORAL AND SOCIAL SCIENCES", "category": "Social Sciences, Mathematical Methods; Psychology, Mathematical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000404028800019", "keywords": "metalorganic vapor phase epitaxy; gallium nitride; chemical adsorption; surface reconstruction; density functional theory calculations; steepest-entropy-ascent quantum thermodynamics", "title": "Performance of Vascular Exposure and Fasciotomy Among Surgical Residents Before and After Training Compared With Experts", "abstract": "IMPORTANCE Surgical patient outcomes are related to surgeon skills. OBJECTIVE To measure resident surgeon technical and nontechnical skills for trauma core competencies before and after training and up to 18 months later and to compare resident performance with the performance of expert traumatologists. DESIGN, SETTING, AND PARTICIPANTS This longitudinal study performed from May 1, 2013, through February 29, 2016, at Maryland State Anatomy Board cadaver laboratories included 40 surgical residents and 10 expert traumatologists. INTERVENTIONS Performance was measured during extremity vascular exposures and lower extremity fasciotomy in fresh cadavers before and after taking the Advanced Surgical Skills for Exposure in Trauma (ASSET) course. MAIN OUTCOMES AND MEASURES The primary outcome variable was individual procedure score (IPS), with secondary outcomes of IPSs on 5 components of technical and nontechnical skills, Global Rating Scale scores, errors, and time to complete the procedure. Two trained evaluators located in the same laboratory evaluated performance with a standardized script and mobile touch-screen data collection. RESULTS Thirty-eight (95%) of 40 surgical residents (mean [SD] age, 31 [2.9] years) who were evaluated before and within 4 weeks of ASSET training completed follow-up evaluations 12 to 18 months later (mean [SD], 14 [2.7] months). The experts (mean [SD] age, 52 [10.0] years) were significantly older and had a longer (mean [SD], 46 [16.3] months) interval since taking the ASSET course (both P < .001). Overall resident cohort performance improved with increased anatomy knowledge, correct procedural steps, and decreased errors from 60% to 19% after the ASSET course regardless of clinical year of training (P < .001). For 21 of 40 residents (52%), correct vascular procedural steps plotted against anatomy knowledge (the 2 IPS components most improved with training) indicates the resident's performance was within 1 nearest-neighbor classifier of experts after ASSET training. Five residents had no improvement with training. The Trauma Readiness Index for experts (mean [SD], 74 [4]) was significantly different compared with the trained residents (mean [SD], 48 [7] before training vs 63 [7] after training [P = .004] and vs 64 [6] 14 months later [P = .002]). Critical errors that might lead to patient death were identified by pretraining IPS decile of less than 0.5. At follow-up, frequency of resident critical errors was no different from experts. The IPSs ranged from 31.6% to 76.9% among residents for core trauma competency procedures. Modeling revealed that interval experience, rather than time since training, affected skill retention up to 18 months later. Only 4 experts and 16 residents (40%) adequately decompressed and confirmed entry into all 4 lower extremity compartments, CONCLUSIONS AND RELEVANCE This study found that ASSET training improved resident procedural skills for up to 18 months. Performance was highly variable. Interval experience after training affected performance. Pretraining skill identified competency of residents vs experts. Extremity vascular and fasciotomy performance evaluations suggest the need for specific anatomical training interventions in residents with IPS deciles less than 0.5.", "journal": "JAMA SURGERY", "category": "Surgery", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000405959800007", "keywords": "end-stage renal disease; interrupted time series; peritoneal dialysis; prospective payment system", "title": "Impact of the End Stage Renal Disease Prospective Payment System on the Use of Peritoneal Dialysis", "abstract": "Introduction: The End Stage Renal Disease (ESRD) Prospective Payment System (PPS), implemented by the Centers for Medicare and Medicaid Services in January 2011, encouraged use of peritoneal dialysis (PD) through various financial incentives. Our goal was to determine whether PPS effectively increased PD use in incident dialysis patients. Methods: Our study used the United States Renal Data System (USRDS) to identify 430,927 adult patients who initiated dialysis between 2009 and 2012. The interrupted time series method was used to evaluate the association Centers for Medicare and Medicaid Services of PPS with PD use at dialysis initiation. We further stratified by patient demographics, predialysis care, and facility chain and profit status. Results: Interrupted time series analysis indicated PPS was associated with increased PD use in the 2-year period after PPS (change in slope = 0.04, P < 0.0001), although there was no immediate change in the level of PD use at the beginning of PPS (P = 0.512). Stratified analyses indicated PPS led to increased PD use across all age, race, and sex groups (P < 0.05) although marginally among females (P = 0.09). Notably, small dialysis organizations and nonprofit organizations appeared to increase use of PD faster compared to large dialysis organizations and for-profit units, respectively. Discussion: Implementation of the Centers for Medicare and Medicaid Services ESRD payment reform was associated with an increased use of PD in the 2 years after PPS. Our findings highlight the role of financial incentives in changing practice patterns to increase use of a dialysis modality considered to be both more cost-effective and empowering to ESRD patients. However, even after PPS, rates of PD use remain low among the dialysis population in the USA.", "journal": "KIDNEY INTERNATIONAL REPORTS", "category": "Urology & Nephrology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000455356600013", "keywords": "Ventricular assist device; Rotary blood pump; Physiological control; Ventricular suction; Heart failure; Starling control", "title": "In Vitro Evaluation of an Immediate Response Starling-Like Controller for Dual Rotary Blood Pumps", "abstract": "Rotary ventricular assist devices (VADs) are used to provide mechanical circulatory support. However, their lack of preload sensitivity in constant speed control mode (CSC) may result in ventricular suction or venous congestion. This is particularly true of biventricular support, where the native flow-balancing Starling response of both ventricles is diminished. It is possible to model the Starling response of the ventricles using cardiac output and venous return curves. With this model, we can create a Starling-like physiological controller (SLC) for VADs which can automatically balance cardiac output in the presence of perturbations to the circulation. The comparison between CSC and SLC of dual HeartWare HVADs using a mock circulation loop to simulate biventricular heart failure has been reported. Four changes in cardiovascular state were simulated to test the controller, including a 700 mL reduction in circulating fluid volume, a total loss of left and right ventricular contractility, reduction in systemic vascular resistance (SVR) from 1300 to 600 dyne s/cm(5), and an elevation in pulmonary vascular resistance (PVR) from 100 to 300 dyne s/cm(5). SLC maintained the left and right ventricular volumes between 69214 mL and 29-182 mL; respectively, for all tests, preventing ventricular suction (ventricular volume = 0 mL) and venous congestion (atrial pressures > 20 mm Hg). Cardiac output was maintained at sufficient levels by the SLC, with systemic and pulmonary flow rates maintained above 3.14 L/min for all tests. With the CSC, left ventricular suction occurred during reductions in SVR, elevations in PVR, and reduction in circulating fluid simulations. These results demonstrate a need for a physiological control system and provide adequate in vitro validation of the immediate response of a SLC for biventricular support.", "journal": "ARTIFICIAL ORGANS", "category": "Engineering, Biomedical; Transplantation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000413760900002", "keywords": "Executive function; Working memory; Bifactor; P-factor; Externalizing", "title": "Is Poor Working Memory a Transdiagnostic Risk Factor for Psychopathology?", "abstract": "In contrast to historical conceptualizations that framed psychological disorders as distinct, categorical conditions, it is now widely understood that co- and multi-morbidities between disorders are extensive. As a result, there has been a call to better understand the dimensional liabilities that are common to and influence the development of multiple psychopathologies, as supported and exemplified by the National Institutes of Mental Health (NIMH) Research Domain Criteria (RDoC) framework. We use a latent variable SEM approach to examine the degree to which working memory deficits represent a cognitive liability associated with the development of common and discrete dimensions of psychopathology. In a sample of 415 community recruited children aged 8-12 (n = 170 girls), we fit a bi-factor model to parent reports of behavior from the DISC-4 and BASC-2, and included a latent working memory factor as a predictor of the internalizing, externalizing, and general \"p-factor.\" We found that both the general \"p-factor\" and externalizing (but not internalizing) latent factor were significantly associated with working memory. When a bi-factor model of externalizing symptomology was fit to further explore this relationship, working memory was only correlated with the general externalizing dimension; correlation with specific inattention, hyperactive/impulsive, and oppositional factors did not survive once the general externalizing dimension was taken into consideration. These findings held regardless of the sex of the child. Our results suggest that working memory deficits represent both a common cognitive liability for mental health disorders, and a specific liability for externalizing disorders.", "journal": "JOURNAL OF ABNORMAL CHILD PSYCHOLOGY", "category": "Psychology, Clinical; Psychology, Developmental", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000404794000008", "keywords": "hepatocellular carcinoma; liver transplantation; mortality; prediction; prognosis", "title": "The Glasgow Prognostic Score and its variants predict mortality in living donor but not in deceased donor liver transplantation for hepatocellular carcinoma: A double-center validation study", "abstract": "AimThis study aimed to evaluate whether the Glasgow Prognostic Score (GPS) and its variants are able to predict mortality in live donor and deceased donor liver transplantation for hepatocellular carcinoma. MethodsData of 29 live donor and 319 deceased donor transplantations from two German transplant centers was analyzed. The GPS, modified GPS, hepatic GPS, and Abe score were investigated. Receiver operating characteristic (ROC) curve analysis was carried out to calculate the sensitivity, specificity, and overall model correctness of the investigated scores as a predictive model. Study end-points were 1-year, 3-year, and long-term mortality. ResultsA 1-year mortality of 19.1% (n=61), 3-year mortality of 26.3% (n=84), and overall mortality of 37.3% (n=119) was observed. All investigated scores failed to predict outcome in deceased donor liver transplantation (areas under ROC curves <0.700), whereas GPS, hepatic GPS, modified GPS, and the Abe score reached areas under ROC curves >0.700 for the prediction of 1-year mortality in live donor transplantation. The GPS and Abe score were also able to predict 3-year mortality. None of the investigated scores was a reliable predictor of long-term mortality. ConclusionSystemic inflammation-based scores have great prognostic potential in live donor transplantation. Abe score could be successfully externally validated in the current study for the first time. In deceased donor transplantation, none of the analyzed scores was able to allow reliable prediction for the investigated study end-points.", "journal": "HEPATOLOGY RESEARCH", "category": "Gastroenterology & Hepatology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000406700200131", "keywords": "optimization; simulated annealing; genetic algorithm; power losses; power consumption", "title": "Electric Power Grids Distribution Generation System for Optimal Location and Sizing-A Case Study Investigation by Various Optimization Algorithms", "abstract": "In this paper, the approach focused on the variables involved in assessing the quality of a distributed generation system are reviewed in detail, for its investigation and research contribution. The aim to minimize the electric power losses (unused power consumption) and optimize the voltage profile for the power system under investigation. To provide this assessment, several experiments have been made to the IEEE 34-bus test case and various actual test cases with the respect of multiple Distribution Generation DG units. The possibility and effectiveness of the proposed algorithm for optimal placement and sizing of DG in distribution systems have been verified. Finally, four algorithms were trailed: simulated annealing (SA), hybrid genetic algorithm (HGA), genetic algorithm (GA), and variable neighbourhood search. The HGA algorithm was found to produce the best solution at a cost of a longer processing time.", "journal": "ENERGIES", "category": "Energy & Fuels", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000405158900007", "keywords": "Sustainable development; Territorial cohesion; Policy conflicts; Co-integration; Time series analysis; EU regions", "title": "Oxidative Cleavage of the C=N Bond on AI(I)", "abstract": "Reaction of the cyclic guanidine TolN= SIMe with the aluminum(I) compound NacNacAl (1) results in the unprecedented cleavage of the C-N multiple bond to give, after rearrangement, the carbene-ligated Al(III) amide, NacNac'Al(NHTol)(SIMe) (6). DFT calculations revealed that these reactions proceed via a bimolecular mechanism in which either the basic Al(I) center or the transient Al=NTol species deprotonates the methyl group of the NacNac ligand.", "journal": "JOURNAL OF THE AMERICAN CHEMICAL SOCIETY", "category": "Chemistry, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000417176100007", "keywords": "People with disabilities; Daily activities; Self-report; Epidemiological surveys; Health inequities", "title": "Social inequalities in limitations caused by chronic diseases and disabilities in Brazil: the 2013 National Health Survey", "abstract": "This paper aims to evaluate the association between social inequalities and self-reported limitations for the performance of daily activities caused by chronic diseases or disabilities. The 2013 National Health Survey evaluated a sample of Brazilians with 18+ years. The outcome was that individuals reported that their daily activities were moderately, severely or very severely limited (LIMIT) by one or more chronic diseases, or mental, physical, hearing or motor impairment. The main exposure was the economy class, classified into five categories, ranging from A (richest) to E (poorest). We estimated a logistic regression model adjusted for economy class and confounding variables, considering the complex sample design and alpha -5%. Around 15.5% of individuals reported having Limit. Comparing social classes, 19.5%, 21.9%, 16.1%, 11.1%, and 7.7% individuals belonging to class E, D, C, B and A reported the outcome. The adjusted model showed greater odds of individuals in class D + E, and D, reporting LIMIT than individuals of class A + B (reference). Public policies for health care and social welfare for people with disabilities should focus on social classes E and D.", "journal": "CIENCIA & SAUDE COLETIVA", "category": "Public, Environmental & Occupational Health", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000403822000016", "keywords": "WDM/SCM-PON; backscattering techniques; OTDR; signal processing; I-OFDR", "title": "Multiple Fiber Fault Location With Low-Frequency Sub-Carrier Tone Sweep", "abstract": "We present a monitoring technique that can be directly integrated in the sub-carrier multiplexing (SCM) transceiver for PtP and wavelength division multiplexing and subcarrier multiplexing passive optical networks applications. The technique is based on the detection of the backscattered signal from a baseband tone (100 Hz to 100 kHz) and interpretation with the least absolute shrinkage and selection operator (LASSO) operator, a signal processing technique which identifies the fault positions based on the mathematical model of the received signal. The proposed single-ended fiber probing method enables the location of multiple faults with no a-priori knowledge of the fiber, and offers low-cost in-service operation with negligible impact on data transmission, few meters spatial resolution, and limited dynamic range. Due to all its advantages, but also considering its reach limitation, the SincLASSO method features as an ideal monitoring candidate for short reach sub-carrier multiplexed optical fiber networks, such as the ones intended for analog mobile Fronthaul based on fiber and fiber-extended copper lines.", "journal": "IEEE PHOTONICS TECHNOLOGY LETTERS", "category": "Engineering, Electrical & Electronic; Optics; Physics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000407655700007", "keywords": "Charging stations; Electric vehicle (EV's); Genetic algorithm (GA); Particle swarm optimization (PSO)", "title": "Optimal planning of electric vehicle charging station at the distribution system using hybrid optimization algorithm", "abstract": "India's ever increasing population has made it necessary to develop alternative modes of transportation with electric vehicles being the most preferred option. The major obstacle is the deteriorating impact on the utility distribution system brought about by improper setup of these charging stations. This paper deals with the optimal planning (siting and sizing) of charging station infrastructure in the city of Allahabad, India. This city is one of the upcoming smart cities, where electric vehicle transportation pilot project is going on under Government of India initiative. In this context, a hybrid algorithm based on genetic algorithm and improved version of conventional particle swarm optimization is utilized for finding optimal placement of charging station in the Allahabad distribution system. The particle swarm optimization algorithm re-optimizes the received sub-optimal solution (site and the size of the station) which leads to an improvement in the algorithm functionality and enhances quality of solution. The genetic algorithm and improved version of conventional particle swarm optimization algorithm will also be compared with a conventional genetic algorithm and particle swarm optimization. Through simulation studies on a real time system of Allahabad city, the superior performance of the aforementioned technique with respect to genetic algorithm and particle swarm optimization in terms of improvement in voltage profile and quality. (C) 2017 Elsevier Ltd. All rights reserved.", "journal": "ENERGY", "category": "Thermodynamics; Energy & Fuels", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000413143000012", "keywords": "Combustion distribution; disturbance rejection; extremum-seeking (ES) control; temperature distribution; thermal power plant", "title": "Control of Thermal Power Plant Combustion Distribution Using Extremum Seeking", "abstract": "High demands for increasing robustness, safety, and efficiency in thermal power plants are the main motivation behind ongoing attempts to optimize combustion. This paper presents a study of modeling and control of the combustion process in a tangentially fired pulverized-coal boiler. It proposes an approach to flame geometry and position control by means of reallocation of firing. Such control ensures flame focus maintenance away from the walls of the boiler, and thus prevents many unwanted by-products of combustion. In addition, uniform heat dissipation over mills enhances the energy efficiency and reliability of the boiler. First, experimental data obtained from the 350-MW boiler of the Nikola Tesla power plant, Serbia, are analyzed in detail. This results in a model identification procedure using an adaptive parameter estimation method. Second, constrained multivariate extremum seeking (ES) is proposed in this paper, to optimally tune boiler operation in order to maintain the desired flame configuration in the furnace. Finally, the effectiveness of the ES adaptive controller in the presence of disturbances is demonstrated through simulations performed on the experimentally identified model of the boiler.", "journal": "IEEE TRANSACTIONS ON CONTROL SYSTEMS TECHNOLOGY", "category": "Automation & Control Systems; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000399544500006", "keywords": "construction morphology; naive discriminative learning; -se/-ra Spanish alternation", "title": "The se-ra Alternation in Spanish Subjunctive", "abstract": "In this paper I take a look at a classic problem in Spanish morphosyntax, namely the alternation between the forms -se and -ra in the Imperfect Subjunctive (Imperfecto de Subjuntivo). Research on this topic has mainly focused on sociolinguistic variation, and has been done almost exclusively with impressionistic data and speakers' intuitions. I address the problem from a usage-based perspective, using corpus linguistics methods. The main claim is that the choice between -se and -ra correlates to a certain extent with morphosyntactic and discourse factors. Through collostructional analysis I also show that there exists repelled and attracted collexemes that distinguish and relate both forms.", "journal": "CORPUS LINGUISTICS AND LINGUISTIC THEORY", "category": "Linguistics; Language & Linguistics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000401063500002", "keywords": "conjugate gradient; sufficient descent; trust region", "title": "A modified three-term PRP conjugate gradient algorithm for optimization models", "abstract": "The nonlinear conjugate gradient (CG) algorithm is a very effective method for optimization, especially for large-scale problems, because of its low memory requirement and simplicity. Zhang et al. (IMA J. Numer. Anal. 26: 629-649, 2006) firstly propose a three-term CG algorithm based on the well known Polak-Ribiere-Polyak (PRP) formula for unconstrained optimization, where their method has the sufficient descent property without any line search technique. They proved the global convergence of the Armijo line search but this fails for the Wolfe line search technique. Inspired by their method, we will make a further study and give a modified three-term PRP CG algorithm. The presented method possesses the following features: (1) The sufficient descent property also holds without any line search technique; (2) the trust region property of the search direction is automatically satisfied; (3) the steplengh is bounded from below; (4) the global convergence will be established under the Wolfe line search. Numerical results show that the new algorithm is more effective than that of the normal method.", "journal": "JOURNAL OF INEQUALITIES AND APPLICATIONS", "category": "Mathematics, Applied; Mathematics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000402747100006", "keywords": "Optical flow; stereo vision; disparity; Viterbi process", "title": "Integration of optical flow and Multi-Path-Viterbi algorithm for stereo vision", "abstract": "This paper proposes a novel stereo matching algorithm to solve environment sensing problems. It integrates a non-convex optical flow and Viterbi process. The non-convex optical flow employs a new adaptive weighted non-convex Total Generalized Variation (TGV) model, which can obtain sharp disparity maps. Structural similarity, total variation constraint, and a specific merging strategy are combined with the 4 bi-directional Viterbi process to improve the robustness. In the fusion of the optical flow and Viterbi process, a new occlusion processing method is incorporated in order to get more sharp disparity and more robust result. Extensive experiments are conducted to compare this algorithm with other state-of-the-art methods. Experimental results show the superiority of our algorithm.", "journal": "INTERNATIONAL JOURNAL OF WAVELETS MULTIRESOLUTION AND INFORMATION PROCESSING", "category": "Computer Science, Software Engineering; Mathematics, Interdisciplinary Applications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000401213300040", "keywords": "Persistent depressive disorder; Placebo effect; Nocebo effect; Meta-analysis; Meta-regression analysis; Systematic review", "title": "Placebo and nocebo reactions in randomized trials of pharmacological treatments for persistent depressive disorder. A meta-regression analysis", "abstract": "Background: We aimed to investigate placebo and nocebo reactions in randomized controlled trials (RCT) of pharmacological treatments for persistent depressive disorder (PDD). Methods: We conducted a systematic electronic search and included RCTs investigating antidepressants for the treatment of PDD. Outcomes were the number of patients experiencing response and remission in placebo arms (=placebo reaction). Additional outcomes were the incidence of patients experiencing adverse events and related discontinuations in placebo arms (=nocebo reaction). A priori defined effect modifiers were analyzed using a series of meta-regression analyses. Results: Twenty-three trials were included in the analyses. We found a pooled placebo response rate of 31% and a placebo remission rate of 22%. The pooled adverse event rate and related discontinuations were 57% and 4%, respectively. All placebo arm outcomes were positively associated with the corresponding medication arm outcomes. Placebo response rate was associated with a greater proportion of patients with early onset depression, a smaller chance to receive placebo and a larger sample size. The adverse event rate in placebo arms was associated with a greater proportion of patients with early onset depression, a smaller proportion of females and a more recent publication. Conclusions: Pooled placebo and nocebo reaction rates in PDD were comparable to those in episodic depression. The identified effect modifiers should be considered to assess unbiased effects in RCTs, to influence placebo and nocebo reactions in practice. Limitations: Limitations result from the methodology applied, the fact that we conducted only univariate analyses, and the number and quality of included trials.", "journal": "JOURNAL OF AFFECTIVE DISORDERS", "category": "Clinical Neurology; Psychiatry", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000404877800015", "keywords": "Falls; Mild cognitive impairment; Intraindividual variability; Reaction time", "title": "Intraindividual Stepping Reaction Time Variability Predicts Falls in Older Adults With Mild Cognitive Impairment", "abstract": "Background: Reaction time measures have considerable potential to aid neuropsychological assessment in a variety of health care settings. One such measure, the intraindividual reaction time variability (IIV), is of particular interest as it is thought to reflect neurobiological disturbance. IIV is associated with a variety of age-related neurological disorders, as well as gait impairment and future falls in older adults. However, although persons diagnosed with Mild Cognitive Impairment (MCI) are at high risk of falling, the association between IIV and prospective falls is unknown. Methods: We conducted a longitudinal cohort study in cognitively intact (n = 271) and MCI (n = 154) community-dwelling adults aged 70-90 years. IIV was assessed through a variety of measures including simple and choice hand reaction time and choice stepping reaction time tasks (CSRT), the latter administered as a single task and also with a secondary working memory task. Results: Logistic regression did not show an association between IIV on the hand-held tasks and falls. Greater IIV in both CSRT tasks, however, did significantly increase the risk of future falls. This effect was specific to the MCI group, with a stronger effect in persons exhibiting gait, posture, or physiological impairment. Conclusions: The findings suggest that increased stepping IIV may indicate compromised neural circuitry involved in executive function, gait, and posture in persons with MCI increasing their risk of falling. IIV measures have potential to assess neurobiological disturbance underlying physical and cognitive dysfunction in old age, and aid fall risk assessment and routine care in community and health care settings.", "journal": "JOURNALS OF GERONTOLOGY SERIES A-BIOLOGICAL SCIENCES AND MEDICAL SCIENCES", "category": "Geriatrics & Gerontology; Gerontology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000412650300078", "keywords": "anatomic variation; D2 lymph node dissection; gastric cancer; robot surgery; situs inversus totalis; total gastrectomy", "title": "Gastric cancer in a situs inversus totalis patient with multiple intestinal and vessel variations related to gastrectomy surgery A case report and literature review", "abstract": "Rationale: Situs inversus totalis (SIT) is a rare congenital anomaly characterized by complete inversion of the abdominal and thoracic organs, and often involves multiple genetic mutations. The most suitable surgical technique for patients with multiple vessel and organ variations as well as SIT remains unclear. Furthermore, there has been insufficient clinical evidence that demonstrates which surgical techniques achieve the best outcomes. Finally, the standard of care has not yet been determined. We present the case of a 60-year-old man with SIT, who was diagnosed with moderately and poorly differentiated adenocarcinoma at the gastroesophageal junction. We further describe the advantage of using robotic-assisted laparoscopic surgery in patients with this anomaly. Patient concerns: A 60-year-old man complained of pain in his upper abdomen for 3 months. Physical examination revealed an apex beat in the right fifth intercostal space, and vascular anomalies were noted on abdominal angiographic computed tomography. Diagnoses: Moderately and poorly differentiated adenocarcinoma at the gastroesophageal junction with SIT. Interventions: Robot-assisted total gastrectomy with D2 lymph node dissection and hand-sewn Roux-en-Y anastomosis was performed. Out comes: The postoperative course was uneventful, and the patient was discharged on the seventh postoperative day. Lessons: Robotic surgery for gastric cancer is a safe and feasible alternative to laparoscopic surgery and it can be successfully used to treat gastric cancer in patients with SIT with multiple anatomic variations. As exemplified by our case, SIT might be accompanied by multiple anatomic variations. Detailed preoperative detailed imaging of the blood vessels and gastrointestinal tract is useful in these patients.", "journal": "MEDICINE", "category": "Medicine, General & Internal", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000401950600017", "keywords": "Visible light communication; flicker mitigation; finite-state-machines; channel coding; OOK; VPPM; Viterbi algorithm", "title": "Code Design for Flicker Mitigation in Visible Light Communications Using Finite State Machines", "abstract": "The IEEE 802.15.7 standard for visible light communication (VLC) includes the use of run-length-limited codes to mitigate modulation-induced flickering and the further use of coding to improve bit error rate performance. In this paper, we introduce algorithms to design codes using finite-state machines, which provide simultaneously a coding gain while also mitigating flicker. The codes have the additional advantage of being optimally soft-decision decodable using the Viterbi algorithm. To compare the flicker mitigation performance of different codes, we further introduce a mathematical measure of flicker based on the power spectrum of the transmitted signals. We discuss tradeoffs between flicker mitigation, code rate, and coding gain, design several codes, and compare their error rate and flicker mitigation performance to some codes in the VLC standard.", "journal": "IEEE TRANSACTIONS ON COMMUNICATIONS", "category": "Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000407617700005", "keywords": "rumor; rumor correction; retransmission; Twitter; retweetability", "title": "The Retransmission of Rumor and Rumor Correction Messages on Twitter", "abstract": "This article seeks to examine the relationships among source credibility, message plausibility, message type (rumor or rumor correction) and retransmission of tweets in a rumoring situation. From a total of 5,885 tweets related to the rumored death of the founding father of Singapore Lee Kuan Yew, 357 original tweets without an RT prefix were selected and analyzed using negative binomial regression analysis. The results show that source credibility and message plausibility are correlated with retransmission. Also, rumor correction tweets are retweeted more than rumor tweets. Moreover, message type moderates the relationship between source credibility and retransmission as well as that between message plausibility and retransmission. By highlighting some implications for theory and practice, this article concludes with some limitations and suggestions for further research.", "journal": "AMERICAN BEHAVIORAL SCIENTIST", "category": "Psychology, Clinical; Social Sciences, Interdisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000412840100004", "keywords": "Regression analysis; Heavy metals; Urban soils pollution; Principal component analysis; Cluster analysis", "title": "Investigating soils retention ratios and modelling geochemical factors affecting heavy metals retention in soils in a tropical urban watershed", "abstract": "This study aimed at investigating the retention of Pb and Cd in soils and the geochemical factors influencing the adsorption of these pollutants. Soil samples were air-dried and ground to pass through a 2-mm sieve, and different soil extracts were prepared for chemical analysis (organic matter, cation exchange capacity and pH). Total Pb and Cd were extracted with diacid using digestion method and determined by atomic adsorption spectrophotometer (AAS) after filtration. Results revealed that the heavy metals retention ratio (RR) of the Rhodic ferralsol, Xanthic ferralsol and Mollic gleysol (2) were very high for Cd (> 80 %) and was relatively low (generally < 60 %) for Pb. In contrast, RRs for the Plinthic gleysol and the Mollic gleysol (1) were relatively low (< 60 %), regardless of the heavy metal concerned. Multiple regression equations indicated for Pb and Cd concentrations different linear relationships over simple linear regression, when pH, organic matter, clay percentage and cation exchange capacity (CEC) were used as independent variables. Results indicate that organic matter exerts major influences on the retention of Pb and Cd in soils, while CEC, clay content and pH have a minor influence in this process in the Ntem watershed. From these observations, the application of soil organic matter could be a solution in protecting shallow aquifers from heavy metal pollution and thus insuring that they are not a hazard to public health.", "journal": "ENVIRONMENT DEVELOPMENT AND SUSTAINABILITY", "category": "Green & Sustainable Science & Technology; Environmental Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000413137900038", "keywords": "Risk-aware decision making; Next release problem; Multi-stakeholder", "title": "Phase Retrieval Approach for DOA Estimation With Array Errors", "abstract": "Direction-of-arrival estimation in the presence of gain and phase errors is investigated from the phase retrieval (PR) perspective. In order to remove the influence of phase errors, they are isolated by taking the absolute values of the elements in the compensated covariance matrix. Finally, the formulated nonconvex PR optimization problem is solved by utilizing the feasible point pursuit algorithm. Simulation results demonstrate the effectiveness of the proposed algorithm.", "journal": "IEEE TRANSACTIONS ON AEROSPACE AND ELECTRONIC SYSTEMS", "category": "Engineering, Aerospace; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000402486200010", "keywords": "Hip fractures; Neck of femur; Time to surgery", "title": "Neck of femur fractures in the elderly: Does every hour to surgery count?", "abstract": "Objectives: To determine if early surgery before 12 h confers a survival or length of stay benefit for patients with neck of femur (NOF) fractures. Design: Retrospective review of prospectively collected data. Setting: District general hospital. Patients: 1913 patients aged over 60 admitted with a fractured NOF who underwent surgery between 2011 and 2015. Mean age was 83.9 years. 73.7% were female. Intervention: Patients had surgery for fractured NOF with data collected on demographics, mortality and length of stay. Main outcome measurements: Data collected included gender, age, ASA grade, fracture anatomy, surgery, time to surgery, days spent in acute hospital and rehabilitation settings and 30-day mortality. Statistical analysis was used to identify independent predictors of mortality and length of stay. Results: 30-day mortality was 6.1% and the mean hospitalisation time was 13 +/- 11.3 days for the acute hospital and 20.2 +/- 17.2 days for the trust. Operations were performed at a mean of 23.8 +/- 14.8 h after presentation. Age, gender, ASA grade and type of fracture were independent predictors of either mortality or length of stay. Timing of surgery had an association with mortality but this only reached statistical significance at 24 h. In line with previous studies we analysed time to surgery in 12 h blocks. We also used logistic regression, recognizing time as a continuous variable, which revealed that every hour of delay to surgery increased the mortality risk by 1.8%. Conclusions: While every hour of delay increased mortality risk, the association with mortality only became statistically significant when delaying over 24 h. This supports a pragmatic approach, with surgery as soon as medically possible without a race to theatre. Level of evidence: Level III retrospective cohort study. (C) 2017 Elsevier Ltd. All rights reserved.", "journal": "INJURY-INTERNATIONAL JOURNAL OF THE CARE OF THE INJURED", "category": "Critical Care Medicine; Emergency Medicine; Orthopedics; Surgery", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000400345200006", "keywords": "forensic science; toolmark comparison; comparison microscope; focus variation microscopy; statistical algorithm; striae; correlation function", "title": "Validation of Toolmark Comparisons Made At Different Vertical and Horizontal Angles", "abstract": "Numerous studies have focused on determining whether objective statistical methods can be used to discriminate between known matches and nonmatches when comparing laboratory prepared toolmarks. This study involved an analysis of striated toolmarks made as a function of varying vertical and horizontal angles of attack. Comparisons based on experimental data show that replicate toolmarks from the same tool show high correlation values at identical vertical and horizontal angles, with the correlation decreasing as the angular difference increases, especially for horizontal angular changes. Comparisons between nonmatching samples produce low correlation values that remain unchanged as horizontal angular differences increase. While complete statistical separation was not achieved between matching and nonmatching samples, there is evidence demonstrating that toolmarks can be identified if the variation in horizontal angle is within 10 degrees. The experiment shows that computer-aided comparison techniques could be viable for identification with the proper statistical algorithm.", "journal": "JOURNAL OF FORENSIC SCIENCES", "category": "Medicine, Legal", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000408007600064", "keywords": "Drug release; Hydrogel; HPMC; MRI; NMR microimaging; Modeling; Extended release", "title": "Effects of HPMC substituent pattern on water up-take, polymer and drug release: An experimental and modelling study", "abstract": "The purpose of this study was to investigate the hydration behavior of two matrix formulations containing the cellulose derivative hydroxypropyl methylcellulose (HPMC). The two HPMC batches investigated had different substitution pattern along the backbone; the first one is referred to as heterogeneous and the second as homogenous. The release of both the drug molecule theophylline and the polymer was determined. Additionally, the water concentrations at different positions in the swollen gel layers were determined by Magnetic Resonance Imaging. The experimental data was compared to predicted values obtained by the extension of a mechanistic Fickian based model. The hydration of tablets containing the more homogenous HPMC batch showed a gradual water concentration gradient in the gel layer and could be well predicted. The hydration process for the more heterogeneous batch showed a very abrupt step change in the water concentration in the gel layer and could not be well predicted. Based on the comparison between the experimental and predicted data this study suggests, for the first time, that formulations with HPMC of different heterogeneities form gels in different ways. The homogeneous HPMC batch exhibits a water sorption behavior ascribable to a Fick's law for the diffusion process whereas the more heterogeneous HPMC batches does not. This conclusion is important in the future development of simulation models and in the understanding of drug release mechanism from hydrophilic matrices. (c) 2017 Elsevier B.V. All rights reserved.", "journal": "INTERNATIONAL JOURNAL OF PHARMACEUTICS", "category": "Pharmacology & Pharmacy", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000407338000002", "keywords": "percutaneous nephrolithotomy; renal stone; complications; intercostal", "title": "Non-Angled Intercostal Percutaneous Access Under Full Expiration: Safety Is Not an Issue Anymore", "abstract": "Objective: Percutaneous nephrolithotomy (PCNL) is a well-established procedure for the management of urinary calculi and can be performed intercostally or subcostally. Favoring one approach vs the other is still debatable, and literature has been inconclusive regarding the efficacy and safety of both approaches. Hence, this study aims to assess the safety and efficacy of direct non-angled intercostal technique performed under full expiration and to compare it to the subcostal approach. Methods: PCNL was conducted among 361 patients during 2010-2015 at Saint George Hospital University Medical Center in Beirut, Lebanon. PCNL was done by one operator and by following a standard technique. After reviewing the medical records, 304 patients were included. Data analysis was conducted using Stata/IC 10.0. Bivariate analysis was conducted using Pearson's Chi-square, and logistic regression model was run. Alpha level was set at 0.05. Results: Of the total patients, 54.6% and 45.4% underwent intercostal (Group I) and subcostal (Group II) access, respectively. Mean drop in hemoglobin in Group II was 1.9 g/dL vs 1.48 g/dL in Group I (p-value = 0.0040). The mean difference in operation time between group I (88.61 minutes) and group II (102.58 minutes) was statistically significant (p-value = 0.0064). Patients were stone free in 88.05% of the intercostal cases and 78.52% of the subcostal cases. Group II patients were twice more likely to have residual stones compared to Group I (p-value = 0.029). No statistical significance was observed in postoperative complications among both groups. In addition, no cases of pneumothorax were reported. Conclusion: Compared to subcostal access, intercostal approach under full expiration is a safe technique that provides optimal approach to the intrarenal collecting system and allows less angulation, less bleeding, and yields higher stone clearance with minimal complications. When performed by a well-trained urologist, intercostal access should be advocated in PCNL to obtain a direct non-angled access to the tip of the desired posterior calix.", "journal": "JOURNAL OF ENDOUROLOGY", "category": "Urology & Nephrology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000404650900014", "keywords": "ARAP3; Prognosis; Breast cancer; NOTH4; CDH5", "title": "Bioinformatic analysis of prognostic value of ARAP3 in breast cancer and the associated signaling pathways", "abstract": "OBJECTIVE: In this study, we tried to pool previous annotated genomic data to assess the association between ARAP3 expression and metastatic relapse (MR) risk in patients with breast cancer. Moreover, we also investigated the signaling pathways in which ARAP3 might be involved in breast cancer. MATERIALS AND METHODS: The raw microarray data (GDS5666) that compared gene transcriptional profiles of 4T1 derived lung-aggressive explant and primary tumor explant were reanalyzed to identify the dysregulated genes. ARAP3 mRNA expression, its association with MR-free survival and its co-upregulated genes in breast cancer, were studied by data mining in TCGA database and Breast Cancer Gene-Expression Miner Version 4.0 (bc-GenExMiner 4.0). RESULTS: A RAP3 is a significantly upregulated gene in the metastatic breast tumor cells. The ER-patients with high ARAP3 expression had significantly increased the risk of MR, regardless of the nodal status. Patients in ER-/Nm group with high ARAP3 expression had the highest risk of MR (HR: 1.25; 95% CI: 1.10-1.41, p<0.001). In patients with basal-like tumors, High ARAP3 level is associated with significantly worse MR-free survival (HR: 1.63, 95% CI: 1.22-2.19, p=0.001). ARAP3 might be closely related to the NOTCH4 and CDH5 signaling pathways in breast cancer. CONCLUSIONS: The expression level of ARAP3 might be a useful indicator of the metastatic likelihood of the basal-like breast tumors. ARAP3 might be involved in NOTCH4 and CDH5 related signaling pathways, but the underlying mechanism should be further studied.", "journal": "EUROPEAN REVIEW FOR MEDICAL AND PHARMACOLOGICAL SCIENCES", "category": "Pharmacology & Pharmacy", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000414611600008", "keywords": "sustainable development; CSR; firm value; SMEs; firm size", "title": "Management for Sustainable Development and Its Impact on Firm Value in the SME Context: Does Size Matter?", "abstract": "With the increasing demands from society towards sustainable and social responsible business practices, management for sustainable development has become a cornerstone to understand the success of many firms in the current competitive context. This article investigates corporate social responsibility (CSR) and examines the links between CSR practices and business outcomes - both financial and non-financial (i.e. image and corporate reputation) - for small-to-medium sized enterprises (SMEs). In addition, we also attempt to determine whether the impact of such relationships is moderated by firm size. To this end, we carry out a quantitative study using PLS techniques to analyze a sample of SME owners and managers, with a view to test the proposed model in the light of social capital theory. In this sense, our study is pioneering in that it aims to determine - from a quantitative viewpoint - the degree to which firm size has a moderating impact on a series of relevant CSR-driven outcomes. The data suggest that, in SME contexts, CSR impacts corporate reputation, brand image and financial value of the company. Importantly, we find that the larger the firm, the greater the intensity of the relationships linking CSR and business outcomes. Hence, our findings have important implications for CSR implementation in SME contexts. Finally, we provide a series of guidelines aimed at maximizing the effectiveness of CSR-based business practices. Copyright (c) 2017 John Wiley & Sons, Ltd and ERP Environment", "journal": "BUSINESS STRATEGY AND THE ENVIRONMENT", "category": "Business; Environmental Studies; Management", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000412408800009", "keywords": "alveolitis; extrinsic allergic; tomography scanners; X-ray computed; survival analysis", "title": "Presence of Air Trapping and Mosaic Attenuation on Chest Computed Tomography Predicts Survival in Chronic Hypersensitivity Pneumonitis", "abstract": "Rationale: Significant heterogeneity of computed tomography (CT) presentation exists within chronic hypersensitivity pneumonitis (HP). There are limited data aimed at delineating the prognostic value of specific CT features, distribution, and patterns in chronic HP. Objectives: To examine whether the presence of CT mosaic attenuation (MA) and air trapping (AT), and the distribution or patterns of fibrosis impact survival in subjects with chronic HP. Methods: We retrospectively identified 110 consecutively enrolled, well-characterized, biopsy-proven adult subjects with chronic HP between 1982 and 2015 from the National Jewish Health interstitial lung disease research database. The first available CT scan of diagnostic quality from each subject was formally evaluated for specific CT findings associated with chronic HP and for overall CT pattern. A Cox proportional hazards model was used to identify independent predictors in time-to-death analysis, and bootstrap analysis was performed for internal model validation. Results: Fibrotic HP(65%; 72/110) was most often peripheral in the axial plane and lower lung preponderant. The distribution of lung disease in those without fibrosis was most often axially and zonally diffuse. There was no association between survival and CT distribution or CT pattern in the whole cohort or within the fibrotic subset of subjects. After multivariate adjustment, AT/MA was independently associated with survival in the whole cohort (HR = 0.26; 95% confidence interval = 0.07-0.97). Results were similar after restricting the analyses to fibrotic HP cases. Conclusions: Among subjects with chronic HP, the presence of CT AT/MA may identify subjects with better prognosis.", "journal": "ANNALS OF THE AMERICAN THORACIC SOCIETY", "category": "Respiratory System", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000411429600013", "keywords": "atopic dermatitis; RNase 7; TH2 cytokines", "title": "APPROXIMATION OF ZEROS OF ACCRETIVE OPERATORS IN A BANACH SPACE", "abstract": "We consider the problem of finding a zero of an accretive operator in a Banach space and prove strong convergence results for resolvents of the accretive operator.", "journal": "ISRAEL JOURNAL OF MATHEMATICS", "category": "Mathematics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000402077800005", "keywords": "Disaster; vulnerability; resilience; human reaction; hazard; threat; systemic reaction", "title": "Resilience 2.0: Computer-aided Disaster Management", "abstract": "Compram Many factors (larger population, more dependency on technology, more human-caused interference in the natural systems and equilibria, climate changes,ai) contribute to the seemingly growing number and severity of disasters. Additional exaggeration is generated by public media. As a consequence Disaster Prevention and Disaster Management must be given increased attention. The ultimate goal of Disaster Management is resilience of the affected system and thus the adequate and acceptable survival of the affected population. We discuss system behavior in the case of an assault or disturbance: from being fragile (loss of their functionality due to the assault) to being resilient (having the capacity... of bouncing back to dynamic stability after a disturbance), or even antifragile (being able to \"learn\" so as to improve disaster resilience). Resilience 2.0 identifies a new paradigm: modern Information and Communication Technologies (ICT) are employed as a basis for enabling and improving resilience of a system. ICT provide the basis for sufficient preparation before an assault, for quick recognition of, and for effective, efficient reactions to disasters. Only the coordinated intra- and interphase deployment of ICT promises sufficient success and can bring resilience to currently as yet fragile systems. We discuss stressors (time and performance pressure, physical and psychological stress on personnel) and problems due to damaged ICT-platforms and communication infrastructure. The basic message is that computer-aided Disaster Management is able to offers a new level of reactivity: Resilience 2.0.", "journal": "JOURNAL OF SYSTEMS SCIENCE AND SYSTEMS ENGINEERING", "category": "Operations Research & Management Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000397366700007", "keywords": "Dynamic Time Warping (DTW); Time series; Distance measure; Warping path; Classification", "title": "Dynamic Time Warping under limited warping path length", "abstract": "Dynamic Time Warping (DTW) is probably the most popular distance measure for time series data, because it captures flexible similarities under time distortions. However, DTW has long been suffering from the pathological alignment problem, and most existing solutions, which essentially impose rigid constraints on the warping path, are likely to miss the correct alignments. A crucial observation on pathological alignment is that it always leads to an abnormally large number of links between two sequences. Based on this new observation, we propose a novel variant of DTW called LDTW, which limits the total number of links during the optimization process of DTW. LDTW not only oppresses the pathological alignment effectively, but also allows more flexibilities when measuring similarities. It is a softer constraint because we still let the optimization process of DTW decide how many links to allocate to each data point and where to put these links. In this paper, we introduce the motivation and algorithm of LDTW and we conduct a nearest neighbor classification experiment on UCR time series archive to show its performance. (C) 2017 Elsevier Inc. All rights reserved.", "journal": "INFORMATION SCIENCES", "category": "Computer Science, Information Systems", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000413762100001", "keywords": "Intracerebral hemorrhage; Hemorrhagic stroke; Intensive care; Outcome; Prognosis; Apache ii; Saps ii; Sofa; Glasgow coma scale score; Age", "title": "Common intensive care scoring systems do not outperform age and glasgow coma scale score in predicting mid-term mortality in patients with spontaneous intracerebral hemorrhage treated in the intensive care unit", "abstract": "Background: Intensive care scoring systems are widely used in intensive care units (ICU) around the world for case-mix adjustment in research and benchmarking. The aim of our study was to investigate the usefulness of common intensive care scoring systems in predicting mid-term mortality in patients with spontaneous intracerebral hemorrhage (ICH) treated in intensive care units (ICU). Methods: We performed a retrospective observational study including adult patients with spontaneous ICH treated in Finnish ICUs during 2003-2012. We used six-month mortality as the primary outcome of interest. We used logistic regression to customize Acute Physiology and Chronic Health Evaluation (APACHE) II, Simplified Acute Physiology Score (SAPS) II and Sequential Organ Failure Assessment (SOFA) for six-month mortality prediction. To assess the usefulness of the scoring systems, we compared their discrimination and calibration with two simpler models consisting of age, Glasgow Coma Scale (GCS) score, and premorbid functional status. Results: Totally 3218 patients were included. Overall six-month mortality was 48%. APACHE II and SAPS II outperformed SOFA (area under the receiver operator curve [AUC] 0.83 and 0.84, respectively, vs. 0.73) but did not show any benefit over the simpler models in terms of discrimination (AUC 0.84, p > 0.05 for all models). SAPS II showed satisfactory calibration (p = 0.058 in the Hosmer-Lemeshow test), whereas all other models showed poor calibration (p < 0.05). Discussion: In this retrospective multi-center study, we found that SAPS II and APACHE II were of no additional prognostic value to a simple model based on only age and GCS score for patients with ICH treated in the ICU. In fact, the major predictive ability of APACHE II and SAPS II comes from their age and GCS score components. SOFA performed significantly poorer than the other models and is not applicable as a prognostic model for ICH patients. All models displayed poor calibration, highlighting the need for improved prognostic models for ICH patients. Conclusion: The common intensive care scoring systems did not outperform a simpler model based on only age and GCS score. Thus, the use of previous intensive care scoring systems is not warranted in ICH patients.", "journal": "SCANDINAVIAN JOURNAL OF TRAUMA RESUSCITATION & EMERGENCY MEDICINE", "category": "Emergency Medicine", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000414484300003", "keywords": "Scoliosis; Kyphosis; Adult degenerative spine; Osteotomy; Interbody fusion", "title": "Surgical management of coronal and sagittal imbalance of the spine without PSO: a multicentric cohort study on compensated adult degenerative deformities", "abstract": "Purpose Sagittal imbalance of severe adult degenerative deformities requires surgical correction to improve pain, mobility and quality of life. Our aim was a harmonic and balanced spine, treating a series of adult degenerative thoracolumbar and lumbar kyphoscoliosis by a non posterior subtraction osteotomy technique. Methods We operated 22 painful thoracolumbar and lumbar compensated degenerative deformities by anterior (ALIF), extreme lateral (XLIF) and transforaminal (TLIF) interbody fusion and grade 2 osteotomy (SPO) to restore lumbar lordosis and mobilize the co ronal curve. Two-stage surgery, first anterior and after 2 or 3 weeks posterior, was proposed when the Oswestry Disability Index (ODI) was equal to or greater than 50% and VAS more than 5. All patients were submitted to X-ray and clinical screening during pre, post-operative and follow-up periods. Results We performed 5 ALIFs, 39 XLIFs, 8 TLIFs, 32 SPOs. No major complications were recorded and complication rate was 18% after lateral fusion and 22.7% after posterior approach. Pelvic tilt, lumbar lordosis, sagittal vertical axis and thoracic kyphosis improved (p < 0.05). Clinical follow-up (mean 20.5; range 18-24) was satisfactory in all cases, except for two due to sacroiliac pain. Mean preoperative VAS was 7.7 (range 6-10), while ODI was 67% on average (range 50-78). After two-stage surgery, VAS and ODI decreased, respectively, to 2.4 (range 2-4) and 31% (range 25-45), while their values were 4 (range 2-6) and 35% (range 20-55) at the final follow-up. Conclusion Current follow-up does not allow definitive conclusions. However, the surgical approach adopted in this study seems promising, improving balance and clinical condition of adult patients with a compensated sagittal degenerative imbalance of the thoracolumbar spine.", "journal": "EUROPEAN SPINE JOURNAL", "category": "Clinical Neurology; Orthopedics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000406976500038", "keywords": "Flexibly reconfigurable roll forming; Response surface methodology; Regression analysis; Experiments and numerical simulations", "title": "Rapid prediction of longitudinal curvature obtained by flexibly reconfigurable roll forming using response surface methodology", "abstract": "Flexibly reconfigurable roll forming (FRRF) is a sheet-forming technology that can be used to produce multi-curvature surfaces by controlling the longitudinal strain distribution. In FRRF, the shape of the formed surface is determined by the curvature of the reconfigurable rollers and the gaps between the rollers. Because FRRF technology is still under development, a simulation model of the physical forming process is conveniently used to investigate the effects of the input parameters. To facilitate the investigation in the present study, the response surface methodology is used to develop a model for predicting the curvature produced in a longitudinal blank. The input parameters are the sheet compression ratio, the curvature radius in the transverse direction, and the initial blank width. Samples are generated using a three-level three-factor full-factorial design, and each convex and saddle curvature is represented by a quadratic regression model with two-factor interactions. The fitted polynomial equations are verified numerically by the R-squared values and root mean square errors and graphically by residual plots. To assess the reliability of the sample data, experiments are performed using pre-FRRF equipment. The proposed analytical procedure is confirmed to be reasonable, and a statistical formula for estimating the longitudinal curvature produced by the FRRF process is established.", "journal": "INTERNATIONAL JOURNAL OF ADVANCED MANUFACTURING TECHNOLOGY", "category": "Automation & Control Systems; Engineering, Manufacturing", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000404532300019", "keywords": "inflammatory bowel disease; Crohn's disease; pediatric; disparities; race", "title": "Race Differences in Initial Presentation, Early Treatment, and 1-year Outcomes of Pediatric Crohn's Disease: Results from the ImproveCareNow Network", "abstract": "Background: Racially disparate care has been shown to contribute to suboptimal health care outcomes for minorities. Using the ImproveCareNow network, we investigated differences in management and outcomes of pediatric patients with Crohn's disease at diagnosis and 1-year postdiagnosis. Methods: ImproveCareNow is a learning health network for pediatric inflammatory bowel disease. It contains prospective, longitudinal data from outpatient encounters. This retrospective study included all patients with Crohn's disease <= 21 years, September 2006 to October 2014, with the first recorded encounter <= 90 days from date of diagnosis and an encounter 1 year +/- 60 days. We examined the effect of race on remission rate and treatment at diagnosis and 1 year from diagnosis using t-tests, Wilcoxon rank-sum tests, chi(2) statistic, and Fisher's exact tests, where appropriate, followed by univariate regression models. Results: Nine hundred seventy-six patients (Black = 118 (12%), White = 858 (88%), mean age = 13 years, 63% male) from 39 sites were included. Black children had a higher percentage of Medicaid insurance (44% versus 11%, P < 0.001). At diagnosis, Black children had more active disease according to physician global assessment (P = 0.027), but not by short Pediatric Crohn's Disease Activity Index (P = 0.67). Race differences in treatment were not identified. Black children had lower hematocrit (34.8 versus 36.7, P < 0.001) and albumin levels (3.6 versus 3.9, P = 0.001). At 1 year, Black children had more active disease according to physician global assessment (P = 0.016), but not by short Pediatric Crohn's Disease Activity Index (P = 0.06). Conclusions: Black children with Crohn's disease may have more severe disease than White children based on physician global assessment. Neither disease phenotype differences at diagnosis nor treatment differences at 1-year follow-up were identified.", "journal": "INFLAMMATORY BOWEL DISEASES", "category": "Gastroenterology & Hepatology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000412583800007", "keywords": "Leishmaniasis; Leishmania amazonensis; cell therapy; mesenchymal stromal cell; BALB/c; IL-10", "title": "The relationship between types of childhood victimisation and young adulthood criminality", "abstract": "BackgroundPrevious research suggests that some types of childhood abuse and neglect are related to an increased likelihood of perpetrating criminal behaviour in adulthood. Little research, however, has examined associations between multiple different types of childhood victimisation and adult criminal behaviour. AimsWe sought to examine the contribution of multiple and diverse childhood victimisations on adult criminal behaviour. Our central hypothesis was that, after controlling for gender, substance use and psychopathy, each type of childhood victimisation - specifically experience of property offences, physical violence, verbal abuse, sexual abuse, neglect and witnessed violence - would be positively and independently related to criminal behaviour in young adults. MethodsWe examined data from a large, nationally representative sample of 2244 young Swedish adults who reported at least one form of victimisation, using hierarchical regression analysis to also account for gender, substance use and psychopathy. ResultsExperiences of physical assaults, neglect and witnessing violence as a child were significantly associated with adult criminal behaviour, but not experiences of property, verbal or sexual victimizations. ConclusionsOur findings help to identify those forms of harm to children that are most likely to be associated with later criminality. Even after accounting for gender, substance misuse and psychopathology, childhood experience of violence - directly or as a witness - carries risk for adulthood criminal behaviour, so such children need targeted support and treatment. Copyright (c) 2016 John Wiley & Sons, Ltd.", "journal": "CRIMINAL BEHAVIOUR AND MENTAL HEALTH", "category": "Criminology & Penology; Psychiatry", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000412367700013", "keywords": "line: profiles; planetary systems; planets and satellites: individual: (HAT-P-41 b, Kepler-448 b, WASP-79 b); techniques: spectroscopic", "title": "Second-Generation Antipsychotic Prescribing Patterns for Pediatric Patients Enrolled in West Virginia Medicaid", "abstract": "Objective: The prescribing of second-generation antipsychotics for young people has increased dramatically. Studies have shown that children enrolled in Medicaid are more likely than those with private insurance to receive antipsychotics, leading many states to require prior authorization (PA) for their use. However, little is known about how PA programs affect prescribing patterns for antipsychotics or other psychotropic medications. This study examined a PA program for second-generation antipsychotic use for children under 18 in West Virginia Medicaid. Prescribing rates for antipsychotics and other psychotropic classes were assessed. Methods: Administrative claims from West Virginia Medicaid and the Children's Health Insurance Program for September 2014 to July 2016 were examined (N=273,369 prescriptions) with an interrupted time-series design. Segmented linear regression was used to model both immediate effects and trends in prescribing rates before and after implementation of the PA program in August 2015. Results: After PA program implementation, the prescribing rate for second-generation antipsychotics immediately dropped by 17% from prior levels, adjusted for preexisting trends, and further declined in the following months. Prescribing rates for all second-generation antipsychotics except for aripiprazole decreased significantly. Benzodiazepine prescribing increased in the month after PA program implementation but immediately returned to prepolicy rates, and sustained compensatory prescribing was not observed for any psychotropic drug class. Conclusions: Implementation of a second-generation antipsychotic PA program for children under age 18 resulted in a significant decrease in the prescribing rate for this class of medication, without sustained compensatory prescribing of other psychotropic classes.", "journal": "PSYCHIATRIC SERVICES", "category": "Health Policy & Services; Public, Environmental & Occupational Health; Psychiatry", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000401240200013", "keywords": "Arctic ocean oscillation; Beaufort Sea; Ecosystem; Polar bear; Reproduction; Ringed seal; Sea ice; Population dynamics; Tooth annuli", "title": "Handgun Acquisitions in California After Two Mass Shootings", "abstract": "Background: Mass shootings are common in the United States. They are the most visible form of firearm violence. Their effect on personal decisions to purchase firearms is not well-understood. Objective: To determine changes in handgun acquisition patterns after the mass shootings in Newtown, Connecticut, in 2012 and San Bernardino, California, in 2015. Design: Time-series analysis using seasonal autoregressive integrated moving-average (SARIMA) models. Setting: California. Population: Adults who acquired handguns between 2007 and 2016. Measurements: Excess handgun acquisitions (defined as the difference between actual and expected acquisitions) in the 6-week and 12-week periods after each shooting, overall and within subgroups of acquirers. Results: In the 6 weeks after the Newtown and San Bernardino shootings, there were 25 705 (95% prediction interval, 17 411 to 32 788) and 27 413 (prediction interval, 15 188 to 37 734) excess acquisitions, respectively, representing increases of 53% (95% CI, 30% to 80%) and 41% (CI, 19% to 68%) over expected volume. Large increases in acquisitions occurred among white and Hispanic persons, but not among black persons, and among persons with no record of having previously acquired a handgun. After the San Bernardino shootings, acquisition rates increased by 85% among residents of that city and adjacent neighborhoods, compared with 35% elsewhere in California. Limitations: The data relate to handguns in 1 state. The statistical analysis cannot establish causality. Conclusion: Large increases in handgun acquisitions occurred after these 2 mass shootings. The spikes were short-lived and accounted for less than 10% of annual handgun acquisitions statewide. Further research should examine whether repeated shocks of this kind lead to substantial increases in the prevalence of firearm ownership.", "journal": "ANNALS OF INTERNAL MEDICINE", "category": "Medicine, General & Internal", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000400202500006", "keywords": "Frailty; Cardiovascular disease; Meta-analysis", "title": "Risk of cardiovascular disease morbidity and mortality in frail and pre-frail older adults: Results from a meta-analysis and exploratory meta-regression analysis", "abstract": "Frailty is common and associated with poorer outcomes in the elderly, but its role as potential cardiovascular disease (CVD) risk factor requires clarification. We thus aimed to meta-analytically evaluate the evidence of frailty and pre-frailty as risk factors for CVD. Two reviewers selected all studies comparing data about CVD prevalence or incidence rates between frail/pre-frail vs. robust. The association between frailty status and CVD in cross-sectional studies was explored by calculating and pooling crude and adjusted odds ratios (ORs)+/- 95% confidence intervals (CIs); the data from longitudinal studies were pooled using the adjusted hazard ratios (HRs). Eighteen cohorts with a total of 31,343 participants were meta-analyzed. Using estimates from 10 cross-sectional cohorts, both frailty and pre-frailty were associated with higher odds of CVD than robust participants. Longitudinal data were obtained from 6 prospective cohort studies. After a median follow-up of 4.4 years, we identified an increased risk for faster onset of any type CVD in the frail (HR= 1.70 [95%CI, 1.18-2.45]; I-2 = 66%) and pre-frail (HR= 1.23 [95%CI, 1.07-1.36]; I-2 = 67%) vs. robust groups. Similar results were apparent for time to CVD mortality in the frail and pre-frail groups. In conclusion, frailty and pre-frailty constitute addressable and independent risk factors for CVD in older adults. (C) 2017 Elsevier B.V. All rights reserved.", "journal": "AGEING RESEARCH REVIEWS", "category": "Cell Biology; Geriatrics & Gerontology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000416779300024", "keywords": "hybrid spatial model; topology; grid; indoor GIS; indoor navigation; path planning; A* algorithm", "title": "Hybrid Spatial Data Model for Indoor Space: Combined Topology and Grid", "abstract": "The construction and application of an indoor spatial data model is an important prerequisite to meet the requirements of diversified indoor spatial location services. The traditional indoor spatial topology model focuses on the construction of topology information. It has high path analysis and query efficiency, but ignores the spatial location information. The grid model retains the plane position information by grid, but increases the data volume and complexity of the model and reduces the efficiency of the model analysis. This paper presents a hybrid model for interior space based on topology and grid. Based on the spatial meshing and spatial division of the interior space, the model retains the position information and topological connectivity information of the interior space by establishing the connection or affiliation between the grid subspace and the topological subspace. The model improves the speed of interior spatial analysis and solves the problem of the topology information and location information updates not being synchronized. In this study, the A* shortest path query efficiency of typical daily indoor activities under the grid model and the hybrid model were compared for the indoor plane of an apartment and a shopping mall. The results obtained show that the hybrid model is 43% higher than the A* algorithm of the grid model as a result of the existence of topology communication information. This paper provides a useful idea for the establishment of a highly efficient and highly available interior spatial data model.", "journal": "ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION", "category": "Computer Science, Information Systems; Geography, Physical; Remote Sensing", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000410010700029", "keywords": "Extreme; Significant wave height; Non-stationary; Splines; Bayesian inference; Return value; Numerical integration", "title": "Efficient estimation of return value distributions from non-stationary marginal extreme value models using Bayesian inference", "abstract": "Extreme values of an environmental response can be estimated by fitting the generalised Pareto distribution to a sample of exceedances of a high threshold. In oceanographic applications to responses such as ocean storm severity, threshold and model parameters are typically functions of physical covariates. A fundamental difficulty is selection or estimation of an appropriate threshold or interval of thresholds, of particular concern since inferences for return values vary with threshold choice. Historical studies suggest that evidence for threshold selection is weak in typical samples. Hence, following Randell et al. (2016), a piecewise gamma-generalised Pareto model for a sample of storm peak significant wave height, non-stationary with respect to storm directional and seasonal covariates, is estimated here using Bayesian inference. Quantile regression (for a fixed quantile threshold probability) is used to partition the sample prior to independent gamma (body) and generalised Pareto (tail) estimation. An ensemble of independent models, each member of which corresponds to a choice of quantile probability from a wide interval of quantile threshold probabilities, is estimated. Diagnostic tools are then used to select an interval of quantile threshold probabilities corresponding to reasonable model performance, for subsequent inference of extreme quantiles incorporating threshold uncertainty. The estimated posterior predictive return value distribution (for a long return period of the order of 10,000 years) is a particularly useful diagnostic tool for threshold selection, since this return value is a key deliverable in metocean design. Estimating the distribution using Monte Carlo simulation becomes computationally demanding as return period increases. We present an alternative numerical integration scheme, the computation time for which is effectively independent of return period, dramatically improving computational efficiency for longer return periods. The methodology is illustrated in application to storm peak and sea state significant wave height at a South China Sea location, subject to monsoon conditions, showing directional and seasonal variability.", "journal": "OCEAN ENGINEERING", "category": "Engineering, Marine; Engineering, Civil; Engineering, Ocean; Oceanography", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000409468300002", "keywords": "DUALEM-421; salinity; viticulture; soil apparent electrical conductivity; inversion modelling", "title": "Utilizing a DUALEM-421 and inversion modelling to map baseline soil salinity along toposequences in the Hunter Valley Wine district", "abstract": "In the oldest commercial wine district of Australia, the Hunter Valley, there is the threat of soil salinization because marine sediments underlie the area. To understand the risk requires information about the spatial distribution of soil properties. Electromagnetic (EM) induction instruments have been used to identify and map the spatial variation of average soil salinity to a certain depth. However, soils vary with depth dependent on soil forming factors. We collected data from a single-frequency and multiple-coil DUALEM-421 along a toposequence. We inverted this data using EM4Soil software and evaluated the resultant 2-dimensional model of true electrical conductivity (sigma - mS/m) with depth against electrical conductivity of saturated soil pastes (ECp - dS/m). Using a fitted linear regression (LR) model calibration approach and by varying the forward model (cumulative function-CF and full solution-FS), inversion algorithm (S1 and S2), damping factor () and number of arrays, we determined a suitable electromagnetic conductivity image (EMCI), which was optimal (R-2=0.82) when using the full solution, S2, =3.6 and all six coil arrays. We conducted an uncertainty analysis of the LR model used to estimate the electrical conductivity of the saturated soil-paste extract (ECe - dS/m). Our interpretation based on estimates of ECe suggests the approach can identify differences in salinity, how these vary with parent material and how topography influences salt distribution. The results provide information leading to insights into how soil forming factors and agricultural practices influence salinity down a toposequence and how this can guide soil management practices.", "journal": "SOIL USE AND MANAGEMENT", "category": "Soil Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000403861800003", "keywords": "Solar tracking; Photovoltaic; Equations", "title": "Mathematical study of the movement of solar tracking systems based on rational models", "abstract": "This paper presents the analytical deduction of generic and unified equations of the movement of solar tracking systems. These equations reproduce published equations, which only consider the sun position, or the equations of the astronomical movement. As a novelty, these equations are more generic, thus allowing the optimization of the positioning of PV installations where diffuse and reflected irradiance are usable. The analysis of the results obtained criticizes the axiomatic idea - widely considered by numerous authors - establishing that the ideal tracking system in PV installations is that tracker providing the best possible alignment with direct sunbeams. (C) 2017 Elsevier Ltd. All rights reserved.", "journal": "SOLAR ENERGY", "category": "Energy & Fuels", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000403455600003", "keywords": "Data replication; binary trees; data grid; file popularity", "title": "Binary-Tree Based Estimation of File Requests for Efficient Data Replication", "abstract": "Recently, data replication has received considerable attention in the field of grid computing. The main goal of data replication algorithms is to optimize data access performance by replicating the most popular files. When a file does not exist in the node where it was requested, it necessarily has to be transferred from another node, causing delays in the completion the file requests. The general idea behind data replication is to keep track of the most popular files requested in the grid and create copies of them in selected nodes. In this way, more file requests can be completed over a period of time and average job execution time is reduced. In this paper, we introduce an algorithm that estimates the potential of the files located in each node of the grid, using a binary tree structure. Also, the file scope and the file type are taken into account. By potential of a file, we mean its increasing or decreasing demand over a period of time. The file scope generally refers to the extent of the group of users which are interested or potentially interested in a file. The file types are divided into read and write intensive. Our scheme mainly promotes the high-potential files for replication, based on the temporal locality principle. The simulation results indicate that the proposed scheme can offer better data access performance in terms of the hit ratio and the average job execution time, compared to other state-of-the-art strategies.", "journal": "IEEE TRANSACTIONS ON PARALLEL AND DISTRIBUTED SYSTEMS", "category": "Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000410061900002", "keywords": "Magnetorheological damper; one-way pumping flow; dynamic modeling; optimization; experiment", "title": "Negative influence of detector noise on ghost imaging based on the photon counting technique at low light levels", "abstract": "The influence of detector noise on ghost imaging (GI) is investigated at low light levels. Based on the characteristics of the additive detector noise, we establish the analytical model and display the ghost images through numerical and experimental demonstrations. It is shown that the contrast-to-noise ratio and visibility of reconstructed images are sharply affected by the detector noise. Following the increase of the ratio of average signal intensity to the average noise, the quality of reconstructions is enhanced. To reduce the measurement numbers and, thus, shorten the consuming time without sacrificing the imaging quality, we propose a sorting technique in the traditional GI algorithm for a high quality image reconstruction. The results demonstrated here will be favorable to the applications of low-light-level imaging. (C) 2017 Optical Society of America", "journal": "APPLIED OPTICS", "category": "Optics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000431849300004", "keywords": "diet; nutritional status; mobile apps; behavior and behavior mechanisms", "title": "Controlling Your \"App\" etite: How Diet and Nutrition-Related Mobile Apps Lead to Behavior Change", "abstract": "Background: In recent years, obesity has become a serious public health crisis in the United States. Although the problem of obesity is being addressed through a variety of strategies, the use of mobile apps is a relatively new development that could prove useful in helping people to develop healthy dietary habits. Though such apps might lead to health behavior change, especially when relevant behavior change theory constructs are integrated into them, the mechanisms by which these apps facilitate behavior change are largely unknown. Objective: The purpose of this study was to identify which behavior change mechanisms are associated with the use of dietand nutrition-related health apps and whether the use of diet-and nutrition-related apps is associated with health behavior change. Methods: A cross-sectional survey was administered to a total of 217 participants. Participants responded to questions on demographics, use of diet and nutrition apps in the past 6 months, engagement and likability of apps, and changes in the participant's dietary behaviors. Regression analysis was used to identify factors associated with reported changes in theory and separately for reported changes in actual behavior, after controlling for potential confounding variables. Results: The majority of study participants agreed or strongly agreed with statements regarding app use increasing their motivation to eat a healthy diet, improving their self-efficacy, and increasing their desire to set and achieve health diet goals. Additionally, majority of participants strongly agreed that using diet/nutrition apps led to changes in their behavior, namely increases in actual goal setting to eat a healthy diet (58.5%, 127/217), increases in their frequency of eating healthy foods (57.6%, 125/217), and increases in their consistency of eating healthy foods (54.4%, 118/217). Participants also responded favorably to questions related to engagement and likability of diet/nutrition apps. A number of predictors were also positively associated with diet-related behavior change. Theory (P<.001), app engagement (P<.001), app use (P<.003), and education (P<.010) were all positively associated with behavior change. Conclusions: Study findings indicate that the use of diet/nutrition apps is associated with diet-related behavior change. Hence, diet-and nutrition-related apps that focus on improving motivation, desire, self-efficacy, attitudes, knowledge, and goal setting may be particularly useful. As the number of diet-and nutrition-related apps continues to grow, developers should consider integrating appropriate theoretical constructs for health behavior change into the newly developed mobile apps.", "journal": "JMIR MHEALTH AND UHEALTH", "category": "Health Care Sciences & Services; Medical Informatics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000404197000010", "keywords": "Bipartite graph; Matching; Identifiable graph; Computational complexity; Polynomial time algorithm; W[1]-hardness", "title": "On the complexity of the identifiable subgraph problem, revisited", "abstract": "A bipartite graph G = (L, R; E) with at least one edge is said to be identifiable if for every vertex v is an element of L, the subgraph induced by its non-neighbors has a matching of cardinality |L| - 1. An l-subgraph of G is an induced subgraph of G obtained by deleting from it some vertices in L together with all their neighbors. The IDENTIFIABLE SUBGRAPH problem is the problem of determining whether a given bipartite graph contains an identifiable l-subgraph. The MIN- and MAX-IDENTIFIABLE SUBGRAPH problems are defined similarly, taking into account a given upper, resp. lower bound on the number of vertices from L in an identifiable l-subgraph. We show that the IDENTIFIABLE SUBGRAPH and MAX-IDENTIFIABLE SUBGRAPH problems are polynomially solvable, thereby answering the question about the complexity of these two problems posed by Fritzilas et al. in 2013. We also complement a known APX-hardness result for the MIN-IDENTIFIABLE SUBGRAPH problem by showing that two parameterized variants of the problem are W[1]-hard. (C) 2017 Elsevier B.V. All rights reserved.", "journal": "DISCRETE APPLIED MATHEMATICS", "category": "Mathematics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000412380500002", "keywords": "Circular data; MCMC; Bayes factor; Savage-Dickey density ratio", "title": "Bayesian estimation and hypothesis tests for a circular Generalized Linear Model", "abstract": "Motivated by a study from cognitive psychology, we develop a Generalized Linear Model for circular data within the Bayesian framework, using the von Mises distribution. Although circular data arise in a wide variety of scientific fields, the number of methods for their analysis is limited. Our model allows inclusion of both continuous and categorical covariates. In a frequentist setting, this type of model is plagued by the likelihood surface of its regression coefficients, which is not logarithmically concave. In a Bayesian context, a weakly informative prior solves this issue, while for other parameters noninformative priors are available. In addition to an MCMC sampling algorithm, we develop Bayesian hypothesis tests based on the Bayes factor for both equality and inequality constrained hypotheses. In a simulation study, it can be seen that our method performs well. The analyses are available in the package CircGLMBayes. Finally, we apply this model to a dataset from experimental psychology, and show that it provides valuable insight for applied researchers. Extensions to dependent observations are within reach by means of the multivariate von Mises distribution. (C) 2017 Elsevier Inc. All rights reserved.", "journal": "JOURNAL OF MATHEMATICAL PSYCHOLOGY", "category": "Mathematics, Interdisciplinary Applications; Social Sciences, Mathematical Methods; Psychology, Mathematical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000403133000007", "keywords": "Flexible production resources; Capacity utilization; Robust optimization", "title": "Flexible production resources and capacity utilization rates: A robust optimization perspective", "abstract": "This paper presents a novel application of robust optimization (RO) in the context of optimal choices of product dedicated and flexible capacities in a spatial setting. Specifically, we examine the impact of acquiring flexible production technologies on a firm's overall capacity utilization rates under demand uncertainty. Uncorrelated, negatively, and positively correlated demands are considered. We implement an efficient RO algorithm, which can solve large samples of robust min-max problems in a realistic amount of time. This paper examines the impact of three critical factors that lead to different capacity utilization and resource flexibility outcomes: the degree of solution robustness selected by the decision-maker to accommodate uncertain demand realizations, the flexible capacity costs relative to dedicated capacity costs, and the level of correlation between product demands. One of our results show that according to robust solutions, the total capacity may not be fully utilized because of the distinction between largest (in unit terms) and costliest demand realizations. The second result shows that increasing the proportion of flexible capacity can increase capacity utilization; however, our results report less than full utilization even when using flexible capacity only. We show that the optimal amount of flexible capacity, and its impact on overall capacity utilization rate substantially depends on demand correlation.", "journal": "INTERNATIONAL JOURNAL OF PRODUCTION ECONOMICS", "category": "Engineering, Industrial; Engineering, Manufacturing; Operations Research & Management Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000412361900055", "keywords": "High-frequency ac (HFAC) microgrids; multilevel inverter (MLI); quasi-resonant switched-capacitor (QRSC); self-voltage balancing", "title": "A Quasi-Resonant Switched-Capacitor Multilevel Inverter With Self-Voltage Balancing for Single-Phase High-Frequency AC Microgrids", "abstract": "In this paper, a quasi-resonant switched-capacitor (QRSC) multilevel inverter (MLI) is proposed with self-voltage balancing for single-phase high-frequency ac (HFAC) microgrids. It is composed of a QRSC circuit (QRSCC) in the frontend and an H-bridge circuit in the backend. The input voltage is divided averagely by the series-connected capacitors in QRSCC, and any voltage level can be obtained by increasing the capacitor number. The different operational mechanism and the resulting different application make up for the deficiency of the existing switched-capacitor topologies. The capacitors are connected in parallel partially or wholly when discharging to the load, thus the self-voltage balancing is realized without any high-frequency balancing algorithm. In other words, the proposed QRSC MLI is especially adapted for HFAC fields, where fundamental frequency modulation is preferred when considering the switching frequency and the resulting loss. The quasi-resonance technique is utilized to suppress the current spikes that emerge from the instantaneous parallel connection of the series-connected capacitors and the input source, decreasing the capacitance, increasing their lifetimes, and reducing the electromagnetic interference, simultaneously. The circuit analysis, power loss analysis, and comparisons with typical switched-capacitor topologies are presented. To evaluate the superior performances, a nine-level prototype is designed and implemented in both simulation and experiment, whose results confirm the feasibility of the proposed QRSC MLI.", "journal": "IEEE TRANSACTIONS ON INDUSTRIAL INFORMATICS", "category": "Automation & Control Systems; Computer Science, Interdisciplinary Applications; Engineering, Industrial", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000413990600020", "keywords": "Proton exchange membrane (PEM) fuel cell; Active power filter; Harmonics isolator; Self-tuning filter; Fuzzy logic control; PWM control", "title": "How Does Augmented Reality Work?", "abstract": "In this paper, a proton exchange membrane PEM fuel cell power plant is used to improve the filtering performance of the conventional active power filter, as well as simultaneously contribute with the electric grid to supply the power to the load. The five-level inverter is used as a shunt active power filter, taking advantages of the multilevel inverter such as low harmonic distortion and reduced switching losses. It is capable of compensating power factor, current harmonics and can also make the interface between renewable energy sources and the electric grid, injecting the energy generated by PEM fuel cell to the load. The active power filter control strategy is based on the use of self tuning filters for reference current generation and a fuzzy logic current controller. The MATLAB Fuzzy Logic Toolbox is used for implementing the fuzzy logic control algorithm. The obtained results show that the PEM fuel cell contributes successfully to supply simultaneously the load with the electric grid and produced a sinusoidal supply current with low harmonic distortion and in phase with the line voltage.", "journal": "HARVARD BUSINESS REVIEW", "category": "Business; Management", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000406756100017", "keywords": "Monte Carlo methods; high-quality data refinement; spectrum modelling; polychromatic methods; quantum energy determination; detector development", "title": "The index of dispersion as a metric of quanta - unravelling the Fano factor", "abstract": "In statistics, the index of dispersion (or variance-to-mean ratio) is unity (sigma(2)/< x > = 1) for a Poisson-distributed process with variance sigma(2) for a variable x that manifests as unit increments. Where x is a measure of some phenomenon, the index takes on a value proportional to the quanta that constitute the phenomenon. That outcome might thus be anticipated to apply for an enormously wide variety of applied measurements of quantum phenomena. However, in a photon-energy proportional radiation detector, a set of M witnessed Poisson-distributed measurements {W-1, W-2,... W-M} scaled so that the ideal expectation value of the quantum is unity, is generally observed to give sigma(2)/< W > < 1 because of detector losses as broadly indicated by Fano [Phys. Rev. (1947), 72, 26]. In other cases where there is spectral dispersion, sigma(2)/< W > > 1. Here these situations are examined analytically, in Monte Carlo simulations, and experimentally. The efforts reveal a powerful metric of quanta broadly associated with such measurements, where the extension has been made to polychromatic and lossy situations. In doing so, the index of dispersion's variously established yet curiously overlooked role as a metric of underlying quanta is indicated. The work's X-ray aspects have very diverse utility and have begun to find applications in radiography and tomography, where the ability to extract spectral information from conventional intensity detectors enables a superior level of material and source characterization.", "journal": "ACTA CRYSTALLOGRAPHICA SECTION B-STRUCTURAL SCIENCE CRYSTAL ENGINEERING AND MATERIALS", "category": "Chemistry, Multidisciplinary; Crystallography", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000465628700058", "keywords": "Rotor router model; Propp machine; Load balancing; Markov chain Monte Carlo (MCMC); Mixing time", "title": "Systematic literature review of PD-L1 assays, their scoring algorithms and validation metrics", "abstract": "Motivated by a derandomization of Markov chain Monte Carlo (MCMC), this paper investigates a deterministic random walk, which is a deterministic process analogous to a random walk. There is some recent progress in the analysis of the vertex-wise discrepancy (i.e., Loo-discrepancy), while little is known about the total variation discrepancy (i.e., L-1-discrepancy), which plays an important role in the analysis of an FPRAS based on MCMC. This paper investigates the L-1-discrepancy between the expected number of tokens in a Markov chain and the number of tokens in its corresponding deterministic random walk. First, we give a simple but nontrivial upper bound O(mt*) of the L-1-discrepancy for any ergodic Markov chains, where m is the number of edges of the transition diagram and t* is the mixing time of the Markov chain. Then, we give a better upper bound O(m t*) for non-oblivious deterministic random walks, if the corresponding Markov chain is ergodic and lazy. We also present some lower bounds. (C) 2016 Elsevier B.V. All rights reserved.", "journal": "JOURNAL FOR IMMUNOTHERAPY OF CANCER", "category": "Oncology; Immunology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000410885700002", "keywords": "parametric filtering and regularization techniques; controlled filtering; multicriteriality; Pareto solutions", "title": "Management of Nasal Fractures in Sports", "abstract": "Nasal fractures represent approximately 60% of all maxillofacial injuries that occur in athletic activities; however, there are no current guidelines regarding immediate sideline management of these injuries. Therefore, the purpose of this article was to (1) summarize the anatomy, etiology, and incidence of nasal fractures, and (2) evaluate the current body of literature regarding immediate on-field and subsequent outpatient management. It is imperative to establish that the athlete's airway is not compromised and there are no other severe concomitant injuries, such as a concussion, ocular injury, or leakage of cerebrospinal fluid. Immediate closed reduction should not be attempted unless there is airway compromise or the practitioner has experience in performing it. The majority of athletes with these injuries in isolation may return to play; however, in our practice, we recommend they wear a face mask for 6 weeks after their injury. Despite our recommendations, we know there is a paucity of clinical studies on immediate sideline and longer-term management. Future studies should focus on establishing therapeutic algorithms that will allow physicians to make treatment recommendations to patients with strong evidence to support their decision.", "journal": "SPORTS MEDICINE", "category": "Sport Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000407292200001", "keywords": "Rainbow trout; Fish; SNPs; Genetic markers; RNA-Seq", "title": "Quaternionic formulation of the two-component Kohn-Sham equations and efficient exploitation of point group symmetry", "abstract": "The quaternionic formulation of the time-reversal invariant quasirelativistic Kohn-Sham equations with exact Hartree-Fock exchange leads to hypercomplex one-component equations with half of the dimension compared to the original two-component problem. The combination of the quaternionic equations with point group symmetry exploitation for D-2h and its subgroups by construction of corepresentations leads to quaternionic, complex, or real algorithms depending on the structure of the point group. In this work, the quaternionic approach with point group symmetry exploitation of the relativistic four-component Dirac-Hartree-Fock theory by Saue and Jensen [J. Chem. Phys. 111, 6211 (1999)] will be adopted to the quasirelativistic two-component Kohn-Sham scheme for closed-shell systems. The implementation in the program system TURBOMOLE is applied to the large lead cluster Pb-56 as a test case. Published by AIP Publishing.", "journal": "JOURNAL OF CHEMICAL PHYSICS", "category": "Chemistry, Physical; Physics, Atomic, Molecular & Chemical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000405296500005", "keywords": "X-ray lasers; XFELs; biology; structure; dynamics", "title": "XFELs for structure and dynamics in biology", "abstract": "The development and application of the free-electron X-ray laser (XFEL) to structure and dynamics in biology since its inception in 2009 are reviewed. The research opportunities which result from the ability to outrun most radiation-damage effects are outlined, and some grand challenges are suggested. By avoiding the need to cool samples to minimize damage, the XFEL has permitted atomic resolution imaging of molecular processes on the 100 fs timescale under near-physiological conditions and in the correct thermal bath in which molecular machines operate. Radiation damage, comparisons of XFEL and synchrotron work, single-particle diffraction, fast solution scattering, pump-probe studies on photosensitive proteins, mix-and-inject experiments, caged molecules, pH jump and other reaction-initiation methods, and the study of molecular machines are all discussed. Sample-delivery methods and data-analysis algorithms for the various modes, from serial femtosecond crystallography to fast solution scattering, fluctuation X-ray scattering, mixing jet experiments and single-particle diffraction, are also reviewed.", "journal": "IUCRJ", "category": "Chemistry, Multidisciplinary; Crystallography; Materials Science, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000402194900002", "keywords": "spreadsheets; seismic; Zoeppritz equations; reflection coefficients", "title": "Employing AVX Vectorization to Improve the Performance of Random Number Generators", "abstract": "By the example of the RNGAVXLIB random number generator library, this paper considers some approaches to employing AVX vectorization for calculation speedup. The RNGAVXLIB library contains AVX implementations of modern generators and the routines allowing one to initialize up to 10(19) independent random number streams. The AVX implementations yield exactly the same pseudorandom sequences as the original algorithms do, while being up to 40 times faster than the ANSI C implementations.", "journal": "PROGRAMMING AND COMPUTER SOFTWARE", "category": "Computer Science, Software Engineering", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000405993200097", "keywords": "Emotion recognition; music personalization; push system", "title": "Research on Personalized Music Push System Based on Emotion Recognition", "abstract": "With the continuous improvement of people's living standards, people's spiritual needs are growing. Therefore, music has become an essential part of people's lives, which can greatly enrich people's lives, and improve people's quality of life. At present, there are many rich and colorful music types. Different people have different needs for music, and they like different kinds of music. Therefore, it is necessary to recommend to our customers different music, to meet the needs of different people. Based on the research of music needs of different groups of people, emotion recognition theory was studied in this paper, in order to establish a music personalized push system for emotion recognition.", "journal": "AGRO FOOD INDUSTRY HI-TECH", "category": "Biotechnology & Applied Microbiology; Food Science & Technology", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000529197700001", "keywords": "birds; habitat suitability model; multi-scale model; presence-only model; spatial scale; the western Swiss Alps", "title": "Toward community predictions: Multi-scale modelling of mountain breeding birds' habitat suitability, landscape preferences, and environmental drivers", "abstract": "Across a large mountain area of the western Swiss Alps, we used occurrence data (presence-only points) of bird species to find suitable modelling solutions and build reliable distribution maps to deal with biodiversity and conservation necessities of bird species at finer scales. We have performed a multi-scale method of modelling, which uses distance, climatic, and focal variables at different scales (neighboring window sizes), to estimate the efficient scale of each environmental predictor and enhance our knowledge on how birds interact with their complex environment. To identify the best radius for each focal variable and the most efficient impact scale of each predictor, we have fitted univariate models per species. In the last step, the final set of variables were subsequently employed to build ensemble of small models (ESMs) at a fine spatial resolution of 100 m and generate species distribution maps as tools of conservation. We could build useful habitat suitability models for the three groups of species in the national red list. Our results indicate that, in general, the most important variables were in the group of bioclimatic variables including \"Bio11\" (Mean Temperature of Coldest Quarter), and \"Bio 4\" (Temperature Seasonality), then in the focal variables including \"Forest\", \"Orchard\", and \"Agriculture area\" as potential foraging, feeding and nesting sites. Our distribution maps are useful for identifying the most threatened species and their habitat and also for improving conservation effort to locate bird hotspots. It is a powerful strategy to improve the ecological understanding of the distribution of bird species in a dynamic heterogeneous environment.", "journal": "ECOLOGY AND EVOLUTION", "category": "Ecology; Evolutionary Biology", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000575728300012", "keywords": "Cooperative localization and tracking; Information entropy; Byzantine sensor identification; Importance sampling; Lyapunov's stability theorem; Maritime search and rescue wireless sensor networks", "title": "NMTLAT: A New robust mobile Multi-Target Localization and Tracking Scheme in marine search and rescue wireless sensor networks under Byzantine attack", "abstract": "After a shipwreck (ship collision, explosion, disappearance, etc.), people rely on the marine search and rescue wireless sensor networks (MSR-WSNs) as one key weapon to locate and track drowning targets to save lives and assets. However, due to the complex and dynamic ocean environment, marine sensors could be captured by malicious attackers and then become Byzantine sensors. Byzantine sensors may tamper with the actual sensor data at a fixed or time-varying probability and transmit it to the data fusion center (DFC), which renders the normal localization and tracking function of the search and rescue system downgrade or even fail. To address this issue, we develop a New robust marine mobile Multi-Target LocAlization and Tracking scheme called NMTLAT by eliminating abnormal measurement data from the initial measurement data. NMTLAT works in several steps. By analyzing and mining sensors data and behavior, we firstly employ the information entropy of the system composed of a single sensor and their neighbor sensors to develop an efficient dynamic threshold based Byzantine node identification method. After migrating Byzantine sensors, the DFC utilizes sensor-aware and preprocessed marine data of the beacon or honest sensors to complete target localization and trajectory tracking. Specifically, NMTLAT consists of a novel distributed and cooperative multi-target localization and tracking algorithm using both received signal strength indication (RSSI) and priori information of sensors location. NMTLAT employs the importance sampling method to approximate the posterior probability distribution of the sensors and targets location. Furthermore, when the marine target is outside MSR-WSNs coverage, a piecewise function is utilized to characterize the likelihood of finding a drowning target in the search and rescue sea area. Finally, the Lyapunov's second stability theorem is adopted to measure the stability of the NMTLAT. Our extensive simulation experiments validate that the NMTLAT performance is superior to existing solutions in various marine search and rescue scenarios.", "journal": "COMPUTER COMMUNICATIONS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000532921800001", "keywords": "Robotics; Mobile robots; Man machine interface (MMI); Cooperative robots; Flexible manufacturing; Automated guided vehicles (AGV); Industry 4; 0; Order-picking; Gripper; Arduino; C#", "title": "Development of a solution for adding a collaborative robot to an industrial AGV", "abstract": "Purpose The Industry 4.0 initiative - with its ultimate objective of revolutionizing the supply-chain - putted more emphasis on smart and autonomous systems, creating new opportunities to add flexibility and agility to automatic manufacturing systems. These systems are designed to free people from monotonous and repetitive tasks, enabling them to concentrate in knowledge-based jobs. One of these repetitive functions is the order-picking task which consists of collecting parts from storage (warehouse) and distributing them among the ordering stations. An order-picking system can also pick finished parts from working stations to take them to the warehouse. The purpose of this paper is to present a simplified model of a robotic order-picking system, i.e. a mobile manipulator composed by an automated guided vehicle (AGV), a collaborative robot (cobot) and a robotic hand. Design/methodology/approach Details about its implementation are also presented. The AGV is needed to safely navigate inside the factory infrastructure, namely, between the warehouse and the working stations located in the shop-floor or elsewhere. For that purpose, an ActiveONE AGV, from Active Space Automation, was selected. The collaborative robot manipulator is used to move parts from/into the mobile platform (feeding the working stations and removing parts for the warehouse). A cobot from Kassow Robots was selected (model KR 810), kindly supplied by partner companies Roboplan (Portugal) and Kassow Robotics (Denmark). An Arduino MKR1000 board was also used to interconnect the user interface, the AGV and the collaborative robot. The graphical user interface was developed in C# using the Microsoft Visual Studio 2019 IDE, taking advantage of this experience in this type of language and programming environment. Findings The resulting prototype was fully demonstrated in the partner company warehouse (Active Space Automation) and constitutes a possible order-picking solution, which is ready to be integrated into advanced solutions for the factories of the future. Originality/value A solution to fully automate the order-picking task at an industrial shop-floor was presented and fully demonstrated. The objective was to design a system that could be easy to use, to adapt to different applications and that could be a basic infrastructure for advanced order-picking systems. The system proved to work very well, executing all the features required for an order-picking system working in an Industry 4.0 scenario where humans and machines must act as co-workers. Although all the system design objectives were accomplished, there are still opportunities to improve and add features to the presented solution. In terms of improvements, a different robotic hand will be used in the final setup, depending on the type of objects that are being required to move. The amount of equipment that is located on-board of the AGV can be significantly reduced, freeing space and lowering the weight that the AGV carries. For example, the controlling computer can be substituted by a single-board-computer without any advantage. Also, the cobot should be equipped with a wrist camera to identify objects and landmark. This would allow the cobot to fully identify the position and orientation of the objects to pick and drop. The wrist camera should also use bin-picking software to fully identify the shape of the objects to pick and also their relative position (if they are randomly located in a box, for example). These features are easy to add to the developed mobile manipulator, as there are a few vision systems in the market (some that integrate with the selected cobot) that can be easily integrated in the solution. Finally, this paper reports a development effort that neglected, for practical reasons, all issues related with certification, safety, training, etc. A future follow-up paper, reporting a practical use-case implementation, will properly address those practical and operational issues.", "journal": "INDUSTRIAL ROBOT-THE INTERNATIONAL JOURNAL OF ROBOTICS RESEARCH AND APPLICATION", "category": "Engineering, Industrial; Robotics", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000518328802381", "keywords": "Neighborhood; Retail food environment; Third places; Cognitive decline; Mixed-methods", "title": "Tumor Infiltrating Lymphocytes in Head and Neck Squamous Cell Carcinoma: Characterization by Digital Image Analysis and Association with Clinicopathological Features and Survival", "abstract": "Service robots have been introduced to hotel industry in the past decade and received various feedback on their performance. To provide better service, one needs to understand how the hotel customers look at the service robots. Understanding their interests, motivation, and behaviors in human-robot interaction is the key to develop high-quality services and improve robot's performance. This is the first work to study human-robot interaction in hotels in China. Frequent pattern mining and social network analysis techniques are used in this work to find out useful suggestions to both hotel management and robot manufactory. Turning on and off lights, TV, curtain, and window screens are popular services that most of the hotel customers preferred during their stays. Service robots are also found to entertain customers to carry out repeated commands for fun or to kill time. Customers also showed various motivations to stay in hotel rooms by calling different commands.", "journal": "LABORATORY INVESTIGATION", "category": "Medicine, Research & Experimental; Pathology", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000550189000001", "keywords": "optimization; nature-inspired; geo-inpired; boost converter; MPPT; EA; PV systems; energy harvesting; DC; DC converters", "title": "Improved MPPT Algorithm for Photovoltaic Systems Based on the Earthquake Optimization Algorithm", "abstract": "Nowadays, owing to the growing interest in renewable energy, Photovoltaic systems (PV) are responsible of supplying more than 500,000 GW of the electrical energy consumed around the world. Therefore, different converters topologies, control algorithms, and techniques have been studied and developed in order to maximize the energy harvested by PV sources. Maximum Power Point Tracking (MPPT) methods are usually employed with DC/DC converters, which together are responsible for varying the impedance at the output of photovoltaic arrays, leading to a change in the current and voltage supplied in order to achieve a dynamic optimization of the transferred energy. MPPT algorithms such as, Perturb and Observe (P&O) guarantee correct tracking behavior with low calibration parameter dependence, but with a compromised relation between the settling time and steady-state oscillations, leading to a trade off between them. Nevertheless, proposed methods like Particle Swarm Optimization- (PSO) based techniques have improved the settling time with the addition of lower steady-state oscillations. Yet, such a proposal performance is highly susceptible and dependent to correct and precise parameter calibration, which may not always ensure the expected behavior. Therefore, this work presents a novel alternative for MPPT, based on the Earthquake Optimization Algorithm (EA) that enables a solution with an easy parameters calibration and an improved dynamic behavior. Hence, a boost converter case study is proposed to verify the suitability of the proposed technique through Simscape Power Systems (TM) simulations, regarding the dynamic model fidelity capabilities of the software. Results show that the proposed structure can easily be suited into different power applications. The proposed solution, reduced between 12% and 36% the energy wasted in the simulation compared to the P&O and PSO based proposals.", "journal": "ENERGIES", "category": "Energy & Fuels", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000511509500005", "keywords": "Magnetic tracking; Laser guidance; Robotic control; Augmented reality; Optical tracking; Microscope; Navigated surgery", "title": "CIGuide: in situ augmented reality laser guidance", "abstract": "Purpose A robotic intraoperative laser guidance system with hybrid optic-magnetic tracking for skull base surgery is presented. It provides in situ augmented reality guidance for microscopic interventions at the lateral skull base with minimal mental and workload overhead on surgeons working without a monitor and dedicated pointing tools. Methods Three components were developed: a registration tool (Rhinospider), a hybrid magneto-optic-tracked robotic feedback control scheme and a modified robotic end-effector. Rhinospider optimizes registration of patient and preoperative CT data by excluding user errors in fiducial localization with magnetic tracking. The hybrid controller uses an integrated microscope HD camera for robotic control with a guidance beam shining on a dual plate setup avoiding magnetic field distortions. A robotic needle insertion platform (iSYS Medizintechnik GmbH, Austria) was modified to position a laser beam with high precision in a surgical scene compatible to microscopic surgery. Results System accuracy was evaluated quantitatively at various target positions on a phantom. The accuracy found is 1.2 mm +/- 0.5 mm. Errors are primarily due to magnetic tracking. This application accuracy seems suitable for most surgical procedures in the lateral skull base. The system was evaluated quantitatively during a mastoidectomy of an anatomic head specimen and was judged useful by the surgeon. Conclusion A hybrid robotic laser guidance system with direct visual feedback is proposed for navigated drilling and intraoperative structure localization. The system provides visual cues directly on/in the patient anatomy, reducing the standard limitations of AR visualizations like depth perception. The custom- built end-effector for the iSYS robot is transparent to using surgical microscopes and compatible with magnetic tracking. The cadaver experiment showed that guidance was accurate and that the end-effector is unobtrusive. This laser guidance has potential to aid the surgeon in finding the optimal mastoidectomy trajectory in more difficult interventions.", "journal": "INTERNATIONAL JOURNAL OF COMPUTER ASSISTED RADIOLOGY AND SURGERY", "category": "Engineering, Biomedical; Radiology, Nuclear Medicine & Medical Imaging; Surgery", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000487078400047", "keywords": "hyperspectral denoising; subspace factorization; l(2,1)-norm; total variation (TV); moreau envelope; moreau-enhanced TV denoising", "title": "RETRACTION: Prediction of Resistance to Water Damage of Geopolymers with Seeded Fly Ash and Rice Husk Bark Ash by Fuzzy Logic (Retraction of Vol 21, Pg 822, 2012)", "abstract": "Hyperspectral images (HSIs) denoising aims at recovering noise-free images from noisy counterparts to improve image visualization. Recently, various prior knowledge has attracted much attention in HSI denoising, e.g., total variation (TV), low-rank, sparse representation, and so on. However, the computational cost of most existing algorithms increases exponentially with increasing spectral bands. In this paper, we fully take advantage of the global spectral correlation of HSI and design a unified framework named subspace-based Moreau-enhanced total variation and sparse factorization (SMTVSF) for multispectral image denoising. Specifically, SMTVSF decomposes an HSI image into the product of a projection matrix and abundance maps, followed by a 'Moreau-enhanced' TV (MTV) denoising step, i.e., a nonconvex regularizer involving the Moreau envelope mechnisam, to reconstruct all the abundance maps. Furthermore, the schemes of subspace representation penalizing the low-rank characteristic and l2,1-norm modelling the structured sparse noise are embedded into our denoising framework to refine the abundance maps and projection matrix. We use the augmented Lagrange multiplier (ALM) algorithm to solve the resulting optimization problem. Extensive results under various noise levels of simulated and real hypspectral images demonstrate our superiority against other competing HSI recovery approaches in terms of quality metrics and visual effects. In addition, our method has a huge advantage in computational efficiency over many competitors, benefiting from its removal of most spectral dimensions during iterations.", "journal": "JOURNAL OF COMPOSITE MATERIALS", "category": "Materials Science, Composites", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000487249900024", "keywords": "ST-VBL; Symplectic geometry similarity transformation; Mixture of Gaussians; Variational Bayesian Learning; De-noising", "title": "Symplectic transformation based Variational Bayesian Learning and its applications to gear fault diagnosis", "abstract": "The feature enhancement is one of the most crucial roles in the fault diagnosis, but the fault information is often contaminated by noise and interference harmonics. Symplectic geometry spectrum analysis (SGSA), as an effective de-noising method, has been attracting great attention in recent years. Unfortunately, SGSA cannot take full account of various noise problem. Therefore, this paper presents a Symplectic transformation based Variational Bayesian Learning (ST-VBL) de-noising method. In this method, the initial de-noising matrix is constructed by using symplectic geometry similarity transformation and the contribution rate method, and most of the noise is removed. Then, using the Variational Bayesian Learning (VBL) and Mixture of Gaussians (MOG), the probability distribution of initial de-noising component matrix is obtained to further de-noise the signal. Moreover, the properties of ST-VBL are demonstrated by simulations and experiment, showing its superiority over the traditional de-noising methods and SGSA de-noising method. (C) 2019 Elsevier Ltd. All rights reserved.", "journal": "MEASUREMENT", "category": "Engineering, Multidisciplinary; Instruments & Instrumentation", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000477784100065", "keywords": "Directional overcurrent relay; Robust protection coordination; Re-optimization; Matheuristics; Load shedding minimization", "title": "Minimizing undesirable load shedding through robust coordination of directional overcurrent relays", "abstract": "The directional overcurrent relay coordination is often performed taking into account the scenario in which the system is expected to operate most of the time. However, such an approach frequently lead to solutions that are not robust enough to deal with perturbed conditions (e.g., topology changes and line losses), which can cause a cascade sequence of unnecessary load disconnections. A new approach to improve the robustness of directional overcurrent relay coordination is presented in this work. At first, relay settings are found to the expected conditions, using a matheuristic algorithm based on Differential Evolution, Linear Programming, and local search techniques. Afterward, the solution found by the algorithm is used as an input on a re-optimization framework, in which protection must remain coordinated and fast, but undesirable load shedding is also minimized in single line dropping scenarios (N-1 criterion). The proposed approach is applied to four test cases: a real meshed network, IEEE-30 bus, IEEE-118 bus, and IEEE-300 bus. These systems have up to 590 relays and 1,044 primary/backup relations.", "journal": "INTERNATIONAL JOURNAL OF ELECTRICAL POWER & ENERGY SYSTEMS", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000525754600002", "keywords": "Time delay; Fractional calculus; Fuzzy logic; PI-PD control; Smith Predictor; Genetic algorithm; Gain-scheduling; Stability analysis", "title": "Design of an optimal fractional fuzzy gain-scheduled Smith Predictor for a time-delay process with experimental application", "abstract": "This study addresses an experimental investigation of a novel modified Smith Predictor (SP) based fractional fuzzy gain-scheduled control scheme in control of a time-delayed thermal process. The control strategy employees a fuzzy algorithm to adjust convenient controller parameters based on the system's operating conditions. Performance enhancement of the closed-loop system enables more robust behavior in the presence of disturbance while reducing energy consumption by producing a smooth control signal in comparison with the traditional integer order SP structures. The proposed controller comprises self-tuning capabilities at runtime which makes it adaptive in nature. The motivation of the present paper is in both points of theory and experimental application. The theoretical contribution is to propose a new Smith Predictor based fractional order fuzzy dead-time compensation scheme that can handle uncertainties, parameter variations, and internal external disturbances. The practical contribution is to apply the proposed control scheme to a real-time air-heating process. The performances of the elaborated control strategies are investigated in both computer simulation and experimental application under different operating conditions. The proposed fractional fuzzy control scheme is found superior to the classical PI-PD SP and integer fuzzy controllers for temperature profile tracking tasks. Moreover, complementary comments are highlighted on the advantages and drawbacks of each controller. (C) 2019 ISA. Published by Elsevier Ltd. All rights reserved.", "journal": "ISA TRANSACTIONS", "category": "Automation & Control Systems; Engineering, Multidisciplinary; Instruments & Instrumentation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000510525700007", "keywords": "ICA; MANET; Clustering; Energy consumption", "title": "The clustering algorithm for efficient energy management in mobile ad-hoc networks", "abstract": "MANET (Mobile Ad-hoc Network) consumes much energy due to their dynamic capabilities, complexities, constraints of design such as lack of a specified communication infrastructure, and their change over time. One strategy to reduce energy consumption is to optimize routing in these networks, which is, in turn, one of the most important challenges in these networks. In addition, optimal routing will increase the network lifetime and lead to its stability. Considering the high efficiency of clustering methods among the routing algorithms we present a new clustering method and considering good performance of Evolutionary Algorithms (EAs) in finding proper head clusters, we present a specific EA-based method named ICA (Imperialist Competitive Algorithm) via numerical coding. By thinking of specific conditions of a MANET and estimating the mobility direction of nodes, we prevent from additional reclusterings leading to reducing the overload. We have evaluated our proposed method for: 1) accuracy (including the reproducibility, convergence and stability criteria) through three case studies with different numbers of nodes and ranges and 2) efficiency (by comparing to other methods). Moreover, we applied our proposed method to a case study (multi-robot system) with low velocity. (C) 2019 Elsevier B.V. All rights reserved.", "journal": "COMPUTER NETWORKS", "category": "Computer Science, Hardware & Architecture; Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000525545900118", "keywords": "Capacitors; Reactive power; Switches; Optimization; Linear programming; Power generation; Genetic algorithms; Exchange market algorithm (EMA); genetic algorithm (GA); particle swarm optimization (PSO); radial distribution system (RDS); switched capacitors", "title": "A Two-Loop Hybrid Method for Optimal Placement and Scheduling of Switched Capacitors in Distribution Networks", "abstract": "This paper presents a method to find the optimal size and place of the switched capacitors using a hybrid optimization algorithm. The objective function includes the active and reactive power of power plants, the capital and maintenance costs of capacitor banks, and the cost of active and reactive power losses in distribution lines and transformers. The impact of the load model on the optimal sizing and placement of switched capacitors is studied using three different scenarios: In the first scenario, all loads are voltage-dependent; in the second scenario, only a portion of loads are voltage-dependent; in the third scenario, all loads are voltage-independent. The proposed hybrid algorithm incorporates an outer and two inner optimization layers. The outer layer is executed by a genetic algorithm (GA), while the inner layer is performed by a GA, an exchange market algorithm (EMA), or a particle swarm optimization (PSO). The performance of GA-GA, GA-EMA, and GA-PSO hybrid schemes are compared on an IEEE 33-bus test system. Moreover, IEEE 33-bus and 69-bus networks are used to verify the effectiveness of proposed hybrid scheme against the gravitational search algorithm (GSA), a combination of PSO and GSA (PSOGSA), cuckoo search algorithm (CSA), teaching learning-based optimization (TLBO), and flower pollination algorithm (FPA). The results highlight the advantage of the proposed hybrid optimization scheme over the other optimization algorithms.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000480510800011", "keywords": "Heat kernel; Littlewood-Paley square function; RCD space; Weight", "title": "Weighted Littlewood-Paley inequalities for heat flows in RCD spaces", "abstract": "We establish inequalities on vertical Littlewood-Paley square functions for heat flows in the weighted L-2 space over metric measure spaces satisfying the RCD* (0, N) condition with N is an element of [1, infinity) and the maximum volume growth assumption. In the noncompact setting, the later assumption can be removed by showing that the volume of the ball growths at least linearly. The estimates are sharp on the growth of the 2-heat weight and the 2-Muckenhoupt weight considered. The p-Muckenhoupt weight and the p-heat weight are also compared for all p is an element of (1, infinity). (C) 2019 Published by Elsevier Inc.", "journal": "JOURNAL OF MATHEMATICAL ANALYSIS AND APPLICATIONS", "category": "Mathematics, Applied; Mathematics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000534341200001", "keywords": "self-replication; self-reproduction; self-assembly; reconfigurable modular robots; manufacturing; additive manufacturing; evolutionary robotics", "title": "Robotic Self-Replication", "abstract": "The concept of an artificial corporeal machine that can reproduce has attracted the attention of researchers from various fields over the past century. Some have approached the topic with a desire to understand biological life and develop artificial versions; others have examined it as a potentially practical way to use material resources from the moon and Mars to bootstrap the exploration and colonization of the solar system. This review considers both bodies of literature, with an emphasis on the underlying principles required to make self-replicating robotic systems from raw materials a reality. We then illustrate these principles with machines from our laboratory and others and discuss how advances in new manufacturing processes such as 3-D printing can have a synergistic effect in advancing the development of such systems.", "journal": "ANNUAL REVIEW OF CONTROL, ROBOTICS, AND AUTONOMOUS SYSTEMS, VOL 3, 2020", "category": "Automation & Control Systems; Robotics", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000524757600001", "keywords": "Job shop scheduling; ant colony system; dynamic scheduling; event-driven", "title": "Efficiency-Oriented Production Scheduling Scheme: An Ant Colony System Method", "abstract": "During the real production system, the scheduling scheme change is mostly changed by dynamic events or new tasks. Due to the different urgency degrees of dynamic events, the corresponding scheduling methods should be adopted to ensure the production efficiency of enterprises. In this paper, an event-driven dynamic workshop scheduling model is established based on Ant Colony System (ACS), and two scheduling methods are designed to deal with dynamic events, namely parallel scheduling and parallel priority scheduling, respectively. The goal of parallel scheduling is to minimize the total makespan, while that of parallel priority scheduling is to minimize the delivery time of dynamic events. Additionally, a selective scheduling strategy is designed to determine the optimal scheduling method according to the urgency degree of dynamic events. Finally, the feasibility of the selective scheduling strategy in solving the dual-objective dynamic job shop scheduling problem (DJSP) is verified by an example experiment on DJSP as well as a large scale problem test set.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000554551300001", "keywords": "Global optimization; unconstrained optimization; constrained optimization; particle swarm optimization; two-stage multi-swarm particle swarm optimizer (TMPSO); multi-point particle swarm optimization (MpPSO)", "title": "Two-Stage Multi-Swarm Particle Swarm Optimizer for Unconstrained and Constrained Global Optimization", "abstract": "This paper presents a new two-stage multi-swarm particle swarm optimizer (TMPSO), which employs the multi-swarm method and takes two-stage different search strategies in the whole iteration process. This new optimizer includes two versions: unconstrained TMPSO (uTMPSO) and constrained TMPSO (cTMPSO) for unconstrained and constrained global optimizations respectively. For the uTMPSO version, TMPSO makes a certain number of sub-swarms in the first stage to iterate to increase the probability to find the global optimum. Further in the second stage, all the sub-swarms are merged into one large swarm to further refine the global best particle. In both these two stages, each sub-swarm of the first stage and the merged swarm of the second stage all employ a local three-stage multi-point particle swarm optimization (MpPSO) algorithm, which is enlightened by human decision-making and cusp catastrophe theory to enhance the local search ability. To solve constrained optimization problems, the uTMPSO is further upgraded to handle the constraints by using trial and error method to form the cTMPSO version, in which constraints violations are checked on each new created particle in the above uTMPSO procedures, and the violating ones are enforced to execute \"retreat\" operations, return into the feasible region and recreate new positions, which replaces the traditional penalty function method. This proposed uTMPSO is tested on two unconstrained optimization test functions benchmark set with 25 and 28 functions (including multimodal hybrid composition functions) respectively, and compared with other twelve particle swarm optimization variants. The test results show that uTMPSO has better performance and outperforms most compared algorithms. The cTMPSO is also tested on eight benchmark constrained optimization functions and five engineering application problems.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000477691600051", "keywords": "Nanocomposite films; kappa-Carrageenan; Cellulose nanocrystals; Interactions; Sorption properties", "title": "Structural and sorption properties of bio-nanocomposite films based on kappa-carrageenan and cellulose nanocrystals", "abstract": "There is an increased interest on changing the synthetic based materials to biodegradable ones, especially with natural polymers, polysaccharides or proteins. In this research we prepared bio-nanocomposite formulations with different component concentrations and investigated their structural features, with focus on the interactions, sorption properties, and how the combination between them influences these properties. By infrared spectroscopy, principal component analysis (PCA) and two-dimensional correlation spectroscopy (2DCOS) was identified that in the blending process are involved the -SO4 and C(4)-O-S groups of beta-d-galactose, C-O groups, (O=S=O) of carrageenan and -OH and C-O groups from CNCs. The water uptake and water sorption properties decrease with increasing the CNCs content in the formulations from about 15% for kappa to 10% for kappa C15 and from about 128% for kappa to 115% for kappa C15, respectively. The increase of the CNCs content induced an increase of the water contact angle from 47 for kappa to 90 for kappa C15, indicating once again the involvement of the free hydroxyl groups in the hydrogen bonded interactions. (C) 2019 Elsevier B.V. All rights reserved.", "journal": "INTERNATIONAL JOURNAL OF BIOLOGICAL MACROMOLECULES", "category": "Biochemistry & Molecular Biology; Chemistry, Applied; Polymer Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000484568900001", "keywords": "Bayesian networks; self-determined motivation; competitive anxiety; athletes; students", "title": "Self-Determined Motivation and Competitive Anxiety in Athletes/Students: A Probabilistic Study Using Bayesian Networks", "abstract": "This study attempts to analyze the relationship between two key psychological variables associated with performance in sports - Self-Determined Motivation and Competitive Anxiety - through Bayesian Networks (BN) analysis. We analyzed 674 university students that are athletes from 44 universities that competed at the University Games in Mexico, with an average age of 21 years (SD = 2.07) and with a mean of 8.61 years' (SD = 5.15) experience in sports. Methods: Regarding the data analysis, firstly, classification using the CHAID algorithm was carried out to determine the dependence links between variables; Secondly, a BN was developed to reduce the uncertainty in the relationships between the two key psychological variables. The validation of the BN revealed AUC values ranging from 0.5 to 0.92. Subsequently, various instantiations were performed with hypothetical values applied to the \"bottom\" variables. Results showed two probability trees that have extrinsic motivation and amotivation at the top, while the anxiety/activation due to worries about performance was at the bottom of the probabilities. The instantiations carried out support the existence of these probabilistic relationships, demonstrating their scarce influence on anxiety about competition generated by the intrinsic motivation, and the complex probabilistic effect of introjected and identified regulation regarding the appearance of anxiety due to worry about performance.", "journal": "FRONTIERS IN PSYCHOLOGY", "category": "Psychology, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000483422300007", "keywords": "Measles; Immunity; Measles-containing vaccine (MCV)", "title": "Study on factors associated with seroprotection after measles vaccination in children of 6-14 years in Eastern China", "abstract": "Measles cases have occurred in individuals with histories of vaccination against the disease in Zhejiang Province, China. The purposes of this study were to determine the seroprevalence of immunoglobulin G (IgG) measles antibodies in vaccinated individuals, to explore the waning kinetics of measles antibody among children after receipt of a measles-containing vaccine, and to define high-risk groups in the population. A seroprevalence survey of measles antibody was conducted with 1900 randomly selected and age-stratified participants aged 6-14 years in Zhejiang province. In our study, seronegative persons accounted for 7.17% of study participants. A case-control study of participants who had received at least one dose of measles-containing vaccine was conducted, with 123 cases of immune failure and 1593 controls with immune success. Multivariate logistic regression analysis showed that age, and number of doses were the influencing factors for measles immunization failure. The older a participant (odds ratio [OR] = 1.164), the more likely that measles vaccine immunity failed. In addition, immune failure was more likely to occur after one dose of MCV than two doses (OR = 0.008) or three doses and more (OR = 0.047). In a univariate logistic regression analysis, we found that immune failure was more likely to occur with MCV vaccination beginning at 8 months than at 9-11 months (OR = 0.562) and the subjects whose registration residence was in other cities in Zhejiang province (OR = 3.527). However, these differences in seropositivity were not significant in the multivariate logistic regression analysis. The exponential regression equation of the attenuation model after measles immunization was y = 884.64e(-0.057x) (R-2 = 0.0521, p < 0.001), and results showed that the measles geometric mean concentration of IgG antibodies was approximately 884.64 mIU/ml after the last MCV vaccination and decreased with time since last vaccination. (C) 2019 Elsevier Ltd. All rights reserved.", "journal": "VACCINE", "category": "Immunology; Medicine, Research & Experimental", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000469688200001", "keywords": "Mixed geographically weighted regression; spatial autoregressive; two stage least squares; large sample properties", "title": "A study on geographically weighted spatial autoregression models with spatial autoregressive disturbances", "abstract": "Spatial heterogeneity and correlation are both considered in the geographical weighted spatial autoregressive model. At present, this kind of model has aroused the attention of some scholars. For the estimation of the model, the existing research is based on the assumption that the error terms are independent and identically distributed. In this article we use a computationally simple procedure for estimating the model with spatially autoregressive disturbance terms, both the estimates of constant coefficients and variable coefficients are obtained. Finally, we give the large sample properties of the estimators under some ordinary conditions. In addition, application study of the estimation methods involved will be further explored in a separate study.", "journal": "COMMUNICATIONS IN STATISTICS-THEORY AND METHODS", "category": "Statistics & Probability", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000464943500015", "keywords": "Outdoor thermal comfort; Subtropical climate region; Thermal neutrality; Logistic regression; Comfort range of meteorological parameters", "title": "Outdoor thermal sensation and logistic regression analysis of comfort range of meteorological parameters in Hong Kong", "abstract": "Warm and hot days account for most of the time in Hong Kong. Outdoor thermal comfort studies in Hong Kong should give its first consideration to warm and hot days. This study presents investigations about thermal comfort through 1600 human subject responses from the onsite survey with concurrent meteorological parameter measurements. Probit analysis was used for searching the thermal neutral range of Hong Kong residents in a year span. Logistic regression was used for locating the meteorological parameter ranges for thermal neutral and comfort conditions. It is shown that people had difficulties defining their actual thermal feelings near the thermal neutral status when being asked to use the nine-point thermal sensation scale. Obvious thermal adaptation effect for thermal neutral conditions were observed among Hong Kong residents over the seasons in a year. The transitional seasons had a wider thermal neutral range than that of winter and summer. Summer had the narrowest thermal neutral range. Wind and solar radiation had an interaction effect with air temperature in determining thermal sensation and thermal comfort. Wind can effectively offset the negative effect of solar radiation in summer when the air temperature was lower than 31 degrees C. The thermal comfort condition allowed a higher limit of solar radiation than the thermal neutral condition when the air temperature was lower than 31 degrees C. The investigations in this study provide some unique insights into the way to assess urban thermal comfort in the building design stage.", "journal": "BUILDING AND ENVIRONMENT", "category": "Construction & Building Technology; Engineering, Environmental; Engineering, Civil", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000474911000022", "keywords": "Extended state observer; input saturation; lumped uncertainty; sliding mode control; trajectory tracking", "title": "Extended state observer based adaptive sliding mode tracking control of wheeled mobile robot with input saturation and uncertainties", "abstract": "This paper proposes the control design for the wheeled mobile robot in the presence of external disturbances, parametric uncertainties together with input saturation. Integrating the extended state observer technique, a practical method named sliding mode control is designed to force the state variables to attain the stable equilibrium with the help of extended state observer by compensating uncertainty and disturbance (called lumped uncertainty). To handle the shortcoming of undesired chattering and the difficulty of choosing the control gain, sliding mode control with adaptive mechanism is applied, which has the ability to automatically adjust the control gain and can even work well without a requirement of knowing the upper bound on lumped uncertainty. Subsequently, an auxiliary system is further developed to cope with input saturation problem. In addition, the stability analysis of the closed-loop system is rigorously proved via Lyapunov theorem, manifesting that the proposed controller can guarantee the ultimate boundedness of all signals in the overall system and make tracking errors converge to an arbitrarily small neighborhood around zero by selecting appropriate control parameters. Finally, simulation results are intuitively carried out to demonstrate the feasibility of the introduced adaptive composite controller.", "journal": "PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART C-JOURNAL OF MECHANICAL ENGINEERING SCIENCE", "category": "Engineering, Mechanical", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000469158300004", "keywords": "Underwater computer vision; Underwater acoustics; Bluefin Tuna biomass estimation", "title": "Automatic Bluefin Tuna (Thunnus thynnus) biomass estimation during transfers using acoustic and computer vision techniques", "abstract": "In this work, acoustic and computer vision techniques are combined to develop an automatic procedure for biomass estimation of tuna during transfers. A side scan sonar working at 200 kHz and a stereo camera, positioned facing towards the surface to record the ventral aspect of fish, are set as acquisition equipment. Moreover, a floating structure has been devised to place the sensors between cages in transfers, creating a transfer canal that allows data acquisition while fish swim from donor to receiving cage. Biomass assessment is computed by counting transferred tuna and sizing a representative sample of the stock. The number of transferred tuna is automatically deduced from acoustic echograms by means of image processing techniques, whereas tuna size is computed from the stereo videos using our automatic computer vision procedure based on a deformable model of the fish ventral silhouette. The results show that the system achieves automatic tuna counting with error below 10%, achieving around 1% error in the best configuration, and automatic tuna sizing of more than 20% of the stock, with highly accurate Snout Fork Length estimation when compared to true data from harvests. These results fulfil the requirements imposed by International Commission for the Conservation of Atlantic Tunas for compliant transfer operations.", "journal": "AQUACULTURAL ENGINEERING", "category": "Agricultural Engineering; Fisheries", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000485711600123", "keywords": "Big data; Finance; Marine economy", "title": "Financial Support to Marine Economy in the Big Data Era", "abstract": "In the big data era, increasing financial support for the marine economy and vigorously developing blue finance have a strong sense of urgency and historical mission, which need a larger macro vision and a more forward-looking strategic layout. Blue finance is related to China's economic and financial security. We must seize the development opportunities of blue finance in the new era and set up a systematic financial thinking of big countries. Only adopting the strategic thinking of innovation and development, can we support the marine economy in the big data era.", "journal": "JOURNAL OF COASTAL RESEARCH", "category": "Environmental Sciences; Geography, Physical; Geosciences, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000483979100001", "keywords": "Tertiary teaching; efficiency; effectiveness; satisfaction; terrain analysis", "title": "Comparative usability of an augmented reality sandtable and 3D GIS for education", "abstract": "Augmented Reality (AR) sandtables facilitate the shaping of sand to form a surface that is transformed into a digital terrain map which is projected back onto the sand. Although a mature technology, there are still few instances of sandtables being used in surface analysis. Fundamentally there has not been any reported formal assessment of how well sandtables perform in an educational context compared to other conventional learning environments. We compared learning outcomes from using an AR sandtable versus a conventional 3D GIS to convey key concepts in terrain and hydrological analyses via usability and knowledge testing. Overall results from students at a research-intensive New Zealand university reveal a faster task performance and more learning satisfaction when using the sandtable to undertake experimental tasks. Effectiveness and knowledge quiz results revealed no significant difference between the technologies though there was a trend for more accurate answers with 3D GIS tasks. Student learning wise, the sandtable integrated core concepts (especially morphometry) more effectively though both technologies were otherwise similar. We conclude that sandtables have high potential in geospatial teaching, fostering accessible and engaging means of introducing terrain and hydrological concepts, prior to undertaking a more accurate and precise surface analysis with 3D GIS.", "journal": "INTERNATIONAL JOURNAL OF GEOGRAPHICAL INFORMATION SCIENCE", "category": "Computer Science, Information Systems; Geography; Geography, Physical; Information Science & Library Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000480051500001", "keywords": "Smooth intrinsic time-scale decomposition; linear local tangent space alignment; multi-fractal detrended fluctuation analysis; vibration signal; bearing fault diagnosis", "title": "Rolling bearing fault diagnosis based on adaptive smooth ITD and MF-DFA method", "abstract": "To effectively utilize a feature set to further improve fault diagnosis of a rolling bearing vibration signal, a method based on multi-fractal detrended fluctuation analysis (MF-DFA) and smooth intrinsic time-scale decomposition (SITD) was proposed. The vibration signal was decomposed into several proper rotation components by applying this new SITD method to overcome noise effects, preserve the effective signal, and improve the signal-to-noise ratio. Wavelet analysis was embedded in iteration procedures of intrinsic time-scale decomposition (ITD). For better results, an adaptive threshold function was used for signal recovery from noisy proper rotation components in the wavelet domain. Additionally, MF-DFA was used to reveal the multi-fractality present in the instantaneous amplitude of the proper rotation components. Finally, linear local tangent space alignment was applied for feature dimension reduction and to obtain fault characteristics of different types, further improving identification accuracy. The performance of the proposed method is determined to be superior to that of the ITD-MF-DFA method.", "journal": "JOURNAL OF LOW FREQUENCY NOISE VIBRATION AND ACTIVE CONTROL", "category": "Acoustics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000465500600005", "keywords": "Finite-time boundedness; prescribed performance control; uncertain nonlinear systems", "title": "Prescribed Performance Finite-Time Tracking Control for Uncertain Nonlinear Systems", "abstract": "This work investigates the finite-time tracking control problem for a class of uncertain strict-feedback nonlinear systems from a new perspective. First, a novel concept called finite-time performance function (FTPF) is defined. Further, a new sufficient condition of finite-time stability is derived and the tracking error can converge to a predefined region within a finite-time interval. The design process of the proposed technique is simpler. Finally, four simulation examples are carried out to illustrate the effectiveness of presented method.", "journal": "JOURNAL OF SYSTEMS SCIENCE & COMPLEXITY", "category": "Mathematics, Interdisciplinary Applications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000472094000023", "keywords": "Multi-grain rice seeds; Variety identification; Near-infrared spectroscopy; Rapid and nondestructive discriminant analysis; Step-by-step phase-out-PLS-DA", "title": "Rapid and non-destructive analysis for the identification of multi-grain rice seeds with near-infrared spectroscopy", "abstract": "The rapid and non-destructive discriminant analysis of rice seeds has great significance for large-scale agriculture. Using near-infrared (MR) diffuse-reflectance spectroscopy with partial least squares-discriminant analysis (PLS-DA), a variety identification method of multi-grain rice seeds was developed. The equidistant combination method was adopted for large-range wavelength screening. A step-by-step phase-out method was proposed to eliminate interference wavelengths and improve the predicted effect. The optimal wavelength model was a combination of 54 wavelengths within 808-974 nm of the short-NIR region. One type of pure rice variety (Y Liangyou 900) was used for identification (negative). Positive samples included the other four pure varieties and contamination of Y Liangyou 900 by the above four varieties. The recognition-accuracy rates for positive, negative and total validation samples reached 93.1%, 95.1%, and 94.3%, respectively. In the long-NIR region, the local optimal wavelength model was a combination of 49 wavelengths within 1188-1650 nm, and the recognition-accuracy rates for positive, negative and total validation samples were 90.3%, 94.1%, and 92.5%, respectively. Results confirmed the feasibility of NIR spectroscopy for variety identification of multi-grain rice seeds. The proposed two discrete-wavelength models located in the short- and long-NIR regions can provide valuable reference to a dedicated spectrometer. (C) 2019 Published by Elsevier B.V.", "journal": "SPECTROCHIMICA ACTA PART A-MOLECULAR AND BIOMOLECULAR SPECTROSCOPY", "category": "Spectroscopy", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000466874200006", "keywords": "evolutionary algorithms; genetic algorithm; water distribution networks; optimization", "title": "Testing evolutionary algorithms for optimization of water distribution networks", "abstract": "Water distribution networks (WDNs) are one of the most important elements of urban infrastructure and require large investment for construction. Design of WDNs is classified as a large combinatorial discrete nonlinear optimization problem. The main concerns associated with the optimization of such networks are the nonlinearity of the discharge-head loss relationships for pipes and the discrete nature of pipe sizes. Due to these issues, this problem is widely considered to be a benchmark problem for testing and evaluating the performance of nonlinear and heuristic optimization algorithms. This paper compares different techniques, all based on evolutionary algorithms (EAs), which yield optimal solutions for least-cost design of WDNs. All of these algorithms search for the global optimum starting from populations of solutions, rather than from a single solution, as in Newton-based search methods. They use different operators to improve the performance of many solutions over repeated iterations. Ten EAs, four of them for the first time, are applied to the design of three networks and their performance in terms of the least cost, under different stopping criteria, are evaluated. Statistical information for 20 executions of the ten algorithms is summarized, and Friedman tests are conducted. Results show that, for the two-loop benchmark network, the particle swarm optimization gravitational search and biology and bioinformatics global optimization algorithms efficiently converge to the global optimum, but perform poorly for large networks. In contrast, given a sufficient number of function evaluations, the covariance matrix adaptation evolution strategy and soccer league competition algorithm consistently converge to the global optimum, for large networks.", "journal": "CANADIAN JOURNAL OF CIVIL ENGINEERING", "category": "Engineering, Civil", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000477037900179", "keywords": "open spaces 2; CKD 3; renal function", "title": "Urban Open Space Is Associated with Better Renal Function of Adult Residents in New Taipei City", "abstract": "The purpose of this study is to explore the association between proximity to open space and adult renal function. This was a cross-sectional study. Adult residents of Taipei metropolis were recruited in the analysis. The proximity of each subject to open space was measured using the Geographic Information System. Residents were divided into two groups: with and without chronic kidney disease (CKD). We made univariable comparisons between the two groups. The logistic regression models were used to estimate the odds ratio of CKD. Forest plot was used to examine the effect of interaction between distance to open space and subgroup variable on CKD. A total number of 21,656 subjects with mean age 53.6 years were enrolled in the study. Of the subjects, 2226 (10.28%) had CKD. The mean and standard deviation of distance to open space were 117.23 m and 80.19 m, respectively. Every 100 m distance to open space was associated with an odds ratio of 1.071 for CKD. Subgroup analysis revealed that residents of female, without hypertension, or without impaired fasting glucose (IFG) living more than 200 m from open spaces have greater odds of CKD than those living less than 200 m. Conclusions: Proximity to open space was associated with a lower prevalence of CKD among adults in Taiwan. Such association was enhanced among females and healthy adults without hypertension or impaired fasting glucose (IFG).", "journal": "INTERNATIONAL JOURNAL OF ENVIRONMENTAL RESEARCH AND PUBLIC HEALTH", "category": "Environmental Sciences; Public, Environmental & Occupational Health", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000466226500033", "keywords": "Digital control; high-gain nonlinear observer; hybrid systems; output feedback; semiglobal asymptotic stabilization; state estimation with saturation", "title": "Semiglobal Asymptotic Stabilization of Lower Triangular Systems by Digital Output Feedback", "abstract": "Digital control of nonlinear systems is studied by output feedback. It is proved that for nonlinear systems with a lower triangular structure, semiglobal asymptotic stabilization is still possible via discretized output feedback as long as a sampling time is small. The establishment of semiglobal asymptotic stabilizability needs no restrictive condition on the nonlinearities and/or unmeasurable states of the system, such as a linear growth condition or an output-dependent growth condition as commonly required in the case of global output feedback stabilization. It involves, however, tedious but subtle stability analysis due to the hybrid nature of the closed-loop system. A design method through \"sample and hold\" is developed to construct a semiglobally asymptotically stabilizing, digital output feedback compensator that consists of a high-gain nonlinear observer and controller, both with saturated states.", "journal": "IEEE TRANSACTIONS ON AUTOMATIC CONTROL", "category": "Automation & Control Systems; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000485928500004", "keywords": "Almond; Spring frost; Breeding; Late-blooming; Yield; Kernel quality", "title": "Identification of late-blooming almond (Prunus dulcis L.) genotypes with high kernel quality", "abstract": "Almond (Prunus dulcis L.) is one of the most important nut crops in the world. The present study was carried out to select the late-blooming almond genotypes with high kernel quantity and quality. In the first step, pre-selection was done based on blooming time among a large number of almond seedling trees. Early and middle blooming genotypes were eliminated and finally, 76 genotypes were selected. Then, in the second step, late-blooming trees were evaluated based on vegetative and fruit traits to identify the superior genotypes. The selected late-blooming genotypes showed significant morphological and pomological differences (P < 0.05). Nut weight ranged from 2.03 to 7.68 g with an average of 4.22. Kernel weight ranged from 1.20 to 2.31 g with an average of 1.52 g. Nut weight and kernel weight with high standardized beta coefficients showed positive and significant associations with kernel percentage and therefore they had a great impact on this key character and should be considered in breeding programs. Considering the ideal values of the important and commercial traits of almond, 14 genotypes were superior and thus they could be cultivated in orchards and/or to be used as a parent for crosses to create suitable populations or to improve blooming time, kernel quality, and nutritional values of almond cultivars.", "journal": "EUPHYTICA", "category": "Agronomy; Plant Sciences; Horticulture", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000470117900004", "keywords": "virtual reality; coordination; motion capture; mirroring; statistical learning", "title": "Audio cues enhance mirroring of arm motion when visual cues are scarce", "abstract": "Swing in a crew boat, a good jazz riff, a fluid conversation: these tasks require extracting sensory information about how others flow in order to mimic and respond. To determine what factors influence coordination, we build an environment to manipulate incoming sensory information by combining virtual reality and motion capture. We study how people mirror the motion of a human avatar's arm as we occlude the avatar. We efficiently map the transition from successful mirroring to failure using Gaussian process regression. Then, we determine the change in behaviour when we introduce audio cues with a frequency proportional to the speed of the avatar's hand or train individuals with a practice session. Remarkably, audio cues extend the range of successful mirroring to regimes where visual information is sparse. Such cues could facilitate joint coordination when navigating visually occluded environments, improve reaction speed in human-computer interfaces or measure altered physiological states and disease.", "journal": "JOURNAL OF THE ROYAL SOCIETY INTERFACE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000472211900018", "keywords": "Anticoagulants; Antiplatelet therapy; Coronary artery disease; Hemorrhage; Thrombogenicity", "title": "Impact of Total Antithrombotic Effect on Bleeding Complications in Patients Receiving Multiple Antithrombotic Agents", "abstract": "Background: Few reports have evaluated the total antithrombotic effect of multiple antithrombotic agents. Methods and Results: Thrombus formation was evaluated with the Total Thrombus-formation Analysis System (T-TAS (R)) using 2 types of microchips in 145 patients with stable coronary artery disease receiving oral anticoagulants plus single-or dual-antiplatelet therapy. The PL-chip coated with collagen is designed for analysis of the platelet thrombus formation process under shear stress condition (18 mu L/min). The AR-chip coated with collagen and tissue thromboplastin is designed for analysis of the fibrin-rich platelet thrombus formation process under shear stress condition (4 mu L/min). The results were expressed as an area under the flow pressure curve (PL18-AUC(10) and AR(4)-AUC(30), respectively). Bleeding events occurred in 43 patients during a 22-month follow-up. AR(4)-AUC(30) was significantly lower in patients with bleeding events than in those without (584 [96-993] vs. 1,028 [756-1,252], P=0.0003). Multivariate logistic regression analysis identified AR(4)-AUC(30) (odds ratio 3.18) as a significant predictor of bleeding events, in addition to baseline anemia and usage of the standard dose of direct oral anticoagulants. However, PL18-AUC(10) was not significantly related to bleeding events. Conclusions: A lower AR(4)-AUC(30) level was associated with increasing risk of subsequent bleeding complications in patients with stable coronary artery disease who received multiple antithrombotic agents.", "journal": "CIRCULATION JOURNAL", "category": "Cardiac & Cardiovascular Systems", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000470756900003", "keywords": "hybrid yarns; stainless steel fibers; spatial arrangement of fibers; cross-section of fibers", "title": "Scale Alignment in Between-Item Multidimensional Rasch Models", "abstract": "Scores estimated from multidimensional item response theory (IRT) models are not necessarily comparable across dimensions. In this article, the concept of aligned dimensions is formalized in the context of Rasch models, and two methods are described-delta dimensional alignment (DDA) and logistic regression alignment (LRA)-to transform estimated item parameters so that dimensions are aligned. Both the DDA and LRA methods are applied to real and simulated data, and it is demonstrated that both methods are broadly effective for achieving aligned scales. The routine use of scale alignment methods is recommended prior to comparing scores across dimensions.", "journal": "JOURNAL OF EDUCATIONAL MEASUREMENT", "category": "Psychology, Educational; Psychology, Applied; Psychology, Mathematical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000465040600016", "keywords": "spectral unmixing; sparse representation; local blocks; noise-adjusted principal analysis; hyperspectral remote sensing imagery", "title": "Parametric sparse representation for autofocused imaging through unknown walls", "abstract": "In order to obtain a high-resolution and well-focused image from compressively sampled echo data in the presence of wall ambiguity, a parametric sparse representation algorithm is proposed in this paper. A parametric dictionary with an unknown wall parameter is designed to represent the wall's ambiguity. Then, imaging through unknown walls problem is converted into a joint optimization one which can be decomposed into sequential sparse imaging and wall parameter estimation. Specifically, the wall parameter estimation is performed by searching the maximum contrast. Numerical results are presented to demonstrate the validity and effectiveness of the proposed algorithm.", "journal": "INTERNATIONAL JOURNAL OF REMOTE SENSING", "category": "Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000483546902089", "keywords": "GaN HEMT; periodic pits; electric field", "title": "The optimal use of image analysis in the assessment of PD-L1 immunostaining in non-small cell lung cancer", "abstract": "In this paper, an AlGaN/GaN high electron mobility transistor (HEMT) by the periodic pits in the end of the buffer layer (PPB-HEMT) is proposed. The main focus of this proposed structure is based on controlling the carrier concentration and uniformity of the electric field distribution in the two dimensional electron gas (2DEG) channel layer that improves the breakdown voltage. The carrier concentrations fall under the gate by making periodic pits in the end of the buffer layer in the PPB-HEMT structure. Then, the electric field value reduces across the 2DEG channel layer and the electric field obtains the critical value at a higher drain-source voltage. Accordingly, the breakdown voltage of the PPB-HEMT improves approximately 32% compared to a conventional HEMT (C-HEMT). Consequently, the maximum output power density of the PPB-HEMT increases by 21% compared to the C-HEMT. Also, the depletion region width across the channel under the gate increases in the PPB-HEMT, and the gate-drain capacitance of the proposed structure decreases by 77% compared to the conventional structure. Therefore, the cut-off frequency of the PPB-HEMT improves by 50% compared to the C-HEMT due to the reduction in gate-drain capacitance. In addition, the current gain, unilateral power gain, and the maximum available power gain of the proposed structure increase. Therefore, the use of PPB-HEMT is more appropriate than the C-HEMT in high power and high frequency applications.", "journal": "VIRCHOWS ARCHIV", "category": "Pathology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000471157200236", "keywords": "benzodiazepine; older adults; pharmacology; psychiatry", "title": "Lipoprotein(a) plasma levels, bone mineral density and risk of hip fracture: a post hoc analysis of the Women's Health Initiative, USA", "abstract": "Objectives Elevated Lipoprotein(a) (Lp[a]) is a well-known risk factor for cardiovascular disease. However, its roles in bone metabolism and fracture risk are unclear. We therefore investigated whether plasma Lp(a) levels were associated with bone mineral density (BMD) and incident hip fractures in a large cohort of postmenopausal women. Design Post hoc analysis of data from the Women's Health Initiative (WHI), USA. Setting 40 clinical centres in the USA. Participants The current analytical cohort consisted of 9698 white, postmenopausal women enrolled in the WHI, a national prospective study investigating determinants of chronic diseases including heart disease, breast and colorectal cancers and osteoporotic fractures among postmenopausal women. Recruitment for WHI took place from 1 October 1993 to 31 December 1998. Exposures Plasma Lp(a) levels were measured at baseline. Outcome measures Incident hip fractures were ascertained annually and confirmed by medical records with follow-up through 29 August 2014. BMD at the femoral neck was measured by dual X-ray absorptiometry in a subset of participants at baseline. Statistical analyses Cox proportional hazards and logistic regression models were used to evaluate associations of quartiles of plasma Lp(a) levels with hip fracture events and hip BMD T-score, respectively. Results During a mean follow-up of 13.8 years, 454 incident cases of hip fracture were observed. In analyses adjusting for confounding variables including age, body mass index, history of hysterectomy, smoking, physical activity, diabetes mellitus, general health status, cardiovascular disease, use of menopausal hormone therapy, use of bisphosphonates, calcitonin or selectiveoestrogen receptor modulators, baseline dietary and supplemental calcium and vitamin D intake and history of fracture, no significant association of plasma Lp(a) levels with low hip BMD T-score or hip fracture risk was detected. Conclusions These findings suggest that plasma Lp(a) levels are not related to hip BMD T-score or hip fracture events in postmenopausal women.", "journal": "BMJ OPEN", "category": "Medicine, General & Internal", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000468797000005", "keywords": "bias; migrant workers; occupational injury; under-reporting; workers' compensation", "title": "Work injuries in internal migrants to Alberta, Canada. Do workers' compensation records provide an unbiased estimate of risk?", "abstract": "Introduction It is not known whether out-of-province Canadians, who travel to Alberta for work, are at increased risk of occupational injury. Methods Workers' compensation board (WCB) claims in 2013 to 2015 for those injured in Alberta were extracted by home province. Denominator data, from Statistics Canada, indicated the numbers from Alberta and Newfoundland and Labrador (NL) employed in Alberta in 2012. Both datasets were stratified by industry, age, and gender. Logistic regression estimated the risk of a worker from NL making a WCB claim in 2013 or 2014, stratified by time lost from work. Bias from under-reporting was examined in responses to injury questions in a cohort of trades' workers across Canada and in a pilot study in Fort McMurray, Alberta. Results Injury reporting rate in workers from NL was lower than those from Alberta, with a marked deficit (odds ratio [OR] = 0.17; 95% confidence interval [CI], 0.12-0.27) for injuries resulting in 1 to 30 days off work. Among the 1520 from Alberta in the trades' cohort, 327 participants reported 444 work injuries: 34.5% were reported to the WCB, rising to 69.4% in those treated by a physician. A total of 52 injuries in Alberta were recorded by 151 workers in the Fort McMurray cohort. In logistic regression, very similar factors predicted WCB reporting in the trades and Fort McMurray cohorts, but those from out-of-province or recently settled in Alberta were much less likely to report (OR = 0.02; 95% CI, 0.00-0.40). Conclusion Differential rates of under-reporting explain in part the overall low estimates of injuries in interprovincial workers but not the deficit in time-loss 1 to 30 days.", "journal": "AMERICAN JOURNAL OF INDUSTRIAL MEDICINE", "category": "Public, Environmental & Occupational Health", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000469184800005", "keywords": "Small renal mass; CT; CT texture analysis; Protein expression", "title": "Texture analysis of small renal cell carcinomas at MDCT for predicting relevant histologic and protein biomarkers", "abstract": "PurposeTo assess CT texture features of small renal cell carcinomas (4cm) for association with key pathologic features including protein biomarkers.MethodsQuantitative CT texture analysis (CTTA) of small renal cancers (4cm) was performed on non-contrast and portal venous phase abdominal MDCT scans with an ROI drawn at the largest cross-sectional diameter of the tumor using commercially available software. Texture parameters including mean pixel attenuation, the standard deviation (SD) of the pixel distribution histogram, entropy, the mean of positive pixels, the skewness (i.e., asymmetry) of the pixel histogram, kurtosis (i.e., peakness) of the pixel histogram, and the percentage of positive pixels were correlated with pathologic data from surgical resection, including histology and nuclear grade, as well as microarray analysis in a subset (n=40) including Ki67 index, CRP, and neovascularization (CD105/CD31).ResultsPortal venous phase images were availablein 249 patients (105women, 144men;mean age,56.7years) with tumors 4cm (mean, median, range,SD; 2.66, 2.60, 0.3-4.0 +/- 0.85cm). CT texture featuresof standard deviation, mean of the positive pixels,and entropy of the pixel histogram were significantly associated with histologic cell type (clear vs. non-clear;p<0.001). Entropy and mean of the positive pixels also showed an association with nuclear grade, although not statistically significant.In themicroarray analysissubset, kurtosis of the pixel histogram was associated with CD105/CD31(p=0.05). SD also showed some association with CD 105 positivity (p=0.02) and CAIX expression (p=0.01). Non-contrastCT images were availablein 174 patients (72women,102men;mean age,57.5years). Although the association with histology was not as strong as on the portal venous phase, in the subset of patients with microarray data, SD was found to correlate with CRP (p=0.08), kurtosis with CRP (p=0.004), CD105/CD31 (p=0.002),and with Ki 67 index (p<0.001).Conclusion p id=Par4 CT texture features were significantly associated withimportanthistopathologic features insmallrenalcancers. These non-invasive measures can be performed retrospectively and may provide useful information when determining follow-up and treatment of small renalcancers.", "journal": "ABDOMINAL RADIOLOGY", "category": "Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000466134800013", "keywords": "Pt nanostructures; Pt thickness; annealing time; nanoparticles; nanoclusters; porous network; GaN (0001); optical property", "title": "Determination of growth regimes of Pt nanostructures on GaN (0001) based on the control of Pt thickness and annealing time: Morphological evolution of Pt nanostructures from the nanoparticles, nanoclusters to porous network", "abstract": "Pt nanostructures are applicable in various applications such as sensors, solar cells, light emitting devices and catalysis and only slight changes in their configuration, density and size can induce significant changes in their properties and thus the functionality in the related applications. In this paper, the systematical evolution of Pt nanostructures such as nanoparticles, nanoclusters and porous network on GaN (0001) is demonstrated by the systematic thermal annealing of Pt thin films based on the combinational effects such as thermal dewetting, Volmer-Weber growth model and coalescence growth. In particular, small dome-shaped self-assembled Pt nanoparticles with relatively smaller deposition amount (<2 nm) and wiggly Pt nanoclusters between 3 and 5 nm are formed based on the Volmer-Weber growth model and the partial coalescence of Pt nanoparticles, respectively. Between 10 and 30 nm, the growth of Pt nanoclusters is observed and eventually with the increased Pt thickness range between 40 and 100 nm, nanoclusters gradually develop into the porous Pt network by connecting neighboring structures owing to the enhanced coalescence growth. Meanwhile, along with the annealing time variation between 0 and 3600 s, the rate of dewetting is increased and as a result, the evolution of densely packed to separated nanoclusters is formed. In addition, the optical properties of corresponding Pt nanostructures demonstrate that the photoluminescence and Raman intensity are reduced along with the evolution of the surface coverage of Pt nanostructures, whereas the average reflectance is significantly enhanced accordingly at the same time.", "journal": "PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART L-JOURNAL OF MATERIALS-DESIGN AND APPLICATIONS", "category": "Materials Science, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000480354800053", "keywords": "Clay mineral ratio (CLM); connected component analysis; iron oxide ratio (IO); K-means clustering; Landsat 8; mine water bodies; surface mining; t-test", "title": "Automated Seasonal Separation of Mine and Non Mine Water Bodies From Landsat 8 OLI/TIRS Using Clay Mineral and Iron Oxide Ratio", "abstract": "Opencast mining has huge effects on water pollution for several reasons. Fresh water is heavily used to process ore. Mine effluent and seepage from various mine related areas especially tailing reservoir, increase water pollution immensely. Monitoring and classification of mine water bodies, which have such environmental impacts, have several research challenges. In the past, land cover classification of a mining region detects mine and non mine water bodies simultaneously. Water bodies inside surface mines have different characteristics from other water bodies. In this paper, a novel method has been proposed to differentiate mine and non mine water bodies over the seasons, which does not require to set a threshold value manually. Here, water body regions are detected over the entire scene by any classical water body detection algorithm. Further, each water body is treated independently, and reflectance properties of a bounding box over each water body region are analyzed. In the past, there were efforts to use clay mineral ratio (CLM) to separate mine and non mine water bodies. In this paper, it has been observed that iron oxide ratio (IO) can also separate mine and non mine water bodies. The accuracy is observed to increase, if the difference of CLM and IO is used for segregation. The proposed algorithm separates these regions by taking into account seasonal variations. Means of differences of CLM and IO of each bounding box have been clustered using K-means clustering algorithm. The automation provides precision and recall for mine, and non mine water bodies as [77.83%, 76.55%] and [75.18%, 75.84%], respectively, using ground truths from high-definition Google Earth images.", "journal": "IEEE JOURNAL OF SELECTED TOPICS IN APPLIED EARTH OBSERVATIONS AND REMOTE SENSING", "category": "Engineering, Electrical & Electronic; Geography, Physical; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000487208100006", "keywords": "Disaster; Time preference; Cognitive self-control; Saving behavior; China", "title": "How a disaster affects household saving: Evidence from China's 2008 Wenchuan earthquake", "abstract": "Previous studies have yielded contradictory findings on whether a natural disaster increases or decreases time preference and hence, respectively, lowers or raises saving. This paper delves into the psychological essence of time preference and proposes a theoretical path by which a disaster affects time preference through its impact on cognitive self-control. The theoretical framework is applied to analyzing the saving behavior of households impacted by the 2008 earthquake in Wenchuan, China. The analysis reveals that the effect on cognitive self-control, as manifested in variables calibrating attention and mood, differs according to severity of damage and level of income. Individuals in more severely affected counties and of lower income levels show diminished cognitive self-control consistent with post-traumatic stress disorder. Conversely, individuals in more moderately affected counties and of higher income levels show enhanced cognitive self-control consistent with the findings of post-traumatic growth reported in some studies. A further stage of the analysis shows cognitive self-control to be directly related to saving behavior. This implies that more severely affected and lower income households should exhibit decreases in saving, and vice versa, and this relationship is confirmed by direct estimation. Estimation of the relationships between the earthquake and both the cognitive self-control indicators and the saving rate makes use of propensity score matching and difference in difference techniques while estimation of the relationship between the cognitive self-control indicators and the saving rate makes use of generalized quantile regression. The finding of differing psychological reactions to the disaster resolves the controversy surrounding the direction of the effect of a disaster on saving behavior. (c) 2019 Elsevier Inc. All rights reserved.", "journal": "JOURNAL OF ASIAN ECONOMICS", "category": "Economics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000481446100003", "keywords": "Emotions; Affect; PLS-SEM; Word of mouth; Rumor; Cognitive appraisal theory", "title": "Role of affect in marketplace rumor propagation", "abstract": "Purpose Rumors about products and brands are common occurrence in the marketplace. Often these rumors are shared among consumers using the word of mouth channel. The spread of these rumors is fast and can lead to significant consequences to products and brands. The purpose of this paper is to explore the dynamics of such rumor sharing behavior among consumers. Specifically, this paper investigates the role of positive affect and negative affect in rumor sharing behavior. Three key rumor characteristics (valence, involvement and credibility) are explored as antecedents to positive affect and negative affect. Design/methodology/approach The paper collects data from 236 respondents using Amazon MTurk, and conducts a PLS-SEM analysis to explore the role of positive affect and negative affect in rumor sharing contexts. Findings Both positive affect and negative affect were found to be significant factors leading to rumor sharing, furthermore positive affect was found to have a stronger influence on rumor sharing as compared to negative affect. The study also delineates the role of valence, involvement and credibility in rumor sharing scenarios, all of which have a strong role in shaping positive affect and negative affect. Originality/value The study is novel in using cognitive appraisal theory to illustrate the formation of positive affect and negative affect in rumor encounters. The study conclusively illustrates the role of cognitive appraisal and emotional experiences in the rumor propagation context, and advances the marketing scholarship's understanding significantly.", "journal": "MARKETING INTELLIGENCE & PLANNING", "category": "Business", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000466048800019", "keywords": "Rheumatoid arthritis; Hand disability; Disease activity; QuickDASH; Patient-reported outcomes", "title": "Validity and interpretability of the QuickDASH in the assessment of hand disability in rheumatoid arthritis", "abstract": "Objective of this study is to evaluate the construct validity and the interpretability of the shortened Disability of Arm, Shoulder and Hand Questionnaire (QuickDASH) in the assessment of rheumatoid arthritis (RA) hand disability. Consecutive RA patients were assessed through the QuickDASH and other function and disease activity indices, respectively, the Health Assessment Questionnaire-Disability Index (HAQ-DI) and the Recent-Onset Arthritis Disability questionnaire (ROAD). For each patient were evaluated the tender and swollen 28-joints counts. Interpretability was defined determining cut-off points of impairment in accordance to the Simplified Disease Activity Index (SDAI) definition of disease activity states. A total of 440 patients (89 men and 351 women, mean age of 57.0 +/- 12.7years) were enrolled. Following the SDAI definition, 98 patients (22.3%) resulted in REM, 115 subjects (26.1%) in LDA, 74 patients (16.8%) in MDA, and 153 subjects (34.8%) in HDA. Mean QuickDASH differed significantly between patients classified as remission (REM), low disease activity (LDA), moderate disease activity (MDA), or high disease activity (HDA) (p<0.001). High correlations were found comparing QuickDASH to composite indices of disease activity and of physical health function: of special interest are the correlations between the comparable dimension of the QuickDASH and the ROAD Upper Extremity Function (rho=0.876; p<0.001). The cut-off points for functional categories (SDAI categories as external criterion) resulted: no impairment13, 13<low impairment18.5, 18.5<moderate impairment31.5, and high impairment>31.5. QuickDASH is useful in clinical practice, for its ease of administration, and positively correlates with the disease activity. It may be a surrogate for evaluating upper extremity impairment, disability index and disease control in RA patients.", "journal": "RHEUMATOLOGY INTERNATIONAL", "category": "Rheumatology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000483808200025", "keywords": "Digital image correlation; Mask image planning; Shrinkage improvement; Stereolithography; Bio-scaffolds resin", "title": "A study of mask planning in projection-based stereolithography using digital image correlation", "abstract": "The use of mask-image-projection-based stereolithography process (MIP-SL) in additive manufacturing (AM)for 3D printing of bio-scaffold materials is becoming more widespread. In general, mask optimization, frame compensation, and resin material performance improvement were adopted to improve the curing accuracy of resin in MIP-SL process. In this paper, a mask optimization scheme based on image segmentation technology is developed, where the mask is divided into multiple little squares, hexagons, and triangles. The large-area warping deformation in MIP-SL is therefore transformed to a multiple small area distortion. Digital image correlation (DIC) method is applied to measure the effect of using mask image planning in bio-scaffolds MIP-SL process. A platform of bio-scaffolds resin shrinkage measurement based on DIC method was built to measure the resin deformation under distinct mask image planning. The results show that the deformation of the bio-scaffolds resin during the light-cured process will increase sharply at first and then tend to decrease partially. In addition, shrinkage of square segmentation and hexagon segmentation are reduced by 33% and 27%, respectively. However, the shrinkage of triangle segmentation is hardly reduced because the required time is twice as that of the other two methods.", "journal": "INTERNATIONAL JOURNAL OF ADVANCED MANUFACTURING TECHNOLOGY", "category": "Automation & Control Systems; Engineering, Manufacturing", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000485672100001", "keywords": "Competitive endogenous RNAs; Gene set enrichment analysis; Long non-coding RNAs; Salt sensitivity of blood pressure; Weighted gene co-expression network analysis", "title": "CeRNA network analysis and functional enrichment of salt sensitivity of blood pressure by weighted-gene co-expression analysis", "abstract": "Background. Salt sensitivity of blood pressure (SSBP) is an independent risk factor for cardiovascular disease. The pathogenic mechanisms of SSBP are still uncertain. This study aimed to construct the co-regulatory network of SSBP and data mining strategy based on the competitive endogenous RNA (ceRNA) theory. Methods. LncRNA and mRNA microarray was performed to screen for candidate RNAs. Four criteria were used to select the potential differently expressed RNAs. The weighted correlation network analysis (WGCNA) package of R software and target miRNA and mRNA prediction online databases were used to construct the ceRNA co-regulatory network and discover the pathways related to SSBP. Gene ontology enrichment, gene set enrichment analysis (GSEA) and KEGG pathway analysis were performed to explore the functions of hub genes in networks. Results. There were 274 lncRNAs and 36 mRNAs that differently expressed between salt-sensitive and salt-resistant groups (P <0.5). Using WGCNA analysis, two modules were identified (blue and turquoise). The blue module had a positive relationship with salt-sensitivity (R = 0.7, P < 0:01), high-density lipoprotein (HDL) (R = 0.53, P =0.02), and total cholesterol (TC) (R=0.55, P =0.01). The turquoise module was positively related with triglyceride (TG) (R=0.8, P <0.01) and low-density lipoprotein (LDL) (R = 0.54, P = 0.01). Furthermore, 84 ceRNA loops were identified and one loop may be of great importance for involving in pathogenesis of SSBP. KEGG analysis showed that differently expressed mRNAs were mostly enriched in the SSBP-related pathways. However, the enrichment results of GSEA were mainly focused on basic physical metabolic processes. Conclusion. The microarray data mining process based on WGCNA co-expression analysis had identified 84 ceRNA loops that closely related with known SSBP pathogenesis. The results of our study provide implications for further understanding of the pathogenesis of SSBP and facilitate the precise diagnosis and therapeutics.", "journal": "PEERJ", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000469560100001", "keywords": "Gender; Ghana; health behaviors; health literacy; sustainable development goals", "title": "Cruciferous Vegetable Consumption and Stomach Cancer: A Case-Control Study", "abstract": "Objective: To investigate the association between regular cruciferous vegetable intake and stomach cancer. Methods: A hospital-based, case-control study was conducted at Roswell Park Comprehensive Cancer Center in Buffalo, NY, which included 292 stomach cancer patients and 1168 cancer-free controls recruited between 1992 and 1998 as part of the Patient Epidemiology Data System (PEDS). Dietary and other epidemiologic and confounding variables were collected by questionnaire. Multivariable logistic regression analyses were utilized to estimate odds ratios (OR) and 95% confidence intervals (CI) for associations between usual pre-diagnostic cruciferous vegetable intake and stomach cancer, with adjustment for other stomach cancer risk factors and dietary characteristics. Results: We observed strong inverse associations between stomach cancer and highest versus lowest intakes of total cruciferous vegetables (OR = 0.59, 95% CI: 0.42-0.83), raw cruciferous vegetables (OR = 0.53, 95% CI: 0.38-0.73), raw broccoli (OR = 0.61, 95% CI: 0.43-0.86), raw cauliflower (OR = 0.51, 95% CI: 0.35-0.73), and Brussels sprouts (OR = 0.66, 95% CI = 0.48-0.91). Conclusions: These data suggest that consuming raw cruciferous vegetables may be associated with a lower odds of stomach cancer, even after considering other dietary characteristics.", "journal": "NUTRITION AND CANCER-AN INTERNATIONAL JOURNAL", "category": "Oncology; Nutrition & Dietetics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000485298000056", "keywords": "Generalized heat equation; Generalized Vigenre-type table on S-n group; Random key sequence; Image encryption", "title": "A new RGB image encryption using generalized heat equation associated with generalized Vigenere-type table over symmetric group", "abstract": "The primary aim of this paper is to provide an efficient encryption algorithm for RGB images. A new, fast and secure RGB image encryption using Generalized Heat Equation (GHE) associated with Generalized re-type Table over Symmetric Group S-n (GVTSG) is proposed. Encryption keys are obtained from Random Key Sequence (RKS). We have generated RKS and GVTSG with the help of GHE. By using this GHE, we are able to test the randomness over the generated key sequence by applying National Institute of Standards and Technology (NIST) statistical test suite. A formula for the keyspace has also been obtained and it is shown that this keyspace resists brute force attack. The proposed encryption algorithm provides high security and a larger key space. The keys for encryption or decryption consume less storage to store it on both sender and receiver ends. Robustness of the proposed algorithm has been analyzed and compared with other competing existing algorithms.", "journal": "MULTIMEDIA TOOLS AND APPLICATIONS", "category": "Computer Science, Information Systems; Computer Science, Software Engineering; Computer Science, Theory & Methods; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000472837000029", "keywords": "Multi-sensing system; Target tracking; Spatial and temporal characteristics; Decision-making strategy", "title": "Mycobacterium tuberculosis whole genome sequencing provides insights into the Manila strain and drug-resistance mutations in the Philippines", "abstract": "The Philippines has a high incidence of tuberculosis disease (TB), with an increasing prevalence of multidrug-resistant Mycobacterium tuberculosis (MDR-TB) strains making its control difficult. Although the M. tuberculosis \" Manila\" ancient lineage 1 strain-type is thought to be prevalent in the country, with evidence of export to others, little is known about the genetic diversity of circulating strains. By whole genome sequencing (WGS) 178 isolates from the Philippines National Drug Resistance Survey, we found the majority (143/178; 80.3%) belonged to the lineage 1 Manila clade, with the minority belonging to lineages 4 (European-American; n = 33) and 2 (East Asian; n = 2). A high proportion were found to be multidrug-resistant (34/178; 19.1%), established through highly concordant laboratory drug susceptibility testing and in silico prediction methods. Some MDR-TB isolates had near identical genomic variation, providing potential evidence of transmission. By placing the Philippine isolates within a phylogeny of global M. tuberculosis (n > 17,000), we established that they are genetically similar to those observed outside the country, including a clade of Manila-like strain-types in Thailand. An analysis of the phylogeny revealed a set of similar to 200 SNPs that are specific for the Manila strain-type, and a subset can be used within a molecular barcode. Sixty-eight mutations known to be associated with 10 anti-TB drug resistance were identified in the Philippine strains, and all have been observed in other populations. Whilst nine putative streptomycin resistance conferring markers in gid (8) and rrs (1) genes appear to be novel and with functional consequences. Overall, this study provides an important baseline characterisation of M. tuberculosis genetic diversity for the Philippines, and will fill a gap in global datasets and aid the development of a nation-wide database for epidemiological studies and clinical decision making. Further, by establishing a molecular barcode for detecting Manila strains it will assist with the design of diagnostic tools for disease control activities.", "journal": "SCIENTIFIC REPORTS", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000476796700013", "keywords": "Data center; digital coherent communication; adaptive equalization (AEQ); finite impulse response (FIR); multiple input multiple output (MIMO)", "title": "Hardware Efficient Adaptive Equalizer for Coherent Short-Reach Optical Interconnects", "abstract": "We propose a novel adaptive equalization (AEQ) algorithm for short-reach coherent optical interconnects targeting intra-datacenter applications. The algorithm consists of 1-tap 2 x 2 complex-valued multiple-input multiple-output (MIMO) finite impulse response (FIR) filter for polarization division de-multiplexing, four N-tap real-valued FIR filters for digital equalization, and a post three-tap T-spaced 4 x 4 real-valued MIMO FIR filter for the timing skew mitigation. Up to 8-km standard single-mode fiber (SSMF) transmission experiments with 28-Gbaud PDM-16QAM signals verifies that less than the 1-dB receiver sensitivity penalty can be tolerated in comparison with conventional 4 x 4 MIMO AEQ, while the number of multipliers can be reduced by similar to 59% if both algorithms use 25 taps. The tolerance of timing skew and chromatic dispersion for the AEQ algorithm is numerically investigated.", "journal": "IEEE PHOTONICS TECHNOLOGY LETTERS", "category": "Engineering, Electrical & Electronic; Optics; Physics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000473612400042", "keywords": "angle-resolved photoluminescence; nitride quantum wells; photo electrochemical etching; polaritons; strong coupling", "title": "Improved GaN Quantum Well Microcavities for Robust Room Temperature Polaritonics", "abstract": "New experiments on state-of-the-art nitride polariton microcavities are reported which consist of high-quality 3?/2-membranes with 8, 10, or 3GaN/Al(0.07)Ga0.93N quantum wells, embedded in all-dielectric distributed Bragg reflectors (DBRs). The optical density of the membranes, extracted from transmittance measurements taking into account standing wave effects, is found proportional to the number of quantum wells. At room temperature, the optical density per quantum well at the exciton wavelength reaches values around 3.5%. It is demonstrated that a top DBR consisting of four alternating pairs of SiO2/Ta2O5 is sufficient to obtain robust polariton features from these membranes at room temperature, with a Rabi splitting of 36 and 71?meV for the 10 and 38 quantum well microcavities, respectively.", "journal": "PHYSICA STATUS SOLIDI B-BASIC SOLID STATE PHYSICS", "category": "Physics, Condensed Matter", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000479122900008", "keywords": "Athletic injuries; football; risk factors; soft tissue injuries; sports medicine", "title": "Functional Movement Screening and injury risk in elite adolescent rugby league players", "abstract": "Research is limited as to whether Functional Movement Screen scores relate to non-contact injury risk in rugby league players. This cohort study investigates whether the Functional Movement Screen score predicts non-contact injuries in elite adolescent rugby league players. Australian adolescent rugby league players (n = 52; mean age 16.0 +/- 1.0 years) from one club participated in this study. Functional Movement Screen scores, height, and mass were collected at the beginning of the preseason. Training, match exposure, and injury incidence data (non-contact match and training injuries with three levels of severity) were recorded for each individual athlete throughout the season. Linear and logistic regression analyses were conducted to investigate the association between Functional Movement Screen score (continuous score, <= 14 or > 14, and three subscores) and injury risk, whilst controlling for exposure time. The mean Functional Movement Screen score for the sample was 13.4 (95% CI: 11.0-14.0). A total of 72 non-contact injuries were recorded (incidence rate: 18.7 per 1000 exposure hours; 95% CI: 11.6-24.8). There were no statistically significant associations between non-contact injury and Functional Movement Screen score for any of the analyses conducted. Our results suggest that the Functional Movement Screen does not reflect non-contact injury risk in elite adolescent rugby league players. Further research should investigate whether a more sport-specific movement screen in the preseason can more effectively predict injury risk in this population.", "journal": "INTERNATIONAL JOURNAL OF SPORTS SCIENCE & COACHING", "category": "Hospitality, Leisure, Sport & Tourism; Psychology, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000471287500008", "keywords": "intraindividual variability; epidemiology; sleep diaries; medical conditions; public health; total sleep time", "title": "Vancomycin-resistant Enterococcus (VRE) outbreak in a neonatal intensive care unit and special care nursery at a tertiary-care hospital in AustraliaA retrospective case-control study", "abstract": "Objective:We investigated the risk factors and origins of the first known occurrence of VRE colonization in the neonatal intensive care unit (NICU) at the Canberra Hospital.Design:A retrospective case-control study.Setting:A 21-bed neonatal intensive care unit (NICU) and a 15-bed special care nursey (SCN) in a tertiary-care adult and pediatric hospital in Australia.Patients:All patients admitted to the NICU and SCN over the outbreak period: January-May 2017. Of these, 14 were colonized with vancomycin-resistant Enterococcus (VRE) and 77 were noncolonized.Methods:Demographic and clinical variables of cases and controls were compared to evaluate potential risk factors for VRE colonization. Whole-genome sequencing of the VRE isolates was used to determine the origin of the outbreak strain.Results:Swift implementation of wide-ranging infection control measures brought the outbreak under control. Multivariate logistic regression revealed a strong association between early gestational age and VRE colonization (odds ratio [OR], 3.68; 95% confidence interval [CI], 1.94-7.00). Whole-genome sequencing showed the isolates to be highly clonal Enterococcus faecium ST1421 harboring a vanA gene and to be closely related to other ST1421 previously sequenced from the Canberra Hospital and the Australian Capital Territory.Conclusion:The colonization of NICU patients was with a highly successful clone endemic to the Canberra Hospital likely introduced into the NICU environment from other wards, with subsequent cross-contamination spreading among the neonate patients. Use of routine surveillance screening may have identified colonization at an earlier stage and have now been implemented on a 6-monthly schedule.", "journal": "INFECTION CONTROL AND HOSPITAL EPIDEMIOLOGY", "category": "Public, Environmental & Occupational Health; Infectious Diseases", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000474925600033", "keywords": "death; emergency department; geriatric; influenza; mortality; prediction", "title": "The Reading House: A Children's Book for Emergent Literacy Screening During Well-Child Visits", "abstract": "BACKGROUND:The American Academy of Pediatrics recommends literacy promotion and developmental assessment during well-child visits. Emergent literacy skills are well defined, and the use of early screening has the potential to identify children at risk for reading difficulties and guide intervention before kindergarten.METHODS:The Reading House (TRH) is a children's book designed to screen emergent literacy skills. These are assessed by sharing the book with the child and using a 9-item, scripted scoring form. Get Ready to Read! (GRTR) is a validated measure shown to predict reading outcomes. TRH and GRTR were administered in random order to 278 children (mean: 43.1 5.6 months; 125 boys, 153 girls) during well-child visits at 7 primary care sites. Parent, child, and provider impressions of TRH were also assessed. Analyses included Rasch methods, Spearman-rho correlations, and logistic regression, including covariates age, sex, and clinic type.RESULTS:Psychometric properties were strong, including item difficulty and reliability. Internal consistency was good for new measures (r(Co-alpha) = 0.68). The mean TRH score was 4.2 (+/- 2.9; range: 0-14), and mean GRTR was 11.1 (+/- 4.4; range: 1-25). TRH scores were positively correlated with GRTR scores (r(s) = 0.66; high), female sex, private practice, and child age (P < .001). The relationship remained significant controlling for these covariates (P < .05). The mean TRH administration time was 5:25 minutes (+/- 0:55; range: 3:34-8:32). Parent, child, and provider impressions of TRH were favorable.CONCLUSIONS:TRH is a feasible, valid, and enjoyable means by which emergent literacy skills in 3- and 4-year-old children can be directly assessed during primary care.", "journal": "PEDIATRICS", "category": "Pediatrics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000484875200005", "keywords": "Fusion techniques; Fused image quality assessment; Spatial quality; Spectral quality", "title": "An effective approach to selecting the appropriate pan-sharpening method in digital change detection of natural ecosystems", "abstract": "In an image fusion process, the spatial resolution of a multispectral image is improved by a panchromatic band. However, due to the spatial and spectral resolution differences between these two data sets, the enhanced image may have two distortions, spatial and spectral. Therefore, to evaluate the efficiency of the pan-sharpening method, the status of these two types of distortions is examined. Unfortunately, there is still no developed acceptance index that can thoroughly investigate the quality of the pansharpened image; moreover, most of the proposed methods for reviewing the quality of output images have been developed with an emphasis on the residential area. Accordingly, to assess the quality of the pansharpened image in this study, we evaluated highly effective conventional methods, such as visual examinations, quantitative evaluation and impact analysis regarding the change detection process of mangrove forests. Finally, we suggested a simple yet efficient approach for such research in natural ecosystems. In the proposed method, based on the nature of the ecosystem, a spectral vegetation index is applied to the pansharpened images; the spectral quality of the images is further evaluated based on two parameters, 1) the areas under the curves of the histogram of the spectral vegetation index in the natural ecosystem region and 2) its centroid. The spatial quality of the pansharpened images is evaluated through implementing of two transects perpendicular to each other in the images of the spectral vegetation index, and creating a spatial deviation on them. With expert reviews and visual evaluation of the pansharpened images, the proposed method, especially in natural ecosystems, has more advantages as regards assessing the quality of the fused images. Based on the evaluations, among 11 methods of pansharpened including Ehlers Fusion, FuzeGO, Gram-Schmidt, HPF, HCS, PCA, Modified IHS, Brovey Transform, Projective Resolution Merge, Wavelet IHS, and Wavelet PCA; the HPF method the Brovey Transform and Modified IHS methods respectively showed the best performance in the digital change detection of Mangrove forests.", "journal": "ECOLOGICAL INFORMATICS", "category": "Ecology", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000467552100030", "keywords": "liver iron quantification; MRI; FerriScan; R2*mapping; iron heterogeneity", "title": "Association of modifiable risk factors and IL-6, CRP, and adiponectin: Findings from the 1993 Birth Cohort, Southern Brazil", "abstract": "Background The literature on the relationship between lifestyle behaviors and inflammatory markers is scarce. Methods A birth cohort was followed since birth up to 22 years in Southern Brazil. Interleukin-6 (IL-6), C-reactive protein (CRP) and adiponectin were measured in nonfasting blood samples drawn at 18 and 22 years of age. Exposures including smoking, alcohol intake, physical inactivity and obesity, were collected at 15, 18 and 22 years. Cross sectional analyses were based on the number of follow-up visits with these exposures and the association with IL-6, CRP and adiponectin at 22 years old. We also carried out a longitudinal Generalized Least Squares (GLS) random-effects analysis with outcomes at 18 and at 22 years old. All analyses were adjusted for several covariates. Results The sample comprised 3,479 cohort members at 22 years. The presence of obesity at >= 2 follow-ups showed the highest mean values (SE) for IL-6 [2.45 (1.05)] and CRP [3.74 (1.11)] and the lowest mean value for adiponectin [8.60 (0.37)] (adjusted analyses, females) compared with other exposures; the highest mean of IL-6 [1.65 (1.05)] and CRP [1.78 (1.11)] and the lowest mean of adiponectin [9.98 (0.38)] were for the number of follow-ups with >= 2 exposures compared to those with no exposures at any follow-up (adjusted analyses, females). The longitudinal analysis showed an increase in obesity associated with IL-6 and CRP in both sexes and an inverse association with adiponectin in females; smoking (in males) was associated with IL-6 and CRP, harmful alcohol intake was associated with CRP in males, and increased in physical activity was inversely associated with CRP in men. Conclusion We concluded that obesity is the main exposure positively associated with IL-6 and CRP and inversely associated with adiponectin (mainly in females). Smoking is also associated with these markers in the longitudinal analysis (in males).", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000482442800018", "keywords": "cyanobacterial blooms; Landsat; lakes; reservoirs; time series satellite images; the middle-lower Yangtze River basin", "title": "Increasing Outbreak of Cyanobacterial Blooms in Large Lakes and Reservoirs under Pressures from Climate Change and Anthropogenic Interferences in the Middle-Lower Yangtze River Basin", "abstract": "In recent decades, the increasing frequency and severity of cyanobacterial blooms in recreational lakes and water supply reservoirs have become a great concern to public health and a significant threat to the environment. Cyanobacterial bloom monitoring is the basis of early warning and treatment. Previous research efforts have always focused on monitoring blooms in a few specific lakes in China using moderate resolution imaging spectroradiometer (MODIS) images, which are available for the years 2000 onward. However, the lack of overall information on long-term cyanobacterial blooms in the lakes and reservoirs in the middle-lower Yangtze River (MLYR) basin is an obstacle to better understanding the dynamics of cyanobacterial blooms at a watershed scale. In this study, we extracted the yearly coverage area and frequency of cyanobacterial blooms that occurred from 1990 to 2016 in 30 large lakes and 10 reservoirs (inundation area >50 km(2)) by using time series Landsat satellite images from Google Earth Engine (GEE). Then, we calculated the cyanobacterial bloom area percentage (CAP) and the cyanobacterial bloom frequency index (CFI) and analyzed their inter-annual variation and trends. We also investigated the main driving forces of changes in the CAP and CFI in each lake and reservoir. We found that all reservoirs and more than 60% of lakes exhibited an increasing frequency and coverage area of cyanobacterial blooms under the pressures of climate change and anthropogenic interferences. Reservoirs were more prone to be affected by fertilizer consumption from their regional surroundings than lakes. High temperatures increased blooms of cyanobacteria, while precipitation in the lake and reservoir regions somewhat alleviated blooms. This study completes the data records of cyanobacterial blooms in large lakes and reservoirs located in hotspots of the MLYR basin and provides more baseline information before 2000, which will present references for water resource management and freshwater conservation.", "journal": "REMOTE SENSING", "category": "Environmental Sciences; Geosciences, Multidisciplinary; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000472239600016", "keywords": "Rat pups; Milk fat globule membrane; Phospholipid concentrate; Sialic acid; Brain development; Gene expression; Neurodevelopment", "title": "Effects of milk fat globule membrane and its various components on neurologic development in a postnatal growth restriction rat model", "abstract": "Background: Milk fat globule membrane (MFGM) is a component of breast milk that consists of glycosylated membrane-bound proteins, polar lipids and carbohydrates originating from the mammary gland plasma membrane. A commercially available bovine MFGM added to infant formula has been shown to improve cognitive development in infants at 12 months of age. Objective: Considering that MFGM is a complex mixture, our aim was to determine which component(s) may be leading to these cognitive outcomes. Methods: Growth-restricted rat pups were supplemented with one of five treatments: (a) bovine MFGM, (b) bovine phospholipid concentrate (PL), (c) sialic acid (SIA) at 200 mg/kg body weight (bw) SIA100, (d) SIA at 2 mg/kg bw and (e) nonfat milk as control. Pups were randomized, cross-fostered into litters of 17 pups per dam and supplemented from postnatal day (PD) 2 to PD 21. The following behavioral tests were performed at adulthood: T-Maze Spontaneous Alternation, Novel Object Recognition and Morris Water Maze. Hippocampus was isolated at PD14 and PD21. Expression of four genes were measured including brain-derived neurotrophic factor (BDNF), dopamine receptor 1, (Drd 1), glutamate receptor (GIuR-1) and ST8 alpha-N-acetyl-neuraminide alpha-2,8-sialytransferase 4 (St8Sia4). Following behavioral testing, brains were collected for nonbiased stereology. Results: Increased expression of genes due to supplementation was most pronounced at the PD14 time point. The MFGM group exhibited higher T-Maze scores compared to the SIA group (P=.01), whereas the SIA100 group visited the novel object more frequently than the MFGM group in the Novel Object test (P=.02). No differences due to supplementation were found in the Morris Water Maze or nonbiased stereology. Conclusions: In this trial, MFGM, compared to its individual components, had the largest impact on neurodevelopment in rat pups through up-regulation of genes and improved T-Maze scores compared to the SIA group. (C) 2019 Elsevier Inc. All rights reserved.", "journal": "JOURNAL OF NUTRITIONAL BIOCHEMISTRY", "category": "Biochemistry & Molecular Biology; Nutrition & Dietetics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000474325900042", "keywords": "Neutron noise; Pulse reactor; IBR-2M; Basic element method", "title": "Neutron noise analysis using the basic element method", "abstract": "Neutron noise spectra in nuclear reactors are a convolution of multiple induced reactivities. Under normal IBR-2M reactor operating conditions the full noise range can reach +/- 22% with respect to average power. Slow changes in average power (induced for instance by the control rods) are thereby much smaller than the neutron noise, however they are of importance in conducting the safe operation of the reactor. The problem we address is the separation of neutron noise from the slow power-changes. For this task we used an algorithm based on the basic element method (BEM) developed at Laboratory of Information Technologies - LIT JINR. The algorithm was applied to both static and dynamic states of the reactor in the 0-2 MW power range. The average processing time for one point on a x86_64 Intel Core i5-4570 Sandy Bridge processor at 3.20 GHz, was approximately 50 mu s, which allows the use of the MSPPA-6 algorithm in real time. (C) 2019 Elsevier Ltd. All rights reserved.", "journal": "ANNALS OF NUCLEAR ENERGY", "category": "Nuclear Science & Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000470828200001", "keywords": "Breast milk production; Domperidone; Mothers of preterm infants; Galactogogue", "title": "Role of days postdelivery on breast milk production: a secondary analysis from the EMPOWER trial", "abstract": "BackgroundWith an increasing demand for mother's own milk to be viewed as a primary source of nutritional support in the care of very small and preterm infants, mothers of preterm infants may be at risk of expressing suboptimal amounts of milk. The use of a galactogogue is often considered when these mothers are still having challenges in breast milk production.MethodsFor this analysis, the study participants were the 90 mothers who participated in the EMPOWER trial and, at the time of randomization, were stratified by days post-delivery, 8-14days and 15-21days. The primary outcome measure was the proportion of mothers in each of the days post-delivery groups who achieved a 50% increase in breast milk volume on day 14 of the study treatment period.ResultsThere was no significant difference in the proportion of mothers in the 8-14days group (75.0%) who achieved a 50% increase in breast milk volume on day 14 of the study treatment period.compared to those in the 15-21days group (60.9%), OR 1.93 (95% CI 0.78, 4.76; p=0.15). Because comorbidities and exposure to antenatal corticosteroids between the groups of mothers were viewed as potential confounders, a logistic regression was performed after controlling for these two variables with the adjusted OR being 1.84 (0.73, 4.64; p=0.19).ConclusionsThis secondary analysis was able to demonstrate that mothers of very preterm infants, <30weeks gestation at birth, were able to respond to the study treatment in a similar fashion regardless of timing of entry and exposure to domperidone. In the presence of a suboptimal breast milk production by the end of the first week postpartum, below 250ml/kg/d based on infant birth weight, a 14day treatment of domperidone could be considered to augment breast milk production.Trial registrationEMPOWER has been registered at http://www.clinicaltrials.gov (identifier NCT 01512225) on January 10, 2012.", "journal": "INTERNATIONAL BREASTFEEDING JOURNAL", "category": "Obstetrics & Gynecology; Pediatrics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000473378000010", "keywords": "HEVC; Fast encoding; Pre-analysis; Partitioning; Mode decision; Inter prediction", "title": "Adaptive inter CU partitioning based on a look-ahead stage for HEVC", "abstract": "High Efficiency Video Coding (HEVC) has become the state-of-the-art video coding standard. It outperforms its predecessors by the introduction of new coding tools, such as the new quadtree-based partitioning scheme called the coding tree unit (CTU), which enables a more flexible partitioning of the input frames. However, selecting the optimal partitioning requires the evaluation of numerous possibilities, which involves long computing times that hinder the applicability of the standard in real-world scenarios. With this in mind, the main focus of this paper is on tackling this complexity by means of a fast partitioning and mode decision algorithm based on a look-ahead stage. This stage performs a preliminary motion estimation that provides the motion costs used to build the least-cost quadtree, which is in turn utilized to conduct the encoding itself. On the basis of this quadtree, the encoder may decide to terminate the partitioning early, or to evaluate additional depth levels adaptively. Furthermore, the encoder may omit some prediction modes according to the costs estimated by the look-ahead stage. A thorough experimental evaluation of the algorithm shows that it can reduce the encoding time by 65.33%, at the expense of only a 1.35% BD-rate for the random access configuration. Combined with a fast inter prediction algorithm, this reduction can rise to 70.55%, while the coding efficiency is maintained at a 1.83% BD-rate. When compared with other related works, these results display an excellent trade-off between the two variables.", "journal": "SIGNAL PROCESSING-IMAGE COMMUNICATION", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000468855800012", "keywords": "Metal implants; Dose calculation algorithms; Monte Carlo; Collapsed cone convolution superposition", "title": "Treatment planning dose accuracy improvement in the presence of dental implants", "abstract": "Streaking artifacts in computed tomography (CT) scans caused by metallic dental implants (MDIs) can lead to inaccuracies in dose calculations. This study quantifies and compares the effect of MDIs on dose distributions using the collapsed cone convolution superposition (CCCS) and Monte Carlo (MC) algorithms, with and without correcting for the density of the MDIs. Ion chamber measurements were taken to test the ability of the algorithms in Pinnacle(3) and Monaco to calculate dose near high-Z materials. Nine previously treated patients with head and neck cancer were included in this study. The MDI and the streaking artifacts on the CT images were carefully contoured. For each patient, a plan was optimized and calculated using the Pinnacle(3) treatment planning system (TPS). Two dose calculations were performed for each patient: one with overridden densities of the MDI and CT artifacts and one without overridden densities of the MDI and CT artifacts. The plans were then exported to the Monaco TPS and recalculated for the same number of monitor units (MUs) using its MC dose calculation algorithm. The changes in dose to the planning target volume (PTV) and surrounding healthy tissues were examined between all the plans using VelocityAl. For the ion chamber measurements, when correct density information was used, Monaco was within 3% of the measured values, whereas the doses calculated in Pinnacle(3) varied up to 7%. The CCCS algorithm in Pinnacle(3) calculated only a significant decrease in PTV coverage for 1 patient when the densities were overridden. The MC algorithm in Monaco was able to calculate a significant change in PTV coverage for five of the patients when the density was overridden. Additionally, when healthy tissues affected by streaking artifacts were assigned the correct density, cumulative (from all the fractions) point doses increased up to 46.2 Gy. Not properly accounting for MDIs can impact both the high-dose regions (PTVs) and surrounding healthy tissues. This study demonstrates that if MDIs and the artifacts are not appropriately accounted for by contouring and assigning to them the correct density, there is a potential risk of compromising the quality of the plan regarding PTV coverage and dose to healthy tissues. (C) 2018 American Association of Medical Dosimetrists. Published by Elsevier B.V. All rights reserved.", "journal": "MEDICAL DOSIMETRY", "category": "Oncology; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000480472500001", "keywords": "Cardiovascular disease; ASCVD; Refugee; Syria; Lebanon; Adherence; Humanitarian assistance; Secondary prevention", "title": "Cardiovascular Disease among Syrian refugees: a descriptive study of patients in two Medecins Sans Frontieres clinics in northern Lebanon", "abstract": "Background Literature on the burden and management of atherosclerotic cardiovascular disease (ASCVD) in humanitarian settings is limited. This study aimed to describe patient characteristics and explore both service use and use of recommended secondary prevention drugs in Syrian refugee patients with ASCVD attending two Medecins Sans Frontieres (MSF) clinics in Lebanon. Methods This study comprised a cross-sectional survey of ASCVD patients attending either MSF clinic over a four-week period in early 2017. Using descriptive statistics, we explored patient demographic characteristics, cardiovascular risk factors and assessed ASCVD secondary prevention medication prescription and patient adherence with a 7-day self-report scale. A retrospective study of routine clinical data explored workload and trends in patient loss to follow-up. We performed logistic regression modelling to explore risk factors for loss to follow-up. Results We included 514 patients with ASCVD in the cross-sectional study, performed in 2017. Most (61.9%) were male and mean age was 60.4 years (95% CI, 59.6-61.3). Over half (58.8%) underwent revascularization and 26.1% had known cerebrovascular disease. ASCVD risk factors included 51.8% with diabetes and 72.2% with hypertension. While prescription (75.7 to 98.2%) and self-reported adherence rates (78.4 to 93.9%) for individual ASCVD secondary prevention drugs (ACE-inhibitor, statin and antiplatelet) were high, the use of all three was low at 41.3% (CI95%: 37.0-45.6). The 5-year retrospective cohort study (ending April 2017) identified 1286 patients with ASCVD and 16,618 related consultations (comprising 24% of all NCD consultations). Over one third (39.7%) of patients were lost to follow-up, with lower risk among men. Conclusions The burden of ASCVD within MSF clinics in Lebanon is substantial. Although prescription and adherence of individual secondary prevention drugs is acceptable, overall use of the three recommended drugs is suboptimal. Loss to follow-up rates were high. Further studies are needed to evaluate innovative strategies to increase the use of the multiple recommended drugs, and to increase the retention of patients with ASCVD in the care system.", "journal": "CONFLICT AND HEALTH", "category": "Public, Environmental & Occupational Health", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000482188200007", "keywords": "Asthma; cost of illness; direct medical cost; military hospital; out-of-pocket payment; Vietnam", "title": "The economic burden attributable to asthmatic inpatients and outpatients in a military hospital, Vietnam: A retrospective 5-year analysis", "abstract": "Objectives: Asthma is a disease that causes significant health and economic burdens worldwide. The prevalence and incidence of asthma have been rising around the world over recent decades. The current study is to capture the direct medical costs of inpatient and outpatient asthma treatment for the period from 2013 to 2017. Methods: This study was conducted at Military Hospital 175 in Vietnam. The study was performed from the patient and social insurance perspective, which means all types of costs were identified and measured based on patients' healthcare insurance. Cost analysis was measured using the medical records for estimating the economic burden of asthma. The study adopted descriptive statistics and bootstrap techniques to calculate asthma-related costs as well as analyze the background characteristics of asthma patients. Results: The average outpatient and inpatient costs were US$64.90 and US$141.20, respectively, over the period from 2013 to 2017, for which out-of-pocket payments accounted for 10-12%. Medications, specifically asthma controller drugs, were the key driver leading to the substantial burden of direct medical costs for treating asthma. The cost burden was also significantly higher for adult patients compared to children. Conclusions: Asthma continues to be a concerning problem in Vietnam. The economic impact of either preventive or promotive health interventions that can reduce the prevalence of asthma can be predicted from the statistics found in this research. Moreover, this data will help policymakers plan and allocate national expenditures for asthma treatment in a more rational way.", "journal": "JOURNAL OF THE PAKISTAN MEDICAL ASSOCIATION", "category": "Medicine, General & Internal; Medicine, Research & Experimental", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000475398600003", "keywords": "Design of experiments; computer experiments; space-filling; constrained space; enhanced stochastic evolutionary algorithm", "title": "Space-filling experimental designs for constrained design spaces", "abstract": "Conventional space-filling experimental design provides uniform coverage of a hypercube design space. When constraints are imposed, the results may contain many infeasible points. Simply omitting these points leads to fewer feasible points than desired and a design of experiments that is not optimally distributed. In this research, an adaptive method is developed to create space-filling points in arbitrarily constrained spaces. First, a design space reconstruction method is developed to reduce the invalid exploration space and enhance the efficiency of experimental designs. Then, a synthetic criterion of uniformity and feasibility is proposed and optimized by the enhanced stochastic evolutionary method to obtain the initial sampling combination. Finally, an adaptive adjustment strategy of design levels is constructed to obtain the required number of feasible points. Various test cases with convex and non-convex, connected and non-connected design spaces are implemented to verify the efficacy of the proposed method.", "journal": "ENGINEERING OPTIMIZATION", "category": "Engineering, Multidisciplinary; Operations Research & Management Science", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000474583700003", "keywords": "Evolutionary algorithm; flexible job shop scheduling; review; swarm intelligence", "title": "A Review on Swarm Intelligence and Evolutionary Algorithms for Solving Flexible Job Shop Scheduling Problems", "abstract": "Flexible job shop scheduling problems (FJSP) have received much attention from academia and industry for many years. Due to their exponential complexity, swarm intelligence (SI) and evolutionary algorithms (EA) are developed, employed and improved for solving them. More than 60% of the publications are related to SI and EA. This paper intents to give a comprehensive literature review of SI and EA for solving FJSP. First, the mathematical model of FJSP is presented and the constraints in applications are summarized. Then, the encoding and decoding strategies for connecting the problem and algorithms are reviewed. The strategies for initializing algorithms? population and local search operators for improving convergence performance are summarized. Next, one classical hybrid genetic algorithm (GA) and one newest imperialist competitive algorithm (ICA) with variables neighborhood search (VNS) for solving FJSP are presented. Finally, we summarize, discus and analyze the status of SI and EA for solving FJSP and give insight into future research directions.", "journal": "IEEE-CAA JOURNAL OF AUTOMATICA SINICA", "category": "Automation & Control Systems", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000470341600013", "keywords": "Land revenue; Market town-bias; Rural land reform; Rural development; China", "title": "Blessing or curse? Impact of land finance on rural public infrastructure development", "abstract": "While land revenue has played an important role in China's fiscal revenue and expenditure, little is known of the linkage between land revenue and rural infrastructure inputs and its impact on the landscape of fiscal spending on rural infrastructure. To investigate this further, we study the effect of land finance on rural infrastructure inputs by a regression analysis of a panel data set collected for 30 provincial-level regions over a ten-year period. The results show that the effect is jointly determined by the amount of land revenue and degree of reliance on land finance. Increases in land revenue contributed to an increase in rural infrastructure inputs while an increase in reliance on land finance decreased rural infrastructure input. Whether this is a blessing or a curse depends on how dependent the region is on land revenue and the region's long-term development strategy. Although land finance has mixed effects on rural infrastructure input, it does not influence the \"market town-biased\" allocation of fiscal spending on rural infrastructure. Land finance can be used to support rural infrastructure development. The challenge for land financed rural public infrastructure development is how to augment land revenue and avoid a heavy dependence on land finance in the long run.", "journal": "LAND USE POLICY", "category": "Environmental Studies", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000467240800197", "keywords": "cancer risk; gout; hyperuricemia; meta-analysis", "title": "Hyperuricemia and gout are associated with cancer incidence and mortality: A meta-analysis based on cohort studies", "abstract": "The association between hyperuricemia or gout and cancer risk has been investigated in various published studies, but their results are conflicting. We conducted a meta-analysis to investigate whether hyperuricemia or gout was associated with the cancer incidence and mortality. Linear and nonlinear trend analyses were conducted to explore the dose-response association between them. The pooled relative risk (RR) and 95% confidence interval (CI) were used to evaluate cancer risk. A total of 24 articles (33 independent studies) were eligible for inclusion. When compared participants with the highest SUA (hyperuricemia) levels and those with the lowest SUA levels, the pooled RR was 1.08 (95% CI, 1.04-1.12), it was significantly associated among males but not among females (males, RR=1.07; 95% CI, 1.03-1.11; females, RR=1.06; 95% CI, 0.96-1.17). Hyperuricemia increased total cancer mortality (RR=1.15; 95% CI, 1.05-1.26), but a significant association was observed in females rather than in males (females: RR=1.26; 95% CI, 1.09-1.45; males, RR=1.02; 95% CI, 0.80-1.30). Linear relationships of SUA levels with overall cancer incidence (p for nonlinearity=0.238) and overall cancer mortality (p for nonlinearity=0.263) were identified. However, 1 mg/dL increment in SUA levels was weakly significant in overall cancer incidence (RR=1.01; 95% CI, 1.01-1.01) but not associated with overall cancer mortality (RR=1.01; 95% CI, 0.99-1.03). Gout was significantly associated with increased cancer incidence (RR=1.19; 95% CI, 1.12-1.25). In conclusion, Hyperuricemia or gout was associated with higher cancer incidence and mortality. Though a potential linear relationship between them was found, we'd better treat this result with caution.", "journal": "JOURNAL OF CELLULAR PHYSIOLOGY", "category": "Cell Biology; Physiology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000481434300013", "keywords": "Lagrangian mechanics; Discrete model of cable; Control of under-actuated systems", "title": "Modelling and control of a spatial dynamic cable", "abstract": "We study the problem of dynamically controlling the shape of a cable that is fixed at one end and attached to an actuated robot at another end. This problem is relevant to unmanned aerial vehicles (UAVs) tethered to a base. While rotorcrafts, such as quadcopters, are agile and versatile in their applications and have been widely used in scientific, industrial, and military applications, one of the biggest challenges with such UAVs is their limited battery life that make the flight time for a typical UAVs limited to twenty to thirty minutes for most practical purposes. A solution to this problem lies in the use of cables that tether the UAV to a power outlet for constant power supply. However, the cable needs to be controlled effectively in order to avoid obstacles or other UAVs. In this paper, we develop methods for controlling the shape of a cable using actuation at one end. We propose a discrete model for the spatial cable and derive the equations governing the cable dynamics for both force controlled system and position controlled system. We design a controller to control the shape of the cable to attain the desired shape and perform simulations under different conditions. Finally, we propose a quasi-static model for the spatial cable and discuss the stability of this system and the proposed controller.", "journal": "ACTA MECHANICA SINICA", "category": "Engineering, Mechanical; Mechanics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000468033700019", "keywords": "Dental arch; Dental arch thickness; Cubic spline; Image enhancement; Panoramic image", "title": "Automatic reconstruction method for high-contrast panoramic image from dental cone-beam CT data", "abstract": "Background and objective: Panoramic images reconstructed from dental cone beam CT (CBCT) data have been effectively used in dental clinics for disease diagnosis. Panoramic images generally have low contrast because excessive non-interest tissues participate in the reconstruction, which may affect the diagnosis. In this study, we developed a fully automatic reconstruction method to improve the global and detail contrast of panoramic images. Methods: The proposed method consists of dental arch thickness detection, image synthesis, and image enhancement. First, the dental arch thickness is detected from an axial maximum intensity projection (MIP) image generated from the axial slices containing the teeth to reduce non-interest tissues in panoramic image reconstruction. Then, a new synthesis algorithm is proposed at image synthesis stage to reduce the effect of non-interest tissues on image contrast. Finally, an image enhancement algorithm is applied to the synthesized image to improve the detail contrast of the final panoramic image. Results: A total of 129 real clinical dental CBCT data sets were used to test the proposed method. The panoramic images generated by three methods were subjectively scored by three experienced dentists who were blinded to the generated method. The evaluation of image contrast included the maxillary, mandible, teeth, and particular region (root canal, crown reconstruction, implants, and metal brackets). The overall image contrast score revealed that the proposed method scored the highest of 11.03 +/- 2.46, followed by the ray sum and x-ray methods with corresponding scores of 6.4 +/- 1.65 and 5.35 +/- 1.56. The results of expert subjective scoring indicated that the image contrast of the panoramic image generated by the proposed method is higher than those of existing methods. Conclusions: The proposed method provides a quick, effective and robust solution to improve the global and detail contrast of the panoramic image generated from dental CBCT data. (C) 2019 Elsevier B.V. All rights reserved.", "journal": "COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE", "category": "Computer Science, Interdisciplinary Applications; Computer Science, Theory & Methods; Engineering, Biomedical; Medical Informatics", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000467315700015", "keywords": "Magnetic resonance imaging; gray matter; white matter; conus medullaris; cross-sectional area", "title": "Gray vs. White Matter Segmentation of the Conus Medullaris: Reliability and Variability in Healthy Volunteers", "abstract": "BACKGROUND AND PURPOSE Magnetic resonance imaging (MRI)-derived spinal cord (SC) gray and white matter (GM/WM) volume are useful indirect measures of atrophy and neurodegeneration over time, typically obtained in the upper SC. Neuropathological evidence suggests that in certain neurological conditions, early degeneration may occur as low as the sacral SC. In this study, the feasibility of GM/WM segmentation of the conus medullaris (CM) was assessed in vivo. METHODS Twenty-three healthy volunteers (11 female, mean age 47 years) underwent high-resolution 3T MRI of the CM using a 3-dimensional fast field echo sequence. Reproducibility of the volume measurements was assessed in 5 subjects (2 female, 25-37 years) by one rater who repeated the analysis 3 times and also with 2 additional raters working independently in order to calculate the intra- and interrater coefficient of variation (COV), respectively. Furthermore, the influence of age, gender, spine and SC metrics on tissue-specific measures of the CM was investigated. RESULTS Volumetric CM analyses (N = 23) for the SC, GM, and WM revealed a mean (SD) total volume of CM-TV = 1746.9 (296.7) mm(3), CM-GM-TV = 731.2 (106.0) mm(3), and CM-WM-TV = 1014.6 (211.3) mm(3), respectively. The intra-rater COV for measuring the CM-TV and CM-GM-TV was 3.38% and 7.42%, respectively; the interrater COV was 3.43% and 10.80%, respectively. Using age, gender, spine and SC metrics in regression models substantially reduced group variability for CM-TV, CM-WM-TV, and CM-GM-TV by up to 39.2%, 42.7%, and 21.2%, respectively. CONCLUSIONS The results from this study demonstrate the feasibility of obtaining tissue-specific volume measurements in the CM by means of MRI with good reproducibility and provide normative data for future applications in neurological diseases affecting the lower SC.", "journal": "JOURNAL OF NEUROIMAGING", "category": "Clinical Neurology; Neuroimaging; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000466259500001", "keywords": "GnRH; Prostaglandin F-2 alpha; CIDR; Bos indicus; Synchronization", "title": "Ovarian follicular and luteal characteristics in Bos indicus-influenced beef cows using prostaglandin F-2 alpha with or without GnRH at the onset of the 5-day CO-Synch plus controlled internal drug release (CIDR) protocol", "abstract": "A modification of the standard 5-day CO-Synch + CIDR procedure (5-day Bee Synch + CIDR; Bee Synch), developed for use in Bos indicus-influenced cows, utilizes the addition of prostaglandin F2 alpha (PGF) on Day 0 of the protocol to eliminate mature corpora lutea (CL) and fixed-time AI (FTAI) at 66 h. Objectives were to test the hypothesis that elimination of GnRH on Day 0 (GnRH1) does not impact significantly the synchronized development of a dominant follicle for presumptive FTAI. Seventy-one estrous cycling Brangus and Brahman x Hereford suckled cows were used in two replicates (35-36/replicate). Following stratification, cows were assigned randomly to a 2 x 3 factorial arrangement of treatments involving two truncated (no FTAI or GnRH-2) versions of Bee Synch (Bee Synch I-t and IIt), each begun 3, 7, and 10 days post-ovulation. Cows in Bee Synch I-t received 100 mu g GnRH (GnRH-1), 25 mg PGF, and a CIDR on Day 0, whereas cows assigned to Bee Synch IIt received the same treatment but without GnRH-1. All cows received 50 mg PGF on Day 5 at CIDR removal. Synchronized new follicular wave emergence (NFWE; days 1-4) was observed in 68.6 and 38.9% of Bee Synch I-t and IIt, respectively (P = 0.01). This increased to 93.3% and 72.2%, respectively, if days 0-4 were considered. Inclusion of GnRH at CIDR insertion improved synchronized NFWE but size of the largest follicle at 66 h, the normal time of FTAI, did not differ due to treatment or day of the estrous cycle.", "journal": "ANIMAL REPRODUCTION SCIENCE", "category": "Agriculture, Dairy & Animal Science; Reproductive Biology; Veterinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000477056500026", "keywords": "clinical research; practice; critical care; intensive care management; lung (allograft) function; dysfunction; lung (native) function; dysfunction; lung disease; lung transplantation; pulmonology; organ transplantation in general", "title": "Pirfenidone exerts beneficial effects in patients with IPF undergoing single lung transplantation", "abstract": "Pirfenidone demonstrated pleiotropic antiinflammatory effects in various experimental and clinical settings. The aim of this study was to assess the impact of previous treatment with pirfenidone on short-term outcomes after single lung transplantation (SLTx). Therefore, patients with idiopathic pulmonary fibrosis (IPF) who were undergoing SLTx were screened retrospectively for previous use of pirfenidone and compared to respective controls. Baseline parameters and short-term outcomes were recorded and analyzed. In total, 17 patients with pirfenidone were compared with 26 patients without antifibrotic treatment. Baseline characteristics and severity of disease did not differ between groups. Use of pirfenidone did not increase blood loss, wound-healing, or anastomotic complications. Severity of primary graft dysfunction at 72 hours was less (0.3 +/- 0.6 vs 1.4 +/- 1.3, P = .002), and length of mechanical ventilation (37.5 +/- 34.8 vs 118.5 +/- 151.0 hours, P = .016) and intensive care unit (ICU) stay (6.6 +/- 7.1 vs 15.6 +/- 20.3, P = .089) were shorter in patients with pirfenidone treatment. An independent beneficial effect of pirfenidone was confirmed by regression analysis while controlling for confounding variables (P = .016). Finally, incidence of acute cellular rejections within the first 30 days after SLTx was lower in patients with previous pirfenidone treatment (0.0% vs 19.2%; P = .040). Our data suggest a beneficial role of previous use of pirfenidone in patients with IPF who were undergoing SLTx.", "journal": "AMERICAN JOURNAL OF TRANSPLANTATION", "category": "Surgery; Transplantation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000472518800010", "keywords": "Lipid; cholesterol; dementia; elderly", "title": "HDL Cholesterol Is Independently Associated with Cognitive Function in Males But Not in Females within a Cohort of Nonagenarians: The Mugello Study", "abstract": "ObjectivesTo investigate the possible relationship between lipid profile and cognitive functions in a cohort of nonagenarians enrolled within the Mugello Study, an epidemiological study aimed at investigating both clinically relevant geriatric items and various health issues.DesignCross-sectional survey.SettingThis study focused on oldest old community-dwelling participants.ParticipantsThree hundred twenty-five nonagenarians (218 F, median age: 92 years).MeasurementsParticipants were evaluated through laboratory, instrumental examinations and questionnaires concerning lifestyle, dietary habits and cognitive status.ResultsFemales are older, with a lower level of education, live more prevalently on their own and have higher values for total cholesterol and high-density lipoprotein cholesterol (HDL) compared to males. With regard to functional and cognitive measures females report a significantly lower skill level in the physical activity performance, with a level of independence that is better for both basic and instrumental activities. In order to investigate whether there was an association between lipid variables and cognitive function as measured by the Mini-Mental State Examination a multiple regression analysis was performed with adjustments for confounding variables based on gender. In males, HDL cholesterol showed a significant relationship with Mini-Mental State Examination after a complete adjustment with years of education, physical activity performance and daily living activities ( = 0.174; p=0.037). In females HDL cholesterol showed a significant association only in the model adjusted for age and body mass index, losing its associations as soon as the cohabitation state and the depression status entered the model.ConclusionOur results support the hypothesis that HDL cholesterol is significantly linked to cognitive functions, especially in males of a cohort of very old people.", "journal": "JOURNAL OF NUTRITION HEALTH & AGING", "category": "Geriatrics & Gerontology; Nutrition & Dietetics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000485436600001", "keywords": "exercise; functional genomics; GWAS; global positioning system; horse; SNP; skeletal muscle", "title": "Analysis of genetic variation contributing to measured speed in Thoroughbreds identifies genomic regions involved in the transcriptional response to exercise", "abstract": "Despite strong selection for athletic traits in Thoroughbred horses, there is marked variation in speed and aptitude for racing performance within the breed. Using global positioning system monitoring during exercise training, we measured speed variables and temporal changes in speed with age to derive phenotypes for GWAS. The aim of the study was to test the hypothesis that genetic variation contributes to variation in end-point physiological traits, in this case galloping speed measured during field exercise tests. Standardisation of field-measured phenotypes was attempted by assessing horses exercised on the same gallop track and managed under similar conditions by a single trainer. PCA of six key speed indices captured 73.9% of the variation with principal component 1 (PC1). Verifying the utility of the phenotype, we observed that PC1 (median) in 2-year-old horses was significantly different among elite, non-elite and unraced horses (P < 0.001) and the temporal change with age in PC1 varied among horses with different myostatin (MSTN) g.66493737C>T SNP genotypes. A GWAS for PC1 in 2-year-old horses (n = 122) identified four SNPs reaching the suggestive threshold for association (P < 4.80 x 10(-5)), defining a 1.09 Mb candidate region on ECA8 containing the myosin XVIIIB (MYO18B) gene. In a GWAS for temporal change in PC1 with age (n = 168), five SNPs reached the suggestive threshold for association and defined candidate regions on ECA2 and ECA11. Both regions contained genes that are significantly differentially expressed in equine skeletal muscle in response to acute exercise and training stimuli, including MYO18A. As MYO18A plays a regulatory role in the skeletal muscle response to exercise, the identified genomic variation proximal to the myosin family genes may be important for the regulation of the response to exercise and training.", "journal": "ANIMAL GENETICS", "category": "Agriculture, Dairy & Animal Science; Genetics & Heredity", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000461762200003", "keywords": "Contrast enhancement; variational method; gamma correction; algorithm; minimization", "title": "A VARIATIONAL GAMMA CORRECTION MODEL FOR IMAGE CONTRAST ENHANCEMENT", "abstract": "Image contrast enhancement plays an important role in computer vision and pattern recognition by improving image quality. The main aim of this paper is to propose and develop a variational model for contrast enhancement of color images based on local gamma correction. The proposed variational model contains an energy functional to determine a local gamma function such that the gamma values can be set according to the local information of the input image. A spatial regularization of the gamma function is incorporated into the functional so that the contrast in an image can be modified by using the information of each pixel and its neighboring pixels. Another regularization term is also employed to preserve the ordering of pixel values. Theoretically, the existence and uniqueness of the minimizer of the proposed model are established. A fast algorithm can be developed to solve the resulting minimization model. Experimental results on benchmark images are presented to show that the performance of the proposed model are better than that of the other testing methods.", "journal": "INVERSE PROBLEMS AND IMAGING", "category": "Mathematics, Applied; Physics, Mathematical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000482792400001", "keywords": "corn; chlorophyll content; chlorophyll fluorescence; derivative spectra; O-2-A absorption", "title": "Estimation of Corn Canopy Chlorophyll Content Using Derivative Spectra in the O-2-A Absorption Band", "abstract": "Chlorophyll (Chl) is one of the most important classes of light-absorbing pigments in photosynthesis, and the proportion of Chl in leaves is closely related to vegetation nutrient status. Remote sensing-based estimation of Chl content holds great potential for evaluating crop growth status in agricultural management, precision farming and ecosystem monitoring. Recent studies have shown that steady-state fluorescence contributed up to 2% on the apparent reflectance in the 750-nm spectral region of plant and also provided additional evidence for fluorescence in-filling of the atmospheric oxygen absorption band at a central wavelength of 760 nm (O-2-A band). In this study, an in situ hyperspectral remote sensing approach zwas employed to estimate corn Chl content at the canopy level by using chlorophyll fluorescence (ChlF) signals in the O-2-A absorption band. Two new spectral indices, REArea(760) (sum of first derivative reflectance between 755 and 763 nm) and REA(760) (maximum of first derivative reflectance between 755 and 763 nm), derived from the first derivative spectra in the O-2-A band, were proposed for estimating the corn canopy Chl content (CCC). They were compared with the performance of published indices measured at ground level, including the MERIS Terrestrial Chlorophyll Index (MTCI), Optimized Soil-Adjusted Vegetation Index 2 (OSAVI2), Modified Chlorophyll Absorption Ratio Index 2 (MCARI2), SR710, REArea (sum of first derivative reflectance between 680 and 780 nm), REA (maximum value of first derivative reflectance between 680 and 780 nm), and mND(705). The results indicated that corn Chl content at the canopy level was better predicted by the new indices (with R-2 = 0.835) than the published indices (with R-2 ranging from 0.676 to 0.826). The two new indices ranked in the top four according to their summed ranks by integrating the ranks of RMSE and R-2 of CCC linear regression models. ChlF originates only from chlorophyll in the photosynthetic apparatus and therefore is less sensitive to soil, wood, and dead biomass interference. Moreover, due to the fluorescence in-filling of the O-2-A band and the amplified effect on spectrum signals by derivative operation, the spectral derivative indices in the O-2-A band have great potential for estimating the CCC.", "journal": "FRONTIERS IN PLANT SCIENCE", "category": "Plant Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000463979000018", "keywords": "Ebinur Lake; Sentienel-1A; Water index; Remote sensing; Eco-environmental impact", "title": "Dynamic detection of water surface area of Ebinur Lake using multi-source satellite data (Landsat and Sentinel-1A) and its responses to changing environment", "abstract": "In arid and semi-arid climatic areas, lakes are extremely essential for fragile ecological environment and regional sustainable development. Ebinur Lake is an important component of the ecological bather of Junggar Basin, Xinjiang Uyghur Autonomous Region (XUAR), China. Due to the tremendous changes in Ebinur Lake and surrounding marshes during the last decades, Ebinur Lake becomes a representative ecological degradation region in northwestern China. The detection of the intra-annual variations of water body and its responses to changing environment are critical for regional ecological security and rehabilitation of degraded ecosystem. To extract more accurate water information using Synthetic Aperture Radar (SAR) data and further fill the gap of intermonth dynamic monitoring of Ebinur Lake, a new SAR water index (modified Sentinel 1A water index, MSWI) was proposed based on the relationship between normalized difference water index (NDWI) imageries and Sentinel 1A data. The dynamic thresholds of classification were selected using Otsu method and the results showed that the classification results were acceptable with the optimal overall accuracy of 99.94% and kappa coefficient of 0.9971, respectively. We conduct a time series analysis of surface areas of Ebinur Lake using S1A data from February 9th, 2017 to February 4th, 2018. The maximum lake surface area was 965.29 km(2) in April 22nd, 2017, while the minimum value was 750.37 km(2) in September 1st, 2017, and the mean area was 831.51 km(2). The seasonal variations showed the stages of \"sharp rising\" - \"significant decreasing\" - \"gradual stabilizing\" in the study period. The water surface area was highly correlated with inflow water volume (correlation coefficient = 0.72, P < 0.001). The variation of Ebinur Lake's water surface area is crucial to monitor the resulting eco-environmental impacts under the changing environmental conditions in the arid and semi-arid areas.", "journal": "CATENA", "category": "Geosciences, Multidisciplinary; Soil Science; Water Resources", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000481328800001", "keywords": "kinetic Monte Carlo; lateral interactions; cluster expansion; algorithms", "title": "Efficient Implementation of Cluster Expansion Models in Surface Kinetic Monte Carlo Simulations with Lateral Interactions: Subtraction Schemes, Supersites, and the Supercluster Contraction", "abstract": "While lateral interaction models for reactions at surfaces have steadily gained popularity and grown in terms of complexity, their use in chemical kinetics has been impeded by the low performance of current kinetic Monte Carlo (KMC) algorithms. The origins of the additional computational cost in KMC simulations with lateral interactions are traced back to the more elaborate cluster expansion Hamiltonian, the more extensive rate updating, and to the impracticality of rate-catalog-based algorithms for interacting adsorbate systems. Favoring instead site-based algorithms, we propose three ways to reduce the cost of KMC simulations: (1) representing the lattice energy by a smaller Supercluster Hamiltonian without loss of accuracy, (2) employing the subtraction schemes for updating key quantities in the simulation that undergo only small, local changes during a reaction event, and (3) applying efficient search algorithms from a set of established methods (supersite approach). The cost of the resulting algorithm is fixed with respect to the number of lattice sites for practical lattice sizes and scales with the square of the range of lateral interactions. The overall added cost of including a complex lateral interaction model amounts to less than a factor 3. Practical issues in implementation due to finite numerical accuracy are discussed in detail, and further suggestions for treating long-range lateral interactions are made. We conclude that, while KMC simulations with complex lateral interaction models are challenging, these challenges can be overcome by modifying the established variable step-size method by employing the supercluster, subtraction, and supersite algorithms (SSS-VSSM). (c) 2019 Wiley Periodicals, Inc.", "journal": "JOURNAL OF COMPUTATIONAL CHEMISTRY", "category": "Chemistry, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000468208700015", "keywords": "shoulder; instability; return to sport; overhead athlete", "title": "Clinical Outcomes After Anterior Shoulder Stabilization in Overhead Athletes: An Analysis of the MOON Shoulder Instability Consortium", "abstract": "Background: Traumatic anterior shoulder instability is a common condition affecting sports participation among young athletes. Clinical outcomes after surgical management may vary according to patient activity level and sport involvement. Overhead athletes may experience a higher rate of recurrent instability and difficulty returning to sport postoperatively with limited previous literature to guide treatment. Purpose: To report the clinical outcomes of patients undergoing primary arthroscopic anterior shoulder stabilization within the Multicenter Orthopaedic Outcomes Network (MOON) Shoulder Instability Consortium and to identify prognostic factors associated with successful return to sport at 2 years postoperatively. Study Design: Case series; Level of evidence, 4. Methods: Overhead athletes undergoing primary arthroscopic anterior shoulder stabilization as part of the MOON Shoulder Instability Consortium were identified for analysis. Primary outcomes included the rate of recurrent instability, defined as any patient reporting recurrent dislocation or reoperation attributed to persistent instability, and return to sport at 2 years postoperatively. Secondary outcomes included the Western Ontario Shoulder Instability Index and Kerlan-Jobe Orthopaedic Clinic Shoulder and Elbow questionnaire score. Univariate regression analysis was performed to identify patient and surgical factors predictive of return to sport at short-term follow-up. Results: A total of 49 athletes were identified for inclusion. At 2-year follow-up, 31 (63%) athletes reported returning to sport. Of those returning to sport, 22 athletes (45% of the study population) were able to return to their previous levels of competition (nonrefereed, refereed, or professional) in at least 1 overhead sport. Two patients (4.1%) underwent revision stabilization, although 14 (28.6%) reported subjective apprehension or looseness. Age (P = .87), sex (P = .82), and baseline level of competition (P = .37) were not predictive of return to sport. No difference in range of motion in all planes (P > .05) and Western Ontario Shoulder Instability Index scores (78.0 vs 80.1, P = .73) was noted between those who reported returning to sport and those who did not. Conclusion: Primary arthroscopic anterior shoulder stabilization in overhead athletes is associated with a low rate of recurrent stabilization surgery. Return to overhead athletics at short-term follow-up is lower than that previously reported for the general athletic population.", "journal": "AMERICAN JOURNAL OF SPORTS MEDICINE", "category": "Orthopedics; Sport Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000471708800005", "keywords": "algebraic approach; Boolean networks; logical systems; semi-tensor product of matrices", "title": "Identification of predictors of Boolean networks from observed attractor states", "abstract": "Predictors of Boolean networks are of significance for biologists to target their research on gene regulation and control. This paper aims to investigate how to determine predictors of Boolean networks from observed attractor states by solving logical equations. The proposed method consists of four steps. First, all possible cycles formed by known attractor states are constructed. Then, for each possible cycle, all data-permitted predictors of each node are identified according to the known attractor states. Subsequently, the data-permitted predictors are incorporated with some common biological constraints to generate logical equations that describe whether such possible predictors can ultimately be chosen as valid ones by the biological constraints. Finally, solve the logical equations; the solutions determine a family of predictors satisfying the known attractor states. The approach is quite different from others such as computer algorithm-based and provides a new angle and means to understand and analyze the structures of Boolean networks.", "journal": "MATHEMATICAL METHODS IN THE APPLIED SCIENCES", "category": "Mathematics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000482935300001", "keywords": "Traction motor; suspension parameters; nonlinear stability; critical speed; Hopf bifurcation", "title": "Nonlinear stability analysis of motor bogies for high-speed trains", "abstract": "The traction motor flexibly suspended on the bogie frame is conducive to the lateral dynamic performance of high-speed trains. At present, most of the researches about the influences of suspension parameters of the traction motor on the stability of the bogie are limited to the linear system. In this paper, according to the suspension mode of the traction motor of a certain type of high-speed train in China, the dynamic equations of the motorized bogie with eight degrees of freedom are derived, and the nonlinear stability of the bogie system is analyzed. The bifurcation diagrams of the bogie system with different motor suspension parameters are obtained by using the continuation algorithm, and the linear and nonlinear critical speeds of the system are studied. The study shows that the suspension stiffness, damping coefficient, and the mass of the motor significantly affect the critical speeds of the bogie system. Then the mechanisms of the influence of suspension parameters on the linear and nonlinear critical speeds of the bogie are analyzed by the root locus method and Hopf bifurcation normal form theory, respectively.", "journal": "PROCEEDINGS OF THE INSTITUTION OF MECHANICAL ENGINEERS PART F-JOURNAL OF RAIL AND RAPID TRANSIT", "category": "Engineering, Civil; Engineering, Mechanical; Transportation Science & Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000474941400006", "keywords": "cervical cancer; endometrial cancer; indocyanine green (ICG); laparoscopy; sentinel lymph node (SLN)", "title": "Sentinel Nodes Detection with Near-infrared Imaging in Gynecological Cancer Patients: Ushering in an Era of Precision Medicine", "abstract": "Background: The sentinel lymph node (SLN) biopsy procedure is a well-known method for identifying solid tumors such as breast cancer, vulvar cancer, and melanoma. In endometrial and cervical cancer, SLN has recently gained acceptance. Objectives: To evaluate the detection rate of SLN with an indocyanine green and near-infrared fluorescent imaging (ICG/NIR) integrated laparoscopic system in clinically uterine-confined endometrial or cervical cancer. Methods: Patients with clinically early-stage endometrial or cervical cancer were included in this retrospective study. ICG was injected into the uterine cervix and an ICG/NIR integrated laparoscopic system was used during the surgeries. The National Comprehensive Cancer Network (NCCN) protocol was followed. SLN and/or suspicious lymph nodes were resected. Side-specific lymphadenectomy was performed when mapping was unsuccessful. Systematic lymphadenectomy was completed in patients with high-grade histology or deep myometrial invasion. Enhanced pathology using ultra-staging and immunohistochemistry were performed in all cases. Results: We analyzed 46 eligible patients: 39 endometrial and 7 cervical cancers. Of these, 44 had at least one SLN (93.6%). In 41 patients (89%) we detected bilateral SLN, in 3 (7%) only unilateral, and in 2 (4%) none were detected. Seven patients presented with lymph node metastasis. All were detected by NCCN/SLN protocol. Of these cases, two were detected with only pathological ultra-staging. Conclusions: SLN mapping in endometrial and cervical cancer can easily be performed with a high detection rate by integrating ICG/NIR into a conventional laparoscopic system. Precision medicine in patients evaluated by SLN biopsy changes the way patients with endometrial or cervical cancer are managed.", "journal": "ISRAEL MEDICAL ASSOCIATION JOURNAL", "category": "Medicine, General & Internal", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000472709100038", "keywords": "Nanomagnetism; Magnetostatic dipolar energy; Magnetic layered materials; Ewald method", "title": "Magnetostatic dipolar energy of large periodic Ni fcc nanowires, slabs and spheres", "abstract": "The computational effort to calculate the magnetostatic dipolar energy, MDE, of a periodic cell of N magnetic moments is an O(N-2) task. Compared with the calculation of the Exchange and Zeeman energy terms, this is the most computationally expensive part of the atomistic simulations of the magnetic properties of large periodic magnetic systems. Two strategies to reduce the computational effort have been studied: An analysis of the traditional Ewald method to calculate the MDE of periodic systems and parallel calculations. The detailed analysis reveals that, for certain types of periodic systems, there are many matrix elements of the Ewald method identical to other elements, due to some symmetry properties of the periodic systems. Computation timing experiments of the MDE of large periodic Ni fcc nanowires, slabs and spheres, up to 32,000 magnetic moments in the periodic cell, have been carried out and they show that the number of matrix elements that should be calculated is approximately equal to N, instead of N-2/2, if these symmetries are used, and that the computation time decreases in an important amount. The time complexity of the analysis of the symmetries is O(N-3), increasing the time complexity of the traditional Ewald method. MDE is a very small energy and therefore, the usual required precision of the calculation of the MDE is so high, about 10(-6) eV/cell, that the calculations of large periodic magnetic systems are very expensive and the use of the symmetries reduces, in practical terms, the computation time of the MDE in a significant amount, in spite of the increase of the time complexity. The second strategy consists of parallel calculations of the MDE without using the symmetries of the periodic systems. The parallel calculations have been compared with serial calculations that use the symmetries.", "journal": "APPLIED SURFACE SCIENCE", "category": "Chemistry, Physical; Materials Science, Coatings & Films; Physics, Applied; Physics, Condensed Matter", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000479747000001", "keywords": "acute liver injury; antidepressants; Germany; pharmacoepidemiology; validation", "title": "Validity of hospital ICD-10-GM codes to identify acute liver injury in Germany", "abstract": "Purpose Acute liver injury (ALI) is an important adverse drug reaction. We estimated the positive predictive values (PPVs) of ICD-10-GM codes of ALI used in an international postauthorisation safety study (PASS). Methods Analyses used routine data (2007 to 2016, adults) from a German academic hospital in a cross-sectional design. Two algorithms from the PASS were applied to extract potential cases from the hospital information system: specific end point (A) (discharge diagnosis of liver disease-specific codes) and less specific end point (B) (discharge and outpatient-specific and nonspecific codes suggestive of liver injury). ALI cases were confirmed on the basis of plasma liver enzyme activity elevation. Secondary analysis was performed following exclusion of cases with known cancer, chronic liver, biliary and pancreatic disease, heart failure, and alcohol-related disorders, as applied in the PASS. Results On the basis of ICD codes: outcome A, 154 cases (143 with case notes and lab data for case verification); outcome B, 485 cases (357 with case notes and lab data). ALI was confirmed in 71 outcome A cases, PPV of 49.7% (95% confidence interval [CI], 41.2%-58.1%), and 100 outcome B cases, PPV of 28.0% (95% CI, 23.4%-33.0%). Applying exclusion criteria increased PPV (95% CI) to 62.7% (50.0%-74.2%) for outcome A and 45.7% (37.2%-54.3%) for outcome B. Conclusions In safety studies on hepatotoxicity based on routine data using ICD-10-GM discharge codes and when validation of potential cases is not feasible, only the more specific codes should be used to describe ALI, and competing diagnoses for liver injury should be excluded to avoid substantial misclassification.", "journal": "PHARMACOEPIDEMIOLOGY AND DRUG SAFETY", "category": "Public, Environmental & Occupational Health; Pharmacology & Pharmacy", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000471232800030", "keywords": "Uncertainty; NLP; Scenario reduction; Sensitivity analysis; Optimisation", "title": "Scenario tree reduction for optimisation under uncertainty using sensitivity analysis", "abstract": "This work addresses the optimal management of a system through a two-stage stochastic Non-Linear Programming (NLP) formulation. This approach uses a scenario-based mathematical formulation to tackle uncertain information. Accurate representation of uncertainty usually involves increased number of scenarios, which may result in large-scale optimisation models. Thus, the proposed formulation aims to reduce the number of scenarios through a sensitivity analysis approach. The proposed model investigates the use of scenario reduction techniques to reduce computational requirements while maintaining good quality of the final optimal solution. (C) 2019 Elsevier Ltd. All rights reserved.", "journal": "COMPUTERS & CHEMICAL ENGINEERING", "category": "Computer Science, Interdisciplinary Applications; Engineering, Chemical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000461445700009", "keywords": "Shallow dome; Genetic algorithms; Worst imperfection pattern", "title": "Finding the worst imperfection pattern in shallow lattice domes using genetic algorithms", "abstract": "Like other structures, shallow domes have imperfections from the construction process. Specifications prescribe some construction tolerance values for imperfections. Despite consideration of these values, the critical load of a dome varies for each imperfection pattern. Analytical and Monte Carlo approaches are among the main methods for finding the worst imperfection pattern that is playing significant role in safety of dome. Analytical methods are reliable but cannot be implemented for large structures. On the other hand, uncertainties and high computational costs are the most important weaknesses of Monte Carlo approaches. In the present research employing genetic algorithms, the worst imperfection pattern is analyzed, calculated, and plotted.", "journal": "JOURNAL OF BUILDING ENGINEERING", "category": "Construction & Building Technology; Engineering, Civil", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000893059100001", "keywords": "Self-interacting proteins; Protein sequence; Gray level co-occurrence matrix; Sparse representation", "title": "Robust and accurate prediction of self-interacting proteins from protein sequence information by exploiting weighted sparse representation based classifier", "abstract": "BackgroundSelf-interacting proteins (SIPs), two or more copies of the protein that can interact with each other expressed by one gene, play a central role in the regulation of most living cells and cellular functions. Although numerous SIPs data can be provided by using high-throughput experimental techniques, there are still several shortcomings such as in time-consuming, costly, inefficient, and inherently high in false-positive rates, for the experimental identification of SIPs even nowadays. Therefore, it is more and more significant how to develop efficient and accurate automatic approaches as a supplement of experimental methods for assisting and accelerating the study of predicting SIPs from protein sequence information. ResultsIn this paper, we present a novel framework, termed GLCM-WSRC (gray level co-occurrence matrix-weighted sparse representation based classification), for predicting SIPs automatically based on protein evolutionary information from protein primary sequences. More specifically, we firstly convert the protein sequence into Position Specific Scoring Matrix (PSSM) containing protein sequence evolutionary information, exploiting the Position Specific Iterated BLAST (PSI-BLAST) tool. Secondly, using an efficient feature extraction approach, i.e., GLCM, we extract abstract salient and invariant feature vectors from the PSSM, and then perform a pre-processing operation, the adaptive synthetic (ADASYN) technique, to balance the SIPs dataset to generate new feature vectors for classification. Finally, we employ an efficient and reliable WSRC model to identify SIPs according to the known information of self-interacting and non-interacting proteins. ConclusionsExtensive experimental results show that the proposed approach exhibits high prediction performance with 98.10% accuracy on the yeast dataset, and 91.51% accuracy on the human dataset, which further reveals that the proposed model could be a useful tool for large-scale self-interacting protein prediction and other bioinformatics tasks detection in the future.", "journal": "BMC BIOINFORMATICS", "category": "Biochemical Research Methods; Biotechnology & Applied Microbiology; Mathematical & Computational Biology", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000898526100001", "keywords": "Chinese herbal injections; XELOX regimen; network meta-analysis; colorectal cancer; randomized controlled trials", "title": "Comparative efficacy and safety of Chinese medicine injections combined with capecitabine and oxaliplatin chemotherapies in treatment of colorectal cancer: A bayesian network meta-analysis", "abstract": "Objective: The aim of the present Bayesian network meta-analysis (NMA) was to explore the comparative effectiveness and safeaty of different Chinese Medicine injections (CMIs) combined with the XELOX regimen versus XELOX alone for colorectal cancer (CRC). Methods: A comprehensive search for randomized controlled trials (RCTs) was performed with regard to different CMIs for the treatment of CRC in several electronic databases up to April 2022. The quality assessment of the included RCTs was conducted according to the Cochrane risk of bias tool. Standard pair-wise and Bayesian NMA were designed to comparethe effectiveness and safety of different CMIs combined with the XELOX regimen by utilizing R 4.0.3 software and Stata 15.1 software simultaneously. Results: Initially, a total of 4296 citations were retrieved through comprehensive searching, and 32 eligible articles involving 2847 participants and 11 CMIs were ultimately included. CMIs combined with XELOX were superior to the XELOX regimen alone, and a total of ten Observation Indicators were included in the study, with the following results. Among all the injections, Shengmaiyin, Shenmai, and Kanglaite combined with the XELOX regimen were the three CMIs with the highest clinical efficiency. The top three in terms of improving CD3(+) values were Shengmaiyin, Shenqifuzheng, and Cinobufacini injections. Shenqifuzheng, Shengmaiyin, and BruceaJavanica oil injections combined with the XELOX regimen performed best at raising CD4(+) values. Kanglaite, Cinobufacini, and Matrine injections combined with the XELOX regimen performed best in improving CD4+/CD8+ rates. The top three in terms of improving performance status were Xiaoaiping, Shenmai, and Kanglaite injections. Cinobufacini and Brucea Javanica oil injections combined with the XELOX regimen performed best at raising CD8(+) values. Shenqifuzheng, Kangai, and Matrine injections combined with the XELOX regimen performed best in improving Gastrointestinal reactions.The top threein terms of improving Leukopenia were Shenqifuzheng, Compound Kushen and Kanglaite injections. The top three in terms of improving Platelet decline were Compound Kushen, Cinobufacini and Shenqifuzheng injections. Additionally, those that were best at improving nausea and vomitting were Cinobufacini, Compound Kushen and Aidi injections. Conclusion: The results of the analysis demonstrated thatShengmaiyin, Kanglaite, and Cinobufacini injections and the XELOX regimen were associated with morepreferable and beneficial outcomes than other CMI groups. Nevertheless, additional results from multicenter trials and high-quality studies will bevital to support our findings.", "journal": "FRONTIERS IN PHARMACOLOGY", "category": "Pharmacology & Pharmacy", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000887801400001", "keywords": "UAV; drone; path planning; trajectory; cellular connected; cellular network; 4G; link reliability; intelligent optimization; PSO; GA", "title": "3D Global Path Planning Optimization for Cellular-Connected UAVs under Link Reliability Constraint", "abstract": "This paper proposes an effective global path planning technique for cellular-connected UAVs to enhance the reliability of unmanned aerial vehicles' (UAVs) flights operating beyond the visual line of sight (BVLOS). Cellular networks are considered one of the leading enabler technologies to provide a ubiquitous and reliable communication link for UAVs. First, this paper investigates a reliable aerial zone based on an extensive aerial drive test in a 4G network within a suburban environment. Then, the path planning problem for the cellular-connected UAVs is formulated under communication link reliability and power consumption constraints. To provide a realistic optimization solution, all constraints of the optimization problem are defined based on real-world scenarios; in addition, the presence of static obstacles and no-fly zones is considered in the path planning problem. Two powerful intelligent optimization algorithms, the genetic algorithm (GA) and the particle swarm optimization (PSO) algorithm, are used to solve the defined optimization problem. Moreover, a combination of both algorithms, referred to as PSO-GA, is used to overcome the inherent shortcomings of the algorithms. The performances of the algorithms are compared under different scenarios in simulation environments. According to the statistical analysis of the aerial drive test, existing 4G base stations are able to provide reliable aerial coverage up to a radius of 500 m and a height of 85 m. The statistical analysis of the optimization results shows that PSO-GA is a more stable and effective algorithm to rapidly converge to a feasible solution for UAV path planning problems, with a far faster execution time compared with PSO and GA, about two times. To validate the performance of the proposed solution, the simulation results are compared with the real-world aerial drive test results. The results comparison proves the effectiveness of the proposed path planning method in suburban environments with 4G coverage. The proposed method can be extended by identifying the aerial link reliability of 5G networks to solve the UAV global path planning problem in the current 5G deployment.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000893527600003", "keywords": "Speech enhancement; Joint sparse representation; Dictionary learning; Modulation transform; Group sparse representation", "title": "Speech enhancement using group complementary joint sparse representations in modulation domain", "abstract": "The internal group structure of signals has been considered for some speech enhancement (SE) algo-rithms, but most of them are conducted in acoustic domain. In this paper, we propose to incorporate the group structure in modulation domain as prior information for complementary joint sparse represen-tations (CJSR). The modulation transform is applied to generate a set of sub-band amplitude spectrums with different modulation frequencies, which contain the novel time-frequency (TF) distributions differ-ent from that in acoustic domain. For each of these spectrums, we learn a couple of joint dictionaries in which the atoms are clustered in groups. The resulted dictionaries have structured characteristics of speech and noise. To represent a signal, we use an objective function based on sparse group lasso to acti-vate atoms on group level. By doing so, the speech is robustly recovered from mixture according to preset group pattern. The results of ablation study show that each part of proposed method, that is, modulation -domain processing and group sparsity, has its benefits for CJSR and combining both parts leads to a fur-ther performance improvement. In the final comparative experiment, the results show that the proposed method produces better objective speech quality, improving PESQ by 6.0% and segSNR by 12.7% com-pared with baseline method.(c) 2022 Elsevier Ltd. All rights reserved.", "journal": "APPLIED ACOUSTICS", "category": "Acoustics", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000755680200013", "keywords": "Automated; Tractography; Brain; Tumor; Corticospinal", "title": "Automatic detection of pupil reactions in cataract surgery videos", "abstract": "In the light of an increased use of premium intraocular lenses (IOL), such as EDOF IOLs, multifocal IOLs or toric IOLs even minor intraoperative complications such as decentrations or an IOL tilt, will hamper the visual performance of these IOLs. Thus, the post-operative analysis of cataract surgeries to detect even minor intraoperative deviations that might explain a lack of a post-operative success becomes more and more important. Up-to-now surgical videos are evaluated by just looking at a very limited number of intraoperative data sets, or as done in studies evaluating the pupil changes that occur during surgeries, in a small number intraoperative picture only. A continuous measurement of pupil changes over the whole surgery, that would achieve clinically more relevant data, has not yet been described. Therefore, the automatic retrieval of such events may be a great support for a post-operative analysis. This would be especially true if large data files could be evaluated automatically. In this work, we automatically detect pupil reactions in cataract surgery videos. We employ a Mask R-CNN architecture as a segmentation algorithm to segment the pupil and iris with pixel-based accuracy and then track their sizes across the entire video. We can detect pupil reactions with a harmonic mean (H) of Recall, Precision, and Ground Truth Coverage Rate (GTCR) of 60.9% and average prediction length (PL) of 18.93 seconds. However, we consider the best configuration for practical use the one with the H value of 59.4% and PL of 10.2 seconds, which is much shorter. We further investigate the generalization ability of this method on a slightly different dataset without retraining the model. In this evaluation, we achieve the H value of 49.3% with the PL of 18.15 seconds.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000706729900001", "keywords": "Energy consumption; Fog computing; Load balancing; Resource utilization; Workflows", "title": "FOCALB: Fog Computing Architecture of Load Balancing for Scientific Workflow Applications", "abstract": "Fog computing has a broad scope in real-time applications. It appears in the middle of Internet of Things (IoT) users and the cloud layer. The main applications of fog computing are to decrease latency and improve resource utilization for the end-users. Along with many advantages, fog computing also faces many challenges such as overloaded resources, security, deployment of nodes, and energy consumption. Load balancing is a challenging problem in a fog computing environment wherein, in more IoTs, the load distribution is required among all resources. Utilization of resources can be increased by the distribution of load in equal proportion among all fog resources. In scientific workflow systems, fog computing aids in proper resource utilization by uniformly dividing the workload. In this paper, we have proposed a Fog Computing Architecture of Load Balancing (FOCALB) for scientific workflow applications. The paper also proposed hybridized load balancing algorithm for scientific workflows (Tabu-GWO-ACO), which is an amalgamation of tabu search, Grey Wolf Optimization (GWO), and Ant Colony Optimization (ACO). The proposed model has been designed to enhance resource utilization by implementing load balancing at the fog layer. In the fog nodes, load scheduling is done when tasks are initialized, and local controller in fog clusters does load balancing. The simulation results are obtained with the help of iFogSim and Eclipse for 20 to 200 fog nodes. Simulated studies based on execution time, cost, and energy compared with various existing models show that FOCALB reduces energy consumption at fog nodes and reduces the execution time and implementation cost as well. The article is summarized by providing open challenges and future research directions.", "journal": "JOURNAL OF GRID COMPUTING", "category": "Computer Science, Information Systems; Computer Science, Theory & Methods", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000725800500017", "keywords": "Robots; Planning; Safety; Trajectory; Probabilistic logic; Heuristic algorithms; Collision avoidance; Linear temporal logic (LTL); path planning; probabilistic guarantee", "title": "Chance-Constrained Multilayered Sampling-Based Path Planning for Temporal Logic-Based Missions", "abstract": "This article introduces a robust and safe path planning algorithm in order to satisfy mission requirements specified in linear temporal logic (LTL). When a path is planned to accomplish a mission, it is possible for a robot to fail to complete the mission or collide with obstacles due to noises and disturbances in the system. Hence, we need to find a robust path against possible disturbances. We introduce a robust path planning algorithm, which maximizes the probability of success in accomplishing a given mission by considering disturbances, while minimizing the moving distance of a robot. The proposed method can guarantee the safety of the planned trajectory by incorporating an LTL formula and chance constraints in a hierarchical manner. A high-level planner generates a discrete plan satisfying the mission requirements specified in LTL. A low-level planner builds a sampling-based rapidly exploring random tree search tree to minimize both the mission failure probability and the moving distance while guaranteeing the probability of collision with obstacles to be below a specified threshold. We have analyzed properties of the proposed algorithm theoretically and validated the robustness and safety of paths generated by the algorithm in simulation and experiments using a quadrotor.", "journal": "IEEE TRANSACTIONS ON AUTOMATIC CONTROL", "category": "Automation & Control Systems; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000703444900008", "keywords": "Nuclear power plant; Actuator stuck fault; Fault accommodation; Fuzzy logic; Target load adjustment", "title": "A fuzzy fault accommodation method for nuclear power plants under actuator stuck faults", "abstract": "This paper proposes a fuzzy fault accommodation (FFA) method for nuclear power plants under actuator stuck faults. It can adjust the plant target load dynamically to match the reactor power during transients in which a few of the actuators lose their regulating capacity completely. The method was applied to a small pressurized water reactor (SPWR) under control rod driven mechanism and steam generator feed water valve stuck faults. The obtained results demonstrate that the method can maintain balance well between the plant load and the reactor power under actuator stuck faults. Accordingly, the coolant average temperature and steam pressure of the SPWR can be restored to their constant setpoints without regulating the corresponding actuators, i.e., the control rods and the feedwater valves, demonstrating the effectiveness of the method. The research is expected to provide possible solutions for the reasonable and satisfactory control of NPPs under the dangerous actuator stuck faults. (c) 2021 Elsevier Ltd. All rights reserved.", "journal": "ANNALS OF NUCLEAR ENERGY", "category": "Nuclear Science & Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000717140800001", "keywords": "miRNA-disease association; credibility similarity; weighted voting; miRNA; disease", "title": "WVMDA: Predicting miRNA-Disease Association Based on Weighted Voting", "abstract": "An increasing number of experiments had verified that miRNA expression is related to human diseases. The miRNA expression profile may be an indicator of clinical diagnosis and provides a new direction for the prevention and treatment of complex diseases. In this work, we present a weighted voting-based model for predicting miRNA-disease association (WVMDA). To reasonably build a network of similarity, we established credibility similarity based on the reliability of known associations and used it to improve the original incomplete similarity. To eliminate noise interference as much as possible while maintaining more reliable similarity information, we developed a filter. More importantly, to ensure the fairness and efficiency of weighted voting, we focus on the design of weighting. Finally, cross-validation experiments and case studies are undertaken to verify the efficacy of the proposed model. The results showed that WVMDA could efficiently identify miRNAs associated with the disease.", "journal": "FRONTIERS IN GENETICS", "category": "Genetics & Heredity", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000695434900001", "keywords": "Virtual machine scheduling; metaheuristic algorithm; whale optimization algorithm; differential evolution; cloud computing", "title": "A hybrid whale optimization algorithm with differential evolution optimization for multi-objective virtual machine scheduling in cloud computing", "abstract": "Virtual machine (VM) scheduling in a dynamic cloud environment is often bound with multiple quality of service parameters; therefore, it is classed as an NP-hard optimization problem. Swarm-based metaheuristics, such as the whale optimization algorithm (WOA), have gained a notable reputation for solving optimization problems. The unique bubble-net hunting behaviour and fast convergence of the algorithm led to the development of a hybrid multi-objective whale optimization algorithm-based differential evolution (M-WODE) technique to solve the VM scheduling problem. The differential evolution (DE) strategy is used to replace the randomly generated solution produced by the WOA to ensure diversity in the solution and to strengthen the local search of the M-WODE. In addition, the DE technique is applied to the Pareto front produced by the WOA to escape local optima entrapment problems. The experimental results showed that the proposed M-WODE outperformed previous algorithms in most cases on makespan and the cost trade-off.", "journal": "ENGINEERING OPTIMIZATION", "category": "Engineering, Multidisciplinary; Operations Research & Management Science", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000749796600003", "keywords": "Camera calibration; Image enhancement; Sub-pixel extraction; Adaptive weight and mutation; Particle swarm optimization", "title": "Comprehensive improvement of camera calibration based on mutation particle swarm optimization", "abstract": "In order to meet the requirements of high-precision measurement, the method of improving camera calibration is studied. In the calibration process, the quality of the calibration image, the extraction accuracy of the calibration image corner and the nonlinear optimization effect of the camera linear parameters directly affect the calibration accuracy. First of all, in order to solve the problems in image acquisition, especially in the case of over exposure, an adaptive gamma correction method is designed to automatically adjust the image brightness, and enhance the contrast of black and white grid to improve the image acquisition quality. Secondly, a sub-pixel corner extraction algorithm based on homography matrix mapping is designed, which overcomes the error and omission of Harris corner extraction algorithm, and improves the accuracy of corner extraction. At last, adaptive weight and mutation particle swarm optimization algorithm are studied to optimize the camera parameters. Compared with other optimization algorithms, this optimization algorithm needs less parameter settings, fast convergence speed, and can obtain more accurate camera parameters. The average calibration error is 0.038 pixels.", "journal": "MEASUREMENT", "category": "Engineering, Multidisciplinary; Instruments & Instrumentation", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000696664600001", "keywords": "Multi-robot systems; distributed robot systems; autonomous agents; formation control", "title": "Distributed Linear Control of Multirobot Formations Organized in Triads", "abstract": "This letter addresses the problem of controlling multiple robots to form a prescribed team shape in two-dimensional space. We consider a team organization in interlaced triads (i.e., groups of three robots). For each triad we define a measure of geometric deformation relative to its prescribed shape. Our main contribution is a novel distributed control law, defined as the gradient descent on the sum of these triangular deformation measures. We show that this geometrically motivated control law is linear, and bears analogies with existing formulations. Moreover, in comparison with these formulations our controller is simpler and more flexible to design, converges to the globally optimal shape by construction, and allows analysis of the team size dynamics. We illustrate the proposed approach in simulation.", "journal": "IEEE ROBOTICS AND AUTOMATION LETTERS", "category": "Robotics", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000736662200001", "keywords": "search and rescue; autonomous drones; path planning; acceleration; deceleration", "title": "Acceleration-Aware Path Planning with Waypoints", "abstract": "In this article we demonstrate that acceleration and deceleration of direction-turning drones at waypoints have a significant influence to path planning which is important to be considered for time-critical applications, such as drone-supported search and rescue. We present a new path planning approach that takes acceleration and deceleration into account. It follows a local gradient ascend strategy which locally minimizes turns while maximizing search probability accumulation. Our approach outperforms classic coverage-based path planning algorithms, such as spiral- and grid-search, as well as potential field methods that consider search probability distributions. We apply this method in the context of autonomous search and rescue drones and in combination with a novel synthetic aperture imaging technique, called Airborne Optical Sectioning (AOS), which removes occlusion of vegetation and forest in real-time.", "journal": "DRONES", "category": "Remote Sensing", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000336707600002", "keywords": "Nondestructive; Quality; Internal rot; Classification; Computer vision", "title": "Postharvest noninvasive assessment of fresh chestnut (Castanea spp.) internal decay using computer tomography images", "abstract": "X-ray computed tomography (CT) is an effective noninvasive tool to visualize fresh agricultural commodities' internal components and quality attributes, including those of chestnuts (Castanea spp). There is no procedure to automatically, effectively and efficiently classify fresh commodities from a continuous inline flow through a CT system. If the information obtained by CT scanning of fresh agricultural commodities is to be used in an industrial application (e.g. inline sorting), automated interpretation of CT images is essential. For this purpose, an image analysis method (algorithm) for the automatic classification of CT images obtained from 2848 fresh chestnuts (cv. 'Colossal' and 'Chinese seedlings'), during the harvesting years from 2009 to 2012, was developed and tested. Classification accuracy was evaluated by comparing the classes obtained from six CT images per chestnut to their internal quality assessment. An experienced human rater performed internal quality assessment by visually and invasively rating fresh chestnut internal decay severity (quality) into 5-, 3- and 2-classes. After CT image preprocessing, cropping and segmentation, 1194 grayscale intensity and textural features were extracted from six resultant CT images per sample. Relevant features were selected using a sequential forward selection algorithm with the Fisher discriminant objective function. 86, 155 and 126 features were effective in designing a quadratic discriminant classifier with a 4-fold cross-validation with a performance accuracy of 85.9%, 91.2% and 96.1% for 5,3 and 2 classes, respectively. This method is accurate and objective in determining fresh chestnut internal quality, and the methodology is applicable to automatic noninvasive inline CT sorting system development. (C) 2014 Elsevier B.V. All rights reserved.", "journal": "POSTHARVEST BIOLOGY AND TECHNOLOGY", "category": "Agronomy; Food Science & Technology; Horticulture", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000339736700021", "keywords": "Amphibians; Birds; Chaco; Hierarchical species distribution modeling; Mammals; Trees", "title": "Land-use and land-cover effects on regional biodiversity distribution in a subtropical dry forest: a hierarchical integrative multi-taxa study", "abstract": "Latin American subtropical dry ecosystems have experienced significant human impact for more than a century, mainly in the form of extensive livestock grazing, forest products extraction, and agriculture expansion. We assessed the regional-scale effect of land use and land cover (LULC) on patterns of richness distribution of trees, birds, amphibians, and mammals in the Northern Argentine Dry Chaco (NADC) over c. 19 million hectares. Using species distribution models in a hierarchical framework, we modeled the distributions of 138 species. First, we trained the models for the entire Argentinean Chaco with climatic and topographic variables. Second, we modeled the same species for the NADC including the biophysical variables identified as relevant in the first step plus four LULC-related variables: woody biomass, distance to crops, density of livestock-based rural settlements (puestos), and vegetation cover. Third, we constructed species richness maps by adding the models of individual species and considering two situations, with and without LULC variables. Four, richness maps were used for assessing differences when LULC variables are added and for determining the main drivers of current patterns of species richness. We found a marked decrease in species richness of the four groups as a consequence of inclusion of LULC variables in distribution models. The main factors associated with current richness distribution patterns (both negatively) were woody biomass and density of livestock puestos. Species richness in present-day Semiarid Chaco landscapes is strongly affected by LULC patterns, even in areas not transformed to agriculture. Regional-scale biodiversity planning should consider open habitats such as grasslands and savannas in addition to woodlands.", "journal": "REGIONAL ENVIRONMENTAL CHANGE", "category": "Environmental Sciences; Environmental Studies", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000335941300021", "keywords": "Cancer; cluster analysis; statistics; symptoms", "title": "Approaching Language Transfer Through Text Classification: Explorations in the Detection- Based Approach.", "abstract": "Context. Statistical methods to identify symptom clusters (SC) have varied between studies. The optimal statistical method to identify SC is unknown. Objectives. Our primary objective was to explore whether eight different statistical techniques applied to a single data set produced different SC. A secondary objective was to investigate whether SC identified by these techniques resembled those from our original study. Methods. We reanalyzed a symptom data set of 1000 patients with advanced cancer. Eight separate cluster analyses were conducted on both prevalence and severity of 38 symptoms. Hierarchical cluster analysis identified clusters at r-values of 0.6, 0.5, and 0.4. For prevalence and severity, the Spearman correlation and Kendall tau-b correlation, respectively, measured the similarity (distance) between symptom pairs. Sensitivity analysis of the prevalence data was done with Cohen kappa coefficient as a similarity measure. The K-means clustering method validated clusters. Results. Hierarchical cluster analysis identified similar cluster configurations from the 38 symptoms using an r-value of 0.6, 0.5, or 0.4. A cutoff point of 0.6 yielded seven clusters. Five of them were identical at all three r-values used: 1) fatigue/anorexia- cachexia: anorexia, dry mouth, early satiety, fatigue, lack of energy, taste changes, weakness, and weight loss (> 10%); 2) gastrointestinal: belching, bloating, dyspepsia, and hiccough; 3) nausea/vomiting: nausea and vomiting; 4) aerodigestive: cough, dysphagia, dyspnea, hoarseness, and wheeze; 5) neurologic: confusion, hallucinations, and memory problems. Regardless of the threshold, there were always some symptoms (e. g., pain) that did not cluster with any others. Seven clusters were validated by K-means analysis. Conclusion. Seven SC identified from both prevalence and severity data were consistently present irrespective of the statistical analysis used. There were only minor variations in the number of clusters and their symptom composition between analytical techniques. All seven clusters originally identified were confirmed. Four consistent SC were found in all analyses: aerodigestive, fatigue/anorexia- cachexia, nausea/vomiting, and upper GI. Our results support the clinical importance of the SC concept. Published by Elsevier Inc. on behalf of American Academy of Hospice and Palliative Medicine.", "journal": "MODERN LANGUAGE JOURNAL", "category": "Education & Educational Research; Linguistics", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000338132800005", "keywords": "QMRCGSTAB algorithm; Lyapunov matrix equation; Sylvester matrix equation; Periodic coupled matrix equations", "title": "Solving the general coupled and the periodic coupled matrix equations via the extended QMRCGSTAB algorithms", "abstract": "By applying the Kronecker product and vectorization operator, we extend the quasi-minimal residual variant of the biconjugate gradient stabilized (QMRCGSTAB) algorithm for solving the general coupled matrix equations Sigma(l)(j=1) (A(i),(1), j X1Bi,(1), (j) + A(i,2,j)X(2)B(i,2,j) + . . . + A(i,l),(j) X-l B-i,B-l,B-j) = C-i for i = 1, 2,..., l, the periodic coupled matrix equations A(1),tXtB1,(t) + C1,tXt+D-1(1),(t) + epsilon 1,tYtF1,t = G(1,t), { for t = 1, 2,..., phi. A(2),tXtB2,(t) + C-2,X-t(t)+D-1(2),(t) + epsilon 2,tYtF2,t = G(2,t), Several numerical examples are given to show the efficiency of the extended QMRCGSTAB algorithms.", "journal": "COMPUTATIONAL & APPLIED MATHEMATICS", "category": "Mathematics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000335897200045", "keywords": "Energy use; Disaggregation; ODA; Accuracy; Unbiased; Convergence", "title": "An analysis of the optimization disaggregation algorithm in the estimation related to energy consumption of appliances in buildings", "abstract": "Generally speaking, energy monitoring system plays an important role in both energy use benchmarking and energy conservation service for large-scale buildings. However, it is often necessary for us to disaggregate the metered data according to types of energy use. Aiming at eliminating unclear concepts between monitoring and disaggregation and incorporating existing algorithms' advantages into the process in which this estimation is generated, the optimal disaggregation algorithm is developed. Based on the assumption of mutually independent ends, a Bayes process is degenerated into maximization likelihood estimation. Following this, the process is expressed by the least squares. With a new algorithm, an unbiased and overall convergent posterior estimation is provided for each terminal category, which is powerfully supported by theoretical analyses and numerical calculation. Actually, it is proved that the disaggregation algorithm is quite appropriate and cost-efficient for sub-item energy monitoring systems in large-scale buildings. Meanwhile, this thesis also focuses on discussing the conditional probability of overall lessening estimation errors specifically. In this way, posterior estimations of overall accuracy are available when prior estimations are concordantly biased with much different accuracy. (C) 2014 Elsevier Inc. All rights reserved.", "journal": "APPLIED MATHEMATICS AND COMPUTATION", "category": "Mathematics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000339474100010", "keywords": "Three-phase power transformer differential protection; Ultra-saturation phenomenon; Magnetic inrush current; Internal faults; External faults; False trip; Harmonic components; Differential current", "title": "A new algorithm for three-phase power transformer differential protection considering effect of ultra-saturation phenomenon", "abstract": "A transient phenomenon that leads to the false trip of the power transformer differential protection during the energization of a loaded power transformer is the ultra-saturation phenomenon. In this paper, at first, a new algorithm for three-phase power transformer differential protection, considering the effect of the ultra-saturation phenomenon, is presented. To model the ultra-saturation phenomenon, the nonlinear characteristic of the transformer core and the effect of saturation of the current transformers are taken into account. It is assumed that the load of the transformer is resistive and inductive. In this algorithm, the ultra-saturation phenomenon, external and internal faults of the power transformer and magnetic inrush current are simulated, and appropriate criteria using the signal harmonic components of the differential current will be presented for the Discrete Fourier Transform (DFT) algorithm to distinguish between. In this paper, simulation is done by PSCAD and MATLAB programs. (C) 2014 Sharif University of Technology. All rights reserved.", "journal": "SCIENTIA IRANICA", "category": "Engineering, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000340019400013", "keywords": "Open channel; Turbulence; Sample size; Particle seeding; LDA", "title": "Effects of sample size and concentration of seeding in LDA measurements on the velocity bias in open channel flow", "abstract": "A Laser Doppler Anemometer was used to measure the mean flow and turbulence in fluid experiments despite it having a problem with the mean velocity bias when a LDA is used to measure turbulent flows. It is generally considered that given a sufficiently large sample size, LDA will produce measurements free of bias, even with high turbulence intensity, but there is no relative experimental validation to demonstrate how the sample size affects the mean velocity bias. This paper first tries to find the reasonable sample size that ensures a mean velocity calculation free of bias. Furthermore, the effects of particle seeding concentration on the measurements of velocity were also considered. Throughout the experimental process the particle velocities were measured using a Dantec 2-D LDA system. To describe the effects of sample size and particle seeding concentration, this paper will also address the reasonable sample size and range of concentration in the design of water flow prior to any experimental application. (C) 2014 Elsevier Ltd. All rights reserved.", "journal": "FLOW MEASUREMENT AND INSTRUMENTATION", "category": "Engineering, Mechanical; Instruments & Instrumentation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000346439500001", "keywords": "Alveolar bone loss; Comorbidity; Periodontitis; Psoriasis", "title": "Association between moderate to severe psoriasis and periodontitis in a Scandinavian population", "abstract": "Background: The aim of the present study was to compare the prevalence of periodontitis and alveolar bone loss among individuals with psoriasis and a group of randomly selected controls. Methods: Fifty individuals with psoriasis and 121 controls completed a structured questionnaire, and were examined clinically and radiographically. Oral examination included numbers of missing teeth, probing pocket depth (PPD), clinical attachment level (CAL), presence of dental plaque and bleeding on probing, as well as alveolar bone loss from radiographs. Questionnaires requested information on age, gender, education, dental care, smoking habits, general diseases and medicament use. For adjustment for baseline differences between psoriasis individuals and controls the propensity score based on gender, age and education was computed using multivariate logistic regression. A subsample analysis for propensity score matched psoriasis individuals (n = 50) and controls (n = 50) was performed. Results: When compared with controls, psoriasis individuals had significantly more missing teeth and more sites with plaque and bleeding on probing. The prevalence of moderate and severe periodontitis was significantly higher among psoriasis individuals (24%) compared to healthy controls (10%). Similarly, 36% of psoriasis cases had one or more sites with radiographic bone loss = 3 mm, compared to 13% of controls. Logistic regression analysis showed that the association between moderate/severe periodontitis and psoriasis remained statistically significant when adjusted for propensity score, but was attenuated when smoking was entered into the model. The association between psoriasis and one or more sites with bone loss = 3 mm remained statistically significant when adjusted for propensity score and smoking and regularity of dental visits. In the propensity score (age, gender and education) matched sample (n = 100) psoriasis remained significantly associated with moderate/severe periodontitis and radiographic bone loss. Conclusions: Within the limits of the present study, periodontitis and radiographic bone loss is more common among patients with moderate/severe psoriasis compared with the general population. This association remained significant after controlling for confounders.", "journal": "BMC ORAL HEALTH", "category": "Dentistry, Oral Surgery & Medicine", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000340391600003", "keywords": "glomerular filtration rate; hepatitis C virus; pegylated interferon; protease inhibitor; renal function; ribavirin; triple therapy; virological response", "title": "Boceprevir and telaprevir-based triple therapy for chronic hepatitis C: virological efficacy and impact on kidney function and model for end-stage liver disease score", "abstract": "Triple therapy using telaprevir or boceprevir [hepatitis C virus (HCV)-NS3/NS4A protease inhibitors (PI)] in association with PEG-IFN/ribavirin has recently become the new standard of care (SOC) for treatment of HCV genotype 1 patients. Our objective was to assess the efficacy and tolerance of triple therapy in routine clinical practice. A total of 186 consecutive HCV patients initiating triple therapy were enrolled in a single centre study. Clinical, biological and virological data were collected at baseline and during follow-up as well as tolerance and side effect details. Among 186 HCV patients initiating triple therapy, 69% received telaprevir and 31% boceprevir. Sixty-one per cent of patients had cirrhosis. The overall extended rapid virological response (eRVR) rate and sustained virological response (SVR) rate were 57.0% and 59.7%, respectively. IL28B CC phenotype was associated with increased probability of achieving eRVR and SVR, whereas previous non-response was associated with low eRVR and SVR rates. The SVR rate increased from 30.8% in previously non-responders to 59.1% in partial non-responders and 75% in relapsers. SVR rate in naive patients was 62.5%. Glomerular filtration rate assessed by MDRD after 12weeks of therapy was significantly reduced for both PI (P<0.001). The model for end-stage liver disease (MELD) score was significantly increased at W12 for telaprevir (P=0.008) and at W24 for boceprevir (P=0.027). PI-based triple therapy leads to high rates of virological response even in previously non-responder patients. Renal function after triple therapy is impaired as well as MELD score in all patients. Cautious clinical monitoring should focus not only on haematological and dermatological side effects but also on renal function.", "journal": "JOURNAL OF VIRAL HEPATITIS", "category": "Gastroenterology & Hepatology; Infectious Diseases; Virology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000340850600027", "keywords": "Elastosonography; Elasticity score; Thyroid micronodules; Ultrasound; Strain ratio", "title": "Diagnostic value of elastosonography for thyroid microcarcinoma", "abstract": "Objective: To assess the diagnostic value of elastosonography for thyroid microcarcinoma (TMC), particularly with regard to elasticity score (ES) and strain ratio (SR). Methods: Conventional ultrasound and elastosonography were performed for 487 thyroid micronodules before surgery. We set the histology as the reference standard. The ES and SR values, as well as their diagnostic threshold and efficiency, were compared and analyzed by the receiver-operating characteristic (ROC) curve. Additional comparisons between TMC patients with and without extracapsular extension were also performed. Results: Statistically significant differences (P < 0.05) in both ES and SR values were detected among the TMC and benign groups. The area under the ROC curve of SR was significantly greater than that of ES (0.956 and 0.844, respectively; P < 0.05). Using ES >= 3 and SR >= 3.65 as diagnostic threshold values, the diagnostic sensitivity, specificity, and accuracy of ES for differentiating benign and malignant nodules were 79.9%, 72.3%, and 80.5%, respectively, whereas those of SR were 86.6%, 85.3%, and 89.4%, respectively. The maximum diameter, microcalcification status, aspect ratio, bilateral cervical lymph node metastasis, and SR values of nodules with extracapsular extension (A1 subgroup) were greater than those of nodules without extracapsular extension (A2 subgroup). Conclusions: Elasticity imaging technology not only can help differentiate between benign and malignant thyroid micronodules but also allow SR values to provide accurate and objective information on tissue hardness and to predict TMC extracapsular extension or even bilateral cervical lymph node metastasis. (C) 2014 Elsevier B.V. All rights reserved.", "journal": "ULTRASONICS", "category": "Acoustics; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000335236800040", "keywords": "premature infant; sepsis; biomarkers; genome-wide expression profile", "title": "Genome-Wide Expression Profiles in Very Low Birth Weight Infants With Neonatal Sepsis", "abstract": "BACKGROUND:Bacterial sepsis is associated with high morbidity and mortality in preterm infants. However, diagnosis of sepsis and identification of the causative agent remains challenging. Our aim was to determine genome-wide expression profiles of very low birth weight (VLBW) infants with and without bacterial sepsis and assess differences.METHODS:This was a prospective observational double-cohort study conducted in VLBW (<1500 g) infants with culture-positive bacterial sepsis and non-septic matched controls. Blood samples were collected as soon as clinical signs of sepsis were identified and before antibiotics were initiated. Total RNA was processed for genome-wide expression analysis using Affymetrix gene arrays.RESULTS:During a 19-month period, 17 septic VLBW infants and 19 matched controls were enrolled. First, a three-dimensional unsupervised principal component analysis based on the entire genome (28000 transcripts) identified 3 clusters of patients based on gene expression patterns: Gram-positive sepsis, Gram-negative sepsis, and noninfected control infants. Furthermore, these groups were confirmed by using analysis of variance, which identified a transcriptional signature of 554 of genes. These genes had a significantly different expression among the groups. Of the 554 identified genes, 66 belonged to the tumor necrosis factor and 56 to cytokine signaling. The most significantly overexpressed pathways in septic neonates related with innate immune and inflammatory responses and were validated by real-time reverse transcription polymerase chain reaction.CONCLUSIONS:Our preliminary results suggest that genome-wide expression profiles discriminate septic from nonseptic VLBW infants early in the neonatal period. Further studies are needed to confirm these findings.", "journal": "PEDIATRICS", "category": "Pediatrics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000336711200011", "keywords": "Nutrients; Peripheral artery disease; Energy; Activity; Cross-sectional; Diet", "title": "Nutrient intake and peripheral artery disease in adults: Key considerations in cross-sectional studies", "abstract": "Background & aims: There are limited studies of nutrient intake and peripheral artery disease (PAD). Some studies have not accounted for the functional consequences of PAD, potentially leading to biased results. To determine the associations between intakes of dietary fiber, folate, vitamins A, C, E, and B6 and PAD. Methods: Cross-sectional analysis of 6534 adults aged 40 years and older in the U.S. National Health and Nutrition Examination Survey between 1999 and 2004, including measurement of ankle-brachial index (ABI) and nutrient intake by 24-h dietary recall. Weighted multivariable logistic regression models to determine odds ratios and 95% confidence intervals. Results: The prevalence of PAD (ABI < 0.9) was 5.3% (4.7-5.9). Inverse associations between PAD and intakes of fiber, folate, and vitamins A, B6, C, and E were statistically significant when adjusting for age, sex, hypertension, diabetes and smoking. In models further adjusted for energy intake and physical activity, these odds ratios all became null (p >= 0.1). Conclusions: In this sample, dietary fiber, folate, and vitamins B6, C, and E were not associated with PAD after accounting for energy intake and activity. Adjustment for energy and physical activity are essential to avoid bias due to reverse causation in cross-sectional studies of diet and PAD. (C) 2013 Elsevier Ltd and European Society for Clinical Nutrition and Metabolism. All rights reserved.", "journal": "CLINICAL NUTRITION", "category": "Nutrition & Dietetics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000342773800006", "keywords": "Formation control; second-order consensus; leader-following; time-varying delays; multi-agent systems", "title": "Leader-following formation control for second-order multi-agent systems with time-varying delays", "abstract": "This paper investigates a leader-following formation control problem for second-order multi-agent systems with nonuniform time-varying communication delays under directed topologies. We first propose a consensus protocol and give a sufficient condition for second-order consensus of the system. Then, under a framework of multiple leaders, the protocol is applied to the formation control, including time-invariant formation and time-varying formation as well as time-varying formation for trajectory tracking. It is shown that the agents will attain the desired formation under the protocol. Finally, several simulations are conducted to illustrate the effectiveness of our theoretical results.", "journal": "TRANSACTIONS OF THE INSTITUTE OF MEASUREMENT AND CONTROL", "category": "Automation & Control Systems; Instruments & Instrumentation", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000340297300049", "keywords": "cerebral hemorrhage; etiology; mortality", "title": "Predictors of Early Mortality in Young Adults After Intracerebral Hemorrhage", "abstract": "Background and Purpose-Patient and radiological characteristics of intracerebral hemorrhage (ICH), surgical treatment, and outcome after ICH are interrelated. Our purpose was to define whether these characteristics or surgical treatment correlate with mortality among young adults. Methods-We retrospectively reviewed clinical and imaging data of all first-ever nontraumatic patients with ICH between 16 and 49 years of age treated in our hospital between January 2000 and March 2010 and linked these data with national causes of death registry. A logistic regression analysis of factors associated with 3-month mortality and a propensity score comparison between patients treated conservatively and operatively was performed. Results-Among the 325 eligible patients (59.4% men), factors associated with 3-month mortality included higher National Institutes of Health Stroke Scale score, infratentorial location, hydrocephalus, herniation, and multiple hemorrhages. Adjusted for these factors, as well as demographics, ICH volume, and the underlying cause, surgical evacuation was associated with lower 3-month mortality (odds ratio, 0.06; 95% confidence interval, 0.02-0.21). In propensity score-matched analysis, 3-month case fatality rates were 3-fold in those treated conservatively (27.5% versus 7.8%; P<0.001). Conclusions-The predictors of short-term case fatality are alike in young and elderly patients with ICH. However, initial hematoma evacuation was associated with lower 3-month case fatality in our young patients with ICH.", "journal": "STROKE", "category": "Clinical Neurology; Peripheral Vascular Disease", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000336875200027", "keywords": "PPM1D; prostate cancer; biomarker; prognosis", "title": "PPM1D as a Novel Biomarker for Prostate Cancer After Radical Prostatectomy", "abstract": "Protein phosphatase magnesium-dependent 1 delta (PPM1D) is involved in several types of cancer. The current study examined the role of PPM1D expression in prostate cancer (PCa) tissues and in PCa cell lines. Expression of PPM1D was evaluated using immunohistochemistry in 234 PCa tissues after radical prostatectomy and 80 benign prostatic hyperplasia (BPH) tissues. The associations of PPM1D expression with clinicopathological parameters and survival were analyzed. In vitro, tumor cells were transfected with small interfering RNA targeting PPM1D (siPPM1D) or si-Scramble, and the cell proliferation, migration and invasion were determined. We found that PPM1D expression was significantly higher in PCa tissues than that in BPH tissues. PPM1D expression was positively correlated with Gleason score (p=0.022), T stage (p=0.015) and lymph node status (p=0.016). Kaplan-Meier curve analysis showed that patients with positive PPM1D expression had shorter biochemical recurrence-free survival and overall survival. Furthermore, multivariate analyses showed that PPM1D expression was an independent predictor of both biochemical recurrence-free (hazard ratio=3.437, 95% confidence interval=1.154-6.209, p=0.016) and overall survival (hazard ratio=5.026, 95% confidence interval=2.545-8.109, p=0.007). Knockdown of PPM1D inhibited the proliferation, migration and invasion capabilities of PC-3 and LNCaP cells. PPM1D expression may predict for both overall and biochemical recurrence-free survival in patients after radical prostatectomy for PCa. Elevated PPM1D expression plays a key role in progression of PCa.", "journal": "ANTICANCER RESEARCH", "category": "Oncology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000347576200091", "keywords": "SOFC system; Simulation; Interconnect; Reformer; Fault; Fault detection and isolation", "title": "Reformer faults in SOFC systems: Experimental and modeling analysis, and simulated fault maps", "abstract": "The effects of fuel processor faults in an solid oxide fuel cell (SOFC) system are analyzed. Focusing on a laboratory-size SOFC system, a reformer fault is investigated both experimentally and through a model; comparison between experimental and modeling results is presented and discussed. The results show that some types of reformer faults can be dangerous, because they can give rise to local thermal gradients as large as 10-20.10(2) K/m or more in the SOFC stack. Simulation results show that SOFC stacks employing metallic interconnects are expected to withstand faults of larger magnitude than SOFC stacks employing ceramic interconnects. Fault maps are presented and discussed, which can be the basis for the development of a fault detection and isolation (FDI) tool. Copyright (C) 2014, Hydrogen Energy Publications, LLC. Published by Elsevier Ltd. All rights reserved.", "journal": "INTERNATIONAL JOURNAL OF HYDROGEN ENERGY", "category": "Chemistry, Physical; Electrochemistry; Energy & Fuels", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000336348300004", "keywords": "cheatgrass; disturbance ecology; ecosystem resilience; restoration ecology; soil resource dynamics; spatial and temporal variation", "title": "Large-Scale Downy Brome Treatments Alter Plant-Soil Relationships and Promote Perennial Grasses in Salt Desert Shrub lands", "abstract": "Because invasive annual grasses can strongly influence soil resource availability and disturbance regimes to favor their own persistence, there is a great need to understand the interrelationships among invasive plant abundance, resource availability, and desirable species prominence. These interrelationships were studied in two salt desert sites where the local abundance of downy brome (Bromus tectorurn L.) varied spatially and increased more than 12-fold over a 3-yr period. We measured downy brome percentage cover, resource availability, and soil chemical and physical properties within 112 plots per site and found significant negative associations between downy brome abundance and both soil water content (P < 0.05; r=-0.27 to -0.49) and nitrate accumulation (P < 0.05; r=-0.34 to -0.45), which corroborated with the direction and strength of multivariate factor loadings assessed with principal component analysis. We then applied factorial combinations of prescribed burning and preemergence herbicide at management-relevant scales (i.e., 6 to 46 ha) as well as biomass removal to smaller plots (12.25 m(2)) at both sites to determine their impact on downy brome, soil resources, and resident plant species. Burning and herbicide applications, especially when combined, significantly reduced downy brome cover (P=0.069 to 0.015), which in turn increased soil nitrate accumulation and water content in the spring. Furthermore, for one shrubland site that was seeded 6 yr previously, the combination of burning and herbicide treatments significantly increased perennial grass percentage cover in the 2 yr posttreatment (P < 0.05). Results not only demonstrate the strong relationships between downy brome abundance, soil resources, and residence species for impoverished salt desert shrub ecosystems, but also suggest that restoration and management efforts must include tactics that facilitate resource use by the residual plant community or establish a greater abundance of species capable of high resource acquisition in the spring.", "journal": "RANGELAND ECOLOGY & MANAGEMENT", "category": "Ecology; Environmental Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000340977400031", "keywords": "Urban air quality; Atmospheric aerosols; FDMS-TEOM; Denuder", "title": "Analysis of semi-volatile materials (SVM) in fine particulate matter", "abstract": "The mass fraction of semi-volatile materials (SVM) in fine particulate matter (PM2.5) was investigated at a subtropical urban aerosol observatory (TARO, 25.0 degrees N, 121.5 degrees E) in Taipei, Taiwan during August 2013. In particular, an integrated Denuder-FDMS-TEOM system was employed to study the effectiveness of the coupling of FDMS and TEOM instruments. The charcoal and MgO denuders used in this study performed a removal efficiency of 89 and 95% for positive interferences in OC and nitrate measurements, respectively, and did not induce a significant particle loss during the field campaign, suggesting that denuders should be considered as a standard device in PM2.5 instrumentation. Analysis on the mass concentration and speciation data found that, as a result of SVM loss, FRM-based measurement underestimated PM2.5 by 21% in our case. Coupling FDMS to TEOM significantly improved the bias in PM2.5 mass concentration from -25% to -14%. The negative bias in FDMS TEOM was attributed to the failure of FDMS in recovering the mass of lost SVOMs in PM2.5. The results of this study highlight the significance of SVM in a subtropical urban environment, give a warning of underestimated health risk relevant to PM2.5 exposure, and necessitate further development of instrument and/or technique to provide accurate ambient levels of fine particulate matters. (C) 2014 The Authors. Published by Elsevier Ltd.", "journal": "ATMOSPHERIC ENVIRONMENT", "category": "Environmental Sciences; Meteorology & Atmospheric Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000343085600007", "keywords": "Job satisfaction; Health Information Systems; Benin", "title": "The Importance of Surgical Volume on Outcomes in Thyroid Surgery Revisited: Old is in Again Editorial Response to \"What's Old is New Again\" by Julie Ann Sosa (doi:10.1245/s10434-014-3850-z)", "abstract": "Introduction: Work engagement, an emerging concept in the field of positive psychology in the workplace is not well known in developing countries. Defined as a positive and fulfilling mindset related to work, it recalls a positive attitude incentive of performance and need to be investigated. In the context of the socioeconomic crisis of health workers, and with the chronic issue of poor quality of data, this study was designed to identify the factors asociated with work engagement among health workers in charge of data collection in the Benin Routine Health Information System. Methods: This study was a cross-sectional and analytical study targeting health workers in charge of data collection in public and private health centres. The dependent variable was work engagement and independent variables were sociodemographic and professional features, personal and professional resources and perception of technical factors. Logistic regression was used. The adequacy of the model was tested with the Hosmer-Lemeshow goodness of fit test. Results: The results indicate that the level of work engagement is similar with that observed in previous studies. Predictors identified in logistic regression are perception of technical factors, location of the job, and personal resources, such as level of effort and overcommitment. Discussion: This study identified factors associated with work engagement in a developing country, and adds to the knowledge concerning this new concept in Benin. The findings can contribute to research for improvement of human resources management in the health sector to achieve real performance and development.", "journal": "ANNALS OF SURGICAL ONCOLOGY", "category": "Oncology; Surgery", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000341775400009", "keywords": "Adiponectin; angiography; coronary artery disease; diabetes mellitus; India; inflammatory markers; TNF-alpha", "title": "Comparison of plasma adiponectin & certain inflammatory markers in angiographically proven coronary artery disease patients with & without diabetes - A study from India", "abstract": "Background & objectives: The association between adiponectin and risk of cardiovascular disease is well known. The aim of the present study was to evaluate adiponectin and certain inflammatory markers and to determine the correlations between them in angiographically proven coronary artery disease (CAD) in subjects with and without diabetes. Methods: A total of 180 subjects who underwent coronary angiography for symptoms suggestive of CAD were categorised into groups based on their diabetes and/or CAD status: group1 (non-diabetic non-CAD); group2 (non-diabetic CAD); group3 (diabetic non-CAD) and group4 (diabetic CAD). Adiponectin, tumour necrosis factor alpha (TNF-alpha) and soluble form of E-selectin (sE-selectin) were estimated using quantitative sandwich enzyme immunoassay and high sensitive C-reactive protein (hsCRP) by particle enhanced immunoturbidimetric method. Results: Adiponectin levels were significantly lower in subjects with either diabetes or CAD and were much lower in subjects who had both. hsCRP was elevated in CAD and diabetes but did not differ significantly between groups. sE-selectin and TNF-alpha levels were elevated in CAD. Adiponectin negatively correlated with age, glucose, sE-selectin, total and LDL cholesterol. hsCRP correlated with BMI, sE-selectin and urea. sE-selectin correlated with BMI, triglycerides and VLDL cholesterol, whereas TNF-alpha correlated with fasting plasma glucose. In the logistic regression analysis, adiponectin had a significant inverse association with CAD. sE-selectin and TNF-alpha also showed significant independent association with CAD. Interpretation & conclusions: Adiponectin and other inflammatory markers such as sE-selectin and TNF-alpha showed a significant association with CAD. Hence, early assessment of such markers can help to identify high risk patients, and to reduce the inflammatory component of diabetes and CAD.", "journal": "INDIAN JOURNAL OF MEDICAL RESEARCH", "category": "Immunology; Medicine, General & Internal; Medicine, Research & Experimental", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000338647400022", "keywords": "Bacterial genomes; Exponential correlation function; Markov model; Second largest eigenvalue; Hexamer; Periodicity of 10-11 bases; Heterogeneity; Codon positions", "title": "TR4 nuclear receptor functions as a tumor suppressor for prostate tumorigenesis via modulation of DNA damage/repair system", "abstract": "Testicular nuclear receptor 4 (TR4), a member of the nuclear receptor superfamily, plays important roles in metabolism, fertility and aging. The linkage of TR4 functions in cancer progression, however, remains unclear. Using three different mouse models, we found TR4 could prevent or delay prostate cancer (PCa)/prostatic intraepithelial neoplasia development. Knocking down TR4 in human RWPE1 and mouse mPrE normal prostate cells promoted tumorigenesis under carcinogen challenge, suggesting TR4 may play a suppressor role in PCa initiation. Mechanism dissection in both in vitro cell lines and in vivo mice studies found that knocking down TR4 led to increased DNA damage with altered DNA repair system that involved the modulation of ATM expression at the transcriptional level, and addition of ATM partially interrupted the TR4 small interfering RNA-induced tumorigenesis in cell transformation assays. Immunohistochemical staining in human PCa tissue microarrays revealed ATM expression is highly correlated with TR4 expression. Together, these results suggest TR4 may function as a tumor suppressor to prevent or delay prostate tumorigenesis via regulating ATM expression at the transcriptional level.", "journal": "CARCINOGENESIS", "category": "Oncology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000336713900002", "keywords": "Bioacoustic monitoring; Acoustic pattern recognition; Hyperbolic localization; Population size", "title": "Applying bioacoustic methods for long-term monitoring of a nocturnal wetland bird", "abstract": "Bioacoustic monitoring is becoming more and more popular as a non-invasive method to study populations and communities of vocalizing animals. Acoustic pattern recognition techniques allow for automated identification of species and an estimation of species composition within ecosystems. Here we describe an approach where on the basis of long term acoustic recordings not only the occurrence of a species was documented, but where the number of vocalizing animals was also estimated. This approach allows us to follow up changes in population density and to define breeding sites in a changing environment. We present the results of five years of continuous acoustic monitoring of Eurasian bittern (Botaurus stellaris) in a recent wetland restoration area. Using a setup consisting of four four-channel recorders equipped with cardioid microphones we recorded vocal activity during entire nights. Vocalizations of bitterns were detected on the recordings by spectrogram template matching. On basis of time differences of arrival (TDOA) of the acoustic signals at different recording devices booming bitterns could be mapped using hyperbolic localization. During the study period not only changes in the number of calling birds but also changes in their spatial distribution connected with changes in habitat structure could be documented. This semi-automated approach towards monitoring birds described here could be applied to a wide range of monitoring tasks for animals with long distance vocalizations. (C) 2013 Elsevier B.V. All rights reserved.", "journal": "ECOLOGICAL INFORMATICS", "category": "Ecology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000338644000037", "keywords": "Beef heifers; estrous synchronization; CIDR; fixed-time AI", "title": "Comparison of three CIDR-based fixed-time AI protocols in beef heifers", "abstract": "Several effective fixed-time AI (FTAI) protocols have been developed to facilitate AI in beef heifers that circumvent the need for estrus detection. Among these are the 5-d CO-Synch + controlled intravaginal progesterone insert (CIDR) protocol (5dCO), PGF(2 alpha) (PG) 6-d CIDR protocol (PG-6dCIDR), and 14-d CIDR-PG protocol (14dCIDR-PG). Although each of these protocols varies in duration and approach to synchronizing estrus and ovulation, each has been reported as an effective method to facilitate FTAI in beef heifers. Therefore, the objective of this study was to compare FTAI pregnancy rates in beef heifers synchronized with these 3 CIDR-based protocols. Virgin beef heifers (n = 801) at 4 locations were synchronized with 1 of 3 protocols: 1) 5dCO, an injection of GnRH (100 mu g) and insertion of a CIDR on d -5, PG (25 mg) and CIDR removal on d 0 with a second injection of PG (>4 h after CIDR removal) on d 0 and FTAI at 72 h after CIDR removal, 2) PG-6dCIDR, PG (25 mg) on d -9, GnRH (100 mu g) and insertion of a CIDR on d -6, PG and CIDR removal on d 0, and FTAI at 66 h after CIDR removal, or 3) 14dCIDR-PG, a 14-d CIDR insert from d -30 to -16, PG (25 mg) on d 0, and FTAI at 66 h after PG. All heifers received an injection of GnRH (100 g) concurrent with FTAI. Timing of treatment initiation was offset to allow all heifers to receive FTAI concomitantly and at random. Pregnancy success was determined between 35 and 40 d after FTAI by transrectal ultrasonography. Blood samples were collected before the beginning of each protocol and at the initiation of each protocol to determine estrous cycling status (77%). Data were analyzed using the GLIMMIX procedures of SAS. As expected, because of the duration of protocols, fewer heifers in the 14dCIDR-PG treatment were pubertal at initiation of synchronization than in the 5dCO (P < 0.05) and PG-6dCIDR (P = 0.10) treatments. Fixed-time AI pregnancy success did not differ between treatments (P = 0.14; 62.6%, 56.9%, and 53.3% for 5dCO, PG-6dCIDR, and 14dCIDR-PG, respectively). However, heifers that had reached puberty by initiation of synchronization had greater (P < 0.01) pregnancy success compared to heifers that were prepubertal (60.7% and 47.3%, respectively). In summary, all 3 protocols had similar FTAI pregnancy success, and puberty status had the greatest impact on pregnancy success.", "journal": "JOURNAL OF ANIMAL SCIENCE", "category": "Agriculture, Dairy & Animal Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000343529400003", "keywords": "Image denoising; Optimization problem; Weighted BPDN; K-SVD; Sparse K-SVD; Closed affine subspace learning", "title": "fMRI-Based Robotic Embodiment: Controlling a Humanoid Robot by Thought Using Real-Time fMR1", "abstract": "We present a robotic embodiment experiment based on real-time functional magnetic resonance imaging (rt-fMRI). In this study fMRI is used as an input device to identify a subject's intentions and convert them into actions performed by a humanoid robot. The process, based on motor imagery, has allowed four subjects located in Israel to control a HOAP3 humanoid robot in France, in a relatively natural manner, experiencing the whole experiment through the eyes of the robot. Motor imagery or movement of the left hand, the right hand, or the legs were used to control the robotic motions of left, right, or walk forward, respectively.", "journal": "PRESENCE-TELEOPERATORS AND VIRTUAL ENVIRONMENTS", "category": "Computer Science, Cybernetics; Computer Science, Software Engineering", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000340259200002", "keywords": "Bouc-Wen model; MR damper; nose landing gear; shimmy", "title": "Application of a magnetorheological damper modeled using the current-dependent Bouc-Wen model for shimmy suppression in a torsional nose landing gear with and without freeplay", "abstract": "In this study, shimmy of a nose landing gear model with torsional degree of freedom is analyzed. Equations governing the torsional nose landing gear model and the stretched string tire model are presented. Freeplay is incorporated into the model. A magnetorheological (MR) damper modeled using the current-dependent Bouc-Wen model is introduced to the torsional landing gear model with and without freeplay. Parameter identification of the Bouc-Wen model is accomplished using genetic algorithms. Incorporation of an MR damper into the landing gear model with and without freeplay is the advantage in this study. Implementation of the current-dependent Bouc-Wen model in such a landing gear model is another brand new concept.", "journal": "JOURNAL OF VIBRATION AND CONTROL", "category": "Acoustics; Engineering, Mechanical; Mechanics", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000337208800004", "keywords": "carbon assimilation; Carya illinoinensis; gas exchange; leaf senescence; nitrogen use efficiency; nutrient deficiency", "title": "Leaf Photosynthesis in Nitrogen-starved 'Western' Pecan Is Lower on Fruiting Shoots than Non-fruiting Shoots during Kernel Fill", "abstract": "Photosynthetic function in nut trees is closely related to nitrogen (N) nutrition because much of tree N is held within the leaf photosynthetic apparatus, but growing fruit and seeds also represent strong N sinks. When soil N availability is low, nut trees remobilize and translocate N from leaves to help satisfy N demand of developing fruit. Our objective was to describe shoot-level impacts of pecan [Carya illinoinensis (Wangenh.) K. Koch.] fruiting on leaf N and photosynthesis (P-n) during kernel fill under a range of tree N statuses. Our study was conducted in a mature 'Western' pecan orchard near Las Cruces, NM. In 2009, 15 trees showing a range of N deficiency symptom severity were grouped according to leaf SPAD into low, medium, and high N status categories. Differential N fertilizer rates were applied to the soil around high and medium N trees to accentuate differences in N status among the three categories. Light-saturated leaf P-n was measured on fruiting and non-fruiting shoots during kernel fill in 2009 and 2010. After measurement of P-n, the leaflet and its leaflet pair partner were collected, dried, and analyzed for tissue N. Leaf N concentration was significantly lower on fruiting shoots than non-fruiting shoots on all three sampling dates. The tree N status main effect was also significant, whereas the two-way interaction of shoot fruiting status and tree N status was not. Photosynthesis of leaves on fruiting shoots was significantly lower than that of non-fruiting shoots on all sampling dates. These data suggest that N demand by the growing kernel reduced N in leaves on the same shoot. Consequently, P-n of those leaves was reduced. The effect of tree N status and shoot fruiting status was best summarized with an additive model where there is a larger relative reduction in leaf N and P-n for fruiting shoots on trees with low N status.", "journal": "JOURNAL OF THE AMERICAN SOCIETY FOR HORTICULTURAL SCIENCE", "category": "Horticulture", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000337642800018", "keywords": "Tumor markers; Lung squamous cell carcinoma; Lung adenocarcinoma; Non-small cell lung cancer; Small cell lung cancer; Fatty acids; Sialic acid; Cytokeratins; Erythrocyte", "title": "Erythrocyte Fatty Acids as Potential Biomarkers in the Diagnosis of Advanced Lung Adenocarcinoma, Lung Squamous Cell Carcinoma, and Small Cell Lung Cancer", "abstract": "Objectives: To analyze the fatty acid profiles of erythrocyte total lipids from patients with advanced squamous cell lung carcinoma (SCC), lung adenocarcinoma (ADC), and small cell lung cancer (SCLC) and benign lung diseases (chronic obstructive pulmonary disease [COPP] and asthma) to reveal the fatty acids that could be used as lung cancer biomarkers. Methods: Thirty, 20, 15, 17, and 19 patients with SCC, ADC, SCLC, COPD, and asthma, respectively, and 55 healthy participants were enrolled in our study Fatty acid profiles were investigated using gas chromatography/mass spectrometry followed by receiver operating characteristic (ROC) curve analysis. Sialic acid (SA) and cytokeratins were measured by the thiobarbituric acid and immunoradiometric methods, respectively Results: At least one of the main fatty acids might be used as a biomarker for every type of lung cancer: arachidonic (20:4n6), linoleic (18:2n6), and stearic (18:0) acids for ADC, SCC, and SCLC, respectively These fatty acids showed diagnostic yields and operating characteristics similar to or higher than the commonly used SA or cytokeratin markers. Conclusions: Fatty acids from erythrocyte total lipids might be used as diagnostic biomarkers of lung ADC, SCC, and SCLC. Their use in different aspects of the disease process needs to be explored.", "journal": "AMERICAN JOURNAL OF CLINICAL PATHOLOGY", "category": "Pathology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000346134400007", "keywords": "brand attitude; green products; innovation; message framing; source credibility", "title": "Green Claims and Message Frames: How Green New Products Change Brand Attitude", "abstract": "In response to a top ten global consumer trend, firms are increasingly introducing environmentally sustainable (\"green\") new products. Firms allocate significant resources to this area; thus, the authors consider the brand-level implications by investigating how the introduction of green new products changes attitude toward the brand. In examining this relationship, they draw from social identity and framing theories to investigate drivers of green new product introductions as well as the moderating effects of message framing, source credibility, and product type. Estimating a three-stage least squares model based on new product introductions from 75 brands across a four-year time period (2009-2012), the authors find that green new product introductions can indeed improve brand attitude and that both the brand and category's positioning influence the introduction of green new products. They also find that the quantity of green messages, the product type, and their source credibility influence the extent to which green new products change brand attitude. The authors use these findings to provide guidance for managers as they attempt to effectively link their green innovation efforts to improve consumer attitudes toward their brands.", "journal": "JOURNAL OF MARKETING", "category": "Business", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000342653900009", "keywords": "Conventional breeding; herbicide-resistant crops; herbicide-resistant lettuce", "title": "Evaluation of Tribenuron-Methyl on Sulfonylurea-Resistant Lettuce Germplasm", "abstract": "A sulfonylurea (SU) herbicide resistance allele discovered in prickly lettuce was previously transferred to domestic lettuce with the cultivar name 'ID-BR1'. ID-BR1 was acquired, and the SU resistance allele was transferred through traditional breeding methods to five common commercial lettuce types: butterhead, crisphead, green leaf, red leaf; and romaine. Field trials were conducted at Salinas, CA during 2011 and 2012 to evaluate POST applications of tribenuron-methyl (tribenuron) on SU-susceptible and SU-resistant lettuce types. Treatments included a nontreated control, pronamide applied PRE at 1,340 g ai ha(-1), and tribenuron at 4, 9, and 17 g ai ha(-1) applied POST. Data collected were: weed control, crop injury estimates (0 = safe, 100 = dead), stand counts, and lettuce yields. Injury to lettuce from tribenuron was high in SU-susceptible lettuce types and low in SU-resistant accessions. With the exceptions of a romaine lettuce line that still may have some susceptible individuals, tribenuron did not reduce yield of SU-resistant lettuce, but did reduce the yield of SU-susceptible lettuce. Suppression of weeds such as common groundsel and annual sowthistle was higher with tribenuron than with pronamide. Tribenuron should be considered for registration as a lettuce herbicide for SU-resistant lettuce to improve current weed management options for that crop.", "journal": "WEED TECHNOLOGY", "category": "Agronomy; Plant Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000339888100009", "keywords": "Knapsack problems; Online algorithms; Competitive ratio", "title": "Texture Evolution as Determined by In situ Neutron Diffraction During Annealing of Iron Deformed by Equal Channel Angular Pressing", "abstract": "In situ neutron diffraction experiments were performed to follow the annealing behavior of iron deformed by equal-channel angular pressing at room temperature using route B (c) to a total von Mises strain of epsilon (vM) = 9.2. The temperature was varied from room temperature to 1223 K (950 A degrees C), while neutron diffraction data for quantitative texture analysis were collected at a given temperature when holding for 5 minutes. Pole figures and orientation distribution function maps from neutron diffraction and electron backscatter diffraction measurements were used to follow the changes in crystallographic texture and grain size during annealing. In situ neutron diffraction experiments allowed understanding and identifying texture-related changes that occur during recrystallization, grain growth, and phase transformation in iron.", "journal": "METALLURGICAL AND MATERIALS TRANSACTIONS A-PHYSICAL METALLURGY AND MATERIALS SCIENCE", "category": "Materials Science, Multidisciplinary; Metallurgy & Metallurgical Engineering", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000348334900005", "keywords": "Attention deficit hyperactivity disorder; risperidone; autistic traits; children", "title": "Autistic Traits and Factors Related to a Clinical Decision to Use Risperidone in Children with Attention Deficit Hyperactivity Disorder", "abstract": "Objective: Our aim was to investigate the factors associated with a clinical decision to use risperidone in children and adolescents with a primary diagnosis of Attention Deficit Hyperactivity Disorder (ADHD) and to investigate autistic traits (ATs) and their influence on treatment decisions in this population Methods: We retrospectively compared four treatment groups of children with a primary diagnosis of ADHD [no psychotropics group, NPG (n=73, mean age (in years)= 9.22 +/- 2.94); stimulant-only, S (n=184, mean age (in years)= 10.52 +/- 2.98); risperidoneonly, R (n=51, mean age (in years)= 10.18 +/- 3.52); and stimulant plus risperidone, SR (n=30, mean age (in years)= 9.37 +/- 2.71] from a private child and adolescent psychiatry clinic. Baseline assessments, in addition to a semistructured interview, included a sociodemographic information form, the parent-rated Child Behavior Checklist for ages 6 to 18 (CBCL-6-18) and the parent and teacher-rated SNAP-IV scale (Swanson, Nolan and Pelham). Results: There were significant between-group differences on CBCL T scores for total problems, externalizing problems, social problems, thought problems, attention problems, and aggression (all p<0.05) and on the parent SNAP inattention and combined scores (one-way ANOVA). The SR group had significantly higher scores (i) on the mentioned subscales of the CBCL when compared with the NPG and S groups, (ii) on the CBCL social problems subscale when compared with the R group, (iii) on the parent SNAP inattention scale when compared with the NPG and R groups and (iv) on the parent-rated SNAP total score when compared with the other 3 groups (Tukey post hoc test). Sixty-four children above the CBCL-AT cutoff had higher scores than those of children below the cutoff on parent and teacher-rated individual ADHD symptoms. In the logistic regression analysis, the clinician's decision to use risperidone (either alone or in combination with stimulants) was significantly related to higher scores on the CBCL social problems (p=0.025) and thought problems (p=0.039) subscales. The presence of AT as a category, however, did not predict treatment assignment. Conclusion: In this clinical sample, parent-rated social problems and thought problems were associated with the clinician's decision to use risperidone in the treatment of ADHD cases (alone or in combination with stimulants). ADHD children with AT had more severe symptoms of ADHD and displayed more learning disability. However, AT profile as a category was not significantly associated with the use of risperidone. The better characterization of non-ADHD symptoms of ADHD children (social and emotional symptoms) may help to develop more individualized clinical interventions, such as nonpharmacological interventions for social development, which may result in a reduction in the use of medications targeting these symptoms in this group of children.", "journal": "KLINIK PSIKOFARMAKOLOJI BULTENI-BULLETIN OF CLINICAL PSYCHOPHARMACOLOGY", "category": "Pharmacology & Pharmacy; Psychiatry", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000344040300013", "keywords": "Grid computing system; Reliability; Redundancy; GA; Deadline; Budget", "title": "Glomerular Autoimmune Multicomponents of Human Lupus Nephritis In Vivo: alpha-Enolase and Annexin Al", "abstract": "Renal targets of autoimmunity in human lupus nephritis (LN) are unknown. We sought to identify autoantibodies and glomerular target antigens in renal biopsy samples from patients with LN and determine whether the same autoantibodies can be detected in circulation. Glomeruli were microdissected from biopsy samples of 20 patients with LN and characterized by proteomic techniques. Serum samples from large cohorts of patients with systemic lupus erythematosus (SLE) with and without LN and other glomerulonephritides were tested. Glomerular IgGs recognized 11 podocyte antigens, with reactivity varying by LN pathology. Notably, IgG2 autoantibodies against alpha-enolase and annexin AI were detected in 11 and 10 of the biopsy samples, respectively, and predominated over other autoantibodies. Immunohistochemistry revealed colocalization of a-enolase or annexin AI with IgG2 in glomeruli. High levels of serum anti-alpha-enolase (>15 mg/L) IgG2 and/or anti-annexin Al (>2.7 mg/L) IgG2 were detected in most patients with LN but not patients with other glomerulonephritides, and they identified two cohorts: patients with high anti-alpha-enolase/low anti-annexin Al IgG2 and patients with low anti-alpha-enolase/high anti-annexin Al IgG2. Serum levels of both autoantibodies decreased significantly after 12 months of therapy for LN. Anti-alpha-enolase IgG2 recognized specific epitopes of alpha-enolase and did not cross-react with dsDNA. Furthermore, nephritogenic monoclonal IgG2 (clone H147) derived from lupus-prone MRL-lpr/lpr mice recognized human alpha-enolase, suggesting homology between animal models and human LN. These data show a multiantibody composition in LN, where IgG2 autoantibodies against alpha-enolase and annexin Al predominate in the glomerulus and can be detected in serum.", "journal": "JOURNAL OF THE AMERICAN SOCIETY OF NEPHROLOGY", "category": "Urology & Nephrology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000335146200001", "keywords": "Light emitting diodes; semiconductor device modeling; wide band gap semiconductors", "title": "Tandem Structure for Efficiency Improvement in GaN Based Light-Emitting Diodes", "abstract": "The improvement in efficiency of nitride-based lighte-mitting diodes by the implementation of a vertically stacked tandem structure is investigated. The electrical and optical characteristics of an LED with a tunnel junction inserted between two active regions are modeled, and the wall-plug efficiency gain of the tandem LED is shown to start at 4.2% at low output powers ( 27.6 mW), with increasing efficiency gains with increased output power due to the alleviation of efficiency droop. The TLED concept further enables optimization of device structure, allowing removal of electron blocking layer, and optimization of number of quantum wells for improvement in efficiency.", "journal": "JOURNAL OF LIGHTWAVE TECHNOLOGY", "category": "Engineering, Electrical & Electronic; Optics; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000340852200050", "keywords": "Metal-semiconductor devices; Electrical properties; Aluminum gallium nitride; Schottky barrier diodes; Current-voltage-temperature characteristics; Gaussian distribution; Thermionic emission theory; Barrier inhomogeneity", "title": "Investigation of temperature dependent electrical properties of Ni/Al0.26Ga0.74N Schottky barrier diodes", "abstract": "The current-voltage (I-V) characteristics of the Ni/Al026Ga0.74N Schottky barrier diodes (SBDs) were measured in the temperature range of 100-310 K by the step of 10 K. The forward I-V characteristics were analyzed on the basis of the thermionic emission theory. The characteristics of diode parameters such as the Schottky height (SBH) and the ideality factor were investigated as a function of temperature. An experimental SBH value about 1.021 eV was obtained for the Ni/Al026Ga0.74N SBD at 300 K. The experimental results show that the values of the ideality factor decrease while the values of the SBH increase with increasing temperature. The temperature dependence of the SBH was explained on the basis of a thermionic emission mechanism with the Gaussian distribution of the SBHs due to the SBH inhomogeneities at the metal-semiconductor interface. The values the mean barrier height (Phi) over bar (bo) and the standard deviation sigma(s0) were 1.362 eV and 133 may in the temperature range of 210-300 K, 1204 eV and 111 meV in the temperature range of 100-210 K, respectively. The modified Richardson plots according to inhomogeneity of the SBHs have a good linearity in the corresponding temperature range. The values of Richardson constant A* were found to be 31.46 Acm(-2) K-2 and 3336 Acm(-2) K-2 in the temperature ranges of 210-310 K and 100-210 K, respectively. The obtained Richardson constant values are good agreement with the theoretical value of 34.56 Acm(-2) K-2 known for n-type Al0.26Ga0.74N. (C) 2014 Elsevier B.V. All rights reserved.", "journal": "THIN SOLID FILMS", "category": "Materials Science, Multidisciplinary; Materials Science, Coatings & Films; Physics, Applied; Physics, Condensed Matter", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000336116600007", "keywords": "Oscillations; MEG; EEG; Spectral analysis; Single-trial; Information theory; Classification", "title": "Analytical methods and experimental approaches for electrophysiological studies of brain oscillations", "abstract": "Brain oscillations are increasingly the subject of electrophysiological studies probing their role in the functioning and dysfunction of the human brain. In recent years this research area has seen rapid and significant changes in the experimental approaches and analysis methods. This article reviews these developments and provides a structured overview of experimental approaches, spectral analysis techniques and methods to establish relationships between brain oscillations and behaviour. (C) 2014 The Author. Published by Elsevier B.V.", "journal": "JOURNAL OF NEUROSCIENCE METHODS", "category": "Biochemical Research Methods; Neurosciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000341366000012", "keywords": "Second-hand smoke; Urinary cotinine; Self-reported exposure; Former smokers; Alcohol drinking", "title": "Obesity in Percutaneous Nephrolithotomy. Is Body Mass Index Really Important?", "abstract": "OBJECTIVE To evaluate the influence of obesity in the results of percutaneous nephrolithotomy (PCNL) in terms of efficacy and safety and to evaluate other aspects such as fluoroscopy time, radiation exposure, total operative time, hemoglobin loss, hospital stay, and the need of auxiliary procedures. MATERIALS AND METHODS We evaluated prospectively all the PCNLs performed at our institution between 2011 and 2012. A series of perioperative and postoperative details were recorded in our database. The patients were distributed in 4 groups using World Health Organization's classification of body mass index (BMI): normal weight, <= 25 kg/m(2); overweight, 25-29.9 kg/m(2); obese, 30-39.9 kg/m(2); and morbidly obese, >= 40 kg/m(2). Modified Clavien classification was used for reporting the complications. Results were compared between the groups using the chi square and multivariate logistic regression tests. RESULTS A total of 255 procedures were performed between January 2011 and December 2012. Overall stone clearance was 76.3% and complication rate using the modified Clavien grading system was 31.4%. No statistical differences in terms of complication rate and stone free rate were noted between the 4 groups. Total operative time and radiation doses increase along with BMI. No difference was found in fluoroscopy time, failure to gain access, hospital stay, or need for auxiliary procedures. CONCLUSION Obesity does not increase complications in PCNL, and the efficacy of the technique is similar to normal weight patients with appropriate expertise. Total operative time and radiation exposure increase along with BMI, putting patients at risk. (C) 2014 Elsevier Inc.", "journal": "UROLOGY", "category": "Urology & Nephrology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000344869500009", "keywords": "Carotid atherosclerosis; carotid contrast ultrasound; carotid stenosis; coronary heart disease; monocyte subsets; plaque neovascularization", "title": "Expression of monocyte subsets and angiogenic markers in relation to carotid plaque neovascularization in patients with pre-existing coronary artery disease and carotid stenosis", "abstract": "Aim. To characterize blood monocyte subsets in patients with different degrees of carotid atherosclerosis and pathological carotid plaque neovascularization. Methods. Assessment of carotid plaque neovascularization using contrast ultrasonography and flow cytometric quantification of monocyte subsets and their receptors involved in inflammation, angiogenesis, and tissue repair was done in 40 patients with carotid stenosis >= 50% and CAD (CS > 50), 40 patients with carotid stenosis < 50% and documented CAD (CS < 50), 40 hypercholesterolaemic controls (HC group), and 40 normocholesterolaemic controls (NC). Results. CS > 50 and CS < 50 groups had increased counts of Mon1 ('classical' CD14++CD16-CCR2 + cells) compared to HCs (P = 0.03, and P = 0.009). Mon3 ('non-classical' CD14 + CD16++ CCR2- cells) were only increased in CS < 50 compared with HCs (P < 0.01). Both CS>50 and CS < 50 groups showed increased expression of proinflammatory interleukin-6 receptor on Mon1 and Mon2 ('intermediate' CD14++CD16+CCR2+cells); TLR4, proangiogenic Tie2 on all subsets (P < 0.01 for all). In multivariate regression analysis only high Mon1 count was a significant predictor of carotid stenosis (P = 0.04) and intima-media thickness (P = 0.02). In multivariate regression analysis only the Mon1 subset was significantly associated with severe, grade 2 neovascularization (P = 0.034). Conclusion. In this pilot study classical monocytes (Mon1) represent the only monocyte subset predictive of the severity of carotid and systemic atherosclerosis, such as carotid intima-media thickness, degree of carotid stenosis, and presence of carotid intraplaque neovascularization.", "journal": "ANNALS OF MEDICINE", "category": "Medicine, General & Internal", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000342470400001", "keywords": "Smoking cessation; Allen Carr training; Quasi-experimental; CO validation", "title": "The effectiveness of the Allen Carr smoking cessation training in companies tested in a quasi-experimental design", "abstract": "Background: The Allen Carr training (ACt) is a popular one-session smoking cessation group training that is provided by licensed organizations that have the permission to use the Allen Carr method. However, few data are available on the effectiveness of the training. Methods: In a quasi-experimental design the effects of the existing practice of providing the ACt to smokers (n = 124) in companies on abstinence, were compared to changes in abstinence in a cohort of similar smokers in the general population (n = 161). To increase comparability of the smokers in both conditions, smokers in the control condition were matched on the group level on baseline characteristics (fourteen variables) to the smokers in the ACt. The main outcome measure was self-reported continuous abstinence after 13 months, which was validated using a CO measurement in the Act condition. Results: Logistic regression analyses showed that when baseline characteristics were comparable, significantly more responding smokers were continuously abstinent in the ACt condition compared to the control condition, Exp(B) = 6.52 (41.1% and 9.6%, respectively). The all-cases analysis was also significant, Exp(B) = 5.09 (31.5% and 8.3%, respectively). Conclusion: Smokers following the ACt in their company were about 6 times more likely to be abstinent, assessed after 13 months, compared to similar smokers in the general population. Although smokers in both conditions did not differ significantly on 14 variables that might be related to cessation success, the quasi-experimental design allows no definite conclusion about the effectiveness of the ACt. Still, these data support the provision of the ACt in companies.", "journal": "BMC PUBLIC HEALTH", "category": "Public, Environmental & Occupational Health", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000346585300040", "keywords": "cesarean; class III obesity; induction of labor; mode of delivery", "title": "Mode of delivery in women with class III obesity: planned cesarean compared with induction of labor", "abstract": "OBJECTIVE: To compare maternal and neonatal outcomes between planned cesarean delivery and induction of labor in women with class III obesity (body mass index >= 40 kg/m(2)). STUDY DESIGN: In this retrospective cohort study, we identified all women with a body mass index >= 40 kg/m(2) who delivered a singleton at our institution from January 2007 to February 2013 via planned cesarean or induction of labor (regardless of eventual delivery route) at 37-41 weeks. Patients in spontaneous labor were excluded. The primary outcome was a composite of maternal morbidity including death as well as operative, infection, and thromboembolic complications. The secondary outcome was a neonatal morbidity composite. Additional outcomes included individual components of the composites. Student t, chi(2), and Fisher exact tests were used for statistical analysis. To calculate adjusted odds ratios, covariates were analyzed via multivariable logistic regression. RESULTS: There are 661 mother-infant pairs that met enrollment criteria-399 inductions and 262 cesareans. Groups were similar in terms of prepregnancy weight, pregnancy weight gain, and delivery body mass index. Of the 399 inductions, 258 had cervical ripening (64.7%) and 163 (40.9%) had a cesarean delivery. After multivariable adjustments, there was no significant difference in the maternal morbidity composite (adjusted odds ratio, 0.98; 95% confidence interval, 0.55-1.77) or in the neonatal morbidity composite (adjusted odds ratio, 0.81; 95% confidence interval, 0.37-1.77) between the induction and cesarean groups. CONCLUSION: In term pregnant women with class III obesity, planned cesarean does not appear to reduce maternal and neonatal morbidity compared with induction of labor.", "journal": "AMERICAN JOURNAL OF OBSTETRICS AND GYNECOLOGY", "category": "Obstetrics & Gynecology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000340333500034", "keywords": "Lambs; Computed tomography; VIAScan (R); lean meat yield", "title": "Comparison of rankings for lean meat based on results from a CT scanner and a video image analysis system", "abstract": "Coopworth cross lambs born over three years were examined in this study. Differences between two machines; a computer tomography (CT) scanner and a VIAScan (R) system for the estimation of carcase lean weight in lamb carcases was examined. The CT scanner provided a significantly higher estimate of carcase lean. The rank correlation (0.84) between the CT scanner and the VIAScan (R) system for the prediction of carcase lean was significant, but there was a different ranking for carcase lean depending on which machine was used. This has important ramifications for the use of VIAScan (R) data in the New Zealand Sheep Improvement Ltd genetic programme. (C) 2014 Elsevier Ltd. All rights reserved.", "journal": "MEAT SCIENCE", "category": "Food Science & Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000339175900011", "keywords": "biomarkers; cytokines; pleural effusion", "title": "Smoking and choroidal thickness in patients over 65 with early-atrophic age-related macular degeneration and normals", "abstract": "Objective To compare macular choroidal thickness between cigarette smokers, those with a history of smoking, and nonsmokers in patients over 65 years of age with early-atrophic age-related macular degeneration (AMD) and normals. Methods Prospective, consecutive, observational case series. Enhanced depth imaging spectral domain optical coherence tomography 12-line radial scans were performed and choroidal thickness manually quantified at 84 points in the central 3mm of the macula. Data of normals, soft drusen alone, and soft drusen with additional features of early AMD were compared. A multivariate analysis of variance (MANOVA) model, controlling for age, was constructed to evaluate the effect of smoking history and AMD features on choroidal thickness. Results A history of smoking was significantly associated with a thinner choroid across all patients via logistic regression (P = 0.004; O.R. = 12.4). Mean macular choroidal thickness was thinner for smokers (148 +/- 63 mu m) than for nonsmokers (181 +/- 65 mu m) among all diagnosis categories (P 0.003). Subgroup analysis of patients with AMD features revealed a similar decreased choroidal thickness in smokers (121 +/- 41 mu m) compared with nonsmokers (146 +/- 46 mu m, P = 0.006). Bivariate analysis revealed an association between increased pack-years of smoking and a thin choroid across all patients (P<0.001) and among patients with features of early AMD (P<0.001). Both the presence of features of macular degeneration (P<0.001) and a history of smoking (P = 0.024) were associated with decreased choroidal thickness in a MANOVA model. Conclusion Chronic cigarette smoke exposure may be associated with decreased choroidal thickness. There may be an anatomic sequelae to chronic tobacco smoke exposure that underlies previously reported AMD risk.", "journal": "EYE", "category": "Ophthalmology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000343381100015", "keywords": "Crassostrea gigas; Retinoic acid-inducible gene I; Antiviral immune response; PAMP challenge", "title": "The first invertebrate RIG-I-like receptor (RLR) homolog gene in the pacific oyster Crassostrea gigas", "abstract": "Retinoic acid-inducible gene I (RIG-I)-like receptor (RLR) is a pivotal receptor that detects numerous RNA and DNA viruses and mediates the innate induction of interferons and pro-inflammatory cytokines upon viral infection. In the present study, we cloned and characterized the first RIG-I gene in a marine mollusk, Crassostrea gigas, and designated it as CgRIG-I. The full-length CgRIG-I cDNA is 3436 bp, including 5'- and 3'-untranslated regions (UTRs) of 93 bp and 286 bp, respectively, and an open reading frame (ORF) of 3057 bp. The gene encodes a 1018 amino acid polypeptide with an estimated molecular mass of 116.5 kDa. SMART analysis showed that the CgRIG-I protein had the typical conserved domains, including the caspase activation and recruitment domains (CARDs), the RNA helicase domain and the C-terminal regulatory domain (RD). Phylogenetic analysis revealed that CgRIG-I was grouped into the clade of its vertebrate homologs. Moreover, CgRIG-I expression could be specifically increased after stimulation by poly(I:C) rather than by other PAMPs such as lipopolysaccharide (LPS), peptidoglycan (PGN), heat-killed Listeria monocytogenes (HKLM) and heat-killed Vibrio alginolyticus (HKVA). Meanwhile, six IRF, three STAT and one NF-kappa B predicted sites were identified in the CgRIG-I promoter, which was consistent with its high responsiveness to poly(I:C). In summary, this report provides the first CgRIG-I sequence of a mollusk, but its function in the antiviral immune response requires further investigation. (C) 2014 Elsevier Ltd. All rights reserved.", "journal": "FISH & SHELLFISH IMMUNOLOGY", "category": "Fisheries; Immunology; Marine & Freshwater Biology; Veterinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000340820700005", "keywords": "Cell motility; Actin cytoskeleton; Electron microscopy; Image analysis; Fitting", "title": "Efficacy and Safety of Extended-Release Quetiapine Fumarate in Youth with Bipolar Depression: An 8 Week, Double-Blind, Placebo-Controlled Trial", "abstract": "Objective: Quetiapine is an atypical antipsychotic with demonstrated efficacy in the treatment of adolescent schizophrenia and pediatric bipolar mania. Large, placebo-controlled studies of interventions in pediatric bipolar depression are lacking. The current study investigated the efficacy and safety of quetiapine extended-release (XR) in patients 10-17 years of age, with acute bipolar depression. Methods: This multicenter, double-blind, randomized, placebo-controlled study investigated quetiapine XR (dose range, 150-300 mg/day) in pediatric outpatients with an American Psychiatric Association, Diagnostic and Statistical Manual of Mental Disorders, 4th ed., Text Revision (DSM-IV-TR) diagnosis of bipolar I or bipolar II disorder (current or most recent episode depressed) treated for up to 8 weeks (ClinicalTrials.gov identifier: NCT00811473). The primary study outcome was mean change in Children's Depression Rating Scale-Revised (CDRS-R) total score. Secondary efficacy outcomes included CDRS-R-based response and remission rates. Results: Of 193 patients randomized to treatment, 144 patients completed the study (75.3% of quetiapine XR group [n = 70]; 74.0% of placebo group [n = 74]). Least squares mean changes in CDRS-R total score at week 8 were: -29.6 (SE, 1.65) with quetiapine XR and -27.3 (SE, 1.60) with placebo, a between-treatment group difference of -2.29 (SE, 1.99; 95% CI, -6.22, 1.65; p = 0.25; mixed-model for repeated measures analysis). Rates of response and remission did not differ significantly between treatment groups. The safety profile of quetiapine XR was broadly consistent with the profile reported previously in adult studies of quetiapine XR and pediatric studies of quetiapine immediate-release (IR). Potentially clinically significant elevations in clinical chemistry values included triglycerides (9.3%, quetiapine XR; 1.4%, placebo group) and thyroid stimulating hormone (4.7%, quetiapine XR; 0%, placebo group). An adverse event potentially related to diabetes mellitus occurred in 3.3% of the quetiapine XR versus no adverse events in the placebo group. Conclusions: Quetiapine XR did not demonstrate efficacy relative to placebo in this 8 week study of pediatric bipolar depression. Quetiapine XR was generally safe and well tolerated.", "journal": "JOURNAL OF CHILD AND ADOLESCENT PSYCHOPHARMACOLOGY", "category": "Pediatrics; Pharmacology & Pharmacy; Psychiatry", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000341185900057", "keywords": "methods: numerical; neutrinos; radiative transfer", "title": "MODA: a new algorithm to compute optical depths in multidimensional hydrodynamic simulations", "abstract": "Aims, We introduce the multidimensional optical depth algorithm (MODA) for the calculation of optical depths in approximate multidimensional radiative transport schemes, equally applicable to neutrinos and photons. Motivated by (but not limited to) neutrino transport in three-dimensional simulations of core-collapse supernovae and neutron star mergers. our method makes no assumptions about the geometry of the matter distribution, apart from expecting optically transparent boundaries. Methods. Based on local information about opacities, the algorithm figures out an escape route that tends to minimize the optical depth without assuming any predefined paths for radiation. Its adaptivity makes it suitable for a variety of astrophysical settings with complicated geometry (e.g., core-collapse supernovae, compact baldly mergers, tidal disruptions, star formation, etc.). We implement the MODA algorithm into both a Eulerian hydrodynamics code with a fixed, uniform grid and into an SPH code where we use a tree structure that is otherwise used for searching neighbors and calculating gravity. Results. In a series of numerical experiments, we compare the MODA results with analytically known solutions. We also use snapshots from actual 3D simulations and compare the results of MODA with those obtained with other methods, such as the global and local ray-by-ray method. It turns out that MODA achieves excellent accuracy at a moderate computational cost In appendix we also discuss implementation details and parallelization strategies.", "journal": "ASTRONOMY & ASTROPHYSICS", "category": "Astronomy & Astrophysics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000341506800010", "keywords": "Joint diseases; Joint prosthesis; Diagnostics; Crystal induced synovitis; Wear particles", "title": "Histopathological particle algorithm. Particle identification in the synovia and the SLIM", "abstract": "In the histopathological diagnostics of synovitis and the synovium-like interface membrane (SLIM) the identification of crystals and crystal-like deposits and the associated inflammatory reactions play an important role. The multitude of endogenous crystals, the range of implant materials and material combinations, and the variability in the formation process of different particles explain the high morphological particle heterogeneity which complicates the diagnostic identification of diagnostic particles. A simple histopathological particle algorithm has been designed which allows methodological particle identification based on (1) conventional transmitted light microscopy with a guide to particle size, shape and color, (2) optical polarization criteria and (3) enzyme histochemical properties (oil red staining and Prussian blue reaction). These methods, the importance for particle identification and the differential diagnostics from non-prosthetic materials are summarized in the so-called histopathological particle algorithm. A total of 35 cases of synovitis and SLIM were analyzed and validated according to these criteria. Based on these criteria and a dichotomous differentiation the complete spectrum of particles in the SLIM and synovia can be defined histopathologically. For histopathological diagnosis a particle score for synovitis and SLIM is recommended to evaluate (1) the predominant type of prothetic wear debris with differentiation between microparticles, and macroparticles, (2) the presence of non-prosthesis material particles and (3) the quantification of particle-association necrosis and lymphocytosis. An open, continuously updated web-based particle algorithm would be helpful to address the issue of particle heterogeneity and include all new particle materials generated in a rapidly changing field.", "journal": "ZEITSCHRIFT FUR RHEUMATOLOGIE", "category": "Rheumatology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000340687700002", "keywords": "Assemblage structure; Foraminifera; Lower-Middle Jurassic boundary; Lusitanian Basin; Unconstrained/constrained ordination; PERMANOVA", "title": "Palaeoecological distribution pattern of Early-Middle Jurassic benthic foraminifera in the Lusitanian Basin (Portugal) based on multivariate analysis", "abstract": "Paleontologists have favoured univariate statistical methods over multivariate ones, using information-rich data to derive diversity measures such as the Simpson's index. However, such palaeontological datasets may benefit from flexible multivariate methods that exploit data directly. Hence in this work, with the aim of quantifying and characterising the spatio-temporal change in the species composition and abundance of benthic foraminiferal assemblages, univariate and multivariate statistical techniques are applied on a Lusitanian Basin dataset from the Lower-Middle Jurassic transition. This paper presents the trends identified at spatial and temporal scales, i.e. among sections corresponding to distal, middle and proximal palaeogeographical positions within the platform and between two successive stratigraphic intervals (upper Toarcian and lower Aalenian). It also compares the results obtained by univariate and multivariate methods. Samples in the reference sections are composed by taxa typical of the Jurassic carbonate marine platforms of the Boreal Realm, such as Vaginulinidae and Lenticulina species. Univariate analysis only found significant change in species diversity between the two most distant sections and failed to detect any temporal change. In contrast, multivariate analysis (PERMANOVA and NMDS) revealed that spatial change in assemblage structure (22%) quadruples temporal change (5%). CAP analysis identified characteristic species for each section along the palaeogradient, with a posterior cross-validation procedure that classified correctly 97% of the samples. In fact, multivariate results suggest the existence of stable palaeoecological conditions across the Lower-Middle Jurassic transition in the Lusitanian Basin. Future work in the field may benefit from multivariate analysis, including hydrocarbon exploration activities where only core samples are available and foraminifera are currently used. (C) 2014 Elsevier B.V. All rights reserved.", "journal": "PALAEOGEOGRAPHY PALAEOCLIMATOLOGY PALAEOECOLOGY", "category": "Geography, Physical; Geosciences, Multidisciplinary; Paleontology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000340220800006", "keywords": "Sablefish (Anoplopoma fimbria); Length/age at maturity; Fishery management; Life history; West Coast groundfish", "title": "Maturity and growth of sablefish, Anoplopoma fimbria, along the US West Coast", "abstract": "Life history parameters such as growth and reproductive processes are dynamic, and shift with environmental and anthropogenic influences. Providing up-to-date life history information is critical for population dynamic models that are used to manage fisheries. Sablefish, Anoplopoma fimbria, are a valuable groundfish species that support commercial fishing throughout the North Pacific Ocean, including off the U.S. West Coast. Ovaries were histologically assessed for maturity status, intensity of atresia, and the presence of post-ovulatory follicles. We evaluated coast-wide length and age at maturity for 477 female sablefish by fitting a logistic regression model to the proportion mature. We also examined length-at-age data for 525 female sablefish based on the von Bertalanffy growth model. Since maturity and growth are important components of models used to assess the status of sablefish, we explored variation in these life history parameters among three regions along the West Coast separated by major biogeographic features at Cape Mendocino, CA (40 degrees 26'N) and Pt. Conception, CA (34 degrees 27' N). Coast-wide estimates of length (L-50) and age (A(50)) at 50% maturity were 54.64 cm and 6.86 years, respectively. Differences in L-50 were found north and south of Cape Mendocino, CA, and by depth (range 55-1280 m). Length at 50% maturity decreased from north to south and with increasing depth within each region. Age at 50% maturity also differed north and south of Cape Mendocino, CA, with female sablefish maturing at a younger age further north. Growth of female sablefish demonstrated similar differences among geographic areas and by depth. Asymptotic size (L-infinity, cm) tended to increase at higher latitude and decrease with depth, while growth rates (k, year(-1)) were elevated north of Point Conception, CA, and generally at depths <= 550 m. For female sablefish, the larger size and younger age at 50% maturity, as well as larger asymptotic sizes and somewhat elevated growth rates, were associated with regions within the California Current Ecosystem characterized by elevated productivity and cooler water temperature. Published by Elsevier B.V.", "journal": "FISHERIES RESEARCH", "category": "Fisheries", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000343925600004", "keywords": "Wireless sensor networks; Broadcast; Scheduling; Minimum latency", "title": "Approximation algorithms for broadcasting in duty cycled wireless sensor networks", "abstract": "Broadcast is a fundamental operation in wireless sensor networks (WSNs). Given a source node with a packet to broadcast, the aim is to propagate the packet to all nodes in a collision free manner whilst incurring minimum latency. This problem, called minimum latency broadcast scheduling (MLBS), has been studied extensively in wireless ad-hoc networks whereby nodes remain on all the time, and has been shown to be NP-hard. However, only a few studies have addressed this problem in the context of duty-cycled WSNs. In these WSNs, nodes do not wake-up simultaneously, and hence, not all neighbors of a transmitting node will receive a broadcast packet at the same time. Unfortunately, the problem remains NP-hard and multiple transmissions may be necessary due to different wake-up times. Henceforth, this paper considers MLBS in duty cycled WSNs and presents two approximation algorithms, BS-1 and BS-2, that produce a maximum latency of at most (Delta- 1) TH and 13TH respectively. Here, Delta is the maximum degree of nodes, T denotes the number of time slots in a scheduling period, and H is the broadcast latency lower bound obtained from the shortest path algorithm. We evaluated our algorithms under different network configurations and confirmed that the latencies achieved by our algorithms are much lower than existing schemes. In particular, compared to OTAB, the best broadcast scheduling algorithm to date, the broadcast latency and transmission times achieved by BS-1 is at least 1/17 and 2/5 that of OTAB respectively.", "journal": "WIRELESS NETWORKS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000338269800009", "keywords": "lipid nanoparticle; terpene; dermal delivery; all-trans-retinoic acid", "title": "Terpene Composited Lipid Nanoparticles for Enhanced Dermal Delivery of All-trans-Retinoic Acids", "abstract": "In the present study, terpene composited lipid nanoparticles and lipid nanoparticles were developed and evaluated for dermal delivery of all-trans-retinoic acids (ATRA). Terpene composited lipid nanoparticles and lipid nanoparticles were investigated for size, size distribution, zeta potential, entrapment efficiency, photostability, and cytotoxicity. In vitro skin permeation of ATRA lipid formulations were also evaluated. To explore the ability of lipid nanocarriers to target the skin, the distribution of rhodamine B base in the skin was investigated using confocal laser scanning microscopy (CLSM). The results indicated that the physicochemical characteristics of terpene composited lipid nanoparticles influenced skin permeability. All lipid nanocarriers significantly protected ATRA from photodegradation and were non-toxic to normal human foreskin fibroblast cells in vitro. Solid lipid nanoparticles containing 10% limonene (10% L-SLN) had the highest ATRA skin permeability. Terpene composited SLN and nanostructured lipid carriers (NLC) showed higher epidermal permeation of rhodamine B across the skin based on CLSM image analysis. Our study suggests that terpene composited SLN and NLC can be potentially used as dermal drug delivery carriers for ATRA.", "journal": "BIOLOGICAL & PHARMACEUTICAL BULLETIN", "category": "Pharmacology & Pharmacy", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000336994400001", "keywords": "E-cadherin; Cox-2 inhibition; Head and neck squamous cell carcinoma (HNSCC); Epithelial-to-mesenchymal transition (EMT); Lymph node metastasis", "title": "Restoration of E-cadherin expression by selective Cox-2 inhibition and the clinical relevance of the epithelial-to-mesenchymal transition in head and neck squamous cell carcinoma", "abstract": "Background: The epithelial-to-mesenchymal transition (EMT) accompanied by the downregulation of E-cadherin has been thought to promote metastasis. Cyclooxygenase 2 (Cox 2) is presumed to contribute to cancer progression through its multifaceted function, and recently its inverse relationship with E-cadherin was suggested. The aim of the present study was to investigate whether selective Cox-2 inhibitors restore the expression of E-cadherin in head and neck squamous cell carcinoma (HNSCC) cells, and to examine the possible correlations of the expression levels of EMT-related molecules with clinicopathological factors in HNSCC. Methods: We used quantitative real-time PCR to examine the effects of three selective Cox-2 inhibitors, i.e., celecoxib, NS 398, and SC 791 on the gene expressions of E-cadherin (CDH 1) and its transcriptional repressors (SIP1, Snail, Twist) in the human HNSCC cell lines HSC-2 and HSC-4. To evaluate the changes in E-cadherin expression on the cell surface, we used a flowcytometer and immunofluorescent staining in addition to Western blotting. We evaluated and statistically analyzed the clinicopathological factors and mRNA expressions of Cox-2, CDH-1 and its repressors in surgical specimens of 40 patients with tongue squamous cell carcinoma (TSCC). Results: The selective Cox-2 inhibitors upregulated the E-cadherin expression on the cell surface of the HNSCC cells through the downregulation of its transcriptional repressors. The extent of this effect depended on the baseline expression levels of both E-cadherin and Cox-2 in each cell line. A univariate analysis showed that higher Cox-2 mRNA expression (p = 0.037), lower CDH-1 mRNA expression (p = 0.020), and advanced T-classification (p = 0.036) were significantly correlated with lymph node metastasis in TSCC. A multivariate logistic regression revealed that lower CDH-1 mRNA expression was the independent risk factor affecting lymph node metastasis (p = 0.041). Conclusions: These findings suggest that the appropriately selective administration of certain Cox-2 inhibitors may have an anti-metastatic effect through suppression of the EMT by restoring E-cadherin expression. In addition, the downregulation of CDH 1 resulting from the EMT may be closely involved in lymph node metastasis in TSCC.", "journal": "JOURNAL OF EXPERIMENTAL & CLINICAL CANCER RESEARCH", "category": "Oncology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000334486600006", "keywords": "qualitative research; ethnographies; ethnography; qualitative case studies; methodologies", "title": "Living Witnesses to Social Change and Family Documents as Community Archive Reconstructing Social Change in a Small Rural Community", "abstract": "This article describes a strategy for studying social change in a community too small and/or remote to generate a substantial corpus of local official records or to receive regular coverage by nearby and regional media. In communities of this size (generally less than 1,000 people), the record of change often resides mostly in the shared memories and family memorabilia of surviving community residents. I outline the kinds of materials and evidence that may be collected and the ways in which such materials can be utilized in the attempt to reconstruct the changes that have occurred in such a locality. My procedure posits that social change can be illuminated through the critical linkage between the experience of participants and temporally ordered community contexts and events. This research made especial use of the experience of a particular core family in the small town I studied. I follow their experience over the period from 1941 through the present. Drawing on photographs and interviews, I seek to connect the biographies of these and other residents to the broader course of community change. The problems and limitations of relying on selected key informants and salvaged materials are discussed. I argue that the accounts provided by community participants and the accumulated family memorabilia may serve as a proxy for more conventional community records in situations where that kind of evidence is sparse or nonexistent.", "journal": "QUALITATIVE INQUIRY", "category": "Social Sciences, Interdisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000349331900017", "keywords": "resistance training; upper body; bench press; perceived exertion; mean velocity prediction", "title": "Predicting Power Output of Upper Body using the OMNI-RES Scale", "abstract": "The main aim of this study was to determine the optimal training zone for maximum power output. This was to be achieved through estimating mean bar velocity of the concentric phase of a bench press using a prediction equation. The values for the prediction equation would be obtained using OMNI-RES scale values of different loads of the bench press exercise. Sixty males (age 23.61 (sic) 2.81 year. body height 1.76.29 (sic) 6.73 cm. body mass 73.2 (sic) 4.75 kg) voluntarily participated in the study and were tested using an incremental protocol on a Smith machine to determine one repetition maximum (1RM) in the bench press exercise. A linear regression analysis produced a strong correlation (r = -0.94) between rating of perceived exertion (RPE) and mean bar velocity (Vel(mean)). The Pearson correlation analysis between real power output (Pot(Real)) and estimated power (Pot(Est)) showed a strong correlation coefficient of r = 0.77, significant at a level of p = 0.01. Therefore, the OMNI-RES scale can be used to predict Vel(mean) in the bench press exercise to control the intensity of the exercise. The positive relationship between Pot(Real) and Pot(Est) allowed for the identification of a maximum power-training zone.", "journal": "JOURNAL OF HUMAN KINETICS", "category": "Sport Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000336035100025", "keywords": "Adaptation; Climate variability and change; Adaptation strategies; Farmers; Iran", "title": "Adaptation of Iranian farmers to climate variability and change", "abstract": "Climate change poses serious challenges for populations whose livelihoods depend principally on natural resources. Given the increases in extreme weather events projected to adversely affect the arid and semi-arid regions of Iran, adaptation of the agricultural sector is imperative. Few studies have addressed the farmers' adaptation in Iran, and little is known about ongoing adaptation strategies in use. Adopting principal component analysis/fuzzy logic-based method, this paper considers the agricultural adaptation to climate variability. A survey of 255 farmers of Fars Province, selected through a multistage stratified random sampling method, revealed different levels of adaptation, specifically the low, moderate and high, which are principally distinguished by various degrees of sensitivity and adaptive capacity. The study also identified the main adaptation strategies used by farmers in response to climate-related shocks. Results indicated that although a large percentage of farmers make some adjustments to their farming practices, there are significant differences in choice of adaptation strategies by the adaptation categories. Some conclusions and recommendations are offered to increase the adaptive capacity of farmers and reduce negative impacts of climate variability and change.", "journal": "REGIONAL ENVIRONMENTAL CHANGE", "category": "Environmental Sciences; Environmental Studies", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000336822300007", "keywords": "Soft decision; maximum a posteriori probability; channel estimation; fading channel; iterative algorithm; LTE turbo code", "title": "Dynamic analysis of linear viscoelastic cylindrical and conical helicoidal rods using the mixed FEM", "abstract": "The objective of this study is to investigate the influence of the rotary inertia on dynamic behavior of linear viscoelastic cylindrical and conical helixes by means of the Laplace transform-mixed finite element formulation and solution. The element matrix is based on the Timoshenko beam theory. The influence of rotary inertias is considered in the dynamic analysis, which is original in the literature. Rectangular, sine and step type of impulsive loads are applied on helices having rectangular cross-sections with various aspect ratios. The Kelvin and standard models are used for defining the linear viscoelastic material behavior; and by means of the correspondence principle (the elastic-viscoelastic analogy), the material parameters are replaced with their complex counterparts in the Laplace domain. The analysis is carried out in the Laplace domain and the results are transformed back to time space numerically by modified Durbin's algorithm. First, the solution algorithm is verified using the existing open sources in the literature and afterwards some benchmark examples such as conical viscoelastic rods are handled. (C) 2014 Elsevier Ltd. All rights reserved.", "journal": "JOURNAL OF SOUND AND VIBRATION", "category": "Acoustics; Engineering, Mechanical; Mechanics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000344991700073", "keywords": "Hierarchical topic visualization; evolutionary tree clustering; data transformation", "title": "How Hierarchical Topics Evolve in Large Text Corpora", "abstract": "Using a sequence of topic trees to organize documents is a popular way to represent hierarchical and evolving topics in text corpora. However, following evolving topics in the context of topic trees remains difficult for users. To address this issue, we present an interactive visual text analysis approach to allow users to progressively explore and analyze the complex evolutionary patterns of hierarchical topics. The key idea behind our approach is to exploit a tree cut to approximate each tree and allow users to interactively modify the tree cuts based on their interests. In particular, we propose an incremental evolutionary tree cut algorithm with the goal of balancing 1) the fitness of each tree cut and the smoothness between adjacent tree cuts; 2) the historical and new information related to user interests. A time-based visualization is designed to illustrate the evolving topics over time. To preserve the mental map; we develop a stable layout algorithm. As a result, our approach can quickly guide users to progressively gain profound insights into evolving hierarchical topics. We evaluate the effectiveness of the proposed method on Amazon's Mechanical Turk and real-world news data. The results show that users are able to successfully analyze evolving topics in text data.", "journal": "IEEE TRANSACTIONS ON VISUALIZATION AND COMPUTER GRAPHICS", "category": "Computer Science, Software Engineering", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000343063500001", "keywords": "Transarterial Chemoembolization; Liver Transplantation; Liver Resection; Intrahepatic Recurrence; Hepatocellular Carcinoma", "title": "Transarterial chemoembolization for intrahepatic multiple recurrent HCC after liver resection or transplantation", "abstract": "Background: Transarterial chemoembolization (TACE) can reduce tumor progression and help achieve good locoregional effect in hepatocellular carcinoma (HCC) patients with intrahepatic multiple recurrence (IHMR) after liver resection (LR). The effect of TACE on HCC patients with IHMR after liver transplantation (LT) remains unclear. The purpose of this study was to investigate the effect of TACE on IHMR after LR or LT. Material/Methods: This hospital-based retrospective study included 968 and 180 HCC patients who had undergone LR or LT, respectively, in the past decade. Parameters included clinical characteristics, alpha-fetoprotein level, Child classification, tumor stage at first treatment, tumor size at recurrence, and recurrence and survival status. The groups were compared using the t test or chi-square test, and univariate and multivariate analyses were performed. Survival and recurrence were analyzed by the Kaplan-Meier method. Differences were significant at P<0.05. Results: During follow-up, 112 patients had IHMR: 101 after LR and 11 after LT. Age, sex distribution, and HCV infection rate differed significantly between the LR and LR groups. All patients in the LT group who had recurrent HCC died within 3 years. The risk factors for death from tumor recurrence included a larger tumor size at recurrence, poor Child classification at recurrence, hyperbilirubinemia, hypoalbuminemia, and no TACE treatment. In Cox regression analysis, only vessel invasion, Child class C, and no TACE treatment were independent risk factors for death from tumor recurrence. Conclusions: TACE is beneficial for treating IHMR in patients after LR or LT.", "journal": "ANNALS OF TRANSPLANTATION", "category": "Surgery; Transplantation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000355389200003", "keywords": "Human auditory nucleus; Ageing; Image analysis; Auditory implants", "title": "Age associated changes in the human cochlear nucleus - A three-dimensional modelling and its potential application for brainstem implants", "abstract": "Introduction: The cochlear nucleus (CN) is vulnerable to physio-pathological alterations with age that may contribute to varied degree of hearing impairment. It was planned to study the age changes in the three-dimensional (3-D) structure of human CN. Methods: Forty-one human brainstems were collected (birth 90 years) from the mortuary of the All India Institute of Medical Sciences (AIIMS), New Delhi, with ethical committee permission. Tissues were fixed in 4% paraforrnaldehyde (pH 7.4), cryosectioned (40 mu m) serially and stained with cresyl violet. A 3-D reconstruction model of the adult human CN was made using the 3-D Solid body tracing probe of the Stereo Investigator software (MBF Biosciences, VT, USA). Results: The CN appeared as a thin, crescent-shaped protuberance along the floor and lateral recess of the 4th ventricle in relation with inferior and middle cerebellar peduncle at the level of the pontomedullary junction. The rostral tip was observed deep in the middle cerebellar peduncle and the dorsal subdivision, within the wall of lateral recess of the 4th ventricle. Microscopically, different types of neurons were observed in all the ages studied and no change in the distribution was noted. The volume of the CN did not change with ageing. Discussion: The 3-D model of the CN can be used as a landmark for safe surgical route to access the intraventricular surface of the 4th ventricle and to engineer brainstem implants. Copyright (C) 2014, Anatomical Society of India. Published by Reed Elsevier India Pvt. Ltd. All rights reserved.", "journal": "JOURNAL OF THE ANATOMICAL SOCIETY OF INDIA", "category": "Anatomy & Morphology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000342199700013", "keywords": "Choroidal basal blood flow; eye tracking; high-resolution MRI; visual fixation", "title": "Choroidal Blood Flow Decreases with Age: An MRI Study", "abstract": "Purpose: To verify that a visual fixation protocol with cued eye blinks achieves sufficient stability for magnetic resonance imaging (MRI) blood-flow measurements and to determine if choroidal blood flow (ChBF) changes with age in humans. Methods: The visual fixation stability achievable during an MRI scan was measured in five normal subjects using an eye-tracking camera outside the MRI scanner. Subjects were instructed to blink immediately after recorded MRI sound cues but to otherwise maintain stable visual fixation on a small target. Using this fixation protocol, ChBF was measured with MRI using a 3 Tesla clinical scanner in 17 normal subjects (24-68 years old). Arterial and intraocular pressures (IOP) were measured to calculate perfusion pressure in the same subjects. Results: The mean temporal fluctuations (standard deviation) of the horizontal and vertical displacements were 29 +/- 9 mu m and 38 +/- 11 mu m within individual fixation periods, and 50 +/- 34 mu m and 48 +/- 19 mu m across different fixation periods. The absolute displacements were 67 +/- 31 mu m and 81 +/- 26 mu m. ChBF was negatively correlated with age (R = -0.7, p = 0.003), declining 2.7 ml/100 ml/min per year. There were no significant correlations between ChBF versus perfusion pressure, arterial pressure, or IOP. There were also no significant correlations between age versus perfusion pressure, arterial pressure, or IOP. Multiple regression analysis indicated that age was the only measured independent variable that was significantly correlated with ChBF (p = 0.03). Conclusions: The visual fixation protocol with cued eye blinks was effective in achieving sufficient stability for MRI measurements. ChBF had a significant negative correlation with age.", "journal": "CURRENT EYE RESEARCH", "category": "Ophthalmology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000336374400003", "keywords": "Fracture; Oral anti-coagulant; Osteoporosis; Warfarin", "title": "Incident long-term warfarin use and risk of osteoporotic fractures: propensity-score matched cohort of elders with new onset atrial fibrillation", "abstract": "Association between warfarin use and fracture risk is unclear. We examined the association between long-term warfarin use and fracture risk at the hip, spine, and wrist in elders. No significant association was found between long-term warfarin use and fracture risk, despite biological plausibility. Prior studies examining the association of warfarin use and osteoporotic fractures have been conflicting, potentially related to methodological limitations. Thus, we examined the association of long-term warfarin use with risk of hip, spine, and wrist fractures among older adults with atrial fibrillation, attempting to address prior methodologic challenges. We included men and women a parts per thousand yen65 years of age with incident atrial fibrillation and without prior history of fractures from The Health Improvement Network followed between 2000 and 2010. Long-term warfarin use was defined in two ways: (1) warfarin use a parts per thousand yen1 year; (2) warfarin use a parts per thousand yen3 years. Propensity-score matched cohorts of warfarin users and nonusers were created to evaluate the association between long-term warfarin use and risk of hip, spine, and wrist fractures separately as well as combined, using Cox-proportional hazards regression models. Among > 20,000 participants with incident atrial fibrillation, the hazard ratios (HR) for hip fracture with warfarin use a parts per thousand yen1 and a parts per thousand yen3 years, respectively, were 1.08 (95%CI 0.87, 1.35) and 1.13 (95 % CI 0.84, 1.50). Similarly, no significant associations were observed between long-term warfarin use and risk of spine or wrist fracture. When risk of any fracture was assessed with warfarin use, no association was found [HR for warfarin use a parts per thousand yen1 year 0.92 (95%CI 0.77, 1.10); HR for warfarin use a parts per thousand yen3 years 1.12 (95%CI 0.88, 1.43)]. Long-term warfarin use among elders with atrial fibrillation was not associated with increased risk of osteoporotic fractures and therefore does not appear to necessitate additional surveillance or prophylaxis.", "journal": "OSTEOPOROSIS INTERNATIONAL", "category": "Endocrinology & Metabolism", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000339707700010", "keywords": "Brownian motion; Finite-element analysis; Kriging; Multifidelity data; Nonstationary Gaussian process models; Tuning parameters", "title": "Surrogate Modeling of Computer Experiments With Different Mesh Densities", "abstract": "This article considers deterministic computer experiments with real-valued tuning parameters which determine the accuracy of the numerical algorithm. A prominent example is finite-element analysis with its mesh density as the tuning parameter. The aim of this work is to integrate computer outputs with different tuning parameters. Novel nonstationary Gaussian process models are proposed to establish a framework consistent with the results in numerical analysis. Numerical studies show the advantages of the proposed method over existing methods. The methodology is illustrated with a problem in casting simulation. Supplementary material for this article is available online.", "journal": "TECHNOMETRICS", "category": "Statistics & Probability", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000335905200025", "keywords": "Calcium; NM-BAPTA; Method comparison; Interferences", "title": "Evaluation of NM-BAPTA method for plasma total calcium measurement on Cobas 8000 (R)", "abstract": "Objectives: A new method was developed by Roche for the measurement of plasma calcium using the chromophore 5-nitro-5 '-methyl-(1,2-bis(o-aminophenoxy) ethan-N, N, N', N'-tetraacetic acid ( NM-BAPTA) which could have several advantages over the CPC method. The aim of our study was to evaluate the analytical performances of the NM-BAPTA assay from Roche on c701/Cobas 8000 (R) and to perform a comparison study of calcium values with CPC and Arsenazo III methods. Methods: The analytical performance including imprecision study, linearity, and stability of the NM-BAPTA assay was tested on the c701/Cobas 8000 (R) analyzer. The most frequent interferences such as magnesium and gadolinium-based contrast agents (Gd-CAs) were examined with spiked human plasma on the selected method. The calcium Arsenazo III method from Horiba (Montpellier, France) installed on ABX Pentra 400 (R) was used as a reference method. Linear regression analysis was performed to compare data from the different methods. Results: The CV of the NM-BAPTA assay showed good analytical performances with CV <1.5%, in agreement with the proposed and interim European biologic goals. We found no interference neither with gadobenate dimeglumine nor with gadoteric acid considering significant findings as interference greater than 5%. In the analytical range from 0.85 to 3.80 mmol/L, the NM-BAPTA method was closely correlated to the Arsenazo III method. Conclusions: Our data demonstrate that this new calcium NM-BAPTA method developed by Roche analyzers perform as well as the conventional method, especially for the outermost values. Thus, this new colorimetric assay could substitute the CPC method on Roche analyzers. (C) 2014 The Canadian Society of Clinical Chemists. Published by Elsevier Inc. All rights reserved.", "journal": "CLINICAL BIOCHEMISTRY", "category": "Medical Laboratory Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000343091400008", "keywords": "Ascular effects; Aging; Hypertension; Atherosclerosis", "title": "Detection of c, d, and e waves in the acceleration photoplethysmogram", "abstract": "Analyzing the acceleration photoplethysmogram (APG) is becoming increasingly important for diagnosis. However, processing an APG signal is challenging, especially if the goal is to detect its small components (c, d, and e waves). Accurate detection of c, d, and e waves is an important first step for any clinical analysis of APG signals. In this paper, a novel algorithm that can detect c, d, and e waves simultaneously in APG signals of healthy subjects that have low amplitude waves, contain fast rhythm heart beats, and suffer from non-stationary effects was developed. The performance of the proposed method was tested on 27 records collected during rest, resulting in 97.39% sensitivity and 99.82% positive predictivity. (C) 2014 Elsevier Ireland Ltd. All rights reserved.", "journal": "COMPUTER METHODS AND PROGRAMS IN BIOMEDICINE", "category": "Computer Science, Interdisciplinary Applications; Computer Science, Theory & Methods; Engineering, Biomedical; Medical Informatics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000346280900007", "keywords": "asthma specific job exposure matrices; Cohens kappa; exposure assignment; hazard ratios; new-onset asthma; occupational asthma; population attributable risk", "title": "Seroprevalence of Herpes Simplex Virus Type 1 and 2 Among Pregnant Women, 1989-2010", "abstract": "Maternal acquisition of genital herpes simplex virus type 1 (HSV-1) or type 2 (HSV-2) near delivery accounts for most cases of neonatal herpes, although neonatal HSV incidence has remained stable in recent decades. A decline in HSV-1, but not HSV-2, seroprevalence has been reported among reproductive-aged women. This study was designed to evaluate trends in the seroprevalence of HSV-1 and HSV-2 among parturients who delivered between 1989 and 2010. Herpes simplex virus infection status was determined using Western blot within routine prenatal tests. The proportion of patients with prenatal HSV serologic results declined from 54.7% during 1989 to 1997 to 44.8% during 1998 to 2010. Yearly changes in HSV-1 and HSV-2 seroprevalence were estimated with Poisson regression. Models were adjusted for race/ethnicity, maternal age, parity, delivery type, and insurance status. Prenatal HSV serologic results were available for 18,993 pregnancies in 15,738 women (median age, 28 years; interquartile range, 23-33 years), of whom 43% were white, 12% were black, 11% were Asian, 7% were Hispanic, and 27% were other or unreported; 26% had private insurance. Nine percent of pregnancies involved women who were seropositive for HSV-2 only, 15% for both HSV-1 and HSV-2, and 53% for HSV-1 only; 24% were seronegative for HSV. Herpes simplex virus type 1 seroprevalence decreased from 69.1% during 1989 to 1999 to 65.5% during 2000 to 2010; HSV-2 seroprevalence decreased from 30.1% to 16.3%, for a 46% relative decline. After adjustment, no significant annual trend in HSV-1 seroprevalence was noted (0.1% per year; 95% CI, 0%-0.3% per year; P = 0.13). Rates of HSV-2 seroprevalence decreased significantly by 4.8% per year (95% CI, 4.3%-5.2% per year; P < 0.001). Herpes simplex virus type 1 seroprevalence increased slightly among black women only (0.9% per year; 95% CI, 0.4%-1.3% per year; P < 0.001). Herpes simplex virus type 2 seroprevalence decreased significantly among women of all races (P < 0.001). Rates per year decreased substantially less for black women relative to white women (2.6% per year vs 5.5% per year, respectively; P < 0.001). Seroprevalence of HSV-2 among pregnant women decreased between 1989 and 2010, with the decrease particularly noted for white women. Herpes simplex virus type 1 did not decrease overall and in fact increased slightly among black women. Women who are seronegative entering pregnancy and acquire HSV during late pregnancy are at higher risk for transmission of HSV to their infants than are seropositive women. These findings offer new data on HSV seroprevalence in the pregnant population.", "journal": "OBSTETRICAL & GYNECOLOGICAL SURVEY", "category": "Obstetrics & Gynecology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000343316600015", "keywords": "Enhanced oil recovery; Density Functional Theory; Dodecyltrimethylammonium chloride; Supramolecular chemistry", "title": "Limited survival in dialysis patients undergoing intact abdominal aortic aneurysm repair", "abstract": "Objective: Elective abdominal aortic aneurysm (AAA) repair in suitable candidates is a standard modality. The outcomes of AAA repair in patients with end-stage renal disease on dialysis are not well characterized, and there is questionable survival advantage in such patients with limited life expectancy. We sought to describe outcomes after AAA repair in U. S. dialysis patients. AMethods: The United States Renal Data System was used to collect data on intact asymptomatic AAA repair procedures in dialysis patients in the United States between 2005 and 2008. Endovascular AAA repair (EVAR) and open aortic repair (OAR) were identified by Current Procedural Terminology codes. Primary outcomes were perioperative (30-day) mortality and long-term survival. Predictors of mortality were identified by multivariate regression models. Results: A total of 1557 patients were identified who had undergone elective AAA repair: 261 OAR and 1296 EVAR. The 30-day mortality was 11.3% (EVAR, 10.3%; OAR, 16.1%; P = .010), with increased age associated with increased mortality (odds ratio, 1.04; 95% confidence interval [CI], 1.02-1.07; P = .001). Kaplan-Meier survival estimates were 66.5% at 1 year (EVAR, 66.2%; OAR, 68%) and 37.4% at 3 years (EVAR, 36.8%; OAR, 40%; P = .33). Median survival was 25.3 months after EVAR and 27.4 months after OAR. Women had a higher mortality rate at 1 year (38.7%) compared with men (32.0%) (P = .015). There was no significant mortality difference at 1 year in comparing type of procedure in both men (EVAR, 31.6%; OAR, 34%; P = .55) and women (EVAR, 39.3%; OAR, 36%; P = .60). A Cox proportional hazards model demonstrated that male gender (hazard ratio [HR], 0.75; 95% CI, 0.62-0.92; P = .005), increased time on dialysis (HR for each year on dialysis, 0.79; 95% CI, 0.75-0.83; P < .001), kidney transplantation history (HR, 0.62; 95% CI, 0.43-0.88; P = .008), and diagnosis of hypertension (HR, 0.60; 95% CI, 0.48-0.75; P < .001) were protective against mortality. Increased age (HR, 1.02; 95% CI, 1.01-1.03; P < .001) and diabetes diagnosis (HR, 1.39; 95% CI, 1.13-1.71; P = .002) predicted increased mortality. Conclusions: AAA patients on dialysis have high perioperative and 1-year mortality rates after EVAR or OAR, particularly diabetics, women, and the elderly. This raises questions about the indications for intact AAA repair in dialysis patients, in whom the size threshold may need to be raised. Dialysis patients may be best served by deferring repair of AAA until AAAs reach large size or become symptomatic, especially if OAR is required, given the higher perioperative mortality compared with EVAR.", "journal": "JOURNAL OF VASCULAR SURGERY", "category": "Surgery; Peripheral Vascular Disease", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000340302400036", "keywords": "Miscibility gap alloy; Effective thermal conductivity; Thermal storage; Lattice Monte Carlo; Microstructural efficiency", "title": "Effective conductivity of Cu-Fe and Sn-Al miscibility gap alloys", "abstract": "The effective thermal conductivity of Cu-Fe and Sn-Al miscibility gap alloys over a range of temperatures and volume fractions was determined using the Lattice Monte Carlo method. The Cu-Fe system was found to have an effective conductivity predictable by the Maxwell-Eucken model. The Sn-Al system was not consistent with any empirical model analysed. The microstructures of physical samples were approximated using a random growth algorithm calibrated to electron or optical microscope images. Charts of effective conductivity against temperature for a number of volume fractions are presented for the two alloys. It was determined that the Cu-Fe alloy would benefit from an interstice type microstructure and the Sn-Al would be more efficient with a hard spheres type microstructure. More general conclusions are drawn about the efficiency of the two observed microstructures. (C) 2014 Elsevier Ltd. All rights reserved.", "journal": "INTERNATIONAL JOURNAL OF HEAT AND MASS TRANSFER", "category": "Thermodynamics; Engineering, Mechanical; Mechanics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000334018600007", "keywords": "wear; total hip arthroplasty; third body; finite element analysis; retrieval", "title": "A novel formulation for scratch-based wear modelling in total hip arthroplasty", "abstract": "Damage to the femoral head in total hip arthroplasty often takes the form of discrete scratches, which can lead to dramatic wear acceleration of the polyethylene ( PE) liner. Here, a novel formulation is reported for finite element (FE) analysis of wear acceleration due to scratch damage. A diffused-light photography technique was used to globally locate areas of damage, providing guidance for usage of high-magnification optical profilometry to determine individual scratch morphology. This multiscale image combination allowed comprehensive input of scratch-based damage patterns to an FE Archard wear model, to determine the wear acceleration associated with specific retrieval femoral heads. The wear algorithm imposed correspondingly elevated wear factors on areas of PE incrementally overpassed by individual scratches. Physical validation was provided by agreement with experimental data for custom-ruled scratch patterns. Illustrative wear acceleration results are presented for four retrieval femoral heads.", "journal": "COMPUTER METHODS IN BIOMECHANICS AND BIOMEDICAL ENGINEERING", "category": "Computer Science, Interdisciplinary Applications; Engineering, Biomedical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000337759900017", "keywords": "Enveloping surface; Digital CAD model; CNC machining; Interference detection; CNC code", "title": "Wide area measurement/wide area information-based control strategy to fast relieve overloads in a self-healing power grid", "abstract": "Self-healing is one of the characteristics of the smart grid. A self-healing power grid can identify and react to disturbance and restore power systems with little or even no human intervention. A wide area measurement (WAM) and wide area information (WAI)-based control strategy is proposed to fast react to overloads and restore the power grid in the self-healing power grid. The basic principle is to redistribute the power flow of a contingency transmission line to other lines with unified power flow controller (UPFC). To implement the control strategy, the reverse current network and nodal analysis method, instead of the iteration algorithm and optimisation method, is applied to redistribute the power flow. The proposed WAM/WAI-based nodal analysis can relieve overloads quickly and effectively while realising the control objective accurately. Moreover, a grid simplification method is applied to further reduce computation cost. IEEE 39-bus test system-based simulation is applied to verify the proposed control strategy. The results show that the control strategy can relieve overloads effectively in about tens of milliseconds. Thus, it could be an online controller for a self-healing power grid to deal with harmful contingencies.", "journal": "IET GENERATION TRANSMISSION & DISTRIBUTION", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000338601500004", "keywords": "Boundary recovery; Fully constrained Delaunay triangulation; Linear programming; Sequence for removal of Steiner points", "title": "A Novel Financial Market for Mitigating Hurricane Risk. Part II: Empirical Validation", "abstract": "This paper explores the empirical features of a novel commodity option trading instrument described in the companion paper (Part I) that allows market participants to hedge against the risk that a coastal county or region in the eastern United States will experience a hurricane landfall. In this instrument investors can speculate on whether a landfall event will occur in any one of a number of coastal counties or regions, with option prices being determined by an adaptive control algorithm that reflects previous purchasing decisions of other market participants. In this paper, the authors report the results of an experiment designed to test the empirical robustness of this mechanism using data from traders buying landfall options over the course of a simulated hurricane season. In the experiment traders are given the opportunity to buy landfall options in the primary market as well as sell and buy options in a conventional bilateral secondary market. The data show that aggregate market prices quickly converge to rational (efficient) levels among market participants after limited amounts of trading experience. Some systematic anomalies are observed in the trading of options for individual outcomes, however, with the most notable being an initial tendency to overvalue landfall options that have the highest prior probabilities and for valuations of the \"No Landfall\" option to be inflated immediately after a storm threat passes without making landfall.", "journal": "WEATHER CLIMATE AND SOCIETY", "category": "Environmental Studies; Meteorology & Atmospheric Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000333620600014", "keywords": "Acetogens; Genome tailoring; UVC-mutagenesis; Continuous CO2 fermentation", "title": "UVC-mutagenesis in acetogens: resistance to methanol, ethanol, acetone, or n-butanol in recombinants with tailored genomes as the step in engineering of commercial biocatalysts for continuous CO2/H-2 blend fermentations", "abstract": "Time- and cost-efficient six-step UVC-mutagenesis was developed and validated to generate acetogen mutants with preliminary reduced genomes to prevent product inhibition in the to-be-engineered commercial biocatalysts. Genome reduction was performed via elimination of pta, ack, spo0A, spo0J and some pro-phage genes. UVC-mutants such as Clostridium sp. MT1784RG, Clostridium sp. MT653RG, Clostridium sp. MT896RG, and Clostridium sp. MT1962RG (all 4 share 97 % DNA homology with Clostridium ljungdahlii ATCC 55383) were selected based on resistance to methanol (3 M), ethanol (3.6 M), acetone (2.5 M), or n-butanol (0.688 M), respectively. As a part of the biocatalyst engineering algorithm, genome reduction step was associated with integration of attTn7 recognition sequence to the chromosomes of each of the above strains to prepare the defined integration sites for future integration of multi-copy synthetic operons encoding biosynthesis of methanol, ethanol, acetone or n-butanol. Reduced genome mutants had cell duplication times decreased compared to the same for the respective parental strains. All groups of mutants had decreased share of palmitic (C16:0) and increased share of oleic (C18:1) acids along with detection of isopropylstearate (C20) compared to the parental strains. Mutants resistant to acetone and n-butanol also had monounsaturated fatty acid (C20:1) not found in parental strains. Cyclopropane fatty acid (C21) was identified only in n-butanol resistant mutants.", "journal": "WORLD JOURNAL OF MICROBIOLOGY & BIOTECHNOLOGY", "category": "Biotechnology & Applied Microbiology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000344326300005", "keywords": "Delirium; Nursing sensitive outcomes; Patient outcomes; Risk assessment", "title": "Automatic delirium prediction system in a Korean surgical intensive care unit", "abstract": "Background: In Korea, regular screening for delirium is not considered essential. In addition, delirium is often associated with vague concepts, making it harder to identify high-risk patients and impeding decision-making. Aims: To assess the impact of the Automatic PREdiction of DELirium in Intensive Care Units (APREDEL-ICU) system on nursing-sensitive and patient outcomes for surgical ICU patients and to evaluate nurse satisfaction with the system and its usability. Methods: A pre-post research design was adopted. Our study included 724 patients admitted before the implementation of the APREDEL-ICU (January to December 2010) and 1111 patients admitted after the system was installed (May 2011 to April 2012). The APREDEL-ICU uses a pop-up window message to inform the nursing staff of patients at risk for delirium, allowing evidence-based nursing interventions to be applied to the identified patients. A total of 42 nurses were surveyed to determine the system's usability and their level of satisfaction with it. Results: After the implementation of APREDEL-ICU, high-risk patients, determined using a prediction algorithm, showed a slight decrease in the incidence of delirium, but the changes were not significant. However, significant decreases in the number and duration of analgesic/narcotic therapies were observed after the implementation of the system. Nurse self-evaluation results showed an improvement in all categories of knowledge regarding delirium care. Conclusion: The use of a prediction and alerting system for ICU patients at high risk of delirium showed a potential increase in the quality of delirium care, including early detection and proper intervention.", "journal": "NURSING IN CRITICAL CARE", "category": "Nursing", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000342690800005", "keywords": "elderly; falling; monitoring", "title": "Fall Detection Devices and Their Use With Older Adults : A Systematic Review", "abstract": "Background: Falls represent a significant threat to the health and independence of adults aged 65 years and older. As a wide variety and large number of passive monitoring systems are currently and increasingly available to detect when individuals have fallen, there is a need to analyze and synthesize the evidence regarding their ability to accurately detect falls to determine which systems are most effective. Objectives: The purpose of this literature review is to systematically assess the current state of design and implementation of fall-detection devices. This review also examines to what extent these devices have been tested in the real world as well as the acceptability of these devices to older adults. Data Sources: A systematic literature review was conducted in PubMed, CINAHL, EMBASE, and PsycINFO from their respective inception dates to June 25, 2013. Study Eligibility Criteria and Interventions: Articles were included if they discussed a project or multiple projects involving a system with the purpose of detecting a fall in adults. It was not a requirement for inclusion in this review that the system targets persons older than 65 years. Articles were excluded if they were not written in English or if they looked at fall risk, fall detection in children, fall prevention, or a personal emergency response device. Study Appraisal and Synthesis Methods: Studies were initially divided into those using sensitivity, specificity, or accuracy in their evaluation methods and those using other methods to evaluate their devices. Studies were further classified into wearable devices and nonwearable devices. Studies were appraised for inclusion of older adults in sample and if evaluation included real-world settings. Results: This review identified 57 projects that used wearable systems and 35 projects using nonwearable systems, regardless of evaluation technique. Nonwearable systems included cameras, motion sensors, microphones, and floor sensors. Of the projects examining wearable systems, only 7.1% reported monitoring older adults in a real-world setting. There were no studies of nonwearable devices that used older adults as subjects in either a laboratory or a real-world setting. In general, older adults appear to be interested in using such devices although they express concerns over privacy and understanding exactly what the device is doing at specific times. Limitations: This systematic review was limited to articles written in English and did not include gray literature. Manual paper screening and review processes may have been subject to interpretive bias. Conclusions and Implications of Key Findings: There exists a large body of work describing various fall-detection devices. The challenge in this area is to create highly accurate unobtrusive devices. From this review it appears that the technology is becoming more able to accomplish such a task. There is a need now for more real-world tests as well as standardization of the evaluation of these devices.", "journal": "JOURNAL OF GERIATRIC PHYSICAL THERAPY", "category": "Geriatrics & Gerontology; Rehabilitation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000344589200011", "keywords": "Computational fluid dynamics (CFD); computational modeling; magnetic resonance imaging (MRI); parallel computing; vascular network", "title": "In Silico Modeling of Magnetic Resonance Flow Imaging in Complex Vascular Networks", "abstract": "The paper presents a computational model of magnetic resonance (MR) flow imaging. The model consists of three components. The first component is used to generate complex vascular structures, while the second one provides blood flow characteristics in the generated vascular structures by the lattice Boltzmann method. The third component makes use of the generated vascular structures and flow characteristics to simulate MR flow imaging. To meet computational demands, parallel algorithms are applied in all the components. The proposed approach is verified in three stages. In the first stage, experimental validation is performed by an in vitro phantom. Then, the simulation possibilities of the model are shown. Flow and MR flow imaging in complex vascular structures are presented and evaluated. Finally, the computational performance is tested. Results show that the model is able to reproduce flow behavior in large vascular networks in a relatively short time. Moreover, simulated MR flow images are in accordance with the theoretical considerations and experimental images. The proposed approach is the first such an integrative solution in literature. Moreover, compared to previous works on flow and MR flow imaging, this approach distinguishes itself by its computational efficiency. Such a connection of anatomy, physiology and image formation in a single computer tool could provide an in silico solution to improving our understanding of the processes involved, either considered together or separately.", "journal": "IEEE TRANSACTIONS ON MEDICAL IMAGING", "category": "Computer Science, Interdisciplinary Applications; Engineering, Biomedical; Engineering, Electrical & Electronic; Imaging Science & Photographic Technology; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000345579700009", "keywords": "Harmonic numbers; Legendre polynomials; Petrov-Galerkin method; collocation method; sixth-order boundary value problems", "title": "A Flexible Quagga-Based Virtual Network with FIB Aggregation", "abstract": "The rapid increase in routers' forwarding table size is raising serious concerns for ISPs. In particular, it exhausts the routers' forwarding hardware capacity, leading to more frequent upgrades and higher cost. In this article, we present the development of a virtual network framework based on open source software that demonstrates how solutions to this impending problem can be implemented and studied. The system is capable of emulating an operational network environment with intra-domain and inter-domain routing protocols as well as real-world Internet routing traffic. By adding performance monitoring and FIB aggregation capabilities to this system, we are able to evaluate the performance of FIB aggregation algorithms in a realistic network environment.", "journal": "IEEE NETWORK", "category": "Computer Science, Hardware & Architecture; Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000344942600050", "keywords": "radiofrequency identification; microswitches; chipless tag; RF MEMS switch; radiofrequency identification tag; microelectromechanical system technology; UHF band; basic fabrication process; programmable elements; RFID interrogators", "title": "Efficient weak manoeuvring target detection method for DSSS signal", "abstract": "A novel efficient detection method is proposed for radar transmitting direct sequences spread spectrum (DSSS) signals with high dynamics in the low signal-to-noise ratio (SNR) scenario. The core idea of the proposed method is first to eliminate the linear code phase drift via keystone transform (KT), then the derived modified Radon-Fourier transform (MRFT) is employed to accumulate the energy of the target effectively to achieve target detection and parameter estimation. Compared with the generalised RFT (GRFT) method, the proposed method has a similar detection performance, but requires much lower computational burden. Simulation results verify the validity of the proposed method.", "journal": "ELECTRONICS LETTERS", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000341712000008", "keywords": "Approximation algorithms; Elastic optical networks; Multiprocessor scheduling; Network design; Routing and spectrum assignment; Spectrum assignment", "title": "Spectrum Assignment in Optical Networks: A Multiprocessor Scheduling Perspective", "abstract": "The routing and spectrum assignment problem has emerged as the key design and control problem in elastic optical networks. In this work, we show that the spectrumassignment (SA) problem in mesh networks transforms to the problem of scheduling multiprocessor tasks on dedicated processors. Based on this new perspective, we show that the SA problem in chain (linear) networks is NP-hard for four or more links, but is solvable in polynomial time for three links. We also develop new constant-ratio approximation algorithms for the SA problem in chains when the number of links is fixed. Finally, we present several list scheduling algorithms that are computationally efficient and simple to implement, yet produce solutions that, on average, are within 1 %-5 % of the lower bound.", "journal": "JOURNAL OF OPTICAL COMMUNICATIONS AND NETWORKING", "category": "Computer Science, Hardware & Architecture; Computer Science, Information Systems; Optics; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000335973800017", "keywords": "respiratory monitoring; dead space; volumetric capnography; mechanical ventilation", "title": "Monitoring of oesophageal pressure", "abstract": "Purpose of review Volumetric capnography (VCap) measures the kinetics of carbon dioxide (CO2) elimination on a breath-by-breath basis. A volumetric capnogram contains extensive physiological information about metabolic production, circulatory transport and CO2 elimination within the lungs. VCap is also the best clinical tool to measure dead spaces allowing a detailed analysis of the functional components of each tidal volume, thereby providing clinically useful hints about the lung's efficiency of gas exchange. Difficulties in its bedside measurement, oversimplifications of its interpretation along with prevailing misconceptions regarding dead space analysis have, however, limited its adoption as a routine tool for monitoring mechanically ventilated patients. Recent findings Improvements in CO2 measuring technologies and more advanced algorithms for faster and more accurate analysis of volumetric capnograms have increased our physiological understanding and thus the clinical usefulness of VCap. The recently validated VCap-based method for estimating alveolar partial pressure of CO2 provided a breakthrough for a fully noninvasive breath-by-breath measurement of physiological dead space. Recent advances in VCap and our improved understanding of its clinical implications may help in overcoming the known limitations and reluctances to include expired CO2 kinetics and dead space analysis in routine bedside monitoring. It is about time to start using this powerful monitoring tool to support decision making in the intensive care environment.", "journal": "CURRENT OPINION IN CRITICAL CARE", "category": "Critical Care Medicine", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000345481400014", "keywords": "Collision avoidance; Vehicular cyber physical system; Driver behavior; Vehicle path prediction; Risk assessment", "title": "A method of vehicle motion prediction and collision risk assessment with a simulated vehicular cyber physical system", "abstract": "Vehicular cyber physical system (VCPS) can comprehensively acquire road traffic safety related information, and provide drivers with early warning or driving assistance in emergency, in order to assist them avoid vehicle crash in the driving process. Literature review shows that previous studies mainly rely on observed vehicle motion/location data for assessing vehicle collision risk, where predicted vehicle motion/location, driver behavior and road geometry (e.g., curvature) are rarely considered. In this study, based on the simulated VCPS, a collision avoidance system that can explicitly consider the above issues is designed and presented in detail. Within the proposed collision avoidance system, an assessment method, which can predict collision risk by comprehensively considering vehicles motion/location, driver behavior and road geometry information from the VCPS, is developed. Firstly, the short-term motion of the objective vehicle and surrounding vehicles are predicted based on the Kalman Filter (KF) algorithm and the vehicle motion model. Furthermore, the proposed method that can explicitly take driver behavior and road curvature into account is used to predict vehicle location and calculate the traveled distance among vehicles in real-time. Then, the predicted vehicle gaps are compared with a safe distance threshold and the vehicle collision risk is predicted. Finally, the accuracy of the proposed collision risk assessment method is examined with a receiver operating characteristic (ROC) curve analysis over a section of curved road. Simulation results show that the proposed method is effective for detecting collision risk and providing accurate warnings in a timely fashion. (C) 2014 Elsevier Ltd. All rights reserved.", "journal": "TRANSPORTATION RESEARCH PART C-EMERGING TECHNOLOGIES", "category": "Transportation Science & Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000826430300001", "keywords": "Magnetic resonance imaging; Imaging; Navigation; Trajectory; Planning; Blood vessels; In vivo; Extended state observer (ESO); magnetic actuation and navigation; microrobot; motion planning; visual feedback", "title": "Image-Guided Corridor-Based Motion Planning and Magnetic Control of Microrotor in Dynamic Environments", "abstract": "The in vivo manipulation of magnetic microrobots has attracted considerable attention because of its advantages of noninvasiveness and high precision in the targeted delivery. This article presents an automated control scheme for a magnetic microrotor (an anchor-like microrobot), which has the potential for microsurgery and delivery under the guidance of optical coherence tomography imaging in real time. A front-end optimal path planner that considers both the path length and the path clearance based on the particle swarm optimization, as well as a back-end dense corridor-based trajectory generator, is designed to ensure the collision avoidance of the microrotor navigation in blood vessels. In addition, to achieve robust and safe trajectory tracking of the microrotor, a model-predictive controller based on the extended state observer is used to compensate for the unmodeled dynamics and unknown disturbance while further restricting the state space. Simulations are performed to tune the parameters and validate the proposed approach. Experiments of navigating the microrotor in dynamic environment with different flow rates are performed to demonstrate the effectiveness of the proposed approach.", "journal": "IEEE-ASME TRANSACTIONS ON MECHATRONICS", "category": "Automation & Control Systems; Engineering, Manufacturing; Engineering, Electrical & Electronic; Engineering, Mechanical", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000820876500001", "keywords": "State of charge; Data fusion; Extended Kalman filter; Adaptive extended Kalman filter; Vanadium redox flow battery", "title": "A Variable Selection Method Based on Fast Nondominated Sorting Genetic Algorithm for Qualitative Discrimination of Near Infrared Spectroscopy", "abstract": "A reliable and effective qualitative near-infrared (NIR) spectroscopy discrimination method is critical for excellent model building, yet the performance of models built by these methods is highly dependent on valid feature extraction. The goal of feature selection is to associate the selected variables with the property of interest, which many have done successfully. However, many of selection methods focus only on strong association with the analytes or properties of interest, neglecting correlations between variables. A variable selection method based on a fast nondominated-ranking genetic algorithm (NSGA-II) was proposed in this paper for qualitative discrimination of NIR spectra. The method had two objective functions: (1) maximizing the sum of ratios of interclass variance to intraclass variance, (2) minimizing the sum of correlation coefficients between the selected variables. FT-NIR spectra of a total of 124 tobacco samples from different origins and parts in Guizhou Province, China, were used as the experimental objects, and the part-grade discrimination models of tobacco leaves were established by combining this method with partial least squares-based discriminant analysis (PLS-DA), and compared with PLS-DA model based on the full spectrum. The results showed that the performance of PLS-DA model with the NSGA-II was improved, with a comparable or better correct discrimination rate and reasonable discrimination rate, and could discriminate different parts of the tobacco leaves well. It indicates that the NSGA-II can select a few and effective feature variables to build a high-performance qualitative discrimination model and is proved to be a promising algorithm. In addition, the method is not designed exclusively for spectral data. It is a general strategy that could be used for variable selection for other types of data.", "journal": "JOURNAL OF SPECTROSCOPY", "category": "Biochemical Research Methods; Spectroscopy", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000835425000001", "keywords": "Stokes flow; incompressibility; state-constrained optimization; Lagrangian; saddle-point problem; adjoint state; inverse identification problem; shape derivative", "title": "Inverse problem of shape identification from boundary measurement for Stokes equations: Shape differentiability of Lagrangian", "abstract": "For Stokes equations under divergence-free and mixed boundary conditions, the inverse problem of shape identification from boundary measurement is investigated. Taking the least-square misfit as an objective function, the state-constrained optimization is treated by using an adjoint state within the Lagrange approach. The directional differentiability of a Lagrangian function with respect to shape variations is proved within the velocity method, and a Hadamard representation of the shape derivative by boundary integrals is derived explicitly. The application to gradient descent methods of iterative optimization is discussed.", "journal": "JOURNAL OF INVERSE AND ILL-POSED PROBLEMS", "category": "Mathematics, Applied; Mathematics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000804479400001", "keywords": "abnormal contradiction detection; fuzzy ontology; JD e-commerce platform; online reviews", "title": "Product online review analysis using fuzzy ontology", "abstract": "This paper analyses the online text comments in the online trading platform and identifies the abnormal and contradictory comments. Firstly, the abnormal contradiction detection and analysis model of online text reviews is constructed based on fuzzy ontology to identify the abnormal and contradictory online text reviews. Secondly, the fuzzy emotional ontology and emotional tendency analysis method identify abnormal and contradictory online text reviews. The online text review anomaly detection and analysis model constructed enriches the process and method of online review anomaly identification.", "journal": "SYSTEMS RESEARCH AND BEHAVIORAL SCIENCE", "category": "Management; Social Sciences, Interdisciplinary", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000807809900011", "keywords": "the Stewart platform; forward kinematics; geometric approach; real-time", "title": "Ultimate Bearing Capacity Analysis of Manned Submersible Based on the Genetic Algorithm Discontinuous and Galerkin Finite Element Method", "abstract": "The pressure hull of deep manned submersible is the most basic component to ensure its intended function. It is necessary to study the influence of initial geometric defects on the bearing capacity of pressure hull of manned submersible with different depths. According to the idea of discontinuous Galerkin finite element method, the theoretical model is constructed and the corresponding algorithm is designed, and the genetic algorithm is combined with discontinuous Galerin finite element method to establish the inverse method to obtain the ultimate bearing capacity of manned submersible. First, the discontinuous Galerkin finite element model is constructed, the inversion model is also established through combing the discontinuous Galerkin finite element method and genetic algorithm, and then the corresponding solution algorithm is designed. Moreover, then, the ultimate bearing analysis of manned submersible for different deep is carried out based on the inversion model combing discontinuous Galerkin finite element method and genetic algorithm. The effect of defect parameters on ultimate bearing capacity of manned submersible is obtained.", "journal": "MATHEMATICAL PROBLEMS IN ENGINEERING", "category": "Engineering, Multidisciplinary; Mathematics, Interdisciplinary Applications", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000862665900008", "keywords": "Costs; Power grids; Communication networks; SCADA systems; Power system protection; Power system faults; Optimization; Blackouts; cascading failure; DC-QSS; non-PLCC; PLCC; preventive control; SCADA", "title": "Intelligent Optimization of Tower Crane Location and Layout Based on Firefly Algorithm", "abstract": "The existing tower crane positioning layout mainly depends on the experience of construction personnel, and the best tower crane positioning can be found through a large number of manual data calculation. This manual method is time-consuming and impractical. In view of this, aiming at the current situation that building information modeling (BIM) software can only obtain the relative coordinates of components, this article puts forward the key technology of importing computer-aided design (CAD) graphics into geographic information system (GIS) software to automatically obtain the world coordinate information. By clarifying the transfer relationship between the component material supply point, the component initial positioning point, and the tower crane optional positioning point, as well as the cooperative relationship between each positioning point and the tower crane operation, the tower crane positioning optimization model is formed, and the firefly algorithm is used to automatically calculate and generate the best positioning layout method of the tower crane on the project site. In this study, the vertical transportation and positioning of components are studied, and intelligent construction is formed by integrating information technology. It can further enrich the functions of perception, analysis, decision-making, and optimization; realize the decision-making intelligence of industrial buildings; and achieve the organic unity of engineering construction execution system and decision-making command system.", "journal": "COMPUTATIONAL INTELLIGENCE AND NEUROSCIENCE", "category": "Mathematical & Computational Biology; Neurosciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000329269300001", "keywords": "Land cover classification; Mexico; MODIS; Species distribution model; Time series; Vegetation phenology", "title": "Evolving dynamical networks", "abstract": "Given the rapid loss of biodiversity worldwide and the resulting impacts on ecosystem functions and services, we more than ever rely on current and spatially continuous assessments of species distributions for biodiversity conservation and sustainable land management. Over the last decade, the usefulness of categorical land cover data to account for the human-induced degradation, transformation and loss of natural habitat in species distribution models (SDMs) has been questioned and the number of studies directly analyzing remotely sensed variables has lately multiplied. While several assumptions support the advantages of remote sensing data, an empirical comparison is still lacking. The objective of this study was to bridge this gap and compare the suitability of an existing categorical land cover classification and of continuous remote sensing variables for modeling the distribution patterns of 30 Mexican tree species. We applied the Maximum Entropy algorithm to predict species distributions based on both data types independently, quantified model performance and analyzed species-land cover relationships in detail. As part of this comparison, we focused on two particular aspects, namely the effects of (1) thematic detail and (2) spatial resolution of the land cover data on model performance. Our analysis revealed that remote sensing data were significantly better model predictors and that the main obstacle of the land cover-based SDMs were their bolder predictions, together with their overall overestimation of suitability. Among the land cover-based models, we found that thematic detail was more important than spatial resolution for SDM performance. However, our results also suggest that the suitability of land cover data differs largely among species and is dependent on their habitat distinctiveness. Our findings have relevant implications for future species distribution modeling studies which aim at complementing their set of topo-climatic predictors by data on land surface characteristics. (C) 2013 Elsevier B.V. All rights reserved.", "journal": "PHYSICA D-NONLINEAR PHENOMENA", "category": "Mathematics, Applied; Physics, Fluids & Plasmas; Physics, Multidisciplinary; Physics, Mathematical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000323512300014", "keywords": "Dynamic flexure; Gauss-Markov process; parameter estimation; transfer alignment; Tufts-Kumaresan method", "title": "Online Estimation of Ship Dynamic Flexure Model Parameters for Transfer Alignment", "abstract": "This paper presents an online approach for estimating the dynamic flexure model parameters in shipboard transfer alignment (TA). Traditionally, the application of Kalman filters (KFs) to the TA process is often restricted because of the lack of real-time information on dynamic flexure characteristics, and a KF designed on the basis of inaccurate parameters of the dynamic flexure model will result in a large alignment error. To overcome this difficulty, a parameter estimation algorithm is proposed in this paper, which utilizes the angular increment difference measured by the master inertial navigation system (MINS) and the slave inertial navigation system. Specifically, the Tufts-Kumaresan method is introduced to compute the unknown parameters of the dynamic flexure model from the angular increment correlation function. Our simulation results show that the proposed method can estimate the dynamic flexure parameters with a high degree of accuracy, even in low signal-to-noise ratio conditions. This parameter estimation method does not require a priori knowledge of dynamic flexure characteristics and, therefore, provides the shipboard sensors with an accurate and rapid-response capability for alignment with the MINS.", "journal": "IEEE TRANSACTIONS ON CONTROL SYSTEMS TECHNOLOGY", "category": "Automation & Control Systems; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000329463100002", "keywords": "Damaged starch; Fonio; Mixolab; Sorghum; Sourdough", "title": "Improvement of fonio dough properties through starch modification by sourdough fermentation", "abstract": "Fonio is a West African millet with potential for gluten-free products. Sourdough fermentation improves fonio bread quality. To determine the cause, the effects of sourdough fermentation on the dough quality and starch characteristics of two white and two black fonio types and a white sorghum type were investigated. Sourdough fermentation substantially improved the dough consistency making it more similar to bread wheat flour, as measured by the Mixolab. Sourdough fermentation also increased pasting viscosity, an indication of effects on starch. SEM indicated that sourdough fermentation caused some slight swelling and starch leaching from the fonio starch granules. This was confirmed by an increase in damaged starch. It also caused a substantial reduction in starch gel firmness. Principal component analysis clearly separated fermented and unfermented fonio flours. Damaged starch was associated with fermented fonio, as were Mixolab parameters related to dough stability. Sourdough fermentation thus improves fonio dough and bread quality by bringing about slight changes in the starch granules, which probably increase water absorption and hence improve dough strength and gas holding.", "journal": "STARCH-STARKE", "category": "Food Science & Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000337739300024", "keywords": "Body sensor networks; e-AR sensor; gait; longest common subsequence (LCSS); singular spectrum analysis (SSA)", "title": "Gait Parameter Estimation From a Miniaturized Ear-Worn Sensor Using Singular Spectrum Analysis and Longest Common Subsequence", "abstract": "This paper presents a new approach to gait analysis and parameter estimation from a single miniaturized ear-worn sensor embedded with a triaxial accelerometer. Singular spectrum analysis combined with the longest common subsequence algorithm has been used as a basis for gait parameter estimation. It incorporates information from all axes of the accelerometer to estimate parameters including swing, stance, and stride times. Rather than only using local features of the raw signals, the periodicity of the signals is also taken into account. The hypotheses tested by this study include: 1) how accurate is the ear-worn sensor in terms of gait parameter extraction compared to the use of an instrumented treadmill; 2) does the ear-worn sensor provide a feasible option for assessment and quantification of gait pattern changes. Key gait events for normal subjects such as heel contact and toe off are validated with a high-speed camera, as well as a force-plate instrumented treadmill. Ten healthy adults walked for 20 min on a treadmill with an increasing incline of 2% every 2 min. The upper and lower limits of the absolute errors using 95% confidence intervals for swing, stance, and stride times were obtained as 35.5 +/- 3.99 ms, 36.9 +/- 3.84 ms, and 17.9 +/- 2.29 ms, respectively.", "journal": "IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING", "category": "Engineering, Biomedical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000332032100006", "keywords": "Fuzzy logic supervisor; genetic algorithms; MATLAB (TM) real time temperature control; PI/PID controllers on-line auto-tuning; T-S plant modeling", "title": "A phylogenetic Kalman filter for ancestral trait reconstruction using molecular data", "abstract": "Motivation: Correlation between life history or ecological traits and genomic features such as nucleotide or amino acid composition can be used for reconstructing the evolutionary history of the traits of interest along phylogenies. Thus far, however, such ancestral reconstructions have been done using simple linear regression approaches that do not account for phylogenetic inertia. These reconstructions could instead be seen as a genuine comparative regression problem, such as formalized by classical generalized least-square comparative methods, in which the trait of interest and the molecular predictor are represented as correlated Brownian characters coevolving along the phylogeny. Results: Here, a Bayesian sampler is introduced, representing an alternative and more efficient algorithmic solution to this comparative regression problem, compared with currently existing generalized least-square approaches. Technically, ancestral trait reconstruction based on a molecular predictor is shown to be formally equivalent to a phylogenetic Kalman filter problem, for which backward and forward recursions are developed and implemented in the context of a Markov chain Monte Carlo sampler. The comparative regression method results in more accurate reconstructions and a more faithful representation of uncertainty, compared with simple linear regression. Application to the reconstruction of the evolution of optimal growth temperature in Archaea, using GC composition in ribosomal RNA stems and amino acid composition of a sample of protein-coding genes, confirms previous findings, in particular, pointing to a hyperthermophilic ancestor for the kingdom.", "journal": "BIOINFORMATICS", "category": "Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000333753700020", "keywords": "aged; benzodiazepines; dementia; outcome assessment; substance withdrawal syndrome", "title": "Benzodiazepine discontinuation and patient outcome in a chronic geriatric medical/psychiatric unit: A retrospective chart review", "abstract": "AimA substantial number of elderly patients take benzodiazepines (BZD) regularly despite concerns about toxicity and possible dependence, and there are relatively few data to guide clinicians' decisions regarding discontinuing benzodiazepines in the elderly. MethodsWe carried out a retrospective chart review of 75 elderly patients admitted to a chronic medical/psychiatric unit who were taking a standing dose of benzodiazepines on admission, comparing 40 patients who discontinued benzodiazepines versus 35 who did not discontinue. PurposeWe examined the association of BZD discontinuation versus continuation with clinical outcomes on discharge, and further examined clinical characteristics associated with BZD discontinuation. ResultsDiscontinuers had shorter length of stay without evidence of worse cognitive and functional outcome except a trend toward increased incidence of agitation. Logistic regression models suggested anxiety, higher age and higher dose of antidepressants at the beginning were significantly related to successful discontinuation during admission after regression. ConclusionThese data imply that BZD withdrawal during admission can be safe and feasible in many elderly frail patients, and that withdrawal might be associated with shorter duration of chronic hospitalization. Geriatr Gerontol Int 2014; 14: 388-394.", "journal": "GERIATRICS & GERONTOLOGY INTERNATIONAL", "category": "Geriatrics & Gerontology; Gerontology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000333835600004", "keywords": "nonlinear algorithm; groundwater; hydrological modeling; PEST; SWAT", "title": "Implementing a nonlinear groundwater module in the soil and water assessment tool (SWAT)", "abstract": "The Soil and Water Assessment Tool (SWAT) is widely used in modeling water quantity and quality. In the original SWAT, groundwater flow is calculated using a linear-reservoir model, with outflow proportional to storage. However, observations show that this assumption is not always applicable; for example, macropores in Karst formations would seriously affect the groundwater behavior. A nonlinear groundwater algorithm was introduced in a new version of the SWAT model, called ISWAT. The Shenandoah Valley area in the Eastern U.S., which includes a number of geologic formations including Karst, was selected to test the modified ISWAT model. Parameter ESTimation (PEST) was coupled with ISWAT to auto-calibrate the nonlinear parameter values. Ten years of record at 15 stream gauges were used to calibrate the model. The nonlinear ISWAT, statistically and visually, performed better in stream discharge estimation especially during baseflow recession and low-flow periods. This indicated that the nonlinear algorithm can better represent groundwater behavior. The coupled ISWAT-PEST approach can be used in future stream discharge simulation. Copyright (c) 2013 John Wiley & Sons, Ltd.", "journal": "HYDROLOGICAL PROCESSES", "category": "Water Resources", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000330573000006", "keywords": "sepsis; innate immunity; pattern recognition receptors; pathogen-associated molecular patterns (PAMP); danger-associated molecular patterns (DAMP); neutrophil extracellular traps (NET); myeloid related protein (Mrp)-8/14; coagulation; activated protein C; protease activated receptors", "title": "Host innate immune responses to sepsis", "abstract": "The immune response to sepsis can be seen as a pattern recognition receptor-mediated dysregulation of the immune system following pathogen invasion in which a careful balance between inflammatory and anti-inflammatory responses is vital. Invasive infection triggers both pro-inflammatory and anti-inflammatory host responses, the magnitude of which depends on multiple factors, including pathogen virulence, site of infection, host genetics, and comorbidities. Toll-like receptors, the inflammasomes, and other pattern recognition receptors initiate the immune response after recognition of danger signals derived from microorganisms, so-called pathogen-associated molecular patterns or derived from the host, so-called danger-associated molecular patterns. Further dissection of the role of host-pathogen interactions, the cytokine response, the coagulation cascade, and their multidirectional interactions in sepsis should lead toward the development of new therapeutic strategies in sepsis.", "journal": "VIRULENCE", "category": "Immunology; Infectious Diseases; Microbiology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000325242100012", "keywords": "Basic analytes; Cationic-exchange; RP phases; Silanol sites; VAChT", "title": "A fluoro versus a nitro derivative-a high-performance liquid chromatography study of two basic analytes with different reversed phases and silica phases as basis for the separation of a positron emission tomography radiotracer", "abstract": "To develop a basis for the separation of a F-18-labeled PET radiotracer from its nitro precursor, we performed an analytical HPLC study using the unlabeled reference compound and the corresponding nitro precursor. Aim of the study was to find a separation in which the fluoro derivative elutes in front of the nitro precursor with appropriate separation parameters. Several RP phases as well as a bare silica column were investigated with ACN and MeOH as organic modifiers and aqueous NH4OAc because of the basic character of the analytes. Four types of separation were observed based on different interaction mechanisms. When ACN/20 mM NH4OAc aq. was used mainly cationic-exchange and hydrophobic interactions contributed to the retention. A reversal of elution order could be observed starting from 95% ACN and subsequent increasing of the water content. This phenomenon was observed for all RP phases and seems to be independent of the different spacers bound to the silica. By contrast, using MeOH/20 mM NH4OAc aq. the elution order depends on the phase material. Two columns with the potential to perform pi-pi interactions showed different separation behavior compared to the other RP phases. (C) 2013 Elsevier B.V. All rights reserved.", "journal": "JOURNAL OF CHROMATOGRAPHY A", "category": "Biochemical Research Methods; Chemistry, Analytical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000320907500009", "keywords": "Semi-automated surveillance; CCTV; Detection; Tracking; Security", "title": "Semi-automated CCTV surveillance: The effects of system confidence, system accuracy and task complexity on operator vigilance, reliance and workload", "abstract": "Recent advances in computer vision technology have lead to the development of various automatic surveillance systems, however their effectiveness is adversely affected by many factors and they are not completely reliable. This study investigated the potential of a semi-automated surveillance system to reduce CCTV operator workload in both detection and tracking activities. A further focus of interest was the degree of user reliance on the automated system. A simulated prototype was developed which mimicked an automated system that provided different levels of system confidence information. Dependent variable measures were taken for secondary task performance, reliance and subjective workload. When the automatic component of a semi-automatic CCTV surveillance system provided reliable system confidence information to operators, workload significantly decreased and spare mental capacity significantly increased. Providing feedback about system confidence and accuracy appears to be one important way of making the status of the automated component of the surveillance system more 'visible' to users and hence more effective to use. (C) 2012 Elsevier Ltd and The Ergonomics Society. All rights reserved.", "journal": "APPLIED ERGONOMICS", "category": "Engineering, Industrial; Ergonomics; Psychology, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000324308600017", "keywords": "Patient-Controlled Analgesia; Morphine; Tramadol; Metoclopramide", "title": "Metoclopramide Improves the Quality of Tramadol PCA Indistinguishable to Morphine PCA: A Prospective, Randomized, Double Blind Clinical Comparison", "abstract": "Abstract Objective Multimodal analgesia has been effectively used in postoperative pain control. Tramadol can be considered \"multimodal\" because it has two main mechanisms of action, an opioid agonist and a reuptake inhibitor of norepinephrine and serotonin. Tramadol is not as commonly used as morphine due to the increased incidence of postoperative nausea and vomiting (PONV). As metoclopramide is an antiemetic and an analgesic, it was hypothesized that when added to reduce PONV, metoclopromide may enhance the multimodal feature of tramadol by the analgesic property of metoclopramide. Therefore, the effectiveness of postoperative patient-controlled analgesia (PCA) with morphine was compared against PCA with combination of tramadol and metoclopramide. Design A prospective, randomized, double blind clinical trial. Setting Academic pain service of a university hospital. Subjects Sixty patients undergoing elective total knee arthroplasty with general anesthesia. Methods Sixty patients were randomly divided into Group M and Group T. In a double-blinded fashion, Group M received intraoperative 0.2 mg/kg morphine and postoperative PCA with 1 mg morphine per bolus, whereas Group T received intraoperative tramadol 2.5 mg/kg and postoperative PCA with 20 mg tramadol plus 1 mg metoclopramide per bolus. Lockout interval was 5 minutes in both groups. Pain scale, satisfaction rate, analgesic consumption, PCA demand, and side effects were recorded by a blind investigator. Results These two groups displayed no statistically significant difference between the items and variables evaluated. Conclusions This combination provides analgesia equivalent to that of morphine and can be used as an alternative to morphine PCA.", "journal": "PAIN MEDICINE", "category": "Anesthesiology; Medicine, General & Internal", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000324380500019", "keywords": "Beam-scanning; bifocal; optimization; phased array; reflectarray; reflector", "title": "Bifocal Design and Aperture Phase Optimizations of Reflectarray Antennas for Wide-Angle Beam Scanning Performance", "abstract": "A new design methodology is proposed for high-gain beam-scanning reflectarray antennas. Various approaches for designing beam-scanning reflectarray antennas are first reviewed and it is shown that for limited scan coverage, utilizing the feed displacement technique is a convenient design approach. To improve the scan coverage, a single-reflector bifocal aperture phase distribution is proposed for the reflectarray antenna, and is further optimized to improve the beam-scanning performance. Four reflectarray prototypes, each corresponding to a specific aperture phase distribution, have been fabricated and tested. A Ka-band reflectarray antenna with 60 degrees scan coverage achieving 30-dB gain and side-lobe level below 15 dB is demonstrated.", "journal": "IEEE TRANSACTIONS ON ANTENNAS AND PROPAGATION", "category": "Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000333206500012", "keywords": "Speech synthesis; HMM-based speech synthesis; Parametric representation of speech; Excitation model; Pitch-scaled spectrum", "title": "Pitch-Scaled Spectrum Based Excitation Model for HMM-based Speech Synthesis", "abstract": "The speech generated by hidden Markov model (HMM)-based speech synthesis systems (HTS) suffers from a 'buzzing' sound, which is due to an over-simplified vocoding technique. This paper proposes a new excitation model that uses a pitch-scaled spectrum for the parametric representation of speech in HTS. A residual signal produced using inverse filtering retains the detailed harmonic structure of speech that is not part of the linear prediction (LP) spectrum. By using pitch-scaled spectrums, we can compensate the LP spectrum using the detailed harmonic structure of the residual signal. This spectrum can be compressed using a periodic excitation parameter so that it can used to train HTS. We define an aperiodic measure as the harmonics-to-noise ratio, and calculate a voicing-cut off frequency to fit the aperiodic measure to a sigmoid function. We combine the LP coefficient, pitch-scaled spectrum, and sigmoid function to create a new parametric representation of speech. Listening tests were carried out to evaluate the effectiveness of the proposed technique. This vocoder received a mean opinion score of 4.0 in analysis-synthesis experiments, before dimensionality reduction. By integrating this vocoder into HTS, we improved the sound of the synthesized speech compared with the pulse train excitation model, and demonstrated an even better result than STRAIGHT-HTS.", "journal": "JOURNAL OF SIGNAL PROCESSING SYSTEMS FOR SIGNAL IMAGE AND VIDEO TECHNOLOGY", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000332161800153", "keywords": "stars: fundamental parameters; stars: early-type; stars: individual: HD 47887; stars: individual: HD 47777; open clusters and associations: individual: NGC 2264; stars: magnetic field", "title": "Two spotted and magnetic early B-type stars in the young open cluster NGC 2264 discovered by MOST and ESPaDOnS", "abstract": "Star clusters are known as superb tools for understanding stellar evolution. In a quest for understanding the physical origin of magnetism and chemical peculiarity in about 7% of the massive main-sequence stars, we analysed two of the ten brightest members of the similar to 10 Myr old Galactic open cluster NGC 2264, the early B-dwarfs HD47887 and HD47777. We find accurate rotation periods of 1.95 and 2.64 days, respectively, from MOST photometry. We obtained ESPaDOnS spectropolarimetric observations, through which we determined stellar parameters, detailed chemical surface abundances, projected rotational velocities, and the inclination angles of the rotation axis. Because we found only small (<5 km s(-1)) radial velocity variations, most likely caused by spots, we can rule out that HD47887 and HD47777 are close binaries. Finally, using the least-squares deconvolution technique, we found that both stars possess a large-scale magnetic field with an average longitudinal field strength of about 400 G. From a simultaneous fit of the stellar parameters we determine the evolutionary masses of HD47887 and HD47777 to be 9.4(-0.7)(+0.6) M-circle dot and 7.6(-0.5)(+0.5) M-circle dot. Interestingly, HD47777 shows a remarkable helium underabundance, typical of helium-weak chemically peculiar stars, while the abundances of HD47887 are normal, which might imply that diffusion is operating in the lower mass star but not in the slightly more massive one. Furthermore, we argue that the rather slow rotation, as well as the lack of nitrogen enrichment in both stars, can be consistent with both the fossil and the binary hypothesis for the origin of the magnetic field. However, the presence of two magnetic and apparently single stars near the top of the cluster mass-function may speak in favour of the latter.", "journal": "ASTRONOMY & ASTROPHYSICS", "category": "Astronomy & Astrophysics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000323177700011", "keywords": "vitamin D deficiency; preeclampsia; eclampsia; hypertension", "title": "Vitamin D Deficiency and the Risk of Preeclampsia and Eclampsia in Bangladesh", "abstract": "Maternal vitamin D deficiency has been proposed as a risk factor for preeclampsia, but no significant studies have been conducted to evaluate its relationship with eclampsia. Our goal in this study was to assess the prevalence and potential risk of vitamin D deficiency for both preeclampsia and eclampsia in Bangladesh. Using a case-control design, we measured serum 25(OH)D levels in pregnant women receiving care at the Dhaka Medical College Hospital with preeclampsia (n=33), eclampsia (n=79), and normal pregnancy (controls, n=76). Odds of developing pre-eclampsia and eclampsia with vitamin D deficiency were calculated using logistic regression analysis. The prevalence of vitamin D insufficiency was very high with more than 3 quarters (78%) of all subjects having a serum 25(OH)D level<30 ng/ml. The mean serum 25(OH)D level was 24.86 ng/ml in controls, 23.96 ng/ml in pre-eclamptic women, and 21.56 ng/ml in eclampsia patients. Comparing to those who had a serum 25(OH)D level of >= 30 ng/ml, the odds ratio (95% CI) of developing preeclampsia and eclampsia in mothers with vitamin D insufficiency were 3.9 (95% CI=1.18-12.87) and 5.14 (95% CI=1.98-13.37), respectively (adjusting for age, BMI and duration of pregnancy). The odds of developing preeclampsia and eclampsia may increase by up to 5-fold in women with vitamin D insufficiency. Since preeclampsia and eclampsia can lead to serious complications for both mother and the offspring, vitamin D may be supplemented during pregnancy in high risk populations to decrease these adverse consequences.", "journal": "HORMONE AND METABOLIC RESEARCH", "category": "Endocrinology & Metabolism", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000325075600032", "keywords": "LTCC; Embedded channel; Thermal analysis; Starch; Burnout; Heating profile", "title": "Thermal analysis of starch for realizing embedded channel in low temperature co-fired ceramic", "abstract": "The rapidly developing biotechnology, automotive industry, chemical and environmental fields have increasing needs for analytical systems with desires such as smaller sizes, lower sample volumes. Reduction in size results in further requirements in functionalities such as multi-sensor devices with low cost. These microsystems usually contain three-dimensional structures. In the fabrication of microfluidic devices ensuring a well-shaped channel is a challenge. During firing of the low temperature co-fired ceramic (LTCC) substrate, these embedded structures tend to deform and sag because the green glass-ceramic material is very weak. Starch was used as sacrificial volume material (SVM) to support the embedded structures of the LTCC during lamination and sintering. As a consequence of burnout, the increased fraction of evolving gases from SVM requires an adequate adaptation of the firing process to control starch degradation and provide a residue-free burnout. Using thermal analysis techniques and describing degradation kinetics of SVM, a new heating profile is demonstrated which insures complete starch burnout without damaging the LTCC structures.", "journal": "JOURNAL OF THERMAL ANALYSIS AND CALORIMETRY", "category": "Thermodynamics; Chemistry, Analytical; Chemistry, Physical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000334001300029", "keywords": "High resolution electrocardiogram; Sustained monomorphic ventricular tachycardia; Principal component analysis; Ventricular late potentials; Risk classification", "title": "Principal component analysis in high resolution electrocardiogram for risk stratification of sustained monomorphic ventricular tachycardia", "abstract": "Ventricular late potentials (VLP) are low amplitude and high frequency transients registered on high resolution electrocardiogram (HRECG), markers of life threatening ventricular tachyarrhythmia. This study assessed a novel VLP group classification method based on principal components (PC) analysis. Thirty-six subjects (mean+/-SD; 55.4+/-11.6 years) divided in two groups, 18 healthy controls and 18 patients with induced sustained monomorphic ventricular tachycardia were included. Four PC data matrix from HRECG signal averaged with no further filtering leads were built, taking QRS onset as reference. Mahalanobis distance calculation combined with classification by logistic regression determined optimal separation threshold between groups for each matrix. HRECG signals were also analyzed using classical approaches. ROC curve analyzes compared novel and classical methods (alpha<0.05). The optimal configuration retained seven initial PCs. Average c-statistic was 0.99 for PC method and 0.65 for classical methods taken together (p < 0.05). PC analysis increases diagnostic accuracy for VLP group classification with potential clinical application. (C) 2013 Elsevier Ltd. All rights reserved.", "journal": "BIOMEDICAL SIGNAL PROCESSING AND CONTROL", "category": "Engineering, Biomedical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000328719500007", "keywords": "Task conflict; Relationship conflict; Information processing; Decision making", "title": "Task conflict, information processing, and decision-making: The damaging effect of relationship conflict Preface", "abstract": "A popular theoretical assumption holds that task-related disagreements stimulate critical thinking, and thus may improve group decision making. Two recent meta-analyses showed, however, that task conflict can have a positive effect, a negative effect, or no effect at all on decision-making quality (De Dreu & Weingart, 2003; De Wit, Greer, & Jehn, 2012). In two studies, we built upon the suggestion of both meta-analyses that the presence of relationship conflict determines whether a task conflict is positively or negatively related to decision making. We hypothesized and found that the level of perceived relationship conflict during task conflict (Study 1), and the actual presence (vs. absence) of relationship conflict during task conflict (Study 2), increased group members' rigidity in holding onto suboptimal initial preferences during decision making and thus led to poor decisions. In both studies the effect of relationship conflict on decision making was mediated by biased use of information. (C) 2013 Elsevier Inc. All rights reserved.", "journal": "ORGANIZATIONAL BEHAVIOR AND HUMAN DECISION PROCESSES", "category": "Psychology, Applied; Management; Psychology, Social", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000321436300018", "keywords": "colorectal cancer; total antioxidant capacity; risk; diet; non enzymatic antioxidant capacity", "title": "Dietary total antioxidant capacity and colorectal cancer: A large case-control study in Italy", "abstract": "A favorable role of fruit and vegetables on colorectal cancer risk has been related to the antioxidant properties of their components. We used data from an Italian case-control study including 1,953 patients with incident, histologically confirmed colorectal cancer (1,225 colon and 728 rectal cancers). Controls were 4,154 patients admitted to hospital for acute, non-neoplastic conditions. A reproducible and valid food frequency questionnaire was used to assess subjects' usual diet. Total antioxidant capacity (TAC) was measured using Italian food composition tables in terms of ferric reducing-antioxidant power (FRAP), Trolox equivalent antioxidant capacity (TEAC) and total radical-trapping antioxidant parameter (TRAP). We estimated the odds ratios (ORs) and the corresponding 95% confidence intervals (CIs) through multiple logistic regression models, including terms for potential confounding factors, and energy intake. TAC was inversely related with colorectal cancer risk: the OR for the highest versus the lowest quintile was 0.68 (95% CI, 0.57-0.82) for FRAP, 0.69 (95% CI, 0.57-0.83) for TEAC and 0.69 (95% CI, 0.57-0.83) for TRAP. Corresponding values, excluding TAC deriving from coffee, were 0.75 (95% CI, 0.61-0.93) for FRAP, 0.76 (95% CI, 0.61-0.93) for TEAC and 0.71 (95% CI, 0.57-0.89) for TRAP. The inverse association was apparentlythough not significantlystronger for rectal than for colon cancer. This is the first case-control study indicating consistent inverse relations between dietary TAC and colorectal cancer risk. What's new? A diet rich in fruit and vegetables has been associated with a reduced risk of common cancers, including colorectal cancer. Total antioxidant capacity (TAC), rather than individual components, has been suggested as a relevant factor for cancer risk. In this case-control study of over 6,000 patients, the authors used several different techniques to measure the dietary TAC of subjects' usual diet, and found a consistent inverse relationship between dietary TAC and colorectal cancer risk.", "journal": "INTERNATIONAL JOURNAL OF CANCER", "category": "Oncology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000433203501200", "keywords": "688 retina; 499 diabetic retinopathy; 550 imaging/image analysis: clinical", "title": "In Vivo Microscopic Assessment Of Perfused Foveal Capillary Lumen Diameters In Diabetic Retinopathy Versus Healthy Controls", "abstract": "Magnetic resonance imaging presents high-resolution preoperative scans of target tissue and allows for the availability of intraoperative real-time images without the exposure of patients to ionizing radiation. This has motivated scientists and engineers to integrate medical robotics with the magnetic resonance imaging modality to allow robot-assisted, image-guided diagnosis and therapy. This article provides a review of the state-of-the-art medical robotic systems available for use in conjunction with intraoperative magnetic resonance imaging. The robot functionalities and mechanical designs for a wide range of magnetic resonance imaging interventions are presented, including their magnetic resonance imaging compatibility, actuation, kinematics and the mechanical and electrical designs of the robots. Classification and comparative study of various intraoperative magnetic resonance image guided robotic systems are provided. The robotic systems reviewed are summarized in a table in detail. Current technologies for magnetic resonance imaging-conditional robotics are reviewed and their potential future directions are sketched.", "journal": "INVESTIGATIVE OPHTHALMOLOGY & VISUAL SCIENCE", "category": "Ophthalmology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000331888200002", "keywords": "Glufosinate resistance; resistance management; weed control", "title": "Herbicide Programs for Controlling Glyphosate-Resistant Johnsongrass (Sorghum halepense) in Glufosinate-Resistant Soybean", "abstract": "Three field experiments were conducted in 2010 and 2012 in a soybean production field near West Memphis, AR, containing glyphosate-resistant johnsongrass. The goal of this research was to develop effective herbicide programs for glyphosate-resistant johnsongrass in glufosinate-resistant soybean. Control of the resistant johnsongrass was greater with glufosinate at 590 and 740 g ai ha(-1) than at 450 g ha(-1). Sequential glufosinate applications were more effective than a single application, irrespective of rate. A PRE application of flumioxazin at 71 g ai ha(-1) immediately after planting provided no more than 26% johnsongrass control 6 wk after soybean emergence (WAE). The addition of clethodim at 136 g ai ha(-1) to sequential applications of glufosinate at 450 g ha(-1) improved control over sequentially applied glufosinate alone. Herbicide programs containing imazethapyr or imazamox in combination with glufosinate followed by clethodim plus glufosinate controlled johnsongrass at least 94% at 10 WAE and provided three distinct mechanisms of action, a highly effective resistance management strategy. Results from this research indicate that a high level of glyphosate-resistant johnsongrass control can be achieved through the use of several herbicide options in glufosinate-resistant soybean.", "journal": "WEED TECHNOLOGY", "category": "Agronomy; Plant Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000325991100020", "keywords": "Bayesian method; calibration; MCMC method; rainfall uncertainty; hydrological model", "title": "Separately accounting for uncertainties in rainfall and runoff: Calibration of event-based conceptual hydrological models in small urban catchments using Bayesian method", "abstract": "Uncertainty analysis of hydrological models is usually based on model calibration, and the Bayesian method is a popular way to evaluate the uncertainty. The traditional Bayesian method usually uses lumped model residuals to form the likelihood function, where uncertainty in inputs (rainfall) is not explicitly addressed. This paper compares three approaches based on Bayesian inferences, considering rainfall uncertainty either implicitly or explicitly in calibration. Consistent parameter estimation and reliable quantification of predictive uncertainty are mainly examined. When rainfall uncertainty is explicitly treated in calibration, several rainfall observations at one-minute time steps are grouped to share one multiplier to consider the possible observation errors. The appropriate grouping strategy that balances the representativeness and the complexity of the problem is suggested. The application of the methods considered in this study focuses on small urban catchments (<200 ha) with a small temporal scale (1 min time step), in contrast to most literature studies dealing with larger catchments monitored at larger time steps. It is found that uncertainty in rainfall has a minor contribution to the total uncertainty in runoff estimation, and this minor role can be explained by the low pass filter effect of the linear reservoir model. However, the approach explicitly accounting for input uncertainty results in more informed knowledge for uncertainties related with hydrological model calibrations, which can possibly provide an estimation of uncertainty attributed to rainfall records. It should be noted that rainfall error estimates can compensate model structural uncertainty that is not explicitly addressed in this study.", "journal": "WATER RESOURCES RESEARCH", "category": "Environmental Sciences; Limnology; Water Resources", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000325304800029", "keywords": "Germanotellurite glasses; Erbium-doped glasses; Glass-ceramics; EXAFS-Extended X-ray Absorption Fine Structure; Photoluminescence", "title": "Local structure around Er3+ in GeO2-TeO2-Nb2O5-K2O glasses and glass-ceramics", "abstract": "Erbium-doped germanotellurite glasses and glass-ceramics in the (80-x) GeO2-xTeO(2)-10Nb(2)O(5)-10K(2)O system (x = 0-80 mol%) have been prepared. The heat treatments were performed based on differential scanning calorimetry data. Several crystalline phases, including alpha-TeO2, delta-TeO2 and GeO2 (alpha-quartz), together with K [Nb1/2TeO2/3](2)O-4.8, could be distinguished in the X-ray diffraction patterns. The 1.5 mu m photoluminescence (PL) emission of Er3+ (I-4(13/2) -> I-4(15/2)) has been studied in the glasses and glass-ceramics and the latter revealed a general increase in the bandwidth of the emission spectra, compared to the PL peak of the starting glass samples. Some heat treated compositions presented Stark splitting of the I-4(13/2) -> I-4(15/2) transition, indicating that the environment around the Er3+ ions had changed from an amorphous matrix to a more ordered environment. Extended X-ray Absorption Fine Structure spectroscopy measurements were performed in order to investigate the Er3+ coordination shell before and after heat treatments. The glass compositions presented only one coordination shell around Er3+, composed of oxygen, while two coordination shells could be distinguished for the heat treated samples: a first oxygen coordination shell for all heat treated compositions and a mixed second nearest neighbor shell of Te and Er ions, for the Te-rich compositions (x = 80 to 50), and of Ge and Er ions, for the Ge-rich compositions (x = 10 and 20). (c) 2012 Elsevier B.V. All rights reserved.", "journal": "JOURNAL OF NON-CRYSTALLINE SOLIDS", "category": "Materials Science, Ceramics; Materials Science, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000331136800008", "keywords": "dynamical downscaling; WRF; Baltic Sea basin; wind field; wind engineering", "title": "Creating wind field time-series over the Southern Baltic area using a dynamical downscaling approach", "abstract": "The aim of the research is to explore the possibility of using the Weather Research and Forecasting Model 3.2.1 (WRF) for creating synthetic wind speed time-series in the southern part of the Baltic Sea. The hourly wind speed time series derived from a mesoscale model were forced by the NCEP/NCAR re-analysis dataset for the period of 1991-2000 using 2-way nesting domains with horizontal resolutions of 27 and 9 km. Realism of spatial and temporal structure of the dataset was validated against in-situ offshore and onshore measurements. The dataset confirms the wind speed patterns over the Baltic Sea obtained in previous studies. The validation procedure proves that the model represents the spatio-temporal structure of the wind field well with a temporal correlation of around 0.80 for those stations with an undisturbed, high quality archive dataset. It was found that the model tends to underestimate wind speeds over offshore areas and overestimate them over onshore areas, especially for the near-surface wind field. Additionally, the author briefly presents the transition of wind speed in coastal areas as they are of great interest to the renewable energy community.", "journal": "METEOROLOGISCHE ZEITSCHRIFT", "category": "Meteorology & Atmospheric Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000327949900002", "keywords": "Finite elements; Functional data analysis; Penalized smoothing; Semiparametric model; Spatial data analysis", "title": "Spatial spline regression models", "abstract": "We describe a model for the analysis of data distributed over irregularly shaped spatial domains with complex boundaries, strong concavities and interior holes. Adopting an approach that is typical of functional data analysis, we propose a spatial spline regression model that is computationally efficient, allows for spatially distributed covariate information and can impose various conditions over the boundaries of the domain. Accurate surface estimation is achieved by the use of piecewise linear and quadratic finite elements.", "journal": "JOURNAL OF THE ROYAL STATISTICAL SOCIETY SERIES B-STATISTICAL METHODOLOGY", "category": "Statistics & Probability", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000327673600004", "keywords": "respiratory syncytial virus; pathogen recognition receptors; innate immunity; pathogenesis", "title": "Pathogen recognition receptor crosstalk in respiratory syncytial virus sensing: a host and cell type perspective", "abstract": "Human respiratory syncytial virus (RSV) is a major cause of acute lower respiratory tract infection in young children, immunocompromised adults, and the elderly. The innate immune response plays a pivotal role in host defense against RSV, but whether severe outcomes following RSV infection result from excessive or poor innate immune recognition remains unclear. Recent research suggests a situation in which crosstalk between families of pattern recognition receptors (PRRs) occurs in a cell type-dependent manner. The current challenge to empower novel therapeutic approaches and vaccine development is to confirm the role of the individual receptors in RSV pathogenesis in humans.", "journal": "TRENDS IN MICROBIOLOGY", "category": "Biochemistry & Molecular Biology; Microbiology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000323940600004", "keywords": "Retained haemothorax; Trauma; Pneumonia; Risk factors; Antibiotics; Prospective observational study", "title": "Risk factors for post-traumatic pneumonia in patients with retained haemothorax: Results of a prospective, observational AAST study", "abstract": "Introduction: Retained haemothorax (RH) is a problematic sequela of thoracic trauma, reported in up to 20% of patients following chest injury. RH is associated with a higher severity of thoracic trauma and may portend the onset of other serious post-traumatic complications, including pneumonia. The development of pneumonia has previously been reported to be as high as 19.5% in the setting of traumatic RH. The purpose of this study was to identify risk factors for the development of pneumonia as a complication in RH. Methods: We utilized the American Association for the Surgery of Trauma Post-Traumatic Retained Haemothorax database. Patients with post-traumatic RH were prospectively enrolled from 2009 to 2011. Inclusion criteria were placement of a thoracostomy tube within 24 h of admission for the evacuation of pneumothorax or haemothorax and subsequent chest computed tomography scan chest showing RH. Patients treated with thoracotomy before placement of tube thoracostomy were excluded. For univariate analysis, the Chi-square test with Yates correction was used for comparison of categorical risk factors and the Student's t-test or the Mann-Whitney test for comparison of continuous risk factors. To identify independent risk factors for the development of pneumonia, variables from the univariate analysis significant at p < 0.2 were entered into a forward logistic regression model. Adjusted odds ratio and 95% confidence intervals (CI) were derived. Results: 328 patients with post-traumatic RH from 20 United States centres were enrolled. After stepwise regression analysis, ISS > 25 (adjusted OR: 7.1; 95% CI: 3.1, 16.4; p < 0.001), blunt mechanism of injury (adjusted OR: 3.5; 95% CI: 1.7, 7.2; p = 0.001), and failure to administer peri-procedural antibiotics on the initial thoracostomy tube placement (adjusted OR: 2.6; 95% CI: 1.30, 5.4; p = 0.01) were found to be independent predictors of the pneumonia in patients with post-traumatic RH. Conclusions: To our knowledge, our current study is the largest attempt to identify the independent predictors for pneumonia in this population. Our data show that elevated ISS, blunt thoracic trauma, and failure to administer peri-procedural antibiotics on tube thoracostomy placement are the statistically significant independent risk factors. Published by Elsevier Ltd.", "journal": "INJURY-INTERNATIONAL JOURNAL OF THE CARE OF THE INJURED", "category": "Critical Care Medicine; Emergency Medicine; Orthopedics; Surgery", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000329151500012", "keywords": "Starch; Ancient manuscripts; Micellar electrokinetic capillary; chromatography Linear discriminant analysis; Artificial ageing", "title": "Identification of starch and determination of its botanical source in ancient manuscripts by MEKC-DAD and LDA", "abstract": "A simple and rapid Micellar Electrokinetic Capillary Chromatography method with UV diode-array detection (MEKC-DAD) has been developed for the identification of the two starch polysaccharides, amylopectin and amylose, in ancient manuscripts. Moreover, a linear discriminant analysis (LDA) has been used in order to determine the botanical source of the starch (wheat, maize or rice). The reason to develop this method is that starch has been used throughout history in paper manufacture as glue and sizing agent. The LDA was applied to the amylopectin/amylose ratio using the area and height data recorded. The separation was performed in an extended path-length fused-silica capillary (bubble capillary') of 36 cm in length and 50 mu m i.d.. The running buffer was composed of 20 mM sodium acetate, 1.2 mM I-2, 7.2 mM KI, and 50 mM sodium dodecyl sulphate (SDS) at pH 6. The potential applied was 22 kV in positive polarity, the temperature was 25 degrees C, and the detection was performed at 560 nm. Injection of the samples was performed at 20 mbar for 2 s. An artificial ageing test was carried out in the three types of starch in order to determine the effect of the temperature, relative humidity and irradiance on this compound. The procedure was performed in an ageing chamber according to the ISO 5630-3:1996 and 11341:2004 standards. The methods were applied to samples from manuscripts preserved in the Historic Archive of the University of Granada and the Royal Chancellery Archive of Granada (Spain). (C) 2013 Elsevier B.V. All rights reserved.", "journal": "MICROCHEMICAL JOURNAL", "category": "Chemistry, Analytical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000327552500018", "keywords": "Electrical winding; lumped parameter (LP); parameter estimation; thermal material data; thermal modeling", "title": "Estimation of Equivalent Thermal Parameters of Impregnated Electrical Windings", "abstract": "It is common practice to represent a composite electrical winding as an equivalent lumped anisotropic material as this greatly simplifies a thermal model and reduces computation times. Existing techniques for estimating the bulk thermal properties of such composite materials use either analytical, numerical, or experimental approaches; however, these methods exhibit a number of drawbacks and limitations regarding their applicability. In this paper, a numerical thermal conductivity and analytical specific heat capacity estimation technique is proposed. The method is validated experimentally against three winding samples with differing configuration. A procedure is presented which enables bulk thermal properties to be estimated with a minimal need for experimental measurement, thereby accelerating the thermal modeling process. The proposed procedure is illustrated by the modeling of three coil exemplars with differing windings. Experimental thermal transients obtained by dc test of the coils show close agreement with a lumped-parameter thermal model utilizing estimated material data.", "journal": "IEEE TRANSACTIONS ON INDUSTRY APPLICATIONS", "category": "Engineering, Multidisciplinary; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000341622700012", "keywords": "Material constitutive; Fracture; Damage; Plane strain; Metal cutting; Triaxiality", "title": "A spurious jump in the satellite record: has Antarctic sea ice expansion been overestimated?", "abstract": "Recent estimates indicate that the Antarctic sea ice cover is expanding at a statistically significant rate with a magnitude one-third as large as the rapid rate of sea ice retreat in the Arctic. However, during the mid-2000s, with several fewer years in the observational record, the trend in Antarctic sea ice extent was reported to be considerably smaller and statistically indistinguishable from zero. Here, we show that much of the increase in the reported trend occurred due to the previously undocumented effect of a change in the way the satellite sea ice observations are processed for the widely used Bootstrap algorithm data set, rather than a physical increase in the rate of ice advance. Specifically, we find that a change in the intercalibration across a 1991 sensor transition when the data set was reprocessed in 2007 caused a substantial change in the long-term trend. Although our analysis does not definitively identify whether this change introduced an error or removed one, the resulting difference in the trends suggests that a substantial error exists in either the current data set or the version that was used prior to the mid-2000s, and numerous studies that have relied on these observations should be reexamined to determine the sensitivity of their results to this change in the data set. Furthermore, a number of recent studies have investigated physical mechanisms for the observed expansion of the Antarctic sea ice cover. The results of this analysis raise the possibility that much of this expansion may be a spurious artifact of an error in the processing of the satellite observations.", "journal": "CRYOSPHERE", "category": "Geography, Physical; Geosciences, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000336052700028", "keywords": "Nonsmooth optimization; Sequential quadratic programming; Gradient sampling; Feasible algorithm", "title": "Online Dynamic Event Region Detection Using Distributed Sensor Networks", "abstract": "Event region detection refers to the process of detecting regions with distinguishable characteristics in an environment, and it can find a broad range of applications from environmental monitoring to system health management. The problem of online dynamic event region detection is studied here. The spatiotemporal relationship of the evolving event regions is assumed and modeled by dynamic Markov random fields. Observations are collected from a network of sensors distributed in the field. To provide detection results at each time step, a distributed event region tracking algorithm is proposed. The system dynamics and information collected from neighbors are used to predict the underlying hypothesis at each sensor node and its local observation is used for update. Mean field approximation is adopted in the algorithm for tractability. The performance of the proposed algorithm is analyzed both theoretically and through simulations. By comparing with static event region detection algorithms and a centralized algorithm (with certain approximation), we demonstrate the effectiveness and efficiency of the proposed algorithm, especially its robustness in low signal-to-noise ratio (SNR) situations.", "journal": "IEEE TRANSACTIONS ON AEROSPACE AND ELECTRONIC SYSTEMS", "category": "Engineering, Aerospace; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000337393300001", "keywords": "stock volatility; stroke; time-series analysis", "title": "Forest Phenology Dynamics and Its Responses to Meteorological Variations in Northeast China", "abstract": "Based on time series of Moderate Resolution Imaging Spectroradiometer (MODIS) Enhanced Vegetation Index (EVI) data (2000-2009), we extracted forest phenological variables in Northeast China using a threshold-based method, which included the start of the growing season (SOS), end of the growing season (EOS), and length of the growing season (LOS). The spatial variation of phenological trends was analyzed using the linear regression method. In Northeast China, SOS was delayed at the rate of <1.5 days per year. The delay trend of EOS was well distributed in the entire region with almost the same rates. LOS increased slightly. The analysis of the relationship between forest phenology and meteorological variations shows that SOS was mainly affected by spring temperature, whereas SOS had a negative relationship with precipitation in the warm-temperate deciduous broadleaf forest region. The EOS in temperate steppe region was affected by temperature and precipitation in August, whereas the others were significantly affected by temperature. Because of the increased temperature in spring, the LOS of the temperate steppe region and temperate mixed forest region increased, and the LOS was positively correlated with the mean temperature of summer in the cool-temperate needleleaf forest region.", "journal": "ADVANCES IN METEOROLOGY", "category": "Meteorology & Atmospheric Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000322203200006", "keywords": "advection-dominated diffusion equations; error estimate; finite volume element method; modified method of characteristics; two grid", "title": "A two-grid characteristic finite volume element method for semilinear advection-dominated diffusion equations", "abstract": "A two-grid finite volume element method, combined with the modified method of characteristics, is presented and analyzed for semilinear time-dependent advection-dominated diffusion equations in two space dimensions. h) is reduced to the solution of two small (one linear and one nonlinear) systems on the coarse-grid space (with grid size H) and a linear system on the fine-grid space. An optimal error estimate in H-1 -norm is obtained for the two-grid method. It shows that the two-grid method achieves asymptotically optimal approximation, as long as the mesh sizes satisfy h = O(H-2). Numerical example is presented to validate the usefulness and efficiency of the method. (c) 2013 Wiley Periodicals, Inc.", "journal": "NUMERICAL METHODS FOR PARTIAL DIFFERENTIAL EQUATIONS", "category": "Mathematics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000332680500002", "keywords": "Reprogramming; wireless sensor networks", "title": "An Efficient Differencing Algorithm for Reprogramming Wireless Sensor Networks", "abstract": "Wireless reprogramming is a crucial technique for managing large-scale wireless sensor networks (WSNs). It is, however, energy intensive to disseminate the code to enable reprogramming. Incremental reprogramming is a promising approach to reduce the dissemination cost. In incremental reprogramming, only the delta between the new code and the old code needs to be disseminated, resulting much less energy consumption. The differencing algorithm plays a key role in incremental reprogramming. It takes inputs of two successive versions of codes and generates a small delta script for dissemination. Existing incremental algorithms have several limitations. First, they do not ensure the smallest delta size for dissemination. Second, some of them may incur a large overhead in terms of execution time and memory consumption. To address these issues, we propose DASA, an efficient differencing algorithm based on suffix array. DASA performs byte-level comparison and ensure the optimal result in terms of the delta size. Moreover, DASA has a low execution overhead. The time complexity and space complexity of DASA are 0 (11 log a) and 0(n), respectively. To the best of our knowledge, DASA is the optimal algorithm with the lowest time and space complexity for reprogramming WSNs.", "journal": "AD HOC & SENSOR WIRELESS NETWORKS", "category": "Computer Science, Information Systems; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000329016600011", "keywords": "Unitary Hessenberg matrix; Isometric Arnoldi algorithm; Toeplitz matrix", "title": "A generalized unitary Hessenberg matrix", "abstract": "This paper describes the use of a generalized isometric Arnoldi algorithm to reduce a unitary matrix, via unitary similarity, to a product of elementary reflectors and permutations. The computation is analogous to the reduction of a unitary matrix to a unitary Hessenberg matrix using the isometric Arnoldi algorithm. In the case in which A is a shift matrix, the reduction provides a novel recurrence for the factor R in the Q R factorization of a Toeplitz-like matrix. (C) 2013 Elsevier Inc. All rights reserved.", "journal": "LINEAR ALGEBRA AND ITS APPLICATIONS", "category": "Mathematics, Applied; Mathematics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000332036900001", "keywords": "Data integration; data processing; motion analysis; pattern recognition", "title": "Flagellin A Toll-Like Receptor 5 Agonist as an Adjuvant in Chicken Vaccines", "abstract": "Chicken raised under commercial conditions are vulnerable to environmental exposure to a number of pathogens. Therefore, regular vaccination of the flock is an absolute requirement to prevent the occurrence of infectious diseases. To combat infectious diseases, vaccines require inclusion of effective adjuvants that promote enhanced protection and do not cause any undesired adverse reaction when administered to birds along with the vaccine. With this perspective in mind, there is an increased need for effective better vaccine adjuvants. Efforts are being made to enhance vaccine efficacy by the use of suitable adjuvants, particularly Toll-like receptor (TLR)-based adjuvants. TLRs are among the types of pattern recognition receptors (PRRs) that recognize conserved pathogen molecules. A number of studies have documented the effectiveness of flagellin as an adjuvant as well as its ability to promote cytokine production by a range of innate immune cells. This minireview summarizes our current understanding of flagellin action, its role in inducing cytokine response in chicken cells, and the potential use of flagellin as well as its combination with other TLR ligands as an adjuvant in chicken vaccines.", "journal": "CLINICAL AND VACCINE IMMUNOLOGY", "category": "Immunology; Infectious Diseases; Microbiology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000341603900035", "keywords": "Optimal control; Nonlinear systems; Fermentation process; Gradient method optimization; Antibiotics production", "title": "CycloTRACK (v1.0) - tracking winter extratropical cyclones based on relative vorticity: sensitivity to data filtering and other relevant parameters", "abstract": "In this study we present a new cyclone identification and tracking algorithm, cycloTRACK. The algorithm describes an iterative process. At each time step it identifies all potential cyclone centers, defined as relative vorticity maxima embedded in smoothed enclosed contours of at least 3x10(-5) s(-1) at the atmospheric level of 850 hPa. Next, the algorithm finds all the potential cyclone paths by linking the cyclone centers at consecutive time steps and selects the most probable track based on the minimization of a cost function. The cost function is based on the average differences of relative vorticity between consecutive track points, weighted by their distance. Last, for each cyclone, the algorithm identifies \"an effective area\" for which different physical diagnostics are measured, such as the minimum sea level pressure and the maximum wind speed. The algorithm was applied to the ERA-Interim reanalyses for tracking the Northern Hemisphere extratropical cyclones of winters from 1989 until 2009, and we assessed its sensitivity for the several free parameters used to perform the tracking.", "journal": "GEOSCIENTIFIC MODEL DEVELOPMENT", "category": "Geosciences, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000328939500011", "keywords": "Canopy phenology; land surface phenology (LSP); scatterometry; start of season", "title": "Assessing the Phenology of Southern Tropical Africa: A Comparison of Hemispherical Photography, Scatterometry, and Optical/NIR Remote Sensing", "abstract": "The seasonal cycle of tree leaf display in the savannas and woodlands of the seasonally dry tropics is complex, and robust observations are required to illuminate the processes at play. Here, we evaluate three types of data for this purpose, comparing scatterometry (QuikSCAT sigma(0)) and optical/near-infrared MODIS EVI remotely sensed data against field observations. At a site in Mozambique, the seasonal cycles from both space-borne sensors are in close agreement with each other and with estimates of tree plant area index derived from hemispherical photography (r > 0.88). This agreement results in similar estimates of the start of the growing season across different data types (range 13 days). Ku-band scatterometry may therefore be a useful complement to vegetation indices such as EVI for estimating the start of the growing season for trees in tropical woodlands. More broadly, across southern tropical Africa there is close agreement between scatterometry and EVI time series in woody ecosystems with > 25% tree cover, but in areas of < 25% tree cover, the two time series diverge and produce markedly different start of season (SoS) dates (difference > 50 days). This is due to increases in sigma(0) during the dry season, not matched by increase in EVI. The reasons for these increases are not obvious, but might relate to soil moisture, flowering, fruiting, or grass dynamics. Further observations and modeling of this phenomenon is warranted to understand the causes of these dry season changes in sigma(0). Finally, three different definitions of the SoS were examined and found to produce only small differences in estimated dates, across all types of data.", "journal": "IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING", "category": "Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000329440300002", "keywords": "substructure identification; multiple damage identification; variable crack location; transient power flow; structural optimization; time domain", "title": "Damage identification using combined transient power flow balance and acceleration matching technique", "abstract": "This paper presents a multiobjective optimization formulation to detect and quantify crack damage in beam structures at variable locations. It is implemented at the substructure level where the concept is to balance the instantaneous power flow by equating the input power to the dissipated power and the time rate of change of kinetic and strain energies and power transferred to adjacent substructures. This imbalance is reduced to zero to identify the structural parameters or damages. For improved results, the power balance method is combined with conventional acceleration matching method where the objective is to minimize the deviation between measured and estimated accelerationsno additional sensors are required to incorporate the extra power flow balance criteria. Numerical simulations are performed for a lumped mass system, a planar truss structure, and a cantilever beam of 20 elements with multiple damages to evaluate the accuracy of the proposed method. Effects of noise are also taken into account by contaminating the measured responses with 3%, 5%, and 10% Gaussian noise. The particle swarm optimization is used as the optimization algorithm, and normalized fitness functions are defined for both power flow and acceleration components with weighted aggregation multiobjective optimization technique. The effects of various weighting factors for the combined objective function are also studied. The results demonstrate that improvement in accuracy of damage detection is achieved by the combined method, when compared with previous acceleration only matching method as well as other methods. Copyright (c) 2013 John Wiley & Sons, Ltd.", "journal": "STRUCTURAL CONTROL & HEALTH MONITORING", "category": "Construction & Building Technology; Engineering, Civil; Instruments & Instrumentation", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000323498000008", "keywords": "Bacterium-inspired algorithm; environmental monitoring; optimal coverage; simplistic agents; visual imaging", "title": "Visual Imaging of Invisible Hazardous Substances Using Bacterial Inspiration", "abstract": "Providing a visual image of a hazardous substance such as nerve gas or nuclear radiation using multiple robotic agents could be very useful particularly when the substance is invisible. Such visual representation could show where the hazardous substance concentration is highest through the deployment of a higher density of robotic agents to that area enabling humans to avoid such areas. We present an algorithm that is capable of doing the aforementioned with very minimal cost when compared with other techniques such as Voronoi partition methods. Using a mathematical proof, we show that the algorithm would always converge to the distribution of a spatial quantity under investigation. The mathematical model of the bacterium as developed by Berg and Brown is used in this paper, and through simulations and physical experiments, we show that a controller based upon the model is capable of being used to visually represent an invisible spatial hazardous substance using simplistic agents with the future possibility of the same algorithm being used to track a rapidly changing spatiotemporal substance. We believe that the algorithm has this potential because of its low communication and computational needs.", "journal": "IEEE TRANSACTIONS ON SYSTEMS MAN CYBERNETICS-SYSTEMS", "category": "Automation & Control Systems; Computer Science, Cybernetics", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000327791600003", "keywords": "readmissions; quality indicators; hospital quality", "title": "Limitations of using same-hospital readmission metrics", "abstract": "Objective. To quantify the limitations associated with restricting readmission metrics to same-hospital only readmission. Design. Using 2000-2009 California Office of Statewide Health Planning and Development Patient Discharge Data Nonpublic file, we identified the proportion of 7-, 15-and 30-day readmissions occurring to the same hospital as the initial admission using All-cause Readmission (ACR) and 3M Corporation Potentially Preventable Readmissions (PPR) Metric. We examined the correlation between performance using same and different hospital readmission, the percent of hospitals remaining in the extreme deciles when utilizing different metrics, agreement in identifying outliers and differences in longitudinal performance. Using logistic regression, we examined the factors associated with admission to the same hospital. Results. 68% of 30-day ACR and 70% of 30-day PPR occurred to the same hospital. Abdominopelvic procedures had higher proportions of same-hospital readmissions (87.4-88.9%), cardiac surgery had lower (72.5-74.9%) and medical DRGs were lower than surgical DRGs (67.1 vs. 71.1%). Correlation and agreement in identifying high-and low-performing hospitals was weak to moderate, except for 7-day metrics where agreement was stronger (r = 0.23-0.80, Kappa = 0.38-0.76). Agreement for within-hospital significant (P < 0.05) longitudinal change was weak (Kappa = 0.05-0.11). Beyond all patient refined-diagnostic related groups, payer was the most predictive factor with Medicare and MediCal patients having a higher likelihood of same-hospital readmission (OR 1.62, 1.73). Conclusions. Same-hospital readmission metrics are limited for all tested applications. Caution should be used when conducting research, quality improvement or comparative applications that do not account for readmissions to other hospitals.", "journal": "INTERNATIONAL JOURNAL FOR QUALITY IN HEALTH CARE", "category": "Health Care Sciences & Services; Health Policy & Services", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000324974500004", "keywords": "Economic growth; Financial development; Heterogeneity; Nonparametric regression; Nonlinearities", "title": "Who benefits from financial development? New methods, new evidence", "abstract": "This paper takes a fresh look at the impact of financial development on economic growth by using recently developed kernel methods that allow for heterogeneity in partial effects, nonlinearities and endogenous regressors. Our results suggest that while the positive impact of financial development on growth has increased over time, it is also highly nonlinear with more developed nations benefiting while low-income countries do not benefit at all. We also conduct a novel policy analysis that confirms these statistical findings. In sum, this set of results contributes to the ongoing policy debate as to whether low-income nations should scale up financial reforms. (C) 2013 Elsevier B.V. All rights reserved.", "journal": "EUROPEAN ECONOMIC REVIEW", "category": "Economics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000344971200005", "keywords": "visual localization; spatial attention; transcranial electrical stimulation; interhemispheric competition; position perception", "title": "Transcranial direct current stimulation over posterior parietal cortex modulates visuospatial localization", "abstract": "Visual localization is based on the complex interplay of bottom-up and top-down processing. Based on previous work, the posterior parietal cortex (PPC) is assumed to play an essential role in this interplay. In this study, we investigated the causal role of the PPC in visual localization. Specifically, our goal was to determine whether modulation of the PPC via transcranial direct current stimulation (tDCS) could induce visual mislocalization similar to that induced by an exogenous attentional cue (Wright, Morris, & Krekelberg, 2011). We placed one stimulation electrode over the right PPC and the other over the left PPC (dual tDCS) and varied the polarity of the stimulation. We found that this manipulation altered visual localization; this supports the causal involvement of the PPC in visual localization. Notably, mislocalization was more rightward when the cathode was placed over the right PPC than when the anode was placed over the right PPC. This mislocalization was found within a few minutes of stimulation onset, it dissipated during stimulation, but then resurfaced after stimulation offset and lasted for another 10-15 min. On the assumption that excitability is reduced beneath the cathode and increased beneath the anode, these findings support the view that each hemisphere biases processing to the contralateral hemifield and that the balance of activation between the hemispheres contributes to position perception (Kinsbourne, 1977; Szczepanski, Konen, & Kastner, 2010).", "journal": "JOURNAL OF VISION", "category": "Ophthalmology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000332165500001", "keywords": "event-related potentials (ERPs); bimodal; alexithymia; emotion; subclinical", "title": "Subclinical alexithymia modulates early audio-visual perceptive and attentional event-related potentials", "abstract": "Introduction: Previous studies have highlighted the advantage of using audio visual oddball tasks (instead of unimodal ones) in order to electrophysiologically index subclinical behavioral differences. Since alexithymia is highly prevalent in the general population, we investigated whether the use of various bimodal tasks could elicit emotional effects in low- vs. high-alexithymic scorers. Methods: Fifty students (33 females and 17 males) were split into groups based on low and high scores on the Toronto Alexithymia Scale (TAS-20). During event-related potential (ERP) recordings, they were exposed to three kinds of audio visual oddball tasks: neutral-AVN (geometrical forms and bips), animal-AVA-(dog and cock with their respective shouts), or emotional-AVE-(faces and voices) stimuli. In each condition, participants were asked to quickly detect deviant events occurring amongst a train of repeated and frequent matching stimuli (e.g., push a button when a sad face voice pair appeared amongst a train of neutral face voice pairs). P100, N100, and P300 components were analyzed: P100 refers to visual perceptive and attentional processing, N100 to auditory ones, and the P300 relates to response-related stages, involving memory processes. Results: High-alexithymic scorers presented a particular pattern of results when processing the emotional stimulations, reflected in early ERP components by increased P100 and N100 amplitudes in the emotional oddball tasks [P100: F-(2,F- 48) = 20, 319, p < 0.001; N100: F-(2,F- 96) = 8,807, p = 0.001] as compared to the animal or neutral ones. Indeed, regarding the P100, subjects exhibited a higher amplitude in the AVE condition (8.717 mu V), which was significantly different from that observed during the AVN condition (4.382 p < 0.001). For the N100, the highest amplitude was found in the AVE condition (-4.035 mu V) and the lowest was observed in the AVN condition (-2.687 mu V, p = 0.003). However, no effect was found on the later P300 component. Conclusions: Our findings suggest that high-alexithymic scorers require heightened early attentional resources in comparison to low scorers, particularly when confronted with emotional bimodal stimuli.", "journal": "FRONTIERS IN HUMAN NEUROSCIENCE", "category": "Neurosciences; Psychology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000327504100011", "keywords": "Atherosclerosis; cyclophilin-A; cytokine; hypertension", "title": "Cyclophilin-A: a novel biomarker for untreated male essential hypertension", "abstract": "Vascular cytokines, total nitrite, and cyclophilin-A (CyP-A) may be related to the pathogenesis of untreated hypertension. Forty males with normotensive and untreated essential hypertension were recruited in this cytokines survey. Body mass index (BMI), hyperlipidemia, and plasma CyP-A were increased in the hypertensive group (p < 0.05). However, only BMI (p = 0.022) and plasma CyP-A (p = 0.020) were found to be significant contributors to hypertension by multiple regression analysis. CyP-A was also positively correlated with systolic blood pressure (p = 0.029) and diastolic blood pressure (p = 0.047). These findings indicated that plasma CyP-A is a critical molecular biomarker in the early pathogenesis of essential hypertension.", "journal": "BIOMARKERS", "category": "Biotechnology & Applied Microbiology; Toxicology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000325797100006", "keywords": "Homotopy method; Newton's method; scalar homotopy function; singular Jacobian; non-linear algebraic equations", "title": "Solving Nonlinear Problems with Singular Initial Conditions Using A Perturbed Scalar Homotopy Method", "abstract": "In this paper, a novel method, named the perturbed scalar homotopy method, is proposed to solve nonlinear systems with a singular Jacobian matrix. The concept of the proposed perturbed scalar homotopy method roots from the conventional homotopy method but it takes the advantages of converting a vector function to a scalar function by using the square norm of the vector function to conduct a scalar-based homotopy method. Then, a small parameter, which is similar to the perturbation theory, is introduced to the singular systems of nonlinear equations such that the modified singular systems of nonlinear equations become nonsingular and the asymptotic solutions may be found. As a result, the proposed novel method does not need to calculate the inverse of the Jacobian matrix and thus has great numerical stability. In addition, the formulation of the proposed method reveals that this new method is exponentially convergent with the use of the exponential time function. Results obtained show that the proposed novel method can be used to solve singular systems of nonlinear equations with high accuracy as well as the convergence and it may be a better alternative for solving a system of non-linear algebraic equations.", "journal": "INTERNATIONAL JOURNAL OF NONLINEAR SCIENCES AND NUMERICAL SIMULATION", "category": "Engineering, Multidisciplinary; Mathematics, Applied; Mechanics; Physics, Mathematical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000329498100009", "keywords": "Deaf; Cardiovascular health; Education; Income; Health disparities", "title": "Higher educational attainment but not higher income is protective for cardiovascular risk in Deaf American Sign Language (ASL) users", "abstract": "Background: Higher educational attainment and income provide cardiovascular protection in the general population. It is unknown if the same effect is seen among Deaf American Sign Language (ASL) users who face communication barriers in health care settings. Objective: We sought to examine whether educational attainment and/or annual household income were inversely associated with cardiovascular risk in a sample of Deaf ASL users. Methods: This cross-sectional study included 302 Deaf respondents aged 18e88 years from the Deaf Health Survey (2008), an adapted and translated Behavioral Risk Factor Surveillance System (BRFSS) administered in sign language. Associations between the self-reported cardiovascular disease equivalents (CVDE; any of the following: diabetes, myocardial infarction (MI), cerebral vascular attack (CVA), and angina) with educational attainment (<= high school [low education], some college, and >= 4 year college degree [referent]), and annual household income (<$ 25,000, $ 25,000-<$50,000, or >=$50,000 [referent]) were assessed using a multivariate logistic regression adjusting for age, sex, race/ethnicity, and smoking history. Results: Deaf respondents who reported <= high school education were more likely to report the presence of a CVDE (OR = 5 5.76; 95% CI = 2.04-16.31) compared to Deaf respondents who reported having > 4 year college degree after adjustment. However, low-income Deaf individuals (i.e., household incomes <$25,000) were not more likely to report the presence of a CVDE (OR = 2.24; 95% CI = 0.76-6.68) compared to high-income Deaf respondents after adjustment. Conclusion: Low educational attainment was associated with higher likelihood of reported cardiovascular equivalents among Deaf individuals. Higher income did not appear to provide a cardiovascular protective effect for Deaf respondents. (C) 2014 Elsevier Inc. All rights reserved.", "journal": "DISABILITY AND HEALTH JOURNAL", "category": "Health Care Sciences & Services; Health Policy & Services; Public, Environmental & Occupational Health; Rehabilitation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000329682000006", "keywords": "Fast Field-Cycling MRI; Two-point method algorithm; Data processing; Dispersion curve", "title": "Rapid multi-field T-1 estimation algorithm for Fast Field-Cycling MRI", "abstract": "Fast Field-Cycling MRI (FFC-MRI) is an emerging MRI technique that allows the main magnetic field to vary, allowing probing T-1 at various magnetic field strengths. This technique offers promising possibilities but requires long scan times to improve the signal-to-noise ratio. This paper presents an algorithm derived from the two-point method proposed by Edelstein that can estimate T-1 using only one image per field, thereby shortening the scan time by a factor of nearly two, taking advantage of the fact that the equilibrium magnetisation is proportional to the magnetic field strength. Therefore the equilibrium magnetisation only needs measuring once, then T-1 can be found from inversion recovery experiments using the Bloch equations. The precision and accuracy of the algorithm are estimated using both simulated and experimental data, by Monte-Carlo simulations and by comparison with standard techniques on a phantom. The results are acceptable but usage is limited to the case where variations of the main magnetic field are fast compared with T-1 and where the dispersion curve is relatively linear. The speed-up of T-1-dispersion measurements resulting from the new method is likely to make FFC-MRI more acceptable when it is applied in the clinic. (C) 2013 Elsevier Inc. All rights reserved.", "journal": "JOURNAL OF MAGNETIC RESONANCE", "category": "Biochemical Research Methods; Physics, Atomic, Molecular & Chemical; Spectroscopy", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000330434600015", "keywords": "Modified teaching-learning-based optimization; Multi objective; Automatic voltage regulator; Pareto method", "title": "A new multi objective optimization approach in distribution systems", "abstract": "This paper presents a multi objective optimal location of AVRs in distribution systems at the presence of distributed generators based on modified teaching-learning-based optimization (MTLBO) algorithm. In the proposed MTLBO algorithm, teacher and learner phases are modified. The proposed objective functions are energy generation costs, electrical energy losses and the voltage deviations. The proposed algorithm utilizes several teachers and considers the teachers as an external repository to save found Pareto optimal solutions during the search process. Since the objective functions are not the same, a fuzzy clustering method is used to control the size of the repository. The proposed technique allows the decision maker to select one of the Pareto optimal solutions (by trade-off) for different applications. The performance of the suggested algorithm on a 70-bus distribution network in comparison with other evolutionary methods such as GA, PSO and TLBO, is extraordinary.", "journal": "OPTIMIZATION LETTERS", "category": "Operations Research & Management Science; Mathematics, Applied", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000344435300003", "keywords": "Blue intensity; Maximum latewood density; Scots pine; Dendroclimatology; Image analysis; Scotland", "title": "Blue intensity for dendroclimatology: Should we have the blues? Experiments from Scotland", "abstract": "Blue intensity (BI) has the potential to provide information on past summer temperatures of a similar quality to maximum latewood density (MXD), but at a substantially reduced cost. This paper provides a methodological guide to the generation of BI data using a new and affordable BI measurement system; CooRecorder. Focussing on four sites in the Scottish Highlands from a wider network of 42 sites developed for the Scottish Pine Project, BI and MXD data from Scots pine (Pious sylvestris L.) were used to facilitate a direct comparison between these parameters. A series of experiments aimed at identifying and addressing the limitations of BI suggest that while some potential limitations exist, these can be minimised by adhering to appropriate BI generation protocols. The comparison of BI data produced using different resin-extraction methods (acetone vs. ethanol) and measurement systems (CooRecorder vs. WinDendro) indicates that comparable results can be achieved. Using samples from the same trees, a comparison of both BI and MXD with instrumental climate data revealed that overall, BI performs as well as, if not better than, MXD in reconstructing past summer temperatures (BI r(2) = 0.38-0.46; MXD r(2) = 0.34-0.35). Although reconstructions developed using BI and MXD data appeared equally robust, BI chronologies were more sensitive to the choice of detrending method due to differences in the relative trends of non-detrended raw BI and MXD data. This observation suggests that the heartwood-sapwood colour difference is not entirely removed using either acetone or ethanol chemical treatment, which may ultimately pose a potential limitation for extracting centennial and longer timescale information when using BI data from tree species that exhibit a distinct heartwood-sapwood colour difference. Additional research is required in order to develop new methods to overcome this potential limitation. However, the ease with which BI data can be produced should help justify and recognise the role of this parameter as a potential alternative to MXD, particularly when MXD generation may be impractical or unfeasible for financial or other reasons. (C) 2014 Elsevier GmbH. All rights reserved.", "journal": "DENDROCHRONOLOGIA", "category": "Forestry; Geography, Physical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000320347600028", "keywords": "Mean-square error; Normalized subband adaptive filter; Regularization", "title": "Steady-state mean-square error analysis of regularized normalized subband adaptive filters", "abstract": "The normalized subband adaptive filter (NSAF) has faster convergence rate than the normalized least-mean-square (NLMS) algorithm for colored input signals. Regularization of the NSAF is of importance in practical applications. In this paper, we analyze the steady-state mean-square error (MSE) of regularized NSAFs. The analysis is carried out based on the derivation of a variable regularization matrix NSAF (VRM-NSAF). Theoretical expressions for the steady-state MSE of two regularized NSAFs are derived under some assumptions. Simulation results are given to support the theoretical analysis. (c) 2013 Elsevier B.V. All rights reserved.", "journal": "SIGNAL PROCESSING", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000345826100071", "keywords": "Coral; South China Sea; Trace metals; Chemometrics; Spatiotemporal change model", "title": "Specific features of electronic structures and nonlinear optical susceptibilities of superhard metallic diamond-like t-B2CN compound", "abstract": "The geometry of diamond-like t-B2CN was relaxed by minimizing the forces acting on each atom using an all-electron full potential linearized augmented plane wave within general gradient approximation (PBEGGA). Using the relaxed geometry, the electronic band structure's dispersion, total and angular momentum resolved projected density of states, anthropomorphic shape of Fermi surface, electronic charge density distribution, and linear and nonlinear optical responses were performed within the local density approximation (CA-LDA), PBE-GGA, Engel-Vosko generalized gradient approximation (EV-GGA) and the modified Becke-Johnson potential (mBJ). Calculations showed that t-B2CN is metallic with two bands overlapping around the Fermi level. The density of states at the Fermi level N(E-F) and the electronic specific heat coefficient (g) were obtained. The electronic charge density showed that there are only two type of bonds (B-C and B-N). The Fermi surface is formed by two bands; the shape of the Fermi surface consists of empty areas representing holes and shaded areas corresponding to electrons. The linear optical response confirmed that there exists a lossless region between 5.0 eV and 7.5 eV, and the existence of a considerable anisotropy favors enhanced phase matching conditions for the second harmonic generation (SHG). The nonlinear optical susceptibilities exhibited that X-113(omega)((2)) is the dominant component, with values of about 5.5 pm V-1 at static limit and 8.8 pm V-1 at lambda = 1064 nm. This suggests that the t-B2CN single crystal is a promising nonlinear crystal in comparison with the well-known KTiOPO4 nonlinear optical single crystal.", "journal": "RSC ADVANCES", "category": "Chemistry, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000332614400003", "keywords": "Acoustic modeling; subspace Gaussian mixture model; cross-lingual speech recognition; regularization; adaptation", "title": "Cross-Lingual Subspace Gaussian Mixture Models for Low-Resource Speech Recognition", "abstract": "This paper studies cross-lingual acoustic modeling in the context of subspace Gaussian mixture models (SGMMs). SGMMs factorize the acoustic model parameters into a set that is globally shared between all the states of a hidden Markov model (HMM) and another that is specific to the HMM states. We demonstrate that the SGMM global parameters are transferable between languages, particularly when the parameters are trained multilingually. As a result, acoustic models may be trained using limited amounts of transcribed audio by borrowing the SGMM global parameters from one or more source languages, and only training the state-specific parameters on the target language audio. Model regularization using l(1)-norm penalty is shown to be particularly effective at avoiding overtraining and leading to lower word error rates. We investigate maximum a posteriori (MAP) adaptation of subspace parameters in order to reduce the mismatch between the SGMM global parameters of the source and target languages. In addition, monolingual and cross-lingual speaker adaptive training is used to reduce the model variance introduced by speakers. We have systematically evaluated these techniques by experiments on the GlobalPhone corpus.", "journal": "IEEE-ACM TRANSACTIONS ON AUDIO SPEECH AND LANGUAGE PROCESSING", "category": "Acoustics; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000340145400007", "keywords": "current account sustainability; unit root tests; structural breaks; nonlinearity", "title": "Current account sustainability in Latin America", "abstract": "This paper examines the sustainability of the current account deficit in eighteen Latin American countries through the analysis of the stationarity properties of the current account balance. First, we apply traditional unit root tests and consider the possibility of structural breaks. Second, since the current account may have a nonlinear behaviour, we test for linearity in the data and analyse current account stationarity by means of a recently developed nonlinear unit root test. Results from linear and nonlinear unit root tests show that current account sustainability is supported for the majority of Latin American countries with the exception of Argentina, Brazil, Chile and Paraguay. For the Dominican Republic, Honduras, Mexico, Panama, Peru, Uruguay and Venezuela the current account dynamics are best described by a stationary linear model, and by a stationary linear model with a mean shift in years 2003, 1982 and 1980 in Bolivia, Costa Rica and Nicaragua, respectively. In the case of Colombia, Ecuador, El Salvador and Guatemala, results show that the current account is best described by a mean-reverting nonlinear process.", "journal": "JOURNAL OF INTERNATIONAL TRADE & ECONOMIC DEVELOPMENT", "category": "Economics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000334425200044", "keywords": "Congestion control; Exponential RED; Hopf bifurcation; State feedback; Bifurcation control", "title": "State feedback control at Hopf bifurcation in an exponential RED algorithm model", "abstract": "In this paper, we show that a state feedback method, which has successfully been used to control unstable steady states or periodic orbits, provides a tool to control the Hopf bifurcation for a novel congestion control model, i.e., the exponential RED algorithm with a single link and single source. We choose the gain parameter as the bifurcation parameter. Without control, the bifurcation will occur early; meanwhile, the model can maintain a stationary sending rate only in a certain domain of the gain parameter. However, outside of this domain the model still possesses a stable sending rate that can be guaranteed by the state feedback control, and the onset of the undesirable Hopf bifurcation is postponed. Numerical simulations are given to justify the validity of the state feedback controller in the bifurcation control.", "journal": "NONLINEAR DYNAMICS", "category": "Engineering, Mechanical; Mechanics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000336350000011", "keywords": "masonry; blast shock; contact algorithm; flexural bond strength; scaled distance", "title": "Numerical Modelling of Infilled Clay Brick Masonry Under Blast Loading", "abstract": "Numerical modeling and simulation of clay brick masonry infilled in a reinforced concrete frame (RC frame) subjected to blast loading has been presented in this paper. The pressure loading generated in blast shock has been applied on the masonry and the reinforced concrete frame and time history analysis has been made using ABAQUS finite element software package. The slip and separation at the joints of RC frame and masonry occurring during blast loading due to large difference in their stiffness has been modeled using contact algorithm. The study of the infilled brick masonry has been carried out with elasto-plastic strain hardening model using Mohr-Coulomb yield and failure criterion and contact algorithm for modeling contact behaviour at the interface of masonry wall and RC frame. The non-linearity in RC beam/column has been modelled using concrete damaged plasticity model. The parameters for non-linear finite element modeling of masonry have been experimentally determined. In order to gain confidence in the analysis, the proposed constitutive models have been validated with available experimental results on infilled masonry walls. The parametric study has been made for surface blast of 100 kg TNT at a detonation distance 20, 30 and 40 m for 340 mm and 235 mm thick masonry walls with three grades of mortar infilled in a RC frame. The effect of variation of contact friction between mortar and RC elements on the behaviour of masonry walls has also been studied.", "journal": "ADVANCES IN STRUCTURAL ENGINEERING", "category": "Construction & Building Technology; Engineering, Civil", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000330841500014", "keywords": "Assistive mechanism; Manipulability inclusive principle; Design optimization; Parallel mechanism; Direct kinematical Jacobian", "title": "Manipulability inclusive principle for hip joint assistive mechanism design optimization", "abstract": "The purpose of the design optimization for assistive mechanism is ensuring that assistive mechanism is able to satisfy assistive feasibility and realizes better assistive effect in the whole expected workspace. For this purpose, based on manipulability comparison between assisted limb and slave-active assistive mechanism, manipulability inclusive principle (MIP) is proposed as an effective and simple method to consider assistive feasibility and assistive effect in terms of kinematics. Then MIP evaluation criterions are proposed to evaluate assistive feasibility and assistive effect in mathematical ways. Concretely, weak inclusive judgment algorithm and strong inclusive judgment algorithm are proposed for judging inclusive cases to evaluate assistive feasibility, and evaluation criterion proposed for evaluating assistive effect is considered in terms of assistive efficiency, assistive ability, and assistive isotropy. The application on human hip joint parallel assistive mechanism shows that design optimization based on MIP can effectively optimize the parallel assistive mechanism to realize better assistance.", "journal": "INTERNATIONAL JOURNAL OF ADVANCED MANUFACTURING TECHNOLOGY", "category": "Automation & Control Systems; Engineering, Manufacturing", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000323290800003", "keywords": "Hypersonic flow; Supersonic combustion; Radical farming; Autoignition scramjet Planar laser-induced fluorescence imaging; Plasma-assisted combustion; Laser-induced ignition; Laser-induced plasma (LIP); Laser spark ignition; Enthalpy-matching", "title": "Laser ignition of hypersonic air-hydrogen flow", "abstract": "An experimental investigation of the behaviour of laser-induced ignition in a hypersonic air-hydrogen flow is presented. A compression-ramp model with port-hole injection, fuelled with hydrogen gas, is used in the study. The experiments were conducted in the T-ADFA shock tunnel using a flow condition with a specific total enthalpy of 2.5 MJ/kg and a freestream velocity of 2 km/s. This study is the first comprehensive laser spark study in a hypersonic flow and demonstrates that laser-induced ignition at the fuel-injection site can be effective in terms of hydroxyl production. A semi-empirical method to estimate the conditions in the laser-heated gas kernel is presented in the paper. This method uses blast-wave theory together with an expansion-wave model to estimate the laser-heated gas conditions. The spatially averaged conditions found with this approach are matched to enthalpy curves generated using a standard chemical equilibrium code (NASA CEA). This allows us to account for differences that are introduced due to the idealised description of the blast wave, the isentropic expansion wave as well as thermochemical effects.", "journal": "SHOCK WAVES", "category": "Mechanics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000327172500012", "keywords": "Reversed-phase liquid chromatography; Cholesterol column; Lipophilicity; Biological descriptors; Principal component analysis", "title": "Reversed-phase liquid chromatography with octadecylsilyl, immobilized artificial membrane and cholesterol columns in correlation studies with in silico biological descriptors of newly synthesized antiproliferative and analgesic active compounds", "abstract": "Reversed-phase liquid chromatography (RPLC) with different stationary phases, i.e., octadecylsilyl, immobilized artificial membrane and immobilized cholesterol, was used to study lipophilicity of 56 newly-designed 7,8-dihydroimidazo[2,1-c][1,2,4]triazin-4(6H)-ones and 2,6,7,8-tetrahydroimidazo[2,1-c][1,2,4]triazine-3,4-diones with potential anti-proliferative, anti-metastatic and analgesic activities. Extrapolated retention parameters that correspond to pure buffer as the mobile phase, i.e., log k(w) values are used as chromatographic lipophilicities. The lipophilic properties of compounds also are characterized by computed log P values and basic pharmacokinetic descriptors calculated in silico with the use of ACD/Percepta software according to Abraham's linear solvation energy relationship. Chromatographic and partitioning parameters are compared with biological descriptors using principal component analysis (PCA), and similarities and dissimilarities between variables and compounds are described. Highly significant, predictive relationships between biological descriptors and chromatographic parameters are obtained. Reversed parabolic relationships, which have very good statistical quality between various biological descriptors, i.e., log K-sc, log K-p, log BB, and logK(hsa), and the log k(w) values, indicate the advantages of a cholesterol column in comparison with immobilized artificial membrane and octadecylsilyl stationary phase. (C) 2013 Elsevier B.V. All rights reserved.", "journal": "JOURNAL OF CHROMATOGRAPHY A", "category": "Biochemical Research Methods; Chemistry, Analytical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000327369100011", "keywords": "Magnetic resonance imaging; Intrinsic connectivity network; Sleep; Pediatric; Biomarker", "title": "TORC1-Dependent Epilepsy Caused by Acute Biallelic Tsc1 Deletion in Adult Mice", "abstract": "ObjectiveSeizure development in tuberous sclerosis complex (TSC) correlates with the presence of specific lesions called cortical tubers. Moreover, heterozygous TSC animal models do not show gross brain pathology and are seizure-free, suggesting that such pathology is a prerequisite for the development of epilepsy. However, cells within TSC lesions show increased activity of the target of rapamycin complex 1 (TORC1) pathway, and recent studies have implicated this pathway in non-TSC-related animal models of epilepsy and neuronal excitability. These findings imply a direct role for TORC1 in epilepsy. Here, we investigate the effect of increased TORC1 signaling induced by acute biallelic deletion of Tsc1 in healthy adult mice. MethodsBiallelic Tsc1 gene deletion was induced in adult Tsc1 heterozygous and wild-type mice. Seizures were monitored by electroencephalographic and video recordings. Molecular and cellular changes were investigated by Western blot analysis, immunohistochemistry, and electrophysiology. ResultsMice developed epilepsy a few days after biallelic Tsc1 deletion. Acute gene deletion was not accompanied by any obvious histological changes, but resulted in activation of the TORC1 pathway, enhanced neuronal excitability, and a decreased threshold for protein-synthesis-dependent long-term potentiation preceding the onset of seizures. Rapamycin treatment after seizure onset reduced TORC1 activity and fully abolished the seizures. InterpretationOur data indicate a direct role for TORC1 signaling in epilepsy development, even in the absence of major brain pathology. This suggests that TORC1 is a promising target for treating seizures not only in TSC but also in other forms of epilepsy that result from increased TORC1 activation. Ann Neurol 2013;74:569-579", "journal": "ANNALS OF NEUROLOGY", "category": "Clinical Neurology; Neurosciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000334339000041", "keywords": "wireless sensor networks; WSNs; multiple-objective ant colony optimisation; MOACO; routing; trust; bio-inspired computation; biological-inspired computation; MARFWSN; ant colony optimisation; ACO; insider attacks", "title": "Lifespan Analyses of Forest Raptor Nests: Patterns of Creation, Persistence and Reuse", "abstract": "Structural elements for breeding such as nests are key resources for the conservation of bird populations. This is especially true when structural elements require a specific and restricted habitat, or if the construction of nests is costly in time and energy. The availability of nesting-platforms is influenced by nest creation and persistence. In a Mediterranean forest in southeastern Spain, nesting-platforms are the only structural element for three forest-dwelling raptor species: booted eagle Aquila pennata, common buzzard Buteo buteo and northern goshawk Accipiter gentilis. From 1998 to 2013, we tracked the fate of 157 nesting-platforms built and reused by these species with the aim of determining the rates of creation and destruction of nesting-platforms, estimating nest persistence by applying two survival analyses, describing the pattern of nest reuse and testing the effects of nest use on breeding success. Nest creation and destruction rates were low (0.14 and 0.05, respectively). Using Kaplan Meier survival estimates and Cox proportional-hazards regression models we found that median nest longevity was 12 years and that this was not significantly affected by nest characteristics, nest-tree dimensions, nest-builder species, or frequency of use of the platform. We also estimated a transition matrix, considering the different stages of nest occupation (vacant or occupied by one of the focal species), to obtain the fundamental matrix and the average life expectancies of nests, which varied from 17.9 to 19.7 years. Eighty six percent of nests were used in at least one breeding attempt, 67.5% were reused and 17.8% were successively occupied by at least two of the study species. The frequency of nest use had no significant effects on the breeding success of any species. We conclude that nesting-platforms constitute an important resource for forest raptors and that their longevity is sufficiently high to allow their reuse in multiple breeding attempts.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000331239000005", "keywords": "catalogs; eclipses; planetary systems; space vehicles", "title": "PLANETARY CANDIDATES OBSERVED BY KEPLER IV: PLANET SAMPLE FROM Q1-Q8 (22 MONTHS)", "abstract": "We provide updates to the Kepler planet candidate sample based upon nearly two years of high-precision photometry (i.e., Q1-Q8). From an initial list of nearly 13,400 threshold crossing events, 480 new host stars are identified from their flux time series as consistent with hosting transiting planets. Potential transit signals are subjected to further analysis using the pixel-level data, which allows background eclipsing binaries to be identified through small image position shifts during transit. We also re-evaluate Kepler Objects of Interest (KOIs) 1-1609, which were identified early in the mission, using substantially more data to test for background false positives and to find additional multiple systems. Combining the new and previous KOI samples, we provide updated parameters for 2738 Kepler planet candidates distributed across 2017 host stars. From the combined Kepler planet candidates, 472 are new from the Q1-Q8 data examined in this study. The new Kepler planet candidates represent similar to 40% of the sample with R-P similar to 1R(circle plus) and represent similar to 40% of the low equilibrium temperature (T-eq < 300 K) sample. We review the known biases in the current sample of Kepler planet candidates relevant to evaluating planet population statistics with the current Kepler planet candidate sample.", "journal": "ASTROPHYSICAL JOURNAL SUPPLEMENT SERIES", "category": "Astronomy & Astrophysics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000332298100001", "keywords": "2(2S+1) formalism; Lorentz Group; Matrix Identities", "title": "Energy- and Spectral-Efficiency Trade-Off in OFDMA-Based Cooperative Cognitive Radio Networks", "abstract": "We study the trade-off between energy efficiency (EE) and spectral efficiency (SE) in cooperative cognitive radio networks (CCRN); joint power and subcarrier allocation scheme is proposed. Resource is assigned to each user in a way which ensures maximizing energy efficiency, maintaining primary and second user quality of service (QoS) requirements. Optimum transmit power of user is got by analysis; validity of theory is verified by simulation, and the proposed algorithm can adaptively allocate resource for CCRN.", "journal": "INTERNATIONAL JOURNAL OF DISTRIBUTED SENSOR NETWORKS", "category": "Computer Science, Information Systems; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000333009600005", "keywords": "benefit-risk assessment; log-rank test; mean frequency function; terminal event; informative censoring", "title": "Analysis of safety data in clinical trials using a recurrent event approach", "abstract": "As an important aspect of the clinical evaluation of an investigational therapy, safety data are routinely collected in clinical trials. To date, the analysis of safety data has largely been limited to descriptive summaries of incidence rates or contingency tables aiming to compare simple rates between treatment arms. Many have argued that this traditional approach failed to take into account important information including severity, onset time, and multiple occurrences of a safety event. In addition, premature treatment discontinuation due to excessive toxicity causes informative censoring and may lead to potential bias in the interpretation of safety events. In this article, we propose a framework to summarize safety data with mean frequency function and compare safety events of interest between treatments with a generalized log-rank test, taking into account the aforementioned characteristics ignored in traditional analysis approaches. In addition, a multivariate generalized log-rank test to compare the overall safety profile of different treatments is proposed. In the proposed method, safety events are considered to follow a recurrent event process with a terminal event for each patient. The terminal event is modeled by a process of two types of competing risks: safety events of interest and other terminal events. Statistical properties of the proposed method are investigated via simulations. An application is presented with data from a phase II oncology trial. Copyright (c) 2014 John Wiley & Sons, Ltd.", "journal": "PHARMACEUTICAL STATISTICS", "category": "Pharmacology & Pharmacy; Statistics & Probability", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000325742500031", "keywords": "Generative design system; Shape grammars; Parametric design; Optimization; Genetic algorithm; Representation", "title": "A general indirect representation for optimization of generative design systems by genetic algorithms: Application to a shape grammar-based design system", "abstract": "Generative design systems coupled with objective functions can be efficiently explored through the use of stochastic optimization algorithms, such as genetic algorithms. The first step in implementing genetic algorithms is to define a representation, that is, the data structure representative of the genotype space and its mathematical relation to the data of the phenotype space the variables of the real problem. This can be a hard task, particularly if the design system contains dependency between variables. This paper presents a general representation, which enables the use of standard variation operators, allows defining both continuous and discrete variables from a single type of gene and is easily adaptable to different problems, with a larger or smaller number of variables. This representation was created to solve the representation problem in the design system for Frank Lloyd Wright's prairie houses, a shape grammar that was converted into a parametric design system. (C) 2013 Elsevier B.V. All rights reserved.", "journal": "AUTOMATION IN CONSTRUCTION", "category": "Construction & Building Technology; Engineering, Civil", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000326613300008", "keywords": "Geometric optimization; Enclosing problems; Computational geometry", "title": "Minimum-area enclosing triangle with a fixed angle", "abstract": "Given a set S of n points in the plane and a fixed angle 0 < omega < pi, we show how to find in O(n log n) time all triangles of minimum area with one angle omega to that enclose S. We prove that in general, the solution cannot be written without cubic roots. We also prove an Omega (n log n) lower bound for this problem in the algebraic computation tree model. If the input is a convex n-gon, our algorithm takes Theta(n) time. (C) 2013 Elsevier B.V. All rights reserved.", "journal": "COMPUTATIONAL GEOMETRY-THEORY AND APPLICATIONS", "category": "Mathematics, Applied; Mathematics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000337496800006", "keywords": "binge drinking; university students; gender; impulsivity; alcohol expectations", "title": "The importance of expectations in the relationship between impulsivity and binge drinking among university students", "abstract": "Alcohol intensive consumption (AIC) in university students has important clinical and social implications that motivate the need to look into the factors that favor its apparition and consolidation. More concretely, this study assesses the role of impulsivity and the associated expectations about consumption, as well as the possible mediation of expectations in the relationship between impulsivity and AIC. Three hundred and three students in the first year at the University Complutense of Madrid that carry out AIC kept a self-record of their consumption, a scale of expectations associated to the ingestion (IECI, 2012), and the BIS-11 scale of impulsiveness. In all cases, both men and women, doubles the grams of alcohol that define an AIC, as well as the frequency in the execution of this behaviour, which increases the probability that these negative consequences come about. No differences were found between men's and women's expectations associated to AIC, nor in their total impulsivity scores. The hierarchy regression analysis shows that expectations do not moderate the relationship between impulsivity and consumption. Both variables influence the independent mode of consumption (grams of ingested alcohol and frequency of ingestion), with a higher weight on expectations from both, men and women, but being significant the input of impulsivity only among males. This justifies the need to plan interventions that address the modification of these expectations, including, in the case of males, the aspects related to impulsivity.", "journal": "ADICCIONES", "category": "Substance Abuse", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000334339000134", "keywords": "fault tolerance; reliability; concurrency control; distributed system; REST; transaction; timestamp; Web services; architectural style", "title": "Automated Quantification Reveals Hyperglycemia Inhibits Endothelial Angiogenic Function", "abstract": "Objective: Diabetes Mellitus (DM) has reached epidemic levels globally. A contributing factor to the development of DM is high blood glucose (hyperglycemia). One complication associated with DM is a decreased angiogenesis. The Matrigel tube formation assay (TFA) is the most widely utilized in vitro assay designed to assess angiogeneic factors and conditions. In spite of the widespread use of Matrigel TFAs, quantification is labor-intensive and subjective, often limiting experiential design and interpretation of results. This study describes the development and validation of an open source software tool for high throughput, morphometric analysis of TFA images and the validation of an in vitro hyperglycemic model of DM. Approach and Results: Endothelial cells mimic angiogenesis when placed onto a Matrigel coated surface by forming tubelike structures. The goal of this study was to develop an open-source software algorithm requiring minimal user input (Pipeline v1.3) to automatically quantify tubular metrics from TFA images. Using Pipeline, the ability of endothelial cells to form tubes was assessed after culture in normal or high glucose for 1 or 2 weeks. A significant decrease in the total tube length and number of branch points was found when comparing groups treated with high glucose for 2 weeks versus normal glucose or 1 week of high glucose. Conclusions: Using Pipeline, it was determined that hyperglycemia inhibits formation of endothelial tubes in vitro. Analysis using Pipeline was more accurate and significantly faster than manual analysis. The Pipeline algorithm was shown to have additional applications, such as detection of retinal vasculature.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000331398300005", "keywords": "Selective image encryption; spatial chaotic system; couple chaotic systems", "title": "A SELECTIVE IMAGE ENCRYPTION BASED ON COUPLE SPATIAL CHAOTIC SYSTEMS", "abstract": "In this paper, we present a selective image encryption system based on couple spatial chaotic systems, the cascade one-dimensional Logistic map and high-dimensional spatial chaotic system has been used to generate the adequate encryption sequence, then the selective gray-level image encryption is implemented with the sequence, which can greatly improve the encryption performance and efficiency. In addition, we also adopt an index array to control the generation of the secret key, a completely different cipher text will be obtained if a pixel's value is altered in the original image, which can resist the differential attack effectively.", "journal": "INTERNATIONAL JOURNAL OF MODERN PHYSICS B", "category": "Physics, Applied; Physics, Condensed Matter; Physics, Mathematical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000341993900003", "keywords": "Satisfaction; Risk; Loyalty; Switching costs; Longitudinal study; Risk perceptions; Remaining contract time; Loyalty intentions; Product usage satisfaction", "title": "Investigating temporal effects of risk perceptions and satisfaction on customer loyalty", "abstract": "Purpose - The purpose of this paper is to examine how product usage satisfaction mediates the link between two types of perceived risk and loyalty intentions, and investigate the three moderating effects (overall satisfaction of provider, switching costs, and remaining contract time) of the product usage satisfaction-loyalty intentions linkage. Design/methodology/approach - Using a longitudinal study, a total of 253 usable responses are collected from time T to time T+1. The paper uses the partial least squares (PLS-Graph 3.0) approach for structural parameters in the proposed model. Findings - The findings show that the temporal effect of performance risk and product usage satisfaction negatively increases, whereas the temporal effect between product usage satisfaction and loyalty intentions decreases over time. While the moderating effect between overall satisfaction of provider and product usage satisfaction strengthens loyalty intentions, the switching costs attenuate loyalty intentions. The proposed model can be used to predict both the change in customer behavior and the moderating effects of overall satisfaction of providers (or switching costs) in the context of smartphone replacement period. Originality/value - This paper makes unique contributions to the literature. First, using the consumption-system approach as a theoretical base, the paper extends the product usage satisfaction-loyalty intentions linkage that can appear as a dynamic spiral during subsequent periods. Second, considering the temporal effects of the proposed relationships, the paper identifies the role of perceived risk that reveals how product usage satisfaction mediates the link between two types of perceived risk and loyalty intentions. Third, the paper emphasizes the importance of moderating effects when considering smartphone usage with a particular brand.", "journal": "MANAGING SERVICE QUALITY", "category": "Management", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000322423200063", "keywords": "Clumsy children; Muscle contraction time; Motor strategy; Sensory inputs; Balance", "title": "Slowed muscle force production and sensory organization deficits contribute to altered postural control strategies in children with developmental coordination disorder", "abstract": "This study aimed to (1) compare the postural control strategies, sensory organization of balance control, and lower limb muscle performance of children with and without developmental coordination disorder (DCD) and (2) determine the association between postural control strategies, sensory organization parameters and knee muscle performance indices among children with DCD. Fifty-eight DCD-affected children and 46 typically developing children participated in the study. Postural control strategies and sensory organization were evaluated with the sensory organization test (SOT). Knee muscle strength and time to produce maximum muscle torque (at 180 degrees/s) were assessed using an isokinetic machine. Analysis of variance was used to compare the outcome variables between groups, and multiple regression analysis was used to examine the relationships between postural control strategies, sensory organization parameters, and isokinetic indices in children with DCD. The DCD group had significantly lower strategy scores (SOT conditions 5 and 6), lower visual and vestibular ratios, and took a longer time to reach peak torque in the knee flexor muscles than the control group (p > 0.05). After accounting for age, sex, and body mass index, the vestibular ratio explained 35.8% of the variance in the strategy score of SOT condition 5 (p < 0.05). Moreover, the visual ratio, vestibular ratio, and time to peak torque of the knee flexors were all significant predictors (p < 0.05) of the strategy score during SOT condition 6, accounting for 14, 19.7, and 19.8% of its variance, respectively. The children with DCD demonstrated deficits in postural control strategy, sensory organization and prolonged duration of muscle force development. Slowed knee muscle force production combined with poor visual and vestibular functioning may result in greater use of hip strategy by children with DCD in sensory challenging environments. (c) 2013 Elsevier Ltd. All rights reserved.", "journal": "RESEARCH IN DEVELOPMENTAL DISABILITIES", "category": "Education, Special; Rehabilitation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000325849500001", "keywords": "Transcription factor binding site; TFBS; Transcription factor binding site model; Binding motif; Jaccard similarity; Position weight matrix; PWM; P-value; Position specific frequency matrix; PSFM; Macroape", "title": "Jaccard index based similarity measure to compare transcription factor binding site models", "abstract": "Background: Positional weight matrix (PWM) remains the most popular for quantification of transcription factor (TF) binding. PWM supplied with a score threshold defines a set of putative transcription factor binding sites (TFBS), thus providing a TFBS model. TF binding DNA fragments obtained by different experimental methods usually give similar but not identical PWMs. This is also common for different TFs from the same structural family. Thus it is often necessary to measure the similarity between PWMs. The popular tools compare PWMs directly using matrix elements. Yet, for log-odds PWMs, negative elements do not contribute to the scores of highly scoring TFBS and thus may be different without affecting the sets of the best recognized binding sites. Moreover, the two TFBS sets recognized by a given pair of PWMs can be more or less different depending on the score thresholds. Results: We propose a practical approach for comparing two TFBS models, each consisting of a PWM and the respective scoring threshold. The proposed measure is a variant of the Jaccard index between two TFBS sets. The measure defines a metric space for TFBS models of all finite lengths. The algorithm can compare TFBS models constructed using substantially different approaches, like PWMs with raw positional counts and log-odds. We present the efficient software implementation: MACRO-APE (MAtrix CompaRisOn by Approximate P-value Estimation). Conclusions: MACRO-APE can be effectively used to compute the Jaccard index based similarity for two TFBS models. A two-pass scanning algorithm is presented to scan a given collection of PWMs for PWMs similar to a given query. Availability and implementation: MACRO-APE is implemented in ruby 1.9; software including source code and a manual is freely available at http://autosome.ru/macroape/ and in supplementary materials.", "journal": "ALGORITHMS FOR MOLECULAR BIOLOGY", "category": "Biochemical Research Methods; Biotechnology & Applied Microbiology; Mathematical & Computational Biology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000324724800006", "keywords": "Fractal image coding; Progressive image transmission; Image compression; Quadtree partitioning; Iterated function system; No-search encoding; Domain block and range block; SSIM measure", "title": "DInSAR estimation of land motion using intermittent coherence with application to the South Derbyshire and Leicestershire coalfields", "abstract": "Differential interferometric synthetic aperture radar (DInSAR) is a recognized remote-sensing method for measuring the land motion occurring between two satellite radar acquisitions. Advanced DInSAR techniques such as persistent scatterers and small baseline methods are excellent over urban and rocky environments but generally poor over more rural and natural terrain where the signal can be intermittently good and bad. Here, we describe the Intermittent Small Baseline Subset (ISBAS) method, which appears to improve results over natural, woodland and agricultural terrain. This technique uses a multi-looked, low-resolution approach, which is particularly suitable for deriving the linear components of subsidence for large-scale deformations. Application of the ISBAS method over a coal mining area in the UK indicates that it is able to significantly improve upon a standard small baseline approach.", "journal": "REMOTE SENSING LETTERS", "category": "Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000343199400001", "keywords": "gaze interaction; gaze control; eye tracking; smooth pursuit", "title": "Entering PIN Codes by Smooth Pursuit Eye Movements", "abstract": "Despite its potential gaze interaction is still not a widely-used interaction concept. Major drawbacks as the calibration, strain of the eyes and the high number of false alarms are associated with gaze based interaction and limit its practicability for every-day human computer interaction. In this paper two experiments are described which use smooth pursuit eye movements on moving display buttons. The first experiment was conducted to extract an easy and fast interaction concept and at the same time to collect data to develop a specific but robust algorithm. In a follow-up experiment, twelve conventionally calibrated participants interacted successfully with the system. For another group of twelve people the eye tracker was not calibrated individually, but on a third person. Results show that for both groups interaction was possible without false alarms. Both groups rated the user experience of the system as positive.", "journal": "JOURNAL OF EYE MOVEMENT RESEARCH", "category": "Ophthalmology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000328246000040", "keywords": "engineering assets; closed-loop life cycle management; asset life cycle management; semantics; ontology; PLM; context aware", "title": "Semantic technologies for engineering asset life cycle management", "abstract": "The use of semantic technologies and ontologies is becoming more and more popular in engineering applications and particularly in product modelling. Still, the use is limited in academia and applications are of a small scale. In this paper we present the research work done by the closed-loop life cycle management (CL2M) team of the Laboratory for Computer-Aided Design and Production (LICP) at the Swiss Federal Institute of Technology Lausanne (EPFL), Switzerland, on the use of ontology-based technologies for the life cycle management of products and engineering assets. This research has been performed through a number of PhD works partially financed by the European Framework Program for research. It aims at providing both a wider understanding of the benefits of applying such technologies in the complex environment of asset life cycle management (ALM) and at providing a platform for implementing ontology models in industrial environments.", "journal": "INTERNATIONAL JOURNAL OF PRODUCTION RESEARCH", "category": "Engineering, Industrial; Engineering, Manufacturing; Operations Research & Management Science", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000335208200011", "keywords": "Digital rock physics; Sandstone; Pore structure; Micro-CT scanning; Petrophysical parameters", "title": "Digital rock physics of sandstone based on micro-CT technology", "abstract": "Digital rock physics technology makes up for the disadvantages of traditional petrophysical experiments, and has opened up a new platform for petrophysics research. Taking sandstone as study object, this paper introduces a systematic process of digital rock physics: based on micro-CT scanning and advanced image processing technology we build a three-dimensional digital core model with real pore structure characteristics; with the application of Avizo software which contains a variety of morphological algorithm, the research of quantification and characterization of the pore structure is conducted, the porosity, pore volume distribution and pore size distribution characteristics are obtained by statistical methods, and the equivalent pore network model is built; the Avizo and Comsol multiphysics softwares are interactively combined in this paper, which realize the pore-scale flow simulation as well as the absolute permeability calculation; for the case of the solid phase pore-filling, the effective elastic parameter of the rock is simulated, the result has a good validation with the approximate Gassmann equation. Our study enriches the existing research approaches of digital rock physics, simultaneously opens up a new pathway for its wide-scale development.", "journal": "CHINESE JOURNAL OF GEOPHYSICS-CHINESE EDITION", "category": "Geochemistry & Geophysics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000326610000032", "keywords": "GRASP with path relinking; Column generation; Hybrid meta-heuristics; Network load balancing", "title": "A hybrid column generation with GRASP and path relinking for the network load balancing problem", "abstract": "In this paper, a hybrid meta-heuristic is proposed which combines the GRASP with path relinking method and Column Generation. The key idea of this method is to run a GRASP with path relinking search on a restricted search space, defined by Column Generation, instead of running the search on the complete search space of the problem. Moreover, column generation is used not only to compute the initial restricted search space but also to modify it during the whole algorithm. The proposed heuristic is used to solve the network load balancing problem: given a capacitated telecommunications network with single path routing and an estimated traffic demand matrix, the network load balancing problem is the determination of a routing path for each traffic commodity such that the network load balancing is optimized, i.e., the worst link load is minimized, among all such solutions, the second worst link load is minimized, and continuing in this way until all link loads are minimized. The computational results presented in this paper show that, for the network load balancing problem, the proposed heuristic is effective in obtaining better quality solutions in shorter running times. (C) 2013 Elsevier Ltd. All rights reserved.", "journal": "COMPUTERS & OPERATIONS RESEARCH", "category": "Computer Science, Interdisciplinary Applications; Engineering, Industrial; Operations Research & Management Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000329921000009", "keywords": "auto-disturbance rejection control (ADRC); doubly fed induction generator (DFIG); proportional resonant (PR) controller; stand-alone wind-power generation; unbalance", "title": "Coordinated control for unbalanced operation of stand-alone doubly fed induction generator", "abstract": "This paper proposes a coordinated control scheme of a stand-alone doubly fed induction generator (DFIG)-based wind energy conversion system to improve the operation performance under unbalanced load conditions. To provide excellent voltage profile for load, a direct stator flux control scheme based on auto-disturbance rejection control (ADRC) is applied, and less current sensors are required. Due to the virtues of ADRC, the controller has good disturbance rejection capability and is robust to parameter variation. In the case of unbalanced loads, the electromagnetic torque pulsations at double synchronous frequency will exist. To eliminate the undesired effect, the stator-side converter (SSC) is used to provide the negative sequence current components for the unbalanced load. Usually, proportional integral controllers in a synchronous reference frame are used to control SSC. To simplify the algorithm, an improved proportional resonant (PR) control is proposed and used in the current loop without involving positive and negative sequence decomposition. The improved PR provides more degree of freedom which could be used to improve the performance. The effectiveness of the proposed control scheme has been validated by the simulation and experimental results. Copyright (c) 2012 John Wiley & Sons, Ltd.", "journal": "WIND ENERGY", "category": "Energy & Fuels; Engineering, Mechanical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000333263300018", "keywords": "2-D-3-D registration; joint-constraint model; knee motion reconstruction; magnetic resonance (MR); single-plane fluoroscopy", "title": "A Joint-Constraint Model-Based System for Reconstructing Total Knee Motion", "abstract": "Comprehending knee motion is an essential requirement for studying the causes of knee disorders. In this paper, we propose a new 2-D-3-D registration system based on joint-constraint model for reconstructing total knee motion. The proposed model that contains bone geometries and an articulated joint mechanism is first constructed from multipostural magnetic resonancevolumetric images. Then, the bone segments of the model are hierarchically registered to each frame of the given single-plane fluoroscopic video that records the knee activity. The bone posture is iteratively optimized using a modified chamfer matching algorithm to yield the simulated radiograph which is the best fit to the underlying fluoroscopic image. Unlike conventional registration methods computing posture parameters for each bone independently, the proposed femorotibial and patellofemoral joint models properly maintain the articulations between femur, tibia, and patella during the registration processes. As a result, we can obtain a sequence of registered knee postures showing smooth and reasonable physiologic patterns of motion. The proposed system also provides joint-space interpolation to densely generate intermediate postures for motion animation. The effectiveness of the proposed method was validated by computer simulation, animal cadaver, and in vivo knee testing. The mean target registration errors for femur, tibia, and patella were less than 1.5 mm. In particular, small out-of-plane registration errors [less than 1 mm (translation) and 2 degrees (rotation)] were achieved in animal cadaver assessments.", "journal": "IEEE TRANSACTIONS ON BIOMEDICAL ENGINEERING", "category": "Engineering, Biomedical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000339828400001", "keywords": "FTIR; Multivariate calibration; Compost; Growing media", "title": "A Delay Decomposition Approach to the Stability Analysis of Singular Systems with Interval Time-Varying Delay", "abstract": "This paper investigates delay-dependent stability problem for singular systems with interval time-varying delay. An appropriate Lyapunov-Krasovskii functional is constructed by decomposing the delay interval into multiple equidistant subintervals, where both the information of every subinterval and time-varying delay have been taken into account. Employing the Lyapunov-Krasovskii functional, improved delay-dependent stability criteria for the considered systems to be regular, impulse-free, and stable are established. Finally, two numerical examples are presented to show the effectiveness and less conservativeness of the proposed method.", "journal": "MATHEMATICAL PROBLEMS IN ENGINEERING", "category": "Engineering, Multidisciplinary; Mathematics, Interdisciplinary Applications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000209535600005", "keywords": "Home healthcare; Therapist scheduling; Mixed-integer programming; Weekly planning", "title": "Weekly scheduling models for traveling therapists", "abstract": "This paper presents a series of models that can be used to find weekly schedules for therapists who provide ongoing treatment to patients throughout a geographical region. In all cases, patient-appointment times and visit days are known prior to the beginning of the planning horizon. Variations in the models include single vs. multiple home bases, homogeneous vs. heterogeneous therapists, lunch break requirements, and a nonlinear cost structure for mileage reimbursement and overtime. The single home base and homogeneous therapist cases proved to be easy to solve and so were not thoroughly investigated. This left two cases of interest: the first included only lunch breaks while the second added nonlinear overtime and mileage reimbursement costs. For the first case, 40 data sets were solved, each consisting of either 15 or 20 therapists and between roughly 300 and 540 patient visits over five days. For each instance, we were able to obtain the minimum cost of providing residential healthcare services using a commercial solver. The results showed that CPU time increases more rapidly than total cost as the total number of visits grows. For the second case, which was much more difficult, it was necessary to develop heuristics to find good solutions quickly. Results for 5- through 20-therapist instances are presented and compared to the linear programming relaxation lower bounds. In the first of two parametric analyses, the tradeoff between the number of therapists on staff and the cost of providing service was examined. In the second, a similar tradeoff was explored between cost can the number of home bases used by the therapists. (C) 2012 Elsevier Ltd. All rights reserved.", "journal": "SOCIO-ECONOMIC PLANNING SCIENCES", "category": "Economics; Management; Operations Research & Management Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000344728900024", "keywords": "Wireless sensor network; Resource allocation; Task allocation", "title": "1D-Var multilayer assimilation of X-band SAR data into a detailed snowpack model", "abstract": "The structure and physical properties of a snowpack and their temporal evolution may be simulated using meteorological data and a snow metamorphism model. Such an approach may meet limitations related to potential divergences and accumulated errors, to a limited spatial resolution, to wind or topography-induced local modulations of the physical properties of a snow cover, etc. Exogenous data are then required in order to constrain the simulator and improve its performance over time. Synthetic-aperture radars (SARs) and, in particular, recent sensors provide reflectivity maps of snow-covered environments with high temporal and spatial resolutions. The radiometric properties of a snowpack measured at sufficiently high carrier frequencies are known to be tightly related to some of its main physical parameters, like its depth, snow grain size and density. SAR acquisitions may then be used, together with an electromagnetic backscattering model (EBM) able to simulate the reflectivity of a snowpack from a set of physical descriptors, in order to constrain a physical snowpack model. In this study, we introduce a variational data assimilation scheme coupling TerraSAR-X radiometric data into the snowpack evolution model Crocus. The physical properties of a snowpack, such as snow density and optical diameter of each layer, are simulated by Crocus, fed by the local reanalysis of meteorological data (SAFRAN) at a French Alpine location. These snowpack properties are used as inputs of an EBM based on dense media radiative transfer (DMRT) theory, which simulates the total backscattering coefficient of a dry snow medium at X and higher frequency bands. After evaluating the sensitivity of the EBM to snowpack parameters, a 1D-Var data assimilation scheme is implemented in order to minimize the discrepancies between EBM simulations and observations obtained from TerraSAR-X acquisitions by modifying the physical parameters of the Crocus-simulated snowpack. The algorithm then re-initializes Crocus with the modified snowpack physical parameters, allowing it to continue the simulation of snowpack evolution, with adjustments based on remote sensing information. This method is evaluated using multi-temporal TerraSAR-X images acquired over the specific site of the Argentiere glacier (Mont-Blanc massif, French Alps) to constrain the evolution of Crocus. Results indicate that X-band SAR data can be taken into account to modify the evolution of snowpack simulated by Crocus.", "journal": "CRYOSPHERE", "category": "Geography, Physical; Geosciences, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000333080900004", "keywords": "Matrix geometric mean; Structured matrices; Manifold optimization", "title": "Geometric means of structured matrices", "abstract": "The geometric mean of positive definite matrices is usually identified with the Karcher mean, which possesses all properties-generalized from the scalar case-a geometric mean is expected to satisfy. Unfortunately, the Karcher mean is typically not structure preserving, and destroys, e.g., Toeplitz and band structures, which emerge in many applications. For this reason, the Karcher mean is not always recommended for modeling averages of structured matrices. In this article a new definition of a geometric mean for structured matrices is introduced, its properties are outlined, algorithms for its computation, and numerical experiments are provided. In the Toeplitz case an existing mean based on the Kahler metric is analyzed for comparison.", "journal": "BIT NUMERICAL MATHEMATICS", "category": "Computer Science, Software Engineering; Mathematics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000331630900001", "keywords": "Thyroid gland; Thermoluminescent dosimetry; Radiotherapy; Child; Adolescent", "title": "In vivo dosimetry of thyroid doses from different irradiated sites in children and adolescents: a cross-sectional study", "abstract": "Background: Scattered radiation can be assessed by in vivo dosimetry. Thyroid tissue is sensitive to radiation, even at doses < 10 cGy. This study compared the scattered dose to the thyroid measured by thermoluminescent dosimeters (TLDs) and the estimated one by treatment planning system (TPS). Methods: During radiotherapy to sites other than the thyroid of 16 children and adolescents, seventy-two TLD measurements at the thyroid were compared with TPS estimation. Results: The overall TPS/TLD bias was 1.02 (95% LA 0.05 to 21.09). When bias was stratified by treatment field, the TPS overestimated TLD values at doses < 1 cGy and underestimated them at doses > 10 cGy. The greatest bias was found in pelvis and abdomen: 15.01 (95% LA 9.16 to 24.61) and 5.12 (95% LA 3.04 to 8.63) respectively. There was good agreement in orbit, head, and spine: bias 1.52 (95% LA 0.48 to 4.79), 0.44 (95% LA 0.11 to 1.82) and 0.83 (0.39 to 1.76) respectively. There was small agreement with broad limits for lung and mediastinum: 1.13 (95% LA 0.03 to 40.90) and 0.39 (95% LA 0.02 to 7.14) respectively. Conclusions: The scattered dose can be measured with TLDs, and TPS algorithms for outside structures should be improved.", "journal": "RADIATION ONCOLOGY", "category": "Oncology; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000345122000024", "keywords": "small-animal SPECT; performance evaluation; HiReSPECT; parallel-hole; resolution recovery", "title": "Performance evaluation of a newly developed high-resolution, dual-head animal SPECT system based on the NEMA NU1-2007 standard", "abstract": "Small-animal single-photon emission computed tomography (SPECT) system plays an important role in the field of drug development and investigation of potential drugs in the preclinical phase. The small-animal High-Resolution SPECT (HiReSPECT) scanner has been recently designed and developed based on compact and high-resolution detectors. The detectors are based on a high-resolution parallel hole collimator, a cesium iodide (sodium-activated) pixelated crystal array and two H8500 position-sensitive photomultiplier tubes. In this system, a full set of data corrections such as energy, linearity, and uniformity, together with resolution recovery option in reconstruction algorithms, are available. In this study, we assessed the performance of the system based on NEMA-NU1-2007 standards for pixelated detector cameras. Characterization of the HiReSPECT was performed by measurement of the physical parameters including planar and tomographic performance. The planar performance of the system was characterized with flood-field phantom for energy resolution and uniformity. Spatial resolution and sensitivity were evaluated as functions of distance with capillary tube and cylindrical source, respectively. Tomographic spatial resolution was characterized as a function of radius of rotation (ROR). A dedicated hot rod phantom and image quality phantom was used for the evaluation of overall tomographic quality of the HiReSPECT. The results showed that the planar spatial resolution was similar to 1.6 mm and similar to 2.3 mm in terms of full-width at half-maximum (FWHM) along short-and long-axis dimensions, respectively, when the source was placed on the detector surface. The integral uniformity of the system after uniformity correction was 1.7% and 1.2% in useful field of view (UFOV) and central field of view (CFOV), respectively. System sensitivity on the collimator surface was 1.31 cps/mu Ci and didn't vary significantly with distance. Mean tomographic spatial resolution was measured similar to 1.7 mm FWHM at the radius of rotation of 25 mm with dual-head configuration. The measured performance demonstrated that the HiReSPECT scanner has acceptable image quality and, hence, is well suited for preclinical molecular imaging research.", "journal": "JOURNAL OF APPLIED CLINICAL MEDICAL PHYSICS", "category": "Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000326610000016", "keywords": "Scheduling; Unrelated parallel batch processing machines; Makespan; Heuristics", "title": "Scheduling unrelated parallel batch processing machines with non-identical job sizes", "abstract": "Scheduling unrelated parallel batch processing machines to minimize makespan is studied in this paper. Jobs with non-identical sizes are scheduled on batch processing machines that can process several jobs as a batch as long as the machine capacity is not violated. Several heuristics based on best fit longest processing time (BFLPT) in two groups are proposed to solve the problem. A lower bound is also proved to evaluate the quality of the heuristics. Computational experiments were undertaken. These showed that J_SC-BFLPT, considering both load balance of machines and job processing times, was robust and outperformed other heuristics for most of the problem categories. (C) 2013 Elsevier Ltd. All rights reserved.", "journal": "COMPUTERS & OPERATIONS RESEARCH", "category": "Computer Science, Interdisciplinary Applications; Engineering, Industrial; Operations Research & Management Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000324677900022", "keywords": "Algorithms; Languages; Performance; Implementation; collection types; memory optimization; dynamic typing", "title": "Differences between true mean daily, monthly and annual air temperatures and air temperatures calculated with three equations: a case study from three Croatian stations", "abstract": "Differences between true mean daily, monthly and annual air temperatures T0 [Eq. (1)] and temperatures calculated with three different equations [(2), (3) and (4)] (commonly used in climatological practice) were investigated at three main meteorological Croatian stations from 1 January 1999 to 31 December 2011. The stations are situated in the following three climatically distinct areas: (1) Zagreb-Gri (mild continental climate), (2) ZaviA3/4an (cold mountain climate), and (3) Dubrovnik (hot Mediterranean climate). T1 [Eq. (2)] and T3 [Eq. (4)] mean temperatures are defined by the algorithms based on the weighted means of temperatures measured at irregularly spaced, yet fixed hours. T2 [Eq. (3)] is the mean temperature defined as the average of daily maximum and minimum temperature. The equation as well as the time of observations used introduces a bias into mean temperatures. The largest differences occur for mean daily temperatures. The calculated daily difference value from all three equations and all analysed stations varies from -3.73 A degrees C to +3.56 A degrees C, from -1.39 A degrees C to +0.79 A degrees C for monthly differences and from -0.76 A degrees C to +0.30 A degrees C for annual differences.", "journal": "THEORETICAL AND APPLIED CLIMATOLOGY", "category": "Meteorology & Atmospheric Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000209715200005", "keywords": "Steganography; Reversible; Graph 3 coloring; Cover image; Stego-image; Chromatic polynomial", "title": "Reversible dynamic secure steganography for medical image using graph coloring", "abstract": "Securing data in telemedicine applications is extremely essential and therefore it is mandatory to develop algorithms which preserve the data transmitted. Steganography (information hiding technique) plays a crucial role in telemedicine applications by providing confidentiality, integrity, availability and authenticity. This paper proposes a novel steganography technique that conceals patient information inside a medical image using a dynamic key generated by graph 3 coloring problem. The proposed method ensures reversibility as the original medical image is restored after extracting the embedded data from the stego medical image. Despite the embedding of patient information in the medical image, the visual quality of the image is preserved. Experimental results show that the proposed method is resistant against uniform affine transformations such as cropping, rotation and scaling. The proposed method is designed by considering issues related to transmission errors which could contaminate the medical images transmitted. The performance of the proposed method is compared to other information hiding methods against various parameters such as robustness of stego-image against affine transformations, toughness of the dynamic key generated, detection of transmission error, embedding rate and reversibility. (C) 2013 Fellowship of Postgraduate Medicine. Published by Elsevier Ltd. All rights reserved.", "journal": "HEALTH POLICY AND TECHNOLOGY", "category": "Health Policy & Services", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000390965300060", "keywords": "Adaptive PI control; General type II fuzzy logic; Load frequency control; Modified Harmony Search Algorithm; MicroGrids", "title": "A new load frequency control strategy for micro-grids with considering electrical vehicles", "abstract": "Owing to the intermittent nature of the renewable energies employed in smart grids, large frequency fluctuations occur when the load frequency control (LFC) capacity is not enough to compensate for the imbalance of generation and demand. This problem may become intensified when the system is working in an island operation mode. Meanwhile, electric vehicles (EVs) are growing in popularity, being used as dispersed energy storage units instead of small batteries in the systems. Accordingly, the vehicle-to-grid (V2G) power control can be applied to compensate for the inadequate LFC capacity and thereby to improve the frequency stability of smart grids, especially in the island operation mode. On the other hand, large scale and complex power systems encounter many different uncertainties. In order to handle these uncertainties, this study proposes a combination of the general type-2 fuzzy logic sets (GT2FLS) and the Modified Harmony Search Algorithm (MHSA) technique, as a novel heuristic algorithm, to adaptively tune the proportional-integral (PI) controller for LFC in islanded MicroGrids (MGs). Although implementing general type-2 fuzzy systems is generally computationally cumbersome, by using a recently introduced plane representation, GT2FLS can be regarded as a combination of several interval type-2 fuzzy logic systems (IT2FLS), each with its own corresponding alpha level and linguistic rules can directly be incorporated into the controller. This paper further presents a new modified optimization algorithm to tune the scaling factors and the membership functions of general type-2 fuzzy PI (GT2FPI) controller and thereby to minimize the frequency deviations of the MG system against load disturbances more effectively. To evaluate the efficiency of the proposed controller, the obtained results are compared with those of the proportional integral derivative (PID), Fuzzy-PID (FPID), and Interval Type II fuzzy based PI (IT2FPI) controllers, which are the most recent methods applied in this respect. Simulation results demonstrate the perfection and efficacy of proposed controller. (C) 2016 Published by Elsevier B.V.", "journal": "ELECTRIC POWER SYSTEMS RESEARCH", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000394080400025", "keywords": "Hedge funds; Hedge fund strategies; Systematic risk exposure; Quantile regression", "title": "Different strokes by different folks: The dynamics of hedge fund systematic risk exposure and performance", "abstract": "We study hedge fund performance and exposure to systematic risk factors over different market cycles with a sample of 1821 hedge funds from January 1994 to June 2008. Our findings suggest that hedge funds are exposed to systematic risk factors and minimizing systematic risk exposure by means of, for example, hedging does not always produce good results. Our quantile regression analyses reveal that systematic risk exposure per se does not separate high-achievers (positive alphas) from low-achievers (negative alphas). Fund performance is also conditioned on the direction of exposure. Moreover, fund exposure to the types of risk factors depends On market regimes, confirming the argument that hedge funds shift strategies. Choosing the exposure to the right risk factors in the right direction according to economic regimes separates good performers from poor ones.", "journal": "INTERNATIONAL REVIEW OF ECONOMICS & FINANCE", "category": "Business, Finance; Economics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000400176500010", "keywords": "Semantic localization; Bayesian networks; Robotics; Robot vision; Structural learning; Indoor scene classification", "title": "Variable selection and accurate predictions in habitat modelling: a shrinkage approach", "abstract": "Habitat modelling is increasingly relevant in biodiversity and conservation studies. A typical application is to predict potential zones of specific conservation interest. With many environmental covariates, a large number of models can he investigated but multi-model inference may become impractical. Shrinkage regression overcomes this issue by dealing with the identification and accurate estimation of effect size for prediction. In a Bayesian framework we investigated the use of a shrinkage prior, the Horseshoe, for variable selection in spatial generalized linear models (GLM). As study cases, we considered 5 datasets on small pelagic fish abundance in the Gulf of Lion (Mediterranean Sea, France) and 9 environmental inputs. We compared the predictive performances of a simple kriging model, a full spatial GLM model with independent normal priors for regression coefficients, a full spatial GLM model with a Horseshoe prior for regression coefficients and 2 zero-inflated models (spatial and non-spatial) with a Horseshoe prior. Predictive performances were evaluated by cross validation on a hold-out subset of the data: models with a Horseshoe prior performed best, and the full model with independent normal priors worst. With an increasing number of inputs, extrapolation quickly became pervasive as we tried to predict from novel combinations of covariate values. By shrinking regression coefficients with a Horseshoe prior, only one model needed to be fitted to the data in order to obtain reasonable and accurate predictions, including extrapolations.", "journal": "ECOGRAPHY", "category": "Biodiversity Conservation; Ecology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000423301600002", "keywords": "augmented reality; digital terrain model; map skills; mesh-processing viewer; 3D printed model", "title": "Map-Reading Skill Development with 3D Technologies", "abstract": "Landforms often are represented on maps using abstract cartographic techniques that the reader must interpret for successful three-dimensional terrain visualization. New technologies in 3D landscape representation, both digital and tangible, offer the opportunity to visualize terrain in new ways. The results of a university student workshop, in which traditional 2D versus 3D digital and tangible models were tested, suggest that map-reading skill development is greater when using the 3D technologies.", "journal": "JOURNAL OF GEOGRAPHY", "category": "Geography", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000390725700004", "keywords": "Image hashing; Structure feature; Selective sampling; Perceptual robustness; Discrimination; Security", "title": "Perceptual image hashing with selective sampling for salient structure features", "abstract": "In this paper, a robust and secure image hashing scheme based on salient structure features is proposed, which can be applied in image authentication and retrieval. In order to acquire the fixed length of image hash, the pre-processing for image regularization is first conducted on input image. Salient edge detection is then applied on the secondary image, and a series of non-overlapping blocks containing the richest structural information in the secondary image are selectively sampled according to the edge binary map. Dominant DCT coefficients of the sampled blocks with their corresponding position information are retrieved as the robust features. After the compression with dimensionality reduction for the concatenated features, the final hash can be produced. Experimental results show that the proposed scheme has better performances of perceptual robustness and discrimination compared with some state-of-the-art schemes. (C) 2016 Elsevier B.V. All rights reserved.", "journal": "DISPLAYS", "category": "Computer Science, Hardware & Architecture; Engineering, Electrical & Electronic; Instruments & Instrumentation; Optics", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000418769600009", "keywords": "Binary data; Censored data; Control chart; Latent quality characteristics; Latent variable; SPC; Up-and-down test", "title": "Statistical Process Control for Latent Quality Characteristics Using the Up-and-Down Test", "abstract": "In many applications, the quality characteristic of a product is continuous but unobservable, for example, the critical electric voltage of electro-explosive devices. It is often important to monitor a manufacturing process of a product with such latent quality characteristic. Existing approaches all involve specifying a fixed stimulus level and testing products under that level to collect a sequence of response outcomes (zeros or ones). Appropriate control charts are then applied to the collected binary data sequence. However, these approaches offer limited performance. Moreover, the collected dataset provides little information for troubleshooting when an out-of-control signal is triggered. To overcome these limitations, this article introduces the up-and-down test for collecting data and proposes a new control chart based on this test. Numerical studies show that the proposed chart is able to detect any shifts effectively and is robust in many situations. Finally, an example involving real manufacturing data is given to demonstrate the use of our proposed chart.", "journal": "TECHNOMETRICS", "category": "Statistics & Probability", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000411789300023", "keywords": "Rhinanthus glacialis; C-13 nuclear magnetic resonance; pattern recognition; iridoids; fatty acids; NADPH oxidase 4", "title": "Identification of constituents of the Alpine plant Rhinanthus glacialis by 13C nuclear magnetic resonance pattern recognition", "abstract": "Background: Diagnosis of incarcerated inguinal hernia (IIH) is not difficult, but currently, there are no diagnostic criteria that can be used to differentiate it from strangulated inguinal hernia (SIH). This research aimed to evaluate the clinical value of the neutrophil/lymphocyte ratio (NLR) in diagnosing SIH. Methods: We retrospectively analyzed 263 patients with IIH who had undergone emergency operation. The patients were divided into two groups according to IIH severity: group A, patients with pure IIH validated during operation as having no bowel ischemia; group B, patients with SIH validated during operation as having obvious bowel ischemia, including bowel necrosis. We statistically evaluated the relation between several clinical features and SIH. The accuracy of different indices was then evaluated and compared using receiver operating characteristic (ROC) curve analyses, and the corresponding cutoff values were calculated. Result: Univariate analysis showed eight clinical features that were significantly different between the two groups. They were then subjected to multivariate analysis, which showed that the NLR, type of hernia, and incarcerated organ were significantly related to SIH. ROC curve analysis showed that the NLR had the largest area under the ROC curve. Conclusion: Among the different clinical features, the NLR appears to be the best index in diagnosing SIH. (C) 2016 IJS Publishing Group Ltd. Published by Elsevier Ltd. All rights reserved.", "journal": "PLANTA MEDICA", "category": "Plant Sciences; Chemistry, Medicinal; Integrative & Complementary Medicine; Pharmacology & Pharmacy", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000398425700077", "keywords": "Carbon dioxide emissions reduction; Bid evaluation; Linguistic group decision-making; Mega public projects", "title": "A linguistic group decision-making framework for bid evaluation in mega public projects considering carbon dioxide emissions reduction", "abstract": "Green construction is to conserve the resources and reduce the negative impact of construction activities on the environment to the maximum extent, with scientific management and technological progress under the premise of ensuring the quality, safety and other basic requirements. Recent years, accompanied by the rapid development of construction industry, a considerable proportion of energy and resources, particularly high-carbon materials, is consumed in the construction activities, emitting mass greenhouse gases, which have a damaging impact on the environment. This is especially for the mega public projects. Therefore, under the green and sustainable perspective, actions to control carbon dioxide (CO2) emissions in the construction industry are imperative. In this case, if CO2 emissions reduction can be considered beforehand during the bid evaluation stage, contractors will be guided by bid evaluation indicators and therefore improve their construction schemes, enabling the construction phase to be green, energy saving and sustainable. This is because bid evaluation indicators can serve as guiding effect on the contractors. Given that CO2 emissions are rarely given sufficient consideration in traditional bid evaluation, this study conducts a systematic analysis to identify major sources of CO2 in construction phase and attempts to develop a conceptual computational model in terms of CO2 emission. Then, a linguistic group decision-making framework for bid evaluation in mega public projects considering CO2 emissions reduction is developed. In this framework, entropy, relative entropy, the standard deviation method and weighted aggregation operators are applied to determine the indicator weights, the expert weights and the information aggregation in bid evaluation decision-making. Finally, a scenario comparison between the proposed linguistic group decision-making bid evaluation framework and the traditional framework is made through a case study, and the scientific basis and reliability of this new framework are validated. The establishment of this framework can enrich the decision-making methods in construction management field. And it can provide meaningful reference for owners to choose the most qualified contractors and spur contractors to improve their construction schemes, enabling construction phase to be green, energy saving and sustainable. (C) 2017 Elsevier Ltd. All rights reserved.", "journal": "JOURNAL OF CLEANER PRODUCTION", "category": "Green & Sustainable Science & Technology; Engineering, Environmental; Environmental Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000423186200001", "keywords": "techniques: image processing; techniques: photometric", "title": "An Adaptive Homomorphic Aperture Photometry Algorithm for Merging Galaxies", "abstract": "We present a novel automatic adaptive aperture photometry algorithm for measuring the total magnitudes of merging galaxies with irregular shapes. First, we use a morphological pattern recognition routine for identifying the shape of an irregular source in a background-subtracted image. Then, we extend the shape of the source by using the Dilation image operation to obtain an aperture that is quasi-homomorphic to the shape of the irregular source. The magnitude measured from the homomorphic aperture would thus have minimal contamination from the nearby background. As a test of our algorithm, we applied our technique to the merging galaxies observed by the Sloan Digital Sky Survey and the Canada-France-Hawaii Telescope. Our results suggest that the adaptive homomorphic aperture algorithm can be very useful for investigating extended sources with irregular shapes and sources in crowded regions.", "journal": "PUBLICATIONS OF THE ASTRONOMICAL SOCIETY OF THE PACIFIC", "category": "Astronomy & Astrophysics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000395743500023", "keywords": "Blind signal separation; Dual-matrix method; Constrained terms; Joint diagonalization", "title": "The Plasmin-Sensitive Protein Pls in Methicillin-Resistant Staphylococcus aureus (MRSA) Is a Glycoprotein", "abstract": "Most bacterial glycoproteins identified to date are virulence factors of pathogenic bacteria, i.e. adhesins and invasins. However, the impact of protein glycosylation on the major human pathogen Staphylococcus aureus remains incompletely understood. To study protein glycosylation in staphylococci, we analyzed lysostaphin lysates of methicillin-resistant Staphylococcus aureus (MRSA) strains by SDS-PAGE and subsequent periodic acid-Schiff's staining. We detected four (> 300, similar to 250, similar to 165, and similar to 120 kDa) and two (> 300 and similar to 175 kDa) glycosylated surface proteins with strain COL and strain 1061, respectively. The similar to 250, similar to 165, and similar to 175 kDa proteins were identified as plasmin-sensitive protein (Pls) by mass spectrometry. Previously, Pls has been demonstrated to be a virulence factor in a mouse septic arthritis model. The pls gene is encoded by the staphylococcal cassette chromosome (SCC) mec type I in MRSA that also encodes the methicillin resistance-conferring mecA and further genes. In a search for glycosyltransferases, we identified two open reading frames encoded downstream of pls on the SCCmec element, which we termed gtfC and gtfD. Expression and deletion analysis revealed that both gtfC and gtfD mediate glycosylation of Pls. Additionally, the recently reported glycosyltransferases SdgA and SdgB are involved in Pls glycosylation. Glycosylation occurs at serine residues in the Pls SD-repeat region and modifying carbohydrates are N-acetylhexosaminyl residues. Functional characterization revealed that Pls can confer increased biofilm formation, which seems to involve two distinct mechanisms. The first mechanism depends on glycosylation of the SD-repeat region by GtfC/GtfD and probably also involves eDNA, while the second seems to be independent of glycosylation as well as eDNA and may involve the centrally located G5 domains. Other previously known Pls properties are not related to the sugar modifications. In conclusion, Pls is a glycoprotein and Pls glycosyl residues can stimulate biofilm formation. Thus, sugar modifications may represent promising new targets for novel therapeutic or prophylactic measures against life-threatening S. aureus infections.", "journal": "PLOS PATHOGENS", "category": "Microbiology; Parasitology; Virology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000395613100018", "keywords": "Laser ultrasonic scanning; Binary search; Compressed sensing; Accelerated damage detection; Noncontact damage detection", "title": "Accelerated noncontact laser ultrasonic scanning for damage detection using combined binary search and compressed sensing", "abstract": "Laser ultrasonic scanning is attractive for damage detection due to its noncontact nature, sensitivity to local damage, and high spatial resolution. However, its practicality is limited because scanning at a high spatial resolution demands a prohibitively long scanning time. Inspired by binary search and compressed sensing, an accelerated laser scanning technique is developed to localize and visualize damage with reduced scanning points and scanning time. First, the approximate damage location is identified by examining the interactions between the ultrasonic waves and damage at the sparse scanning points that are selected by the binary search algorithm. Here, a time -domain laser ultrasonic response is transformed into a spatial ultrasonic domain using a basis pursuit approach so that the interactions between the ultrasonic waves and damage, such as reflections and transmissions, can be better identified in the spatial ultrasonic domain. Second, wavefield images around the damage are reconstructed from the previously selected scanning points using compressed sensing. The performance of the proposed accelerated laser scanning technique is validated using a numerical simulation performed on an aluminum plate with a notch and experiments performed on an aluminum plate with a crack and a carbon fiber-reinforced plastic plate with delamination. The number of scanning points that is necessary for damage localization and visualization is dramatically reduced from N.M to 2log(2)N.log(2)M.N and M represent the number of equally spaced scanning points in the x and y directions, respectively, which are required to obtain full-field wave propagation images of the target inspection region. For example, the number of scanning points in the composite plate experiment is reduced by 97.1% (from 2601 points to 75 points).(C) 2017 Elsevier Ltd. All rights reserved.", "journal": "MECHANICAL SYSTEMS AND SIGNAL PROCESSING", "category": "Engineering, Mechanical", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000394578100005", "keywords": "retinal neurons; retrograde labeling; cluster analysis; structural heterogeneity; bony fish; RRID:SCR_013597; RRID:SCR_014213", "title": "Structure and diversity of retinal ganglion cells in steller's sculpin Myoxocephalus stelleri tilesius, 1811", "abstract": "We studied the morphology and diversity of retinal ganglion cells in Steller's sculpin Myoxocephalus stelleri. The cells were retrogradely labeled with horseradish peroxidase and examined in retinal wholemounts. A sample of 123 cells were camera lucida-drawn and digitized. A total of 18 structural parameters were estimated for each cell. The cells were classified using 10 clustering algorithms. The optimum solution was determined using silhouette analysis. It contained eight clusters and was based on three variables: dendritic field area, vitread stratification boundary, and stratification range. Kruskal-Wallis analysis of variance (ANOVA)-on-Ranks with post-hoc Mann-Whitney U-tests showed significant pairwise between-cluster differences in two or more of the original variables. The present classification is compared with those proposed for other teleosts. J. Comp. Neurol. 525:1122-1138, 2017. (c) 2016 Wiley Periodicals, Inc.", "journal": "JOURNAL OF COMPARATIVE NEUROLOGY", "category": "Neurosciences; Zoology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000405597400012", "keywords": "Batch-reactor; chain-growth; control; convergence; DE; multi-objective; optimization; polymerization; temperature; trajectory", "title": "Optimized on-line control of MMA polymerization using fast multi-objective DE", "abstract": "Optimized on-line control (OOC) of polymerization reactors combine the optimization with the on-line operation and control. In this, re-optimized control variable trajectories, in the presence of unplanned disturbances, are obtained and implemented on-line to save the batch. Also, the available computational time for the optimization is limited as the re-optimized trajectories need to be implemented in real time on the actual system. In the present study, the OOC of such a system, i.e., bulk polymerization of methyl methacrylate (MMA) in a batch reactor, is carried out in the occurrence of heater malfunction. To solve the underlying multi-objective problem, a multi-objective variant of differential evolution with an improved mutation strategy is developed. The developed algorithm shows faster convergence with respect to other compared algorithms for a large number of benchmark problems. Finally, this algorithm is used to find the optimal temperature trajectories and the OOC with these trajectories found to be successfully countering the effect of heater malfunction.", "journal": "MATERIALS AND MANUFACTURING PROCESSES", "category": "Engineering, Manufacturing; Materials Science, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000403061100009", "keywords": "Cross-validation; eigenfunctions; eigenvalues; functional linear model; functional principal components; functional partial least squares", "title": "Functional Principal Component Regression and Functional Partial Least-squares Regression: An Overview and a Comparative Study", "abstract": "Functional data analysis is a field of growing importance in Statistics. In particular, the functional linear model with scalar response is surely the model that has attracted more attention in both theoretical and applied research. Two of the most important methodologies used to estimate the parameters of the functional linear model with scalar response are functional principal component regression and functional partial least-squares regression. We provide an overview of estimation methods based on these methodologies and discuss their advantages and disadvantages. We emphasise that the role played by the functional principal components and by the functional partial least-squares components that are used in estimation appears to be very important to estimate the functional slope of the model. A functional version of the best subset selection strategy usual in multiple linear regression is also analysed. Finally, we present an extensive comparative simulation study to compare the performance of all the considered methodologies that may help practitioners in the use of the functional linear model with scalar response.", "journal": "INTERNATIONAL STATISTICAL REVIEW", "category": "Statistics & Probability", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000399326300005", "keywords": "Demographics; epidemiology; noise; hearing conservation; instrumentation", "title": "Uncertainty quantification and design-of-experiment in absorption-based aqueous film parameter measurements using Bayesian inference", "abstract": "Diode laser-based multi-wavelength near-infrared (NIR) absorption in aqueous films is a promising diagnostic for making temporally resolved, simultaneous measurements of film thickness, temperature, and concentration of a solute. Our previous work in aqueous urea solutions aimed at determining simultaneously two of these system parameters, while the third one must be fixed or specified by additional measurements. The current work presents a simultaneous NIR absorption-based multi-parameter measurement of thickness, temperature, and solute concentration coupled with the Bayesian methodology that is used to infer probability densities for the obtained data. The Bayesian analysis is based on a temperature-and concentration-dependent spectral database generated with a Fourier transform infrared spectrometer in the range 5500-8000 cm(-1) for water with variable temperature and urea concentration. The concept was first validated with measurements using a calibration cell. Probability densities in the measured parameters were quantified using aMarkov chain Monte Carlo algorithm, which were used to derive credibility intervals. As a practical demonstration, the temporal variation of film thickness, urea concentration, and liquid temperature were recorded during evaporation of a liquid film deposited on a transparent heated quartz plate. (C) 2017 Optical Society of America", "journal": "APPLIED OPTICS", "category": "Optics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000391468200024", "keywords": "GaN; Impurities; Electronic properties; Optical properties; Density functional theory", "title": "Density functional theories study on optoelectronic properties of arsenic-doped GaN nanowires", "abstract": "The electronic and optical properties of As-doped GaN nanowires have been investigated using density functional theory. The energy gaps of As-doped GaN nanowires with different concentrations exhibit indirect band gaps, and the band gaps are decreasing with the increasing doping concentration. The results predicate that alloys exhibit a typical semiconductivity. Moreover, the optical properties, including the complex dielectric function, optical refractive index, energy-loss function, reflectivity, and absorption coefficient are discussed for radiation up to 30 eV. With the increase of the As-fraction, the material gradually exhibits noticeable anisotropy in the photon energy range of 0-15 eV. Quickly increases the static dielectric constant and obviously red-shifts the absorption edge. The results are in good agreement with experimental data reported previously. Furthermore, the work gives a theoretical guidance for the preparation of As-doped GaN optoelectronic nanodevices.", "journal": "OPTICAL AND QUANTUM ELECTRONICS", "category": "Engineering, Electrical & Electronic; Quantum Science & Technology; Optics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000394544100003", "keywords": "Satureja subspicata; Lamiaceae; n-Alkanes; Morphology; Balkan Peninsula; Multivariate analysis", "title": "Chemodiversity of Epicuticular n-Alkanes and Morphological Traits of Natural Populations of Satureja subspicata Bartl. ex Vis. along Dinaric Alps - Ecological and Taxonomic Aspects", "abstract": "Morphological characters and the composition of epicuticular leaf n-alkanes of two Satureja subspicata Bartl. ex Vis. subspecies (subsp. liburnica Sili and subsp. subspicata) from nine natural populations along Dinaric Alps range were studied. Morphological characters were chosen based on Silis subspecies separation. Seventeen n-alkane homologues (C-19 - C-35) were identified using gas chromatography/mass spectrometry (GC/MS) and GC/flame ionisation detector (FID). The most abundant n-alkane in all populations was n-nonacosane (C-29), followed by n-hentriacontane (C-31), with the exception of Divaa population where these two alkanes were co-dominant. Diversity and variability of n-alkane patterns and morphological characters and their relation to different geographic and bioclimatic parameters, including exposure, were analysed by several statistical multivariate methods (PCA, HCA, Discriminant Analysis, Mantel test). These tests showed clear separation of subsp. liburnica from subsp. subspicata, even though population Velebit showed separation from other subsp. liburnica populations based on phytochemical characters. Mantel test showed high correlation with geographical distribution in both investigated data sets. High correlation between morphological and phytochemical characters was also established. However, exposure can influence n-alkane profile, suggesting precaution while taking samples from natural habitats.", "journal": "CHEMISTRY & BIODIVERSITY", "category": "Biochemistry & Molecular Biology; Chemistry, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000399745300016", "keywords": "Beef; Muscle; Breed; Multi-spectral; Textural analysis", "title": "Discrimination of beef muscle based on visible-near infrared multi-spectral features: Textural and spectral analysis", "abstract": "The potential of multi-spectral visible-near infrared imaging to discriminate beef meat muscles in relation with their type and animal origin was examined in the present study. Two hundred forty muscles of three types (longissimus thoracis, biceps femoris, and semimembranosus) were obtained from the carcasses of three types of animals, two late-maturing cattle types of animals (Limousin and Blond d'Aquitaine) that grow slowly and deposit more muscles and less fat, compared to one early-maturing cattle types of animals (Angus) which tends to have muscles richer in collagen and in intramuscular fat. Two hundred forty cube images were collected with nineteen Ligth Emitting Diodes (405 to 1050 nm) using the Videometer Lab2 device. The image cubes were processed in order to extract image mean spectra and image shape features from co-occurrence and difference of histogram matrices. The results of the partial least square discriminant analysis performed on image texture features and spectral data show a maximum ranging from 63.5 to 83% of good classification depending on the muscle and breed considered. This study demonstrated the promising potential of the visible-near infrared multi-spectral imager to characterize beef meat muscles based on muscle type and its animal origin.", "journal": "INTERNATIONAL JOURNAL OF FOOD PROPERTIES", "category": "Food Science & Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000396442600002", "keywords": "antioxidants; diet; ovarian cancer; African American; women", "title": "The Status of Women's Reproductive Rights and Adverse Birth Outcomes", "abstract": "Background: Reproductive rightsdthe ability to decide whether and when to have childrendshape women's socioeconomic and health trajectories across the life course. The objective of this study was to examine reproductive rights in association with preterm birth (PTB; < 37 weeks) and low birth weight (LBW; < 2,500g) across states in the United States. Methods: Analysis included records for all live births in the United States in 2012 grouped by state. A reproductive rights composite index score was assigned to records from each state based on the following indicators for the year before birth (2011): mandatory sex education, expanded Medicaid eligibility for family planning services, mandatory parental involvement for minors seeking abortion, mandatory abortion waiting periods, public funding for abortion, and percentage of women in counties with abortion providers. Scores were ranked by tertile with the highest tertile reflecting states with strongest reproductive rights. We fit logistic regression models with generalized estimating equations to estimate the odds ratios and 95% confidence intervals for PTB and LBW associated with reproductive rights score controlling for maternal race, age, education, and insurance and state-level poverty. Results: States with the strongest reproductive rights had the lowest rates of LBWand PTB (7.3% and 10.6%, respectively) compared with states with more restrictions (8.5% and 12.2%, respectively). After adjustment, women in more restricted states experienced 13% to 15% increased odds of PTB and 6% to 9% increased odds of LBW compared with women in states with the strongest rights. Conclusions: State-level reproductive rights may influence likelihood of adverse birth outcomes among women residents. (C) 2016 Jacobs Institute of Women's Health. Published by Elsevier Inc.", "journal": "WOMENS HEALTH ISSUES", "category": "Public, Environmental & Occupational Health; Women's Studies", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000391897400009", "keywords": "Spatiotemporal dynamic analysis; Forest ecosystem service evaluation; Big data", "title": "Spatiotemporal dynamic analysis of forest ecosystem services using \"big data\": A case study of Anhui province, central-eastern China", "abstract": "Although a large number of reviews have studied different scales of forest ecosystem services, few have concentrated on accounting the spatiotemporal dynamic changes of forest ecosystem services in a region. To make a more accurate accounting, this paper selects intense data sets that assess the physical quantity and value of forest ecosystem services in Anhui provinces from 2009 to 2014. Regulating and supporting services such as water conservation, carbon sequestration and oxygen release, soil conservation and air purification are included in the paper as variables. Optimize analysis was further conducted in order to identify the spatial and temporal heterogeneity during the study period. We establish a dynamic ecosystem services assessment model with the coefficient of spatial heterogeneity and coefficients of scarce resources and social development to analyze the trend of the value in Anhui province. This model is designed for comparisons of forest services in different scales in Anhui province. The results demonstrate that the total value of forest ecosystem services is beyond the previous estimation while using spatiotemporal visualization. The implications are that the evaluation of forest ecosystem services should be measured more accurately with the help of big data in the future. (C) 2016 Elsevier Ltd. All rights reserved.", "journal": "JOURNAL OF CLEANER PRODUCTION", "category": "Green & Sustainable Science & Technology; Engineering, Environmental; Environmental Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000397459800009", "keywords": "partial delamination; finite elements; frequency shift; fractal dimension; damage detection", "title": "Detecting width-wise partial delamination in the composite beam using generalized fractal dimension", "abstract": "Generalized fractal dimension is used to detect the presence of partial delamination in a composite laminated beam. The effect of boundary conditions and location of delamination on the fractal dimension curve is studied. Appropriability of higher mode shape data for detection of delamination in the beam is evaluated. It is shown that fractal dimension measure can be used to detect the presence of partial delamination in composite beams. It is found that the torsional mode shape is well suited for delamination detection in beams. First natural frequency of delaminated beam is found to be higher than the healthy beam for certain small and partial width delaminations and some boundary conditions. An explanation towards this counter intuitive phenomenon is provided.", "journal": "SMART STRUCTURES AND SYSTEMS", "category": "Engineering, Civil; Engineering, Mechanical; Instruments & Instrumentation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000397875500012", "keywords": "birth defects; spina bifida; neural tube defects; occupational paternal and parental exposures; pesticides; insecticides; fungicides; herbicides", "title": "Inducible microRNA-214 contributes to the suppression of NF-kappa B-mediated inflammatory response via targeting myd88 gene in fish", "abstract": "Upon recognition of bacterial pathogens by pattern recognition receptors, cells are activated to produce pro-inflammatory cytokines and type I IFN by multiple signaling pathways. Every step of the process must be precisely regulated to prevent dysregulation. MicroRNAs (miRNAs) have been shown to be important regulators with profound effects on inflammatory response. Nevertheless, the miRNA-mediated regulatory mechanism remains unclear in fish species. Here, we addressed the role of miiuy croaker miR-214 in the bacteria triggered inflammatory response. miR-214 could significantly be up-regulated by Vibro harveyi and LPS stimulation. Up-regulating miR-214 subsequently inhibits the production of inflammatory cytokines by targeting myd88 to avoid excessive inflammation. Moreover, the negative regulatory mechanism of miR-214 has been demonstrated to be via the myd88-mediated NF-kappa B pathway. This is the first to focus on miR-214 acting as the negative regulator involved in the bacteria-triggered inflammatory response and thus may provide knowledge on the host-cell regulator responses to microbial infection.", "journal": "JOURNAL OF BIOLOGICAL CHEMISTRY", "category": "Biochemistry & Molecular Biology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000416702100004", "keywords": "Step-stress accelerated life test; constrained randomization; generalized linear mixed model; adaptive Gaussian quadrature; integrated nested Laplace approximation", "title": "Data analysis of step-stress accelerated life tests with heterogeneous group effects", "abstract": "Step-Stress Accelerated Life Testing (SSALT) is a special type of experiment that tests a product's lifetime with time-varying stress levels. Typical testing protocols deployed in SSALTs cannot implement complete randomization of experiments; instead, they often result in grouped structures of experimental units and, thus, correlated observations. In this article, we propose a Generalized Linear Mixed Model (GLMM) approach to take into account the random group effect in SSALT. Failure times are assumed to be exponentially distributed under any stress level. Two parameter estimation methods, Adaptive Gaussian Quadrature (AGQ) and Integrated Nested Laplace Approximation (INLA), are introduced. A simulation study is conducted to compare the proposed random effect model with the traditional model, which pools data groups together, and with the fixed effect model. We also compare AGQ and INLA with different priors for parameter estimation. Results show that the proposed model can validate the existence of group-to-group variation. Lastly, the GLMM model is applied to a real data and it shows that disregarding experimental protocols in SSALT may result in large bias in the estimation of the effect of stress variable.", "journal": "IISE TRANSACTIONS", "category": "Engineering, Industrial; Operations Research & Management Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000387333600003", "keywords": "HEMT; field plates; InAlN; GaN; electrical properties", "title": "Impact of dual field plates on drain current degradation in InAlN/AlN/GaN HEMTs", "abstract": "InAlN/AlN/GaN HEMTs with both source and gate dual field-plates (FPs) are proposed. To investigate the influence of dual FPs on the devices characteristics, two types of devices with gate FP and without FPs were fabricated and tested. The devices were subjected to different kinds of short-term direct current bias (DC-bias) stress conditions. The results show that after the off-state bias stress, the drain current reduction rate of the devices with dual FPs was 3.32%, which was less than that in both devices with a gate FP of 7.57% and devices without FPs of 14.63%. The current collapse of the HEMTs with dual FPs was relieved due to the increase of electric field uniformity between the gate and drain. The degradation of the output characteristics was more serious after the on-state bias stress. In addition, the effects of bias stress on the transfer characteristics of the devices were studied, and the trapping processes under different stress conditions in the devices were discussed.", "journal": "SEMICONDUCTOR SCIENCE AND TECHNOLOGY", "category": "Engineering, Electrical & Electronic; Materials Science, Multidisciplinary; Physics, Condensed Matter", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000396175600008", "keywords": "cfs evaluator; cross-validation; data mining; forecasting; time series; wrapper evaluator", "title": "Forecasting sea level changes applying data mining techniques to the Cristobal Bay time series, Panama", "abstract": "Time series forecasting using data mining models applied to various time sequence data of a wide variety of domains has been well documented. In this work, time series of water level data recorded every hour at 'Cristobal Bay' in Panama during the years 1909-1980 are employed to construct a model(s) that can be suitable for predicting changes in sea level patterns. Four time lag assemblages of variable combinations of the time series information are fully explored to identify the optimal combinations for the dataset using a data mining tool. The results, based on the assessment using time series of Cristobal data, show that in general using cross-validation and a longer time lag period of the time series led to more accurate forecasting of the model than that of a shorter lag period of the time series. The study also suggests that data mining techniques using cross-validation and the aid of an attribute evaluator can be effectively used in modeling time series for changes in sea level at coastal areas, and changes in ecosystems that by their nature are characterized by nonlinearity and presentation of chaotic climatic changes in their physical behavior.", "journal": "JOURNAL OF WATER AND CLIMATE CHANGE", "category": "Water Resources", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000396303600013", "keywords": "Chronic conditions; community-based; depression; low-and middle-income countries; psychotic experience", "title": "Epidemiology of depression with psychotic experiences and its association with chronic physical conditions in 47 low- and middle-income countries", "abstract": "Background. The co-existence of depression and psychotic experiences (PEs) is associated with more pronounced adverse health outcomes compared to depression alone. However, data on its prevalence and correlates are lacking in the general adult population, and there is no published data on its association with chronic physical conditions. Method. Cross-sectional, community-based data from 201 337 adults aged >= 18 years from 47 low-and middle-income countries from the World Health Survey were analyzed. The presence of past 12-month PE and DSM-IV depression was assessed with the Composite International Diagnostic Interview (CIDI). Information on six chronic medical conditions (chronic back pain, edentulism, arthritis, angina, asthma, diabetes) were obtained by self-report. Multivariable logistic regression analysis was performed. Results. The crude overall prevalence of co-morbid depression/PEs was 2.5% [95% confidence interval (CI) 2.3-2.7%], with the age-and sex-adjusted prevalence ranging from 0.1% (Sri Lanka, Vietnam) to 9.03% (Brazil). Younger age, urban setting, current smoking, alcohol consumption, and anxiety were significant correlates of co-existing depression/PEs. Co-occurring depression/PEs was associated with significantly higher odds for arthritis, angina, and diabetes beyond that of depression alone after adjusting for sociodemographics, anxiety, and country, with odds ratios (depression/ PEs v. depression only) being: arthritis 1.30 (95% CI 1.07-1.59, p = 0.0086); angina 1.40 (95% CI 1.18-1.67, p = 0.0002); diabetes 1.65 (95% CI 1.21-2.26, p = 0.0017). Conclusions. The prevalence of co-existing depression/PEs was non-negligible in most countries. Our study suggests that when depression/PE or a chronic condition (e. g. arthritis, angina, diabetes) is detected, screening for the other may be important to improve clinical outcomes.", "journal": "PSYCHOLOGICAL MEDICINE", "category": "Psychology, Clinical; Psychiatry; Psychology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000385595000022", "keywords": "Sunspot number; Precipitation index", "title": "Influence of solar activity on the precipitation in the North-central China", "abstract": "The time series of sunspot number and the precipitation in the north-central China (108 degrees similar to 115 degrees E, 33 degrees similar to 41 degrees N) over the past 500 years (1470-2002) are investigated, through periodicity analysis, cross wavelet transform and ensemble empirical mode decomposition analysis. The results are as follows: the solar activity periods are determined in the precipitation time series of weak statistical significance, but are found in decomposed components of the series with statistically significance; the Quasi Biennial Oscillation (QBO) is determined to significantly exist in the time series, and its action on precipitation is opposite to the solar activity; the sun is inferred to act on precipitation in two ways, with one lagging the other by half of the solar activity period. (C) 2016 Elsevier B.V. All rights reserved.", "journal": "NEW ASTRONOMY", "category": "Astronomy & Astrophysics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000394436700002", "keywords": "Damping; exponential stability; generalized thermoelasticity; Riesz basis", "title": "Spectral analysis of thermoelastic systems under nonclassical thermal models", "abstract": "We study some spectral properties of the solutions to generalized thermoelastic systems under Lord-Shulman, Green-Lindsay, and Green-Naghdi of type-II models. First, we prove that the linear operator of each model has compact resolvent and generates a C-0-semigroup in an appropriate Hilbert space. We also show that there is a sequence of generalized eigenfunctions of the linear operator that forms a Riesz basis. By a detailed spectral analysis, we obtain the expressions of the spectrum and we deduce that the spectrum-determined growth condition holds. Therefore, if the imaginary axis is not an asymptote of the spectrum, we prove that the energy of each model decays exponentially to a rate determined explicitly by the physical parameters. Finally, some simulations are given for each model to support our results.", "journal": "JOURNAL OF THERMAL STRESSES", "category": "Thermodynamics; Mechanics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000396184900013", "keywords": "Space greenhouse; Light spectrum; Pulsed light; Optimality criterion; Regression analysis; A fractional multiple-factor experiment", "title": "CO2 emissions, energy consumption, economic growth, and financial development in GCC countries: Dynamic simultaneous equation models", "abstract": "The aim of this work were to choose a quantitative optimality criterion for estimating the quality of plant LED lighting regimes inside space greenhouses and to construct regression models of crop productivity and the optimality criterion depending on the level of photosynthetic photon flux density (PPFD), the proportion of the red component in the light spectrum and the duration of the duty cycle (Chinese cabbage Brassica. hinensis L. as an example). The properties of the obtained models were described in the context of predicting crop dry weight and the optimality criterion behavior when varying plant lighting parameters. Results of the fractional 3-factor experiment demonstrated the share of the PPFD level participation in the crop dry weight accumulation was 84.4% at almost any combination of other lighting parameters, but when PPFD value increased up to 500 mu mol m(-2) s(-1) the pulse light and supplemental light from red LEDs could additionally increase crop productivity. Analysis of the optimality criterion response to variation of lighting parameters showed that the maximum coordinates were the following: PPFD = 500 mu mol m(-2) s(-1), about 70%-proportion of the red component of the light spectrum (PPFDLEDred/PPFDLEDwhite = 1.5) and the duty cycle with a period of 501 mu s. Thus, LED crop lighting with these parameters was optimal for achieving high crop productivity and for efficient use of energy in the given range of lighting parameter values. (C) 2016 The Committee on Space Research (COSPAR). Published by Elsevier Ltd. All rights reserved.", "journal": "RENEWABLE & SUSTAINABLE ENERGY REVIEWS", "category": "Green & Sustainable Science & Technology; Energy & Fuels", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000399762700013", "keywords": "Cerro Prieto Geothermal Field; ground deformation; afterslip; El Mayor-Cucapah earthquake; subsidence; DInSAR", "title": "Subsidence at Cerro Prieto Geothermal Field and postseismic slip along the Indiviso fault from 2011 to 2016 RADARSAT-2 DInSAR time series analysis", "abstract": "We present RADARSAT-2 Differential Interferometric Synthetic Aperture Radar (DInSAR) observations of deformation due to fluid extraction at the Cerro Prieto Geothermal Field (CPGF) and afterslip on the 2010 M7.2 El Mayor-Cucapah (EMC) earthquake rupture during 2011-2016. Advanced multidimensional time series analysis reveals subsidence at the CPGF with the maximum rate greater than 100mm/yr accompanied by horizontal motion (radial contraction) at a rate greater than 30mm/yr. During the same time period, more than 30mm of surface creep occurred on the Indiviso fault ruptured by the EMC earthquake. We performed inversions of DInSAR data to estimate the rate of volume changes at depth due to the geothermal production at the CPGF and the distribution of afterslip on the Indiviso fault. The maximum coseismic slip due to the EMC earthquake correlates with the Coulomb stress changes on the Indiviso fault due to fluid extraction at the CPGF. Afterslip occurs on the periphery of maximum coseismic slip areas. Time series analysis indicates that afterslip still occurs 6years after the earthquake.", "journal": "GEOPHYSICAL RESEARCH LETTERS", "category": "Geosciences, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000393077900012", "keywords": "Cholesterol; injury; ischaemic stroke; PCSK9", "title": "Decreased serum PCSK9 levels after ischaemic stroke predict worse outcomes", "abstract": "Background Soluble mediators have been investigated to predict the prognosis of acute ischaemic stroke (AIS). Among them, proprotein convertase subtilisin/kexin type 9 (PCSK9) might have both clinical and pathophysiological relevance. Materials and methods All available serum samples from a cohort of patients with first AIS (n = 72) were tested for PCSK9 and included in this substudy analysis. The primary endpoint investigated the predictive value of early PCSK9 level variations (DPCSK9) from AIS onset to day 7 or from day 1 to day 7, towards a 90-day outcome by modified Rankin Scale (mRS). The secondary endpoint explored the association between DPCSK9 and the risk of major adverse cardiovascular events (MACEs). Results Decreased serum PCSK9 levels at days 1 and 7 were associated with poor clinical outcomes at day 90. At the cut-off point identified by ROC curve analysis (-61.28 ng/mL), DPCSK9 day 7-day 1 predicted a poor mRS at day 90 after AIS. DPCSK9 day 7-day 1 <= -61.28 ng/mL was associated with an increased rate of MACEs. Conclusion A decrease in PCSK9 levels was a predictor for poor outcome and increased MACEs after AIS. Additional studies targeting post-AIS PCSK9 levels and activity are required to clarify the prognostic and pathophysiological relevance of PCSK9 after AIS.", "journal": "EUROPEAN JOURNAL OF CLINICAL INVESTIGATION", "category": "Medicine, General & Internal; Medicine, Research & Experimental", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000388065100022", "keywords": "DC-DC converter; digital controller; fast transient response; SSA-PID algorithm", "title": "Analysis and Design of a Separate Sampling Adaptive PID Algorithm for Digital DC-DC Converters", "abstract": "Based on the conventional PID algorithm and the adaptive PID (AD-PID) algorithm, a separate sampling adaptive PM (SSA-PID) algorithm is proposed to improve the transient response of digitally controlled DC-DC converters. The SSA-PID algorithm, which can be divided into an oversampled adaptive P (AD-P) control and an adaptive ID (AD -ID) control, adopts a higher sampling frequency for AD-P control and a conventional sampling frequency for AD-ID control. In addition, it can also adaptively adjust the PID parameters (i.e. K-p, K-i and K-d) based on the system state. Simulation results show that the proposed algorithm has better line transient and load transient responses than the conventional PM and AD-PID algorithms. Compared with the conventional PID and AD-PID algorithms, the experimental results based on a FPGA indicate that the recovery time of the SSA-PID algorithm is reduced by 80% and 67% separately, and that overshoot is decreased by 33% and 12% for a 700mA load step. Moreover, the SSA-PID algorithm can achieve zero overshoot during startup.", "journal": "JOURNAL OF POWER ELECTRONICS", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000407567700001", "keywords": "Traffic network modeling; transportation network simulation; simulation model; multiagent system", "title": "Testing the Effectiveness of Cognitive Analytic Therapy for Hypersexuality Disorder: An Intensive Time-Series Evaluation", "abstract": "The evidence base for treatment of hypersexuality disorder (HD) has few studies with appropriate methodological rigor. This study therefore conducted a single case experiment of cognitive analytic therapy (CAT) for HD using an A/B design with extended follow-up. Cruising, pornography usage, masturbation frequency and associated cognitions and emotions were measured daily in a 231-day time series. Following a three-week assessment baseline (A: 21days), treatment was delivered via outpatient sessions (B: 147days), with the follow-up period lasting 63days. Results show that cruising and pornography usage extinguished. The total sexual outlet score no longer met caseness, and the primary nomothetic hypersexuality outcome measure met recovery criteria. Reduced pornography consumption was mediated by reduced obsessionality and greater interpersonal connectivity. The utility of the CAT model for intimacy problems shows promise. Directions for future HD outcome research are also provided.", "journal": "JOURNAL OF SEX & MARITAL THERAPY", "category": "Psychology, Clinical; Family Studies", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000389909100160", "keywords": "Gluten-friendly; SEM; Immunofluorescent microscopy; Antigenic epitopes", "title": "Impact of gluten-friendly (TM) technology on wheat kernel endosperm and gluten protein structure in seeds by light and electron microscopy", "abstract": "The main aim of this paper was to assess the impact of Gluten-Friendly (TM) (GF) technology (Italian priority patent n degrees 102015000084813 filed on 17th December 2015) on wheat kernel endosperm morphology and gluten protein structure, using SEM, light and immunofluorescent microscopy. Microscopy was combined with immunodetection with specific antibodies for gliadins, gamma-gliadins, LMW subunits and antigenic epitopes to gain a better understanding of the technology at a molecular level. The results showed significant changes to gluten proteins after GF treatment; cross-reactivity towards the antibodies recognizing almost the entire range of gluten proteins as well as the antigenic epitopes through the sequences QQSF, QQSY, PEQPFPQGC and QQPFP was significantly reduced. The present study confirms the results from our previous work and shows, for the first time, the mechanism by which a chemical-physical treatment abolishes the antigenic capacity of gluten. (C) 2016 Elsevier Ltd. All rights reserved.", "journal": "FOOD CHEMISTRY", "category": "Chemistry, Applied; Food Science & Technology; Nutrition & Dietetics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000402671400008", "keywords": "Stand structure; forest compartment; indicators; adaptive forest management; Italy", "title": "Toward sustainable forest management indicators? A data mining approach to evaluate the impact of silvicultural practices on stand structure", "abstract": "Indicators are increasingly required to support a fine-tuning between sustainable forestry and multiple environmental targets. A data mining strategy was implemented in this study to assess the overall impact of traditional and innovative silviculture on stand structure in a sample of beech forests with varying dominant age, management history and stand structure in Italy. Harvesting intensity and stand sensitivity to treatment were investigated using a principal component analysis (PCA) run on a set of dendrometric and stand-structure variables measured before and after practice implementation at the scale of forest compartment. The PCA decomposed the overall impact of silviculture on forest structure in two manipulative effects: (i) structural changes between control and treatments, and (ii) the net manipulative effect of innovative versus traditional treatment. Our approach informs the sustainable management of forests, outlining between-site differences in stand structure and identifying a diversity gradient shaped by silvicultural practices. Multivariate analysis of forest indicators following practice's implementation is a promising tool to design innovative silviculture coherent with conservation of forests' structural diversity.", "journal": "INTERNATIONAL JOURNAL OF SUSTAINABLE DEVELOPMENT AND WORLD ECOLOGY", "category": "Green & Sustainable Science & Technology; Ecology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000392921700008", "keywords": "compressed sensing (CS); inverse synthetic aperture radar (ISAR); log-sum", "title": "NEW COMPRESSED SENSING ISAR IMAGING ALGORITHM BASED ON LOG-SUM MINIMIZATION", "abstract": "To improve the performance of inverse synthetic aperture radar (ISAR) imaging based on compressed sensing (CS), a new algorithm based on log-sum minimization is proposed. A new interpretation of the algorithm is also provided. Compared with the conventional algorithm, the new algorithm can recover signals based on fewer measurements, in looser sparsity condition, with smaller recovery error, and it has obtained better sinusoidal signal spectrum and imaging result for real ISAR data. Therefore, the proposed algorithm is a promising imaging algorithm in CS ISAR.", "journal": "Journal of Electrical Engineering-Elektrotechnicky Casopis", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000389759500007", "keywords": "Vibrational spectroscopy; DFT calculations; organic arsenate; NLO; AC conductivities; dielectric measurements; proton hopping mechanism", "title": "Method for hue plane preserving color correction", "abstract": "Hue plane preserving color correction (HPPCC), introduced by Andersen and Hardeberg [Proceedings of the 13th Color and Imaging Conference (CIC) (2005), pp. 141-146], maps device-dependent color values (RGB) to colorimetric color values (XY Z) using a set of linear transforms, realized by white point preserving 3 x 3 matrices, where each transform is learned and applied in a subregion of color space, defined by two adjacent hue planes. The hue plane delimited subregions of camera RGB values are mapped to corresponding hue plane delimited subregions of estimated colorimetric XYZ values. Hue planes are geometrical half-planes, where each is defined by the neutral axis and a chromatic color in a linear color space. The key advantage of the HPPCC method is that, while offering an estimation accuracy of higher order methods, it maintains the linear colorimetric relations of colors in hue planes. As a significant result, it therefore also renders the colorimetric estimates invariant to exposure and shading of object reflection. In this paper, we present a new flexible and robust version of HPPCC using constrained least squares in the optimization, where the subregions can be chosen freely in number and position in order to optimize the results while constraining transform continuity at the subregion boundaries. The method is compared to a selection of other state-of-the-art characterization methods, and the results show that it outperforms the original HPPCC method. Published by The Optical Society under the terms of the Creative Commons Attribution 4.0 License.", "journal": "JOURNAL OF THE OPTICAL SOCIETY OF AMERICA A-OPTICS IMAGE SCIENCE AND VISION", "category": "Optics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000381980700002", "keywords": "Graphic Processing Unit (GPU); Parallel Processing; Heterogeneous Computing; Optimization; Extensive Cancellation Algorithm (ECA)", "title": "A Parallel Implementation of Extensive Cancellation Algorithm (ECA) for Passive Bistatic Radar (PBR) on a GPU", "abstract": "Passive Bistatic Radar (PBR) receives high interest because of exploiting existing signals of opportunity from the surrounding environment such as TV and Radio signals. It reduces the pollution and the interference since it doesn't require a dedicated transmitter. However, PBR needs a novel algorithm to detect the target accurately since the RF transmitted signals is not under the control of the radar designer and has a variable structure of the ambiguity function. So, novel adaptive cancellation filters such as Extensive Cancellation Algorithm (ECA) was designed which has proven to detect the target accurately. However, ECA is a computationally intensive algorithm. This work involves transformation of ECA by exploring opportunities of any computation and storage that can be eliminated. ECA algorithm also has been implemented on GPU by exploiting parallel and pipelining approaches. The computation time of our transformed algorithm has improved by a factor of 3.8. Also, the achieved speed-up of GPU over our sequentially transformed algorithm is improved by up to 20.8.", "journal": "JOURNAL OF SIGNAL PROCESSING SYSTEMS FOR SIGNAL IMAGE AND VIDEO TECHNOLOGY", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000399507300017", "keywords": "Mixed convection; Nanoparticles; Variable thermal conductivity; Heat source/sink; Stretching cylinder", "title": "Effectiveness of magnetic nanoparticles in radiative flow of Eyring-Powell fluid", "abstract": "The present work studies the MHD two-dimensional flow of Eyring-Powell fluid with thermophoresis and Brownian motion. Flow caused is due to convection type stretching cylinder. Thermal radiation and heat source/sink phenomenon characterizes the heat transfer process. Computations for strong nonlinear systems are presented after non-dimensionalization. Thermal and nanoparticles concentration fields for nonlinear boundary value problems are calculated and discussed. The velocity, temperature and concentration gradients are also evaluated. The major outcome of the present study is that Brownian motion and thermophoretic phenomenon boosts temperature however nanoparticles concentration distribution has opposite behavior for these phenomena. Moreover thermal and concentration Biot numbers have similar impacts on temperature and concentration. (C) 2017 Elsevier B.V. All rights reserved.", "journal": "JOURNAL OF MOLECULAR LIQUIDS", "category": "Chemistry, Physical; Physics, Atomic, Molecular & Chemical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000409101400024", "keywords": "Absolute value equation; Picard algorithm; matrix splitting iteration method; conjugate gradient method", "title": "Picard splitting method and Picard CG method for solving the absolute value equation", "abstract": "In this paper, we combine matrix splitting iteration algorithms, such as, Jacobi, SSOR or SAOR algorithms with Picard method for solving absolute value equation. Then, we propose Picard CG for solving the absolute value equation. We discuss the convergence of those methods we proposed. At last, some examples are provided to illustrate the efficiency and validity of methods that we present. (C) 2017 All rights reserved.", "journal": "JOURNAL OF NONLINEAR SCIENCES AND APPLICATIONS", "category": "Mathematics, Applied; Mathematics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000397127900010", "keywords": "global sensitivity analysis; meta-heuristic; single-solution; sensitivity indicator; truss structures", "title": "Optimal design of truss structures using a new optimization algorithm based on global sensitivity analysis", "abstract": "Global sensitivity analysis (GSA) has been widely used to investigate the sensitivity of the model output with respect to its input parameters. In this paper a new single-solution search optimization algorithm is developed based on the GSA, and applied to the size optimization of truss structures. In this method the search space of the optimization is determined using the sensitivity indicator of variables. Unlike the common meta-heuristic algorithms, where all the variables are simultaneously changed in the optimization process, in this approach the sensitive variables of solution are iteratively changed more rapidly than the less sensitive ones in the search space. Comparisons of the present results with those of some previous population-based meta-heuristic algorithms demonstrate its capability, especially for decreasing the number of fitness functions evaluations, in solving the presented benchmark problems.", "journal": "STRUCTURAL ENGINEERING AND MECHANICS", "category": "Engineering, Civil; Engineering, Mechanical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000386071900027", "keywords": "Visual scanpath; Visual attention; Inhibition of return; Eye movements; Saliency", "title": "A comparison of two photosynthesis parameterization schemes for an alpine meadow site on the Qinghai-Tibetan Plateau", "abstract": "Photosynthesis is a very important sub-process in the carbon cycle and is a crucial sub-modular function in carbon cycle models. In this study, two typical photosynthesis parameterization schemes were compared based on meteorological and eddy covariance (EC) observations at an alpine meadow site. The photosynthesis model parameters were estimated using the Markov Chain Monte Carlo (MCMC) method. The results indicated that the Farquhar-conductance coupled model better predicted the gross primary production (GPP) for the alpine meadow ecosystem at an hourly time scale than the light use efficiency (LUE) model even though the Farquhar-conductance coupled model has a lower computational efficiency than the LUE model. Compared to the Ball-Woodrow-Berry (BWB) stomatal conductance model, coupling the Farquhar model with the Leuning stomatal conductance model more accurately simulated GPP.", "journal": "THEORETICAL AND APPLIED CLIMATOLOGY", "category": "Meteorology & Atmospheric Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000397253900033", "keywords": "Intracerebral hemorrhage; recurrent; incidence; risk factors", "title": "Incidence of Recurrent Intracerebral Hemorrhages in a Multiethnic South Asian Population", "abstract": "Introduction: Spontaneous primary intracerebral hemorrhage (ICH) accounts for approximately 25% of all strokes in Singapore. Incidence of recurrent ICH is not well studied, and previous studies have reported inconsistent findings in the rate and risk factors associated with ICH recurrences. We aimed to study the incidence of recurrent ICHs in Singapore and to identify the associated risk factors as well as pattern of ICH recurrence. Methods: A retrospective review of all consecutive admissions for intracerebral hemorrhage at the National Neuroscience Institute between January 2006 and November 2013 was performed. Imaging and computerized clinical records were reviewed. The demographic, clinical, and radiological characteristics of index and recurrent ICH were compared. Univariate analysis was performed using chi-square and Student's t-test, and logistic regression was used to analyze the predictors of ICH recurrence. Results: In total, 1708 patients who survived the index ICH beyond 14 days were followed up for 6398 person-years. Sixty patients developed 68 recurrences of ICH, giving rise to an annual incidence rate of ICH recurrence of 1.1%. A history of previous ischemic stroke (P=.001) and index lobar location of ICH (P=.004) were significantly associated with the occurrence of ICH recurrences on multivariate analysis. The most common pattern on ICH recurrence was ganglionic-ganglionic (44.1%), followed by lobar-lobar (17.6%). Overall mortality of recurrent ICH was 17.6%. Conclusion: The average annual incidence rate of primary ICH recurrence in Singapore is 1.1%, and is associated with previous ischemic stroke and lobar location of index ICH.", "journal": "JOURNAL OF STROKE & CEREBROVASCULAR DISEASES", "category": "Neurosciences; Peripheral Vascular Disease", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000399133400033", "keywords": "variational mode decomposition (VMD); cross correlation coefficient; parameter estimation; rolling bearing", "title": "Adaptive estimation of VMD modes number based on cross correlation coefficient", "abstract": "The variational mode decomposition (VMD) proposed recently is a kind of time-frequency signal analysis method. VMD has some advantages on signal decomposition such as high precision and noise robustness, but its serious shortcoming is that the number of modes (kappa) should be given in advance. And if the number is chosen inappropriately, VMD will lead to larger decomposition error. In this paper, the VMD method is introduced and the over-and under-segment characters of VMD are discussed. The cross correlation coefficients can express the similarity between the two signals. Cross correlation coefficients among VMD components and the original signal are used to judge whether over-segment takes place. As a result, the estimation method of VMD parameter kappa is proposed. Based on the method, the tri-harmonic signal and the vibration signals of ball bearings are analyzed in detail. The results show that the proposed method is feasible and effective.", "journal": "JOURNAL OF VIBROENGINEERING", "category": "Engineering, Biomedical; Engineering, Mechanical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000402713800016", "keywords": "Preterm premature rupture of membranes (PPROM); neonatal intensive care unit (NICU); neonatal morbidity; neonate survival", "title": "Major determinants of survival and length of stay in the neonatal intensive care unit of newborns from women with premature preterm rupture of membranes", "abstract": "Objective: To assess the predictors of outcome in terms of length of stay in the neonatal intensive care unit (NICU) and survival of neonates from women with preterm premature rupture of membranes (PPROM).Methods: A population-based retrospective study including 331 singleton pregnant women with PPROM at 24-34 gestational weeks between January 2013 and December 2015 was conducted. Gestational age at delivery, birth weight, route of delivery, newborn gender, maternal age, oligohydramnios, premature retinopathy (ROP), necrotising enterocolitis (NEC), sepsis, fetal growth retardation (FGR), intracranial hemorrhagia (ICH), bronchopulmonary dysplasia (BPD), respiratory distress syndrome (RDS), primary pulmonary hypertension (PPH), congenital cardiac disease (CCD), patent ductus arteriosus (PDA), use of cortisol (betamethasone) and maternal complications including gestational diabetes, preeclampsia and chorioamnionitis were used to predict neonatal outcomes in terms of length of stay in the NICU and survival.Results: In linear regression analyses, birth weight, ROP, CCD, BPD, PDA, NEC and preeclampsia were significant confounders for length of stay in the NICU. Among them, birth weight was the most powerful confounder for prolongation of the NICU stay (t: -6.43; p<0.001).In multivariate logistic regression analyses, birth weight, PDA, ROP and PPH were significantly correlated with neonatal survival. PPH was the most powerful confounder in neonatal survival (: 7.22; p=0.005).Conclusion: Prematurity-related complications are the most important problems for which precautions should be taken. Therefore, premature deliveries should be avoided to prevent infection and to prolong the latent period in cases of PPROM in order to decrease prematurity-related outcomes.", "journal": "JOURNAL OF MATERNAL-FETAL & NEONATAL MEDICINE", "category": "Obstetrics & Gynecology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000390551000007", "keywords": "Chronic hepatitis B; Hepatitis B virus; Metabolic syndrome", "title": "Hepatitis B e antigen-positive and high levels of alanine aminotransferase are associated with prevalence of metabolic syndrome in chronic HBV patients", "abstract": "Objective: The interactions between hepatitis B virus (HBV) infection and metabolic syndrome (MS) have not been elucidated. This study was aimed to investigate the relationship between metabolic profile and HBV infection. Methods: A retrospective cross-sectional study including patients infected by HBV (HBV group, n = 121) and healthy volunteers (control group, n = 263) was conducted, serum HBV viral load and markers, serum alanine aminotransferase (ALT) levels and MS were analyzed. Factors associated with prevalence of MS were explored with multivariate adjusted logistic regression analyses. Results: The prevalence of MS was 9.9% in HBV infected patients and 19.4% in controls (p = 0.011). Factors associated with the prevalence of MS were (odds ratio, 95% confidence interval, p value): hepatitis B e antigen (HBeAg) positive (0.368, 0.107-0.653, 0.008) and high levels of ALT (0.183, 0.120-0.268, <0.001) in HBV patients. But clinical and virological factors (including age, HBV DNA level, male gender, BMI, and fatty liver) were not found to be associated with prevalence of MS in HBV patients who were HBeAg positive with high levels of ALT. Conclusion: These findings suggest that HBeAg positive and high levels of ALT are independently associated with lower prevalence of MS in HBV patients. But HBV DNA may not have impact on the lipid metabolism. HBV-related immune reactions may play a certain role in the mechanism of MS. (C) 2015 Asia Oceania Association for the Study of Obesity. Published by Elsevier Ltd. All rights reserved.", "journal": "OBESITY RESEARCH & CLINICAL PRACTICE", "category": "Endocrinology & Metabolism; Nutrition & Dietetics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000387201000056", "keywords": "Juvenile rheumatic diseases; Caregiver demand; Illness intrusiveness; Parenting stress; Psychological distress; Parent-child adjustment", "title": "Long QT Syndrome: A Genetic Test in the Context of a Diagnostic Algorithm", "abstract": "To examine illness intrusiveness and parenting stress as potential serial mediators in the relationship between parents' illness-specific caregiver demand and psychological distress in parents of youth with juvenile rheumatic diseases (JRDs). Sixty-eight caregivers of youth diagnosed with a JRD completed measures of illness-specific caregiver demand (CD), illness intrusiveness (IIS-P), general parenting stress (PSI-SF), and psychological distress (BSI). Bootstrap regression analyses revealed a significant CD -> IIS-P -> PSI-SF -> BSI complex serial mediation path. Results also revealed significant CD -> IIS-P -> BSI and CD -> PSI-SF -> BSI simple indirect effects. Results provide support for examining both illness-specific (caregiver demand, illness intrusiveness) and illness non-specific (general parenting stress) parent appraisal variables in determining psychological distress among parents of youth with JRDs. Additionally, results provide clarification regarding the mechanisms that may drive psychological distress in parents, and suggest directions for parent-targeted interventions with this population.", "journal": "JOURNAL OF MOLECULAR DIAGNOSTICS", "category": "Pathology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000391739500045", "keywords": "Compressed sensing; inverse problems; state estimation; system recovery; quantum mechanics", "title": "Improving Compressed Sensing With the Diamond Norm", "abstract": "In low-rank matrix recovery, one aims to reconstruct a low-rank matrix from a minimal number of linear measurements. Within the paradigm of compressed sensing, this is made computationally efficient by minimizing the nuclear norm as a convex surrogate for rank. In this paper, we identify an improved regularizer based on the so-called diamond norm, a concept imported from quantum information theory. We show that-for a class of matrices saturating a certain norm inequality-the descent cone of the diamond norm is contained in that of the nuclear norm. This suggests superior reconstruction properties for these matrices. We explicitly characterize this set of matrices. Moreover, we demonstrate numerically that the diamond norm indeed outperforms the nuclear norm in a number of relevant applications: These include signal analysis tasks, such as blind matrix deconvolution or the retrieval of certain unitary basis changes, as well as the quantum information problem of process tomography with random measurements. The diamond norm is defined for matrices that can be interpreted as order-4 tensors and it turns out that the above condition depends crucially on that tensorial structure. In this sense, this paper touches on an aspect of the notoriously difficult tensor completion problem.", "journal": "IEEE TRANSACTIONS ON INFORMATION THEORY", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000397832000017", "keywords": "Development; Acoustic-to-articulatory inversion; Articulation; Speech", "title": "Acquisition of vowel articulation in childhood investigated by acoustic-to-articulatory inversion", "abstract": "While the acoustical features of speech sounds in children have been extensively studied, limited information is available as to their articulation during speech production. Instead of directly measuring articulatory movements, this study used an acoustic-to-articulatory inversion model with scalable vocal tract size to estimate developmental changes in articulatory state during vowel production. Using a pseudo-inverse Jacobian matrix of a model mapping seven articulatory parameters to acoustic ones, the formant frequencies of each vowel produced by three Japanese children over time at ages between 6 and 60 months were transformed into articulatory parameters. We conducted the discriminant analysis to reveal differences in articulatory states for production of each vowel. The analysis suggested that development of vowel production went through gradual functionalization of articulatory parameters. At 6-9 months, the coordination of position of tongue body and lip aperture forms three vowels: front, back, and central. At 10-17 months, recruitments of jaw and tongue apex enable differentiation of these three vowels into five. At 18 months and older, recruitment of tongue shape produces more distinct vowels specific to Japanese. These results suggest that the jaw and tongue apex contributed to speech production by young children regardless of kinds of vowel. Moreover, initial articulatory states for each vowel could be distinguished by the manner of coordination between lip and tongue, and these initial states are differentiated and refined into articulations adjusted to the native language over the course of development. (C) 2017 Elsevier Inc. All rights reserved.", "journal": "INFANT BEHAVIOR & DEVELOPMENT", "category": "Psychology, Developmental", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000399322900017", "keywords": "acute myocardial infarction; chronic obstructive pulmonary disease; mortality; temporal trends; beta-blockers", "title": "Temporal Trends in Treatment and Outcomes of Acute Myocardial Infarction in Patients With Chronic Obstructive Pulmonary Disease: A Nationwide Population-Based Observational Study", "abstract": "Background-Acute myocardial infarction is a major cause of hospitalization and death in patients with chronic obstructive pulmonary disease (COPD); however, temporal trends in the management and clinical outcomes of these patients remain unclear. Methods and Results-We conducted an observational study by using a representative sample of 1 million beneficiaries from the Taiwan National Health Insurance Research Database. Comorbidities, in-hospital treatment, and outcomes were compared for patients with acute myocardial infarction with and without COPD between 2004 and 2013. Temporal trends in treatment and outcomes were analyzed. We included 6770 patients admitted to hospitals with acute myocardial infarction diagnoses, of whom 1921 (28.3%) had COPD. Fewer patients with COPD received beta-blockers (adjusted odds ratio 0.66, 95% CI 0.59-0.74), angiotensinconverting enzyme inhibitors/angiotensin II receptor blockers (adjusted odds ratio 0.83, 95% CI 0.73-0.93), statins, anticoagulants, dual antiplatelets, and coronary interventions. These patients had higher mortality (in hospital: adjusted hazard ratio 1.25 [95% CI 1.11-1.41]; 1 year: adjusted hazard ratio 1.20 [95% CI 1.09-1.32]) and respiratory failure risk during admission. Temporal trends showed little improvement in mortality in patients with COPD over 10 years. Multivariable logistic regression indicated that dual antiplatelets, b-blockers, angiotensin-converting enzyme inhibitors/angiotensin II receptor blockers, statins, coronary angiography, and coronary artery bypass grafting surgery were significantly correlated with improved mortality in patients with COPD. Conclusions-In Taiwan, a lower proportion of patients with COPD received evidence-based therapies for acute myocardial infarction than did patients without COPD, and their clinical outcomes were inferior. Limited improvement in mortality was observed over the preceding 10 years and is attributable to the underuse of evidence-based treatments.", "journal": "JOURNAL OF THE AMERICAN HEART ASSOCIATION", "category": "Cardiac & Cardiovascular Systems", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000419993600003", "keywords": "Malaria; time-to-treatment; Brazil", "title": "Factors associated with timely treatment of malaria in the Brazilian Amazon: a 10-year population-based study", "abstract": "Objective. To identify factors associated with timely treatment of malaria in the Brazilian Amazon. Malaria, despite being treatable, has proven difficult to control and continues to be an important public health problem globally. Brazil accounted for almost half of the 427 000 new malaria cases notified in the Americas in 2013. Methods. This was a cross-sectional study using secondary data on all notified malaria cases for the period from 2004 - 2013. Timely treatment was considered to be all treatment started within 24 hours of symptoms onset. Multivariate logistic regression was used to identify independent factors associated with timely treatment. Results. The proportion of cases starting treatment on a timely basis was 41.1%, tending to increase in more recent years (OR = 1.40; 95% CI: 1.37-1.42 in 2013). Furthermore, people starting within < 24 hours were more likely to: reside in the states of Rondonia (OR = 1.50; 95% CI: 1.49-1.51) or Acre (OR = 1.53; 95% CI: 1.55-1.57); be 0-5 years of age (OR = 1.39; 95% CI: 1.34-1.44) or 6-14 years of age (OR = 1.34; 95% CI: 1.32-1.36); be indigenous (OR = 1.41; 95% CI: 1.37-1.45); have a low level of schooling (OR = 1.20; 95% CI: 1.19-1.22); and be diagnosed by active detection (OR = 1.39; 95% CI: 1.38-1.39). Conclusion. In the Brazilian Amazon area, individuals were more likely to have timely treatment of malaria if they were young, residing in Acre or Rondonia states, have little schooling, and be identified through active detection. Identifying groups vulnerable to late treatment is important for preventing severe cases and malaria deaths.", "journal": "REVISTA PANAMERICANA DE SALUD PUBLICA-PAN AMERICAN JOURNAL OF PUBLIC HEALTH", "category": "Public, Environmental & Occupational Health", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000386241200008", "keywords": "Common path pessimism removal (CPPR); static timing analysis (STA)", "title": "UI-Timer 1.0: An Ultrafast Path-Based Timing Analysis Algorithm for CPPR", "abstract": "The recent TAU computer-aided design (CAD) contest has aimed to seek novel ideas for accurate and fast common path pessimism removal (CPPR). Unnecessary pessimism forces the static timing analysis tool to report worse violation than the true timing properties owned by physical circuits, thereby misleading signoff timing into a lower clock frequency at which circuits can operate than actual silicon implementations. Therefore, we introduce in this paper UI-Timer 1.0, a powerful CPPR algorithm which achieves high accuracy and ultrafast runtime. Unlike existing approaches which are dominated by explicit path search, UI-Timer 1.0 proves that by implicit path representation the amount of search effort can be significantly reduced. Our timer is superior in both space and time saving, from which memory storage and important timing quantities are available in constant space and constant time per path during the search. Experimental results on industrial benchmarks released from TAU 2014 CAD contest have justified that UI-Timer 1.0 achieved the best result in terms of accuracy and runtime over existing CPPR algorithms.", "journal": "IEEE TRANSACTIONS ON COMPUTER-AIDED DESIGN OF INTEGRATED CIRCUITS AND SYSTEMS", "category": "Computer Science, Hardware & Architecture; Computer Science, Interdisciplinary Applications; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000397193100006", "keywords": "Neutron transport; discrete ordinates; KBA method", "title": "An S-N Algorithm for Modern Architectures", "abstract": "Discrete ordinates transport packages from the Los Alamos National Laboratory are required to perform large computationally intensive time-dependent calculations on massively parallel architectures, where even a single such calculation may need many months to complete. While Koch-Baker-Alcouffe (KBA) methods scale well to very large numbers of compute nodes, we are limited by practical constraints on the number of such nodes we can actually apply to any given calculation. Instead, this paper describes a modified KBA algorithm that allows realization of the reductions in solution time offered by both the current and future architectural changes within a compute node.", "journal": "NUCLEAR SCIENCE AND ENGINEERING", "category": "Nuclear Science & Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000392767000020", "keywords": "ANFIS; Evapotranspiration; FAO-56; Fuzzy; Penman-Monteith; Saudi Arabia", "title": "Regional fuzzy chain model for evapotranspiration estimation", "abstract": "Evapotranspiration (ET) is one of the main hydrological cycle components that has extreme importance for water resources management and agriculture especially in arid and semi-arid regions. In this study, regional ET estimation models based on the fuzzy logic (FL) principles are suggested, where the first stage includes the ET calculation via Penman-Monteith equation, which produces reliable results. In the second phase, ET estimations are produced according to the conventional FL inference system model. In this paper, regional fuzzy model (RFM) and regional fuzzy chain model (RFCM) are proposed through the use of adjacent stations' data in order to fill the missing ones. The application of the two models produces reliable and satisfactory results for mountainous and sea region locations in the Kingdom of Saudi Arabia, but comparatively RFCM estimations have more accuracy. In general, the mean absolute percentage error is less than 10%, which is acceptable in practical applications. (C) 2016 Elsevier B.V. All rights reserved.", "journal": "JOURNAL OF HYDROLOGY", "category": "Engineering, Civil; Geosciences, Multidisciplinary; Water Resources", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000395603500007", "keywords": "Diabetic neuropathies; High-density EMG; Sample entropy; Complexity; Force", "title": "Reduced complexity of force and muscle activity during low level isometric contractions of the ankle in diabetic individuals", "abstract": "Background: This study evaluated the structure and amount of variability of surface electromyography (sEMG) patterns and ankle force data during low-level isometric contractions in diabetic subjects with different degrees of neuropathy. Methods: We assessed 10 control subjects and 38 diabetic patients, classified as absent, mild, moderate, or severe neuropathy, by a fuzzy system based on clinical variables. Multichannel sEMG (64-electrode matrix) of tibialis anterior and gastrocnemius medialis muscles were acquired during isometric contractions at 10%, 20%, and 30% of the maximum voluntary contraction, and force levels during dorsi- and plantarfiexion were recorded. Standard deviation and sample entropy of force signals were calculated and root mean square and sample entropy were calculated from sEMG signals. Differences among groups of force and sEMG variables were verified using a multivariate analysis of variance. Findings: Overall, during dorsiflexion contractions, moderate and severe subjects had higher force standard deviation and moderate subjects had lower force sample entropy. During plantarflexion, moderate subjects had higher force standard deviation and all diabetic subjects had lower entropy. Tibialis anterior presented higher root mean square in absent group and lower entropy in mild subjects. For gastrocnemius medialis, entropy was higher in severe and lower in moderate subjects. Interpretation: Diabetic neuropathy affects the complexity of the neuromuscular system during low-level isometric contractions, reducing the system's capacity to adapt to challenging mechanical demands. The observed patterns of neuromuscular complexity were not associated with disease severity, with the majority of alterations recorded in moderate subject. (C) 2017 Elsevier Ltd. All rights reserved.", "journal": "CLINICAL BIOMECHANICS", "category": "Engineering, Biomedical; Orthopedics; Sport Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000401399900035", "keywords": "Stochastic actuator faults; Non-cooperative targets; Attitude takeover control; Fault tolerant control; Dual-arm space robot", "title": "Adaptive attitude takeover control for space non-cooperative targets with stochastic actuator faults", "abstract": "This paper focuses on the attitude takeover control for the space non-cooperative targets with stochastic actuator faults. In this paper, the stochastic deviation faults and gain faults of the actuators, as well as the inertia tensor calculated errors are all under consideration. By introducing an adaptive deviation fault compensation term and an input uncertainty compensation law in a nonlinear feedback controller, an adaptive fault tolerant attitude takeover control scheme is synthesized in this paper. The considered stochastic thruster faults, the calculated inertia tensor error and the external disturbance can be compensated, the stochastic attitude stabilization and tracking are maintained as a result. Based on a quadratic Lyapunov function, the proof of the stochastic convergence is completed. Simulation results demonstrate the effectiveness and advantages of the proposed method. (C) 2017 Elsevier GmbH. All rights reserved.", "journal": "OPTIK", "category": "Optics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000401988800001", "keywords": "bureaucratic transparency; California Coastal Commission; coastal regulation; permitting process; text mining", "title": "Shaping the Coast with Permits: Making the State Regulatory Permitting Process Transparent with Text Mining", "abstract": "This paper examines the California Coastal Commission's permitting process. Using several text mining techniques, including web scraping, information extraction, and supervised classification, I demonstrate how to retrieve empirical data from unstructured texts, namely public meeting agendas and staff reports. Contrary to the concern that the Commission routinely delays or rejects permitting requests, the data reveal that outright rejection of permit applications is rare. On average, eight of ten applications were approved. Single-family homes and commercial development projects were approved about 80% of the time; the rates were about 70% for seawalls and retaining walls, and 60% for land-use changes. Most applications were processed swiftly, with a median application length of 3months. The agency's influence comes primarily from negotiating each application. Qualitative study of 50 cases pertaining to single-family home construction reveals that the agency adopts a managed development approach, that is, allowing development but scrupulously managing various aspects of development. These case studies illustrate how the agency interprets the broad, abstract state laws and translates the mandates into enforceable actions as permitting conditions. In areas where the state mandates conflict, particularly over development in receding shorelines, the agency has the largest leverage in creating and implementing its preferred policies. The text mining techniques demonstrated in this paper can be applied to study any governmental agency. These techniques help to extract information from a massive volume of papers and organize them into a database for analyses. The empirical data extracted from texts can significantly increase bureaucratic transparency.", "journal": "COASTAL MANAGEMENT", "category": "Environmental Sciences; Environmental Studies", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000389053900039", "keywords": "Sensor; structural health monitoring; composites; smart thread; flexible resistor", "title": "Development of Flexible Cotton-Polystyrene Sensor for Application as Strain Gauge", "abstract": "A smart sensing thread based on a cotton substrate and coating realized from polystyrene and carbon nanoparticles has been developed. Manufacturing method used is an innovative extru-coating procedure developed especially for the purpose. The strain response of the smart sensing thread has been recorded. It has been observed that the sensing thread has a heterogeneous structure along the cross section. This is because of the fact that most of the coating material is deposited at the surface and the sub-surface layer. The core receives the least amount of absorbed solution. This has been confirmed by SEM image analysis and visual analysis of the fracture specimens as well. The dynamic range of the sensor was found to be 11%, while the sensitivity value was calculated to be 15.5. The smart sensing threads are able to detect strains as the electrical resistance changes with strain with a rather high gauge factor and can be used for various sensing applications as strain gauges and flexible sensors.", "journal": "IEEE SENSORS JOURNAL", "category": "Engineering, Electrical & Electronic; Instruments & Instrumentation; Physics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000388616100009", "keywords": "Running detection; Benchmark database; Baseline algorithm", "title": "A reactant-coordinate-based approach to state-to-state differential cross sections for tetratomic reactions", "abstract": "A new algorithm is proposed to compute quantum mechanically state-to-state differential cross sections for reactions involving four atoms in full dimensionality. This algorithm, which is based on the propagation of an initial state specific wave packet exclusively in reactant coordinates, extracts the S-matrix elements in the product channel by first interpolating the time-dependent wave packet using a collocation method at selected time intervals on the product coordinate grid and then projecting out the contributions of all final product states. This approach is efficient and accurate, particularly for reactions that are dominated by a product well or long-range interactions. Validation of this approach is demonstrated for the H-2 + OH -> H + H2O reaction. Published by AIP Publishing.", "journal": "JOURNAL OF CHEMICAL PHYSICS", "category": "Chemistry, Physical; Physics, Atomic, Molecular & Chemical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000391724800018", "keywords": "super-resolution (SR); mixed-resolution; structure similarity; amplitude similarity; transform domain", "title": "Frequency domain based super-resolution method for mixed-resolution multi-view images", "abstract": "Super-resolution (SR) techniques, which are based on single or multi-frame low-resolution (LR) images, have been extensively investigated in the last two decades. Mixed-resolution multi-view video format plays an important role in three-dimensional television (3DTV) coding scheme. Previous work considers multi-view or multi-camera images and videos at the same resolution, which performs well under the planar model without or with little projection error among the videos captured by different cameras. In recent years, several researchers have discussed the SR problem in mixed-resolution multi-view video format, where the super-resolved image is created using the up-sampled version of the LR image and the high frequency components extracted from the warped image in the adjacent high-resolution (HR) views. Unfortunately, the output HR images suffer from artifacts caused by depth error. To obtain the detailed texture and edge information from the HR image as much as possible, while preserving the structure of the LR image, a novel SR reconstruction algorithm is proposed. The algorithm is composed of three components: the structure term, the detail information term, and the regularization term. The first term preserves the structure similarity of the LR image; the second term extracts detailed information from the adjacent HR image; and the last term ensures the uniqueness of the solution. Experimental results show the effectiveness and robustness of the proposed algorithm, which achieves high performance both subjectively and objectively.", "journal": "JOURNAL OF SYSTEMS ENGINEERING AND ELECTRONICS", "category": "Automation & Control Systems; Engineering, Electrical & Electronic; Operations Research & Management Science", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000397985400005", "keywords": "HIV; Early infant diagnosis; Prevention of Mother to Child Transmission; Africa", "title": "Lower extremity weakness is associated with elevated blood and cerebrospinal fluid glucose levels following multibranched endovascular aortic aneurysm repair", "abstract": "Objective: Hyperglycemia is associated with worsened clinical outcomes after central nervous system injury. The purpose of this study was to examine the association between lower extremity weakness (LEW) and the glucose levels of blood and cerebrospinal fluid (CSF) in patients undergoing multibranched endovascular aneurysm repair (MBEVAR) of thoracoabdominal and pararenal aortic aneurysms. Methods: Blood and CSF samples were collected preoperatively, immediately after aneurysm repair, and on postoperative day 1 in 21 patients undergoing MBEVAR. Data on demographics, operative repair, complications, and outcomes were collected prospectively. Results: There were 21 patients who underwent successful MBEVAR. Two patients had pre-existing paraplegia from prior open aortic surgery and were excluded from the current analysis. The mean age was 73 +/- 8 years, and 15 of 19 (79%) were men. In the postoperative period, 7 of 19 (37%) patients developed LEW. This was temporary in 5 of 19 (26%) patients and permanent in 2 of 19 (11%) patients. The LEW group was older than the non-LEW group (77 +/- 6 vs 70 +/- 9 years, respectively; P = .10), had a lower preoperative glomerular filtration rate (58.6 +/- 18.5 vs 71.4 +/- 23.5 mL/min per 1.73 m(2); P = .24), and was more likely to be taking a statin (100% vs 67%, respectively; P = .13), but these did not reach statistical significance. There was no significant difference in the prevalence of diabetes mellitus, hypertension, coronary artery disease, lung disease, or peripheral artery disease between the LEW and non-LEW groups. There was also no difference in operative time, blood loss, contrast material volume, or fluoroscopy times between the two groups. Preoperative blood and CSF glucose levels were similar in those with and without LEW. During the postoperative period, glucose values in the blood and CSF were significantly higher in those patients who developed LEW compared with those who did not develop LEW. In all patients with LEW, the elevation in the blood or CSF glucose level preceded the development of LEW. In a multivariable logistic regression model, CSF glucose concentration on postoperative day 1 was significantly and independently associated with the development of LEW (odds ratio, 2.30 [1.03-5.14] per 10 mg/dL increase in CSF glucose; P = .04). Conclusions: Elevated blood glucose and CSF glucose levels are associated with postoperative LEW in patients undergoing MBEVAR. The protective effect of euglycemia deserves further study in patients at risk for spinal cord ischemia.", "journal": "JOURNAL OF VASCULAR SURGERY", "category": "Surgery; Peripheral Vascular Disease", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000386069200001", "keywords": "Geometric multigrid; Atmospheric modelling; Preconditioner; (Mixed) finite elements; Domain-specific compilers", "title": "High level implementation of geometric multigrid solvers for finite element problems: Applications in atmospheric modelling", "abstract": "The implementation of efficient multigrid preconditioners for elliptic partial differential equations (PDEs) is a challenge due to the complexity of the resulting algorithms and corresponding computer code. For sophisticated (mixed) finite element discretisations on unstructured grids an efficient implementation can be very time consuming and requires the programmer to have in-depth knowledge of the mathematical theory, parallel computing and optimisation techniques on manycore CPUs. In this paper we show how the development of bespoke multigrid preconditioners can be simplified significantly by using a framework which allows the expression of the each component of the algorithm at the correct abstraction level. Our approach (1) allows the expression of the finite element problem in a language which is close to the mathematical formulation of the problem, (2) guarantees the automatic generation and efficient execution of parallel optimised low-level computer code and (3) is flexible enough to support different abstraction levels and give the programmer control over details of the preconditioner. We use the composable abstractions of the Firedrake/PyOP2 package to demonstrate the efficiency of this approach for the solution of strongly anisotropic PDEs in atmospheric modelling. The weak formulation of the PDE is expressed in Unified Form Language (UFL) and the lower PyOP2 abstraction layer allows the manual design of computational kernels for a bespoke geometric multigrid preconditioner. We compare the performance of this preconditioner to a single-level method and hypre's BoomerAMG algorithm. The Firedrake/PyOP2 code is inherently parallel and we present a detailed performance analysis for a single node (24cores) on the ARCHER supercomputer. Our implementation utilises a significant fraction of the available memory bandwidth and shows very good weak scaling on up to 6,144compute cores. (C) 2016 Elsevier Inc. All rights reserved.", "journal": "JOURNAL OF COMPUTATIONAL PHYSICS", "category": "Computer Science, Interdisciplinary Applications; Physics, Mathematical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000397343200006", "keywords": "Chemokine; MIG; Atherosclerosis", "title": "Serum Monokine Induced by Gamma Interferon Is Associated With Severity of Coronary Artery Disease", "abstract": "The immune system may play important roles in the pathogenesis of cardiovascular disease. T-cell mediated immune responses in human progression of atherosclerotic disease and hypertension have recently been revealed, but the significance of T-cell specific chemokines in coronary artery heart disease has not been confirmed. In our study, we sought to examine the association between serum levels of the monokine induced by gamma interferon (MIG)/CXCL9 and the severity of coronary artery disease. We studied 117 patients with coronary heart disease and 80 patients with no coronary heart disease. The severity of coronary artery disease was assessed via coronary artery angiography and the Gensini score was calculated. Clinical and biochemical indices, including serum levels of MIG, CD4OL, and IFN-y were analyzed in all subjects. Finally, we found there was a significant correlation between serum MIG levels and the severity of coronary artery disease, quantified by the Gensini score (r = 0.122, P = 0.009). Furthermore, multivariate regression analysis revealed that serum MIG levels were independently associated with the severity of coronary artery disease, quantified by the Gensini score (beta = 0.100, P = 0.021). Our findings could indicate the potential clinical implication of MIG with respect to early coronary artery atherosclerosis in humans.", "journal": "INTERNATIONAL HEART JOURNAL", "category": "Cardiac & Cardiovascular Systems", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000396950400008", "keywords": "Image retrieval; Feature matching; Twin Feature; Similarity maximal matching; Dynamic normalization", "title": "Improving feature matching strategies for efficient image retrieval", "abstract": "A number of state-of-the-art image retrieval systems have been built upon non-aggregated techniques such as Hamming Embedding (HE) and Selective Match Kernel (SMK). However, the retrieval performances of these techniques are directly affected by the quality of feature matching during the search process. In general, undesirable matched results appear mainly due to the following three aspects: (1) the locality of local features, (2) the quantization errors and (3) the phenomenon of burstiness. In this paper, starting from the framework of SMK, an in-depth study of the integration of Twin Feature (TF) and Similarity Maximal Matching (SMM) is fully investigated. To be specific, two effective modifications based on TF and SMM are proposed to further improve the quality of feature matching. On one hand, the original float vectors of TF are replaced with efficient binary signatures, which achieve relatively high efficiency and comparable accuracy of retrieval. On the other hand, Dynamic Normalization (DN) is designed to effectively control the impact of penalization generated by SMM and improve the performance with almost no extra cost. At last, an efficient image retrieval system is designed and realized based on a cloud-based heterogeneous computing framework through Apache Spark and multiple GPUs to deal with large-scale tasks. Experimental results demonstrate that the proposed system can greatly refine the visual matching process and improve image retrieval results.", "journal": "SIGNAL PROCESSING-IMAGE COMMUNICATION", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000385188200007", "keywords": "Hierarchical multiresolution Markov random fields; multitemporal classification; satellite image time series", "title": "A New Cascade Model for the Hierarchical Joint Classification of Multitemporal and Multiresolution Remote Sensing Data", "abstract": "In this paper, we propose a novel method for the joint classification of both multidate and multiresolution remote sensing imagery, which represents an important and relatively unexplored classification problem. The proposed classifier is based on an explicit hierarchical graph-based model that is sufficiently flexible to address a coregistered time series of images collected at different spatial resolutions. Within this framework, a novel element of the proposed approach is the use of multiple quadtrees in cascade, each associated with the images available at each observation date in the considered time series. For each date, the input images are inserted in a hierarchical structure on the basis of their resolutions, whereas missing levels are filled in with wavelet transforms of the images embedded in finer-resolution levels. This approach is aimed at both exploiting multiscale information, which is known to play a crucial role in high-resolution image analysis, and supporting input images acquired at different resolutions in the input time series. The experimental results are shown for multitemporal and multiresolution optical data.", "journal": "IEEE TRANSACTIONS ON GEOSCIENCE AND REMOTE SENSING", "category": "Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000402624400001", "keywords": "Underactuated systems; legged robots; stability & control; hybrid systems; mechanisms & models", "title": "A brief review of dynamics and control of underactuated biped robots", "abstract": "Humans as bipeds enjoy certain advantages over other terrestrial systems, which motivate us to study and develop biped robots. Underactuated biped robots adopt the energy efficient gait of the biological counterparts and passive walkers. However, the control design for such robots is challenging due to lesser controllable joints, non-linear hybrid system dynamics and the goal of utilizing the natural dynamics. This paper summarizes various designs, models and control strategies used to enable stable walking and running for the underactuated biped robots. It gives a brief about how the mechanism of such bipeds evolved to incorporate the design variations which significantly improved the system performance. The few basic mathematical models which are used to simulate, analyze and predict the system dynamics and test control designs, are described, highlighting the difference in walking and running models. An introduction to the various stability criteria and control methods, successful in enabling stable walking for the robots on flat or uneven terrains, is provided. This paper gives a brief of the significant achievements in this field and ends with the highlights of the abilities inherent to humans but lacking in underactuated bipeds, and adopting or improving which should be the focus of the future research.", "journal": "ADVANCED ROBOTICS", "category": "Robotics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000393995500001", "keywords": "Morphogenesis; computational modeling; artificial development; mutational analysis; viability theory; epigenetics", "title": "Energy Efficiency and Capacity Tradeoff in Cloud Radio Access Network of High-Speed Railways", "abstract": "To meet the increasing demand of high-data-rate services of high-speed railway (HSR) passengers, cloud radio access network (C-RAN) is proposed. This paper investigates the tradeoff between energy efficiency (EE) performance and capacity in C-RAN of HSR. Considering that the train location can be predicted, we propose a predictable path loss based time domain power allocation method (PPTPA) to improve EE performance of HSR communication system. First, we consider that the communication system of HSR only bears the passenger information services (PISs). The energy-efficient power allocation problem with delay constraint is studied. The formulated problem is nonconvex. To deal with it, an equivalent convex problem is reformulated. Based on PPTPA, we propose an iterative algorithm to improve the EE performance. Second, we consider that the PISs and the train control services (TCSs) are all bore. A capacity optimization problem with joint EE and services transmission delay constraints is formulated. Based on PPTPA, we propose a hybrid power allocation scheme to improve the capacity of the system. Finally, we analyze the effect of small-scale fading on EE performance. The effectiveness of the proposed power allocation algorithm is validated by HSR channel measurement trace based emulation results and extensive simulation results.", "journal": "MOBILE INFORMATION SYSTEMS", "category": "Computer Science, Information Systems; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000387724300048", "keywords": "Barrett's esophagus; Quality of life; GERD", "title": "Outcomes of Nigeria's HIV/AIDS Treatment Program for Patients Initiated on Antiretroviral Treatment between 2004-2012", "abstract": "Background The Nigerian Antiretroviral therapy (ART) program started in 2004 and now ranks among the largest in Africa. However, nationally representative data on outcomes have not been reported. Methods We evaluated retrospective cohort data from a nationally representative sample of adults aged >= 15 years who initiated ART during 2004 to 2012. Data were abstracted from 3,496 patient records at 35 sites selected using probability-proportional-to-size (PPS) sampling. Analyses were weighted and controlled for the complex survey design. The main outcome measures were mortality, loss to follow-up (LTFU), and retention (the proportion alive and on ART). Potential predictors of attrition were assessed using competing risk regression models. Results At ART initiation, 66.4 percent (%) were females, median age was 33 years, median weight 56 kg, median CD4 count 161 cells/mm(3), and 47.1% had stage III/IV disease. The percentage of patients retained at 12, 24, 36 and 48 months was 81.2%, 74.4%, 67.2%, and 61.7%, respectively. Over 10,088 person-years of ART, mortality, LTFU, and overall attrition (mortality, LTFU, and treatment stop) rates were 1.1 (95% confidence interval (CI): 0.7-1.8), 12.3 (95% CI: 8.9-17.0), and 13.9 (95% CI: 10.4-18.5) per 100 person-years (py) respectively. Highest attrition rates of 55.4/100py were witnessed in the first 3 months on ART. Predictors of LTFU included: lower-than-secondary level education (reference: Tertiary), care in North-East and South-South regions (reference: North-Central), presence of moderate/severe anemia, symptomatic functional status, and baseline weight <45kg. Predictor of mortality was WHO stage higher than stage I. Male sex, severe anemia, and care in a small clinic were associated with both mortality and LTFU. Conclusion Moderate/Advanced HIV disease was predictive of attrition; earlier ART initiation could improve program outcomes. Retention interventions targeting men and those with lower levels of education are needed. Further research to understand geographic and clinic size variations with outcome is warranted.", "journal": "PLOS ONE", "category": "Multidisciplinary Sciences", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000390401400009", "keywords": "Deep sea; hard substrata; image analysis; megafauna; Mid-Atlantic Ridge; Porifera", "title": "Benthic megafauna on steep slopes at the Northern Mid-Atlantic Ridge", "abstract": "The role of small-scale (<10 km) habitat availability in structuring deep-sea hard substratum assemblages is poorly understood. Epibenthic megafauna and substratum availability were studied on steep slopes at the Mid-Atlantic Ridge from May to July 2010 northwest, northeast, southwest and southeast of the Charlie-Gibbs Fracture Zone (CGFZ; 48-54 degrees N) at between 2095 and 2601 m depth. Megafauna were six times denser north of the CGFZ compared with the south and differences in density were almost entirely driven by sessile fauna. There was no significant difference in habitat availability amongst sites. Rocky substratum made up 48% of the total area surveyed, with individual transects having between 0% and 82% rock. Assemblage structures were different amongst all superstations. The north was dominated by demospongids and hexactinellids, whereas the southern superstations were dominated by antho-zoans and hexactinellids. Differences in megafaunal assemblages north and south of the CGFZ primarily reflected variations in demospongid and antho-zoan species composition. With 213-1825 individuals.ha(-1), and 7-24 species per superstation, hexactinellids were the most species-rich (36 species) and cosmopolitan taxa at the study site, supporting observations elsewhere along the ridge and in the CGFZ. The absence of significant differences in substrata availability suggested alternative drivers for density or percentage cover. The amount of hard substratum available only limited sessile megafauna density at one transect that was entirely covered with sediments. Species richness was highest for areas with intermediate values of substratum coverage (35-43% rock).", "journal": "MARINE ECOLOGY-AN EVOLUTIONARY PERSPECTIVE", "category": "Marine & Freshwater Biology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000388691600025", "keywords": "Interstage; Home monitoring program; Single ventricle; Sociodemographic; Social; Demographics", "title": "Home Monitoring Program Reduces Mortality in High-Risk Sociodemographic Single-Ventricle Patients", "abstract": "A clinician-driven home monitoring program can improve interstage outcomes in single-ventricle patients. Sociodemographic factors have been independently associated with mortality in interstage patients. We hypothesized that even in a population with high-risk sociodemographic characteristics, a home monitoring program is effective in reducing interstage mortality. We defined interstage period as the time period between discharge following Norwood palliation and second-stage surgery. We reviewed the charts of patients for the three-year period before (group 1) and after (group 2) implementation of the home monitoring program. Clinical variables around Norwood palliation, during the interstage period, and at the time of second-stage surgery were analyzed. There were 74 patients in group 1 and 52 in group 2. 59 % patients were Hispanic, and 84 % lived in neighborhoods where over 5 % families lived below poverty line. There was no significant difference in pre-Norwood variables, Norwood discharge variables, age at second surgery, or outcomes at second surgery. There were more Sano shunts performed at the Norwood procedure as the source of pulmonary blood flow in group 2 (p value < 0.05). There were more unplanned hospital admissions and percutaneous re-interventions in group 2. Patients in group 2 whose admission criteria included desaturation had a 45 % likelihood of having an unplanned re-intervention. Group 2 noted an 80 % relative reduction in interstage mortality (p < 0.01). In a multiple regression analysis, after accounting for ethnicity, socio-economic status, and source of pulmonary blood flow, enrollment in a home monitoring program independently predicted improved interstage survival (p < 0.01). A clinician-driven home monitoring program reduces interstage mortality even when the majority of patients has high-risk sociodemographic characteristics.", "journal": "PEDIATRIC CARDIOLOGY", "category": "Cardiac & Cardiovascular Systems; Pediatrics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000392366500009", "keywords": "Phthalate metabolites; Thyroid hormones; Growth hormone; Biomonitoring; Taiwanese", "title": "Does exposure to phthalates influence thyroid function and growth hormone homeostasis? The Taiwan Environmental Survey for Toxicants (TEST) 2013", "abstract": "Background: Previous epidemiologic and toxicological studies provide some inconsistent evidence that exposure to phthalates may affect thyroid function and growth hormone homeostasis. Objective: To assess the relations between exposure to phthalates and indicators of thyroid function and growth hormone homeostasis disturbances both among adults and minors. Methods: We conducted a population-based cross-sectional study of 279 Taiwanese adults (>= 18 years old) and 79 minors ( < 18 years old) in 2013. Exposure assessment was based on urinary biomarkers, 11 phthalate metabolites measured by using online liquid chromatography/tandem mass spectrometry. Indicators of thyroid function included serum levels of thyroxine (T-4), free T-4, triiodothyronine, thyroid-stimulating hormone, and thyroxine-binding globulin (TBG). Growth hormone homeostasis was measured as the serum levels of insulin like growth factor 1 (IGF-1) and insulin-like growth factor binding protein 3 (IGFBP3). We applied multivariate linear regression models to examine these associations after adjusting for covariates. Results: Among adults, serum T-4 levels were negatively associated with urinary mono-(2-ethyl-5-hydroxyhexyl) phthalate (beta=-0.028, P=0.043) and the sum of urinary di-(2-ethylhexyl) phthalate (DEHP) metabolite (beta=-0.045, P=0.017) levels. Free T-4 levels were negatively associated with urinary mono-ethylhexyl phthalate (MEHP) (beta=-0.013, P=0.042) and mono-(2-ethyl-5-oxohexyl) phthalate (beta=-0.030, P=0.003) levels, but positively associated with urinary monoethyl phthalate (beta=0.014, P=0.037) after adjustment for age, BMI, gender, urinary creatinine levels, and TBG levels. Postive associations between urinary MEHP levels and IGF-1 levels (beta=0.033, P=0.006) were observed. Among minors, free T-4 was positively associated with urinary mono benzyl phthalate levels (beta=0.044, P=0.001), and IGF-1 levels were negatively associated with the sum of urinary DEHP metabolite levels (beta=-0.166, P=0.041) after adjustment for significant covariance and IGFBP3. Conclusions: Our results are consistent with the hypothesis that exposure to phthalates influences thyroid function and growth hormone homeostasis.", "journal": "ENVIRONMENTAL RESEARCH", "category": "Environmental Sciences; Public, Environmental & Occupational Health", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000398482200001", "keywords": "Lagrangian model; plume rise; entrapment", "title": "Designing Vehicle Turning Restrictions Based on the Dual Graph Technique", "abstract": "This paper investigates the turning restriction design problem that optimizes the turning restriction locations so as to minimize the total system travel time under the assumption of asymmetric user equilibrium. We first transform a transportation network into a dual graph, where traffic turning movements are explicitly modeled as dual links. The dual transformation allows us to derive a link-based formulation for the turning restriction design problem. Asymmetric user equilibrium is incorporated in the model as a set of nonlinear constraints. A dual-based heuristic algorithm is employed to solve the problem, by sequentially solving a relaxed turning restriction design problem and a design updating problem.", "journal": "MATHEMATICAL PROBLEMS IN ENGINEERING", "category": "Engineering, Multidisciplinary; Mathematics, Interdisciplinary Applications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000403826600007", "keywords": "rotational atherectomy; complications; effectiveness; major adverse cardiac and cerebrovascular events", "title": "Long-term effects of rotational atherectomy in patients with heavy calcified coronary artery lesions: a single-centre experience", "abstract": "Background: Rotational atherectomy (RA) plays a significant role in contemporary percutaneous coronary interventions (PCI), especially in the era of population aging and expansion of PCI indications. Aim: The aim of the current study was to evaluate the rate of periprocedural complications, the long-term effectiveness of RA, and potential factors influencing the incidence of major adverse cardiac events (MACE) and major cardiac as well as cerebrovascular events (MACCE) after RA. Methods: The study included 60 consecutive patients who underwent effective RA between January 2002 and May 2016. Patients were followed-up for 2,616 days for MACE and MACCE. Results: The mean age of the enrolled patients was 72.1 years, and 78.3% were males. The mean follow-up period lasted 835.3 +/- 611.8 days. Periprocedural complications occurred in 12 (20.0%) patients. In the follow-up of up to 2,616 days, 64% of patients were free of MACCE and 68% were free of MACE. Univariate Cox analysis revealed that MACCE occurred more often in patients from the high-risk group based on the EuroSCORE II and those with longer lengths of the implanted stent(s) after the RA procedure. In multivariate Cox regression analysis, both high-risk category and mean stent(s) length were identified as independent predictors of MACCE. EuroSCORE II was confirmed to be the only independent predictor of MACE after RA. Conclusions: Rotational atherectomy is a safe and sufficient technique for the endovascular treatment of heavily calcified coronary artery lesions. Individuals at a higher risk as assessed by the EuroSCORE II before RA and those with longer stent(s) implanted after RA are predisposed to MACCE in the follow-up.", "journal": "KARDIOLOGIA POLSKA", "category": "Cardiac & Cardiovascular Systems", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000386559100001", "keywords": "ASEP; extrapolation; Bethe ansatz; asymptotics of determinants", "title": "Extrapolation methods and Bethe ansatz for the asymmetric exclusion process", "abstract": "The one-dimensional asymmetric simple exclusion process (ASEP), where N hard-core particles hop forward with rate 1 and backward with rate q < 1, is considered on a periodic lattice of L site. Using KPZ universality and previous results for the totally asymmetric model q = 0, precise conjectures are formulated for asymptotics at finite density rho = N/L of ASEP eigenstates close to the stationary state. The conjectures are checked with high precision using extrapolation methods on finite size Bethe ansatz numerics. For weak asymmetry 1 - q similar to 1 / root L, double extrapolation combined with an integer relation algorithm gives an exact expression for the spectral gap up to 10th order in the asymmetry.", "journal": "JOURNAL OF PHYSICS A-MATHEMATICAL AND THEORETICAL", "category": "Physics, Multidisciplinary; Physics, Mathematical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000394436200033", "keywords": "Polar codes; Reed-Solomon codes; concatenation; multiplicity assignment; threshold", "title": "Real-time current-sensing feedback system for compensating process-voltagete-temperature variations of display using double-gate oxide TFT", "abstract": "The shift register of display panel employing double-gate oxide thinfilm transistors (TFTs) to compensate the severe degradation of the threshold voltage (V-TH) is proposed. In double-gate TFTs, VTH is controlled by adjusting top-gate bias, so that VTH degradation can be stabilised by top-gate bias control. However, an optimum top-gate voltage varies from product to product due to process-voltage-temperature variation. To overcome this variation, a feedback system is designed and fabricated in a 0.18 mu m BCDMOS process. The system consists of the shift register current sensing and the searching algorithm for finding optimal top-gate bias of double-gate TFTs. It is verified that proposed system successfully stabilised the operation of the shift register up to 80 degrees C.", "journal": "ELECTRONICS LETTERS", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000393296100001", "keywords": "Laser-induced breakdown spectroscopy; High resolution spectroscopy; Carbonaceous shale; Principal component analysis", "title": "Analysis of Carbonaceous Shale by Laser. induced Breakdown Spectroscopy", "abstract": "Gas shale is one of the important unconventional hydrocarbon source rocks, whose composition, such as mineral components and trace elements, has been proven as important geochemical proxies playing essential roles in indicating the gas potential and gas productivity in recent geological researches. Fast and accurate measurements of the shale composition will reveal rich information for understanding and evaluation of gas shale reservoirs. In this paper, we demonstrated the potentiality as well as feasibility of laser. induced breakdown spectroscopy (LIBS) as an effective technique to perform spectrochemical analysis for shale samples. For this experiment, a Nd : YAG laser at the fundamental wavelength of 1064 nm provided pulses for the shale materials. An echelle spectrometer equipped with an ICCD camera was employed to disperse and record the spectra. Meanwhile, five shale samples were collected at different depth from 2396 m to 3428 m. The LIBS device was used to obtain the spectrum, and combined with the principal component score of each spectrum to draw a two. dimensional diagram. The obtained results revealed that more than 350 lines emitted by 22 different elements were found. Among these species, major elements like Si, Al, Fe, Ca, Mg, K and Na, and redox sensitive trace elements such as Cu, Cr, Ni, Sr, and Ni were detected with high signal. to. noise ratio. In principal component scores diagram, different types of carbonaceous shale were obviously separated, and the results were consistent with the spectra classification. The observed results also show that laser. induced breakdown spectroscopy combined with principal component analysis (PCA) method can be used for carbonaceous shale discriminant field in the future, providing scientific data and means to improve classification performance and enhance the exploitation and evaluation of gas shale reservoirs.", "journal": "CHINESE JOURNAL OF ANALYTICAL CHEMISTRY", "category": "Chemistry, Analytical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000401684100003", "keywords": "pregnancy; venous thromboembolism; low-molecular-weight heparin", "title": "Pulmonary Complications of Pregnancy: Venous Thromboembolism", "abstract": "Unique considerations are needed when diagnosing and treating venous thromboembolism (VTE) in women who are pregnant or postpartum. What are the risks to the fetus, such as drug exposure or the risk of radiation with diagnostic imaging? How does the physiology of pregnancy affect imaging techniques and anticoagulation management? How should anticoagulation be managed around labor and delivery? These questions highlight some of the important considerations needed when managing a pregnant patient with suspected or confirmed VTE. This review outlines what is known about the epidemiology, pathophysiology, clinical risk factors, diagnosis, and therapeutic management of VTE in pregnancy. We also review our preferred diagnostic and treatment algorithm for a pregnant patient with suspected or confirmed VTE.", "journal": "SEMINARS IN RESPIRATORY AND CRITICAL CARE MEDICINE", "category": "Critical Care Medicine; Respiratory System", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000396683300005", "keywords": "elliptic optimal control problem; optimal shape design; pointwise state constraints; Moreau-Yosida regularization; error estimates", "title": "AN OPTIMAL SHAPE DESIGN PROBLEM FOR PLATES", "abstract": "We consider an optimal shape design problem for the plate equation, where the variable thickness of the plate is the design function. This problem can be formulated as a control in the coefficient PDE-constrained optimal control problem with additional control and state constraints. The state constraints are treated with a Moreau-Yosida regularization of a dual problem. Variational discretization is employed for discrete approximation of the optimal control problem. For discretization of the state in the mixed formulation we compare the standard continuous piecewise linear ansatz with a piecewise constant one based on the lowest-order Raviart-Thomas mixed finite element. We derive bounds for the discretization and regularization errors and also address the coupling of the regularization parameter and finite element grid size. The numerical solution of the optimal control problem is realized with a semismooth Newton algorithm. Numerical examples show the performance of the method.", "journal": "SIAM JOURNAL ON NUMERICAL ANALYSIS", "category": "Mathematics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000398818700165", "keywords": "wearable; wireless sensor node; textile antenna; GPS; SIGFOX; SENTILO", "title": "An Alternative Wearable Tracking System Based on a Low-Power Wide-Area Network", "abstract": "This work presents an alternative wearable tracking system based on a low-power wide area network. A complete GPS receiver was integrated with a textile substrate, and the latitude and longitude coordinates were sent to the cloud by means of the SIM-less SIGFOX network. To send the coordinates over SIGFOX protocol, a specific codification algorithm was used and a customized UHF antenna on jeans fabric was designed, simulated and tested. Moreover, to guarantee the compliance to international regulations for human body exposure to electromagnetic radiation, the electromagnetic specific absorption rate of this antenna was analyzed. A specific remote server was developed to decode the latitude and longitude coordinates. Once the coordinates have been decoded, the remote server sends this information to the open source data viewer SENTILO to show the location of the sensor node in a map. The functionality of this system has been demonstrated experimentally. The results guarantee the utility and wearability of the proposed tracking system for the development of sensor nodes and point out that it can be a low cost alternative to other commercial products based on GSM networks.", "journal": "SENSORS", "category": "Chemistry, Analytical; Engineering, Electrical & Electronic; Instruments & Instrumentation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000393846200013", "keywords": "coefficient of variation; personnel monitoring; India", "title": "STUDY ON THE COEFFICIENT OF VARIATION IN INDIAN PERSONNEL MONITORING SYSTEM", "abstract": "The primary parameters for testing an individual monitoring system are standard deviation and the coefficient of variation. The International Electrotechnical Commission (IEC) standard 62387-1 recommends testing the coefficient of variation of dosemeters for various doses because the acceptable coefficient of variation changes with the dose level. However, for dose quantity Hp (10), i.e. doses greater than 1.1 mSv, the acceptable limit is 5 % and remains unchanged up to the highest dose in the measurable range. This study was carried out to confirm whether the same is followed in the Indian personnel monitoring system when measuring H (10) and also in order to study the variation in the coefficient of variation with a given dose. It was observed that even if the coefficient of variation at doses between 0.1 mSv and 1.1 mSv is lower than the IEC requirement, at higher doses, the same may not be true. In routine monitoring, since the anticipated doses are less than 1 mSv, a monitoring system which performs better than the IEC requirement at these levels of doses is an advantage. However, good performance at said dose levels does not naturally indicate good performance at higher doses.", "journal": "NUCLEAR TECHNOLOGY & RADIATION PROTECTION", "category": "Nuclear Science & Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000391441900012", "keywords": "Flash memory; mobile virtualization; non-volatile memory; metadata", "title": "Image-Content-Aware I/O Optimization for Mobile Virtualization", "abstract": "Mobile virtualization introduces extra layers in software stacks, which leads to performance degradation. Notably, each I/O operation has to pass through several software layers to reach the NAND-flash-based storage systems. This article targets at optimizing I/O for mobile virtualization, since I/O becomes one of major performance bottlenecks that seriously affects the performance of mobile devices. Among all the I/O operations, a large percentage is to update metadata. Frequently updated metadata not only degrade overall I/O performance but also severely reduce flash memory lifetime. In this article, we propose a novel I/O optimization technique to identify the metadata of a guest file system that is stored in a virtual machine image file and frequently updated. Then, these metadata are stored in a small additional non-volatile memory (NVM), which is faster and more endurable to greatly improve flash memory's performance and lifetime. To the best of our knowledge, this is the first work to identify the file system metadata from regular data in a guest OS image file with NVM optimization. The proposed scheme is evaluated on a real hardware embedded platform. The experimental results show that the proposed techniques can improve write performance to 45.21% in mobile devices with virtualization.", "journal": "ACM TRANSACTIONS ON EMBEDDED COMPUTING SYSTEMS", "category": "Computer Science, Hardware & Architecture; Computer Science, Software Engineering", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000395218100008", "keywords": "Multiphase flow; High-order scheme; WENO; Two-phase model; Shock-bubble interaction", "title": "Numerical simulations of compressible multicomponent and multiphase flow using a high-order targeted ENO (TENO) finite-volume method", "abstract": "High-order numerical simulations of compressible multicomponent and multiphase flows are challenging due to the need to resolve both complex flow features and sharp gradients associated with material interfaces or shocks with minimal spurious oscillations. Recently, in the context of the WENO family of schemes, increasing the ENO property and incorporating improved convergence properties near local extrema points, has resulted in the targeted ENO or TENO scheme. In this study, a robust high-order finite-volume method based on the TENO scheme is implemented and tested for simulating multicomponent and multiphase compressible flows. A fifth-order spatial reconstruction is combined with a high resolution modifiedHLLC Riemann solver, adjusted for the six-equation formulation of the diffuse interface model, and a third-order TVD Runge-Kutta explicit time-stepping scheme. Multidimensional extension is handled utilizing Gauss-Legendre quadrature points to evaluate both the flux and gas void fraction inter-cell terms. Several challenging 1D and 2D test cases are performed and compared to previously published experimental data and numerical simulations where available. A parametric study of the user-defined threshold parameter in the TENO algorithm is also studied and the TENO scheme is found to be more robust and less dissipative than both the WENO-Z and WEND JS schemes. (C) 2017 Elsevier Ltd. All rights reserved.", "journal": "COMPUTERS & FLUIDS", "category": "Computer Science, Interdisciplinary Applications; Mechanics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000394971800017", "keywords": "Roots of polynomials; Eigenvalues; Companion matrices; QR algorithm; Fiedler matrices; Conditioning; Pseudospectrum; Pseudozero sets of polynomials", "title": "Eigenvalue condition numbers and pseudospectra of Fiedler matrices", "abstract": "The aim of the present paper is to analyze the behavior of Fiedler companion matrices in the polynomial root-finding problem from the point of view of conditioning of eigenvalues. More precisely, we compare: (a) the condition number of a given root of a monic polynomial p(z) with the condition number of as an eigenvalue of any Fiedler matrix of p(z), (b) the condition number of as an eigenvalue of an arbitrary Fiedler matrix with the condition number of as an eigenvalue of the classical Frobenius companion matrices, and (c) the pseudozero sets of p(z) and the pseudospectra of any Fiedler matrix of p(z). We prove that, if the coefficients of the polynomial p(z) are not too large and not all close to zero, then the conditioning of any root of p(z) is similar to the conditioning of as an eigenvalue of any Fiedler matrix of p(z). On the contrary, when p(z) has some large coefficients, or they are all close to zero, the conditioning of as an eigenvalue of any Fiedler matrix can be arbitrarily much larger than its conditioning as a root of p(z) and, moreover, when p(z) has some large coefficients there can be two different Fiedler matrices such that the ratio between the condition numbers of as an eigenvalue of these two matrices can be arbitrarily large. Finally, we relate asymptotically the pseudozero sets of p(z) with the pseudospectra of any given Fiedler matrix of p(z), and the pseudospectra of any two Fiedler matrices of p(z).", "journal": "CALCOLO", "category": "Mathematics, Applied; Mathematics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000390597500007", "keywords": "Cervical cancer; Four-filed box technique; Irregular surface compensator; Intensity modulated radiation therapy; DVH analysis", "title": "Performance of the irregular surface compensator compared with four-field box and intensity modulated radiation therapy for gynecologic cancer", "abstract": "Purpose: A retrospective planning study was undertaken to evaluate the dosimetric advantages of the irregular surface compensator (ISC) technique, a forward planning technique with electronic compensation algorithm available on Varian Eclipse treatment planning system. This was extensively compared to the conventional four-field box (4FB) and intensity modulated radiation therapy using 5 fields (IMRT5F) on gynecologic cancer patients. Methods: Twenty-two patients were enrolled. The prescribed dose was 50.4 Gy in 28 fractions to the primary target including pelvic lymph nodes. 4FB treatment plans were generated, then fluence of anterior and posterior fields were modified to generate ISC plans. IMRT5F were inversely optimized with equally spaced five coplanar fields. Dose-volume parameters were evaluated for the comparison of three planning techniques. The MU and delivery time were also estimated. Results: In terms of target coverage, the conformity and homogeneity index of ISC (1.67 and 1.03, respectively) were superior to those of 4FB (2.43 and 1.06, respectively) but slightly inferior to those of IMRT5F (1.10 and 1.02, respectively). ISC also illustrated an overall improvement in normal organ saving. Compared to 4FB, the mean dose of the rectum was reduced by about 4.0-5.0 Gy with ISC and IMRT5F. The volume receiving large doses was reduced for bladder with statistical significance with ISC and more with IMRT5F relative to 4FB. The mean number of MU per fraction were 200.86 (4FB), 446.09 (ISC) and 895.59 (IMRT5F). Conclusion: The ISC technique has the superior target coverage and healthy tissue sparing in comparison with conventional 4FB and comparable normal organ saving compared to IMRT5F. The ISC can be an available option for gynecologic radiotherapy. (C) 2016 Associazione Italiana di Fisica Medica. Published by Elsevier Ltd. All rights reserved.", "journal": "PHYSICA MEDICA-EUROPEAN JOURNAL OF MEDICAL PHYSICS", "category": "Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000389232600007", "keywords": "Face perception; Face recognition; Congenital prosopagnosia; Image variability; Unfamiliar face matching", "title": "Face matching impairment in developmental prosopagnosia", "abstract": "Developmental prosopagnosia (DP) is commonly referred to as 'face blindness', a term that implies a perceptual basis to the condition. However, DP presents as a deficit in face recognition and is diagnosed using memory-based tasks. Here, we test face identification ability in six people with DP, who are severely impaired on face memory tasks, using tasks that do not rely on memory. First, we compared DP to control participants on a standardized test of unfamiliar face matching using facial images taken on the same day and under standardized studio conditions (Glasgow Face Matching Test; GFMT). Scores for DP participants did not differ from normative accuracy scores on the GFMT. Second, we tested face matching performance on a test created using images that were sourced from the Internet and so varied substantially due to changes in viewing conditions and in a person's appearance (Local Heroes Test; LHT). DP participants showed significantly poorer matching accuracy on the LHT than control participants, for both unfamiliar and familiar face matching. Interestingly, this deficit is specific to 'match' trials, suggesting that people with DP may have particular difficulty in matching images of the same person that contain natural day-to-day variations in appearance. We discuss these results in the broader context of individual differences in face matching ability.", "journal": "QUARTERLY JOURNAL OF EXPERIMENTAL PSYCHOLOGY", "category": "Psychology, Biological; Physiology; Psychology; Psychology, Experimental", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000399006400010", "keywords": "noninvasive; pharmacodynamics; depth of anesthesia; sevoflurane; child; infant", "title": "Effect of age on the performance of bispectral and entropy indices during sevoflurane pediatric anesthesia: a pharmacometric study", "abstract": "Background: Bispectral index (BIS) and entropy monitors have been proposed for use in children, but research has not supported their validity for infants. However, effective monitoring of young children may be even more important than for adults, to aid appropriate anesthetic dosing and reduce the chance of adverse consequences. This prospective study aimed to investigate the relationships between age and the predictive performance of BIS and entropy monitors in measuring the anesthetic drug effects within a pediatric surgery setting. Methods: We concurrently recorded BIS and entropy (SE/RE) in 48 children aged 1 month-12 years, undergoing general anesthesia with sevoflurane and fentanyl. Nonlinear mixed effects modeling was used to characterize the concentration-response relationship independently between the three monitor indicators with sevoflurane. The model's goodness-of-fit was assessed by prediction-corrected visual predictive checks. Model fit with age was evaluated using absolute conditional individual weighted residuals (vertical bar CIWRES vertical bar). The ability of BIS and entropy monitors to describe the effect of anesthesia was compared with prediction probabilities (PK) in different age groups. Intraoperative and awakening values were compared in the age groups. The correlation between BIS and entropy was also calculated. Results: vertical bar CIWRES vertical bar vs age showed an increasing trend in the model's accuracy for all three indicators. PK probabilities were similar for all three indicators within each age group, though lower in infants. The linear correlations between BIS and entropy in different age groups were lower for infants. Infants also tended to have lower values during surgery and at awakening than older children, while toddlers had higher values. Conclusions: Performance of both monitors improves as age increases. Our results suggest a need for the development of new monitor algorithms or calibration to better account for the age-specific EEG dynamics of younger patients.", "journal": "PEDIATRIC ANESTHESIA", "category": "Anesthesiology; Pediatrics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000390073900005", "keywords": "Control valves; Friction; Friction compensation; Nonlinear control; Sliding mode control", "title": "Friction compensation in control valves: Nonlinear control and usual approaches", "abstract": "This work presents different approaches to reduce the control valve friction effect on a process. One is to use the sliding mode control in different conditions and then to compare this controller to widely used algorithms and devices that reduce the control loop variability. The experiments were performed in the Flow Pilot Plant of Polytechnic School of the University of Sao Paulo with a pneumatic control valve with high friction in a flow control loop. The sliding mode controller yielded promising results, which can represent new horizons with respect to friction compensation in control valves.", "journal": "CONTROL ENGINEERING PRACTICE", "category": "Automation & Control Systems; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000389990300001", "keywords": "Fractional difference co-array; Wideband signal DOA estimation; Enhancement of DOFs; Virtual sensor", "title": "Fractional difference co-array perspective for wideband signal DOA estimation", "abstract": "In recent years, much attention has been focused on difference co-array perspective in DOA estimation field due to its ability to increase the degrees of freedom and to detect more sources than sensors. In this article, a fractional difference co-array perspective (FrDCA) is proposed by vectorizing structured second-order statistics matrices instead of conventional zero-lag covariance matrix. As a result, not only conventional virtual sensors but also the fractional ones can be utilized to further increase the degrees of freedom. In a sense, the proposed perspective can be viewed as an extended structured model to generate virtual sensors. Then, as a case study, four DOA estimation algorithms for wideband signal based on the FrDCA perspective are specifically presented. The fractional virtual sensors can be generated by dividing the wideband signal into many sub-band signals. Accordingly, the degree of freedom and the maximum number of resolvable sources are increased. The corresponding numerical simulation results validate the advantages and the effectiveness of the proposed perspective.", "journal": "EURASIP JOURNAL ON ADVANCES IN SIGNAL PROCESSING", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000387983700014", "keywords": "Block two-stage methods; Alternating iterations; Overlapping; Parallel computing; Shared memory; Distributed memory; Laplace's equation; Markov chains", "title": "Icarus: visualizer for de novo assembly evaluation", "abstract": "Data visualization plays an increasingly important role in NGS data analysis. With advances in both sequencing and computational technologies, it has become a new bottleneck in genomics studies. Indeed, evaluation of de novo genome assemblies is one of the areas that can benefit from the visualization. However, even though multiple quality assessment methods are now available, existing visualization tools are hardly suitable for this purpose. Here, we present Icarus-a novel genome visualizer for accurate assessment and analysis of genomic draft assemblies, which is based on the tool QUAST. Icarus can be used in studies where a related reference genome is available, as well as for non-model organisms. The tool is available online and as a standalone application.", "journal": "BIOINFORMATICS", "category": "Biochemical Research Methods; Biotechnology & Applied Microbiology; Computer Science, Interdisciplinary Applications; Mathematical & Computational Biology; Statistics & Probability", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000853231500028", "keywords": "MCDM; supply chain risks; spherical fuzzy; AHP; Vietnam", "title": "Agricultural Supply Chain Risks Evaluation with Spherical Fuzzy Analytic Hierarchy Process", "abstract": "The outbreak of the COVID-19 pandemic has impacted the development of the global economy. As most developing and third world countries are heavily dependent on agriculture and agricultural imports, the agricultural supply chains (ASC) in all these countries are exposed to unprecedented risks following COVID-19. Therefore, it is vital to investigate the impact of risks and create resilient ASC organizations. In this study, critical risks associated with ASC were assessed using a novel Analytical Hierarchy Process based on spherical fuzzy sets (SF-AHP). The findings indicated that depending on the scope and scale of the organization, supply risks, demand risks, financial risks, logistics and infrastructure risks, management and operational risks, policy and regulatory risks, and biological and environmental risks all have a significant impact on ASC. This research highlighted that themost significant criterion is specified as Transportation (TP), followed by Market (MA) and Policy (PO), respectively. Meanwhile, Technology (TL) is the least significant criterion. The study's findings can help managers with a holistic view of the agriculture supply chain risk mitigation. Furthermore, this study may assist managers in sharing information about the processing of agricultural products from top to bottom to manage risk in the supply chain.", "journal": "CMC-COMPUTERS MATERIALS & CONTINUA", "category": "Computer Science, Information Systems; Materials Science, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000773706000001", "keywords": "DPMSG; Parameter identification; Least square method", "title": "Application of the Improved Multipopulation Genetic Algorithm in the TMD Controlled System considering Soil-Structure Interaction", "abstract": "With the advent of globalization, computing speed has increased tremendously, greatly advancing algorithm research in multiple fields. This paper studies the parameter optimization problem of the improved multipopulation genetic algorithm in the tuned mass damper (TMD) structure considering the soil-structure interaction (SSI) effect. The Newmark time-domain analysis method was used to analyze the dynamic response of a 40-story building under the excitation of EL Centro waves and Tangshan waves in China, respectively. The mass, damping coefficient, and spring stiffness of TMD system are used as the design variables of the controller. To reduce structural damage and obtain better comfort, the displacement response and acceleration response are optimized simultaneously in this paper, achieving multiobjective optimization. The results show that the improved multipopulation genetic algorithm method has faster convergence speed and greater accuracy than the traditional genetic algorithm; thus it can be applied to the TMDs parameter optimization of high-rise buildings. Besides, the soil types have a great influence on TMD parameter optimization and structural time history response. If ignoring SSI effect will lead to underestimation of parameter design, the reason is that the soft soil foundations can absorb a lot of seismic energy compared with rigid foundations and then reduce the effect of seismic excitation on the structure. The intention of the research helps researchers to better understand vibration control and provides suggestions for the application of TMD in high-rise buildings.", "journal": "ADVANCES IN CIVIL ENGINEERING", "category": "Construction & Building Technology; Engineering, Civil", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000791357800007", "keywords": "Energy efficiency; Process intensification; Ternary azeotropic mixture; Multi-objective optimization; Sustainability", "title": "Towards sustainable separation of the ternary azeotropic mixture based on the intensified reactive-extractive distillation configurations and multi-objective particle swarm optimization", "abstract": "The separation of ternary azeotropic systems has received significant interest as it enables the recovery of value-added organic solvents, subsequently contribute towards environmental protection. In this work, we propose a novel approach that involves the conceptual design, multi-objective optimization, and process evaluations for developing two different processes, i.e., double-column reactive-extractive distillation (DCRED) and reactive-extractive dividing wall column (REDWC), for the separation of ethanol/tert-butanol/water ternary azeotropic mixture. The conceptual design of the proposed processes was conducted using kinetic and thermodynamic analysis while optimal operating conditions of the established processes were obtained via multi-objective particle swarm optimization algorithm. Then, both developed processes were evaluated based on the total annual cost (TAC), CO2 emissions, and thermodynamic efficiency. From the steady-state simulation, DCRED and REDWC provides a TAC of 1.056 x 10(6) US$ and 1.117 x 10(6) US$, respectively. Likewise, it provides CO2 emissions of 731.27 kg/h and 733.42 kg/h, respectively. The energy efficiency of the DCRED and REDWC were found to be 1.285% and 1.055%, respectively. Relative to the conventional extractive distillation process, the TAC and CO2 emission for the proposed DCRED reduced significantly by 55.4% and 61.8%, respectively. Similar reduction was also observed for the REDWC which provides 52.8% and 61.7% lower TAC and CO2 with respect to the conventional process. In addition, the thermodynamic efficiency of the developed DCRED and REDWC processes are improved by 40.4% and 15.3% in comparison to the conventional extractive distillation scheme.", "journal": "JOURNAL OF CLEANER PRODUCTION", "category": "Green & Sustainable Science & Technology; Engineering, Environmental; Environmental Sciences", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000733269400008", "keywords": "Image edge detection; Optical imaging; Optical sensors; Radar polarimetry; Adaptive optics; Synthetic aperture radar; Feature extraction; Canny edge detection; optical image; registration; scale-invariant feature transform (SIFT); synthetic aperture radar (SAR)", "title": "Combination of SIFT and Canny Edge Detection for Registration Between SAR and Optical Images", "abstract": "Scale-invariant feature transform (SIFT) has been successfully used for optical image registration, but it cannot produce satisfying results when directly applied to synthetic aperture radar (SAR) images. In the present study, a novel method is proposed for registration between SAR and optical images. First, candidate keypoints are detected using SIFT algorithm. Then, Canny edge detection algorithm is adopted to remove the edge points which are wrong candidate points. Next, SIFT descriptors are generated from these correct keypoints. Last, the fast library for approximate nearest neighbors (FLANNs) algorithm is applied to search matching points in high-dimensional space. Experimental results show that the proposed approach is significantly more accurate and much faster than the original SIFT algorithm.", "journal": "IEEE GEOSCIENCE AND REMOTE SENSING LETTERS", "category": "Geochemistry & Geophysics; Engineering, Electrical & Electronic; Remote Sensing; Imaging Science & Photographic Technology", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000770088700001", "keywords": "CSF infection; External ventricular drain; Network meta-analysis; Tunneled; Tunnel length", "title": "Electromechanical conversion efficiency of GaN NWs: critical influence of the NW stiffness, the Schottky nano-contact and the surface charge effects", "abstract": "The piezoelectric nanowires (NWs) are considered as promising nanomaterials to develop high-efficient piezoelectric generators. Establishing the relationship between their characteristics and their piezoelectric conversion properties is now essential to further improve the devices. However, due to their nanoscale dimensions, the NWs are characterized by new properties that are challenging to investigate. Here, we use an advanced nano-characterization tool derived from AFM to quantify the piezo-conversion properties of NWs axially compressed with a well-controlled applied force. This unique technique allows to establish the direct relation between the output signal generation and the NW stiffness and to quantify the electromechanical coupling coefficient of GaN NWs, which can reach up to 43.4%. We highlight that this coefficient is affected by the formation of the Schottky nano-contact harvesting the piezo-generated energy, and is extremely sensitive to the surface charge effects, strongly pronounced in sub-100 nm wide GaN NWs. These results constitute a new building block in the improvement of NW-based nanogenerator devices.", "journal": "NANOSCALE", "category": "Chemistry, Multidisciplinary; Nanoscience & Nanotechnology; Materials Science, Multidisciplinary; Physics, Applied", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000745763800004", "keywords": "Cloud computing; Big data; IoT; Mapreduce; Stream cipher algorithm; Orthogonal learning particle swarm optimization; Improved K mediod clustering", "title": "Internet of Things Big Data Security in Cloud via Stream Cipher and Clustering Model", "abstract": "Big Data in IoT and Cloud Computing are two important developments over the years, enabling companies to provide efficient and effective IT services. To ensure the security of the information being processed, the information is usually stored within a year in a secure database. However, Encrypted Data Cloud introduces new duplication problems, which are important for storing large data. The proposed methodology consists of three segments that are Mapreduce framework, security, and authentication. Initially, the input dataset is mapped into multiple groups to minimize the volume of big data to maintain a strategic distance from scalability issues. To enhance the performance of SCA, the key values are optimally selected with the help of orthogonal learning particle swarm optimization (OLPSO) algorithm is utilized. After encryption process, the data are stored in cloud. In authentication phase, attribute-based access control mechanism is utilized. The proposed methodology performance is analyzed in terms of different metrics and performance is compared with different algorithms.", "journal": "WIRELESS PERSONAL COMMUNICATIONS", "category": "Telecommunications", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000773235100001", "keywords": "Wind farms; Optimization; Wind turbines; Linear programming; Costs; Layout; Genetic algorithms; Electric charge particles optimization; wind farms; optimization", "title": "Design of 3D Wind Farm Layout Using an Improved Electric Charge Particles Optimization With Hub-Height Variety", "abstract": "The limited resources of land and wind have increased the requirement for better designing of wind farm layouts in the wind industry. A three-dimensional layout of wind turbines (WTs) is proposed in this paper to optimize the horizontal and vertical layouts of wind farms. The issue of optimization is a highly challenging task as it involves many variables and requires handling conflicting criteria. Classical optimization algorithms cannot handle this problem due to discontinuity and nonlinear behavior. Considering this, a metaheuristic algorithm called improved electric charged particle optimization (ECPO) is developed and implemented in four different shapes and cases studies. All the scenarios implemented have the same wind distribution and obstacles. The result shows that ECPO achieves better performance in the case study when the maximum number of a wind turbine is the same as the number of grids when compared to three well-known metaheuristic algorithms, which are binary particle swarm optimization (BPSO), genetic algorithm (GA), and artificial bee colony (ABC). By implementing the three-dimensional WFLO, the levelized cost of energy (LCOE) will increase by 7% in the case of the optimal number of WT and 3% in the case of the fixed number of WT.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000748957000012", "keywords": "shilling attack; heterogeneous information network; recommend system; semantic information", "title": "Semantic Shilling Attack against Heterogeneous Information Network Based Recommend Systems", "abstract": "The recommend system has been widely used in many web application areas such as e-commerce services. With the development of the recommend system, the HIN modeling method replaces the traditional bipartite graph modeling method to represent the recommend system. But several studies have already showed that recommend system is vulnerable to shilling attack (injecting attack). However, the effectiveness of how traditional shilling attack has rarely been studied directly in the HIN model. Moreover, no study has focused on how to enhance shilling attacks against HIN recommend system by using the high-level semantic information. This work analyzes the relationship between the high-level semantic information and the attacking effects in HIN recommend system. This work proves that attack results are proportional to the high-level semantic information. Therefore, we propose a heuristic attack method based on high-level semantic information, named Semantic Shilling Attack (SSA) on a HIN recommend system (HERec). This method injects a specific score into each selected item related to the target in semantics. It ensures transmitting the misleading information towards target items and normal users, and attempts to interfere with the effect of the recommend system. The experiment is dependent on two real-world datasets, and proves that the attacking effect is positively correlate with the number of meta-paths. The result shows that our method is more effective when compared with existing baseline algorithms.", "journal": "IEICE TRANSACTIONS ON INFORMATION AND SYSTEMS", "category": "Computer Science, Information Systems; Computer Science, Software Engineering", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000554401700189", "keywords": "Pool-seq; RAD-seq; Seascape genomics; Environmental association; Comparative phylogeography; Marine invertebrates", "title": "Brain Science, AI and Humanoid Robots", "abstract": "Background As global change and anthropogenic pressures continue to increase, conservation and management increasingly needs to consider species' potential to adapt to novel environmental conditions. Therefore, it is imperative to characterise the main selective forces acting on ecosystems, and how these may influence the evolutionary potential of populations and species. Using a multi-model seascape genomics approach, we compare putative environmental drivers of selection in three sympatric southern African marine invertebrates with contrasting ecology and life histories: Cape urchin (Parechinus angulosus), Common shore crab (Cyclograpsus punctatus), and Granular limpet (Scutellastra granularis). Results Using pooled (Pool-seq), restriction-site associated DNA sequencing (RAD-seq), and seven outlier detection methods, we characterise genomic variation between populations along a strong biogeographical gradient. Of the three species, onlyS. granularisshowed significant isolation-by-distance, and isolation-by-environment driven by sea surface temperatures (SST). In contrast, sea surface salinity (SSS) and range in air temperature correlated more strongly with genomic variation inC. punctatusandP. angulosus. Differences were also found in genomic structuring between the three species, with outlier loci contributing to two clusters in the East and West Coasts forS. granularisandP. angulosus, but not forC. punctatus. Conclusion The findings illustrate distinct evolutionary potential across species, suggesting that species-specific habitat requirements and responses to environmental stresses may be better predictors of evolutionary patterns than the strong environmental gradients within the region. We also found large discrepancies between outlier detection methodologies, and thus offer a novel multi-model approach to identifying the principal environmental selection forces acting on species. Overall, this work highlights how adding a comparative approach to seascape genomics (both with multiple models and species) can elucidate the intricate evolutionary responses of ecosystems to global change.", "journal": "BASIC & CLINICAL PHARMACOLOGY & TOXICOLOGY", "category": "Pharmacology & Pharmacy; Toxicology", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000572926900011", "keywords": "Optimal design; Particle swarm optimization (PSO); Chaotic PSO; Cellular PSO; Quantum-behaved PSO; Kaibel dividing wall column", "title": "Optimal design of Kaibel dividing wall columns based on improved particle swarm optimization methods", "abstract": "Dividing wall columns (DWCs) are able to reduce operating costs and save capital costs for distillation columns, which verifies that DWC is a useful strategy in terms of distillation process intensification. DWCs are better choices than the corresponding conventional distillation sequences from both economical and environmental point of views. As DWC saves energy considerably and reduces CO2 emissions, it proves to be a breakthrough towards sustainable distilling. The four-product Kaibel DWC can further intensify the distillation process, and save energy cost about 40%. However, due to the complicated structures and strong interactions, the optimal design of the four-product Kaibel DWC is challenging to solve through conventional optimization methods. Therefore, this paper investigates the applicability of probabilistic global optimization algorithms, including standard particle swarm optimization (PSO), chaotic PSO, cellular PSO, and quantum-behaved PSO algorithms. This paper aims to explore the most appropriate improved PSO algorithm for the Kaibel DWC. The standard PSO and chaotic PSO show the premature convergence in the optimization process, while the cellular PSO and quantum-behaved PSO can prevent premature convergence. The chaotic PSO algorithm is more appropriate for the optimization of the continuous function, and it is less suitable for the optimization of the Kaibel DWC. The quantum-behaved PSO algorithm is relatively not appropriate for the optimization of the Kaibel DWC because of the long calculation tiyme for rigorous simulation. The cellular PSO algorithm, based on the concept of cellular neighborhood, is the most appropriate algorithm for the optimization of the Kaibel DWC. (C) 2020 Elsevier Ltd. All rights reserved.", "journal": "JOURNAL OF CLEANER PRODUCTION", "category": "Green & Sustainable Science & Technology; Engineering, Environmental; Environmental Sciences", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000586234300001", "keywords": "software testing; branch coverage; genetic algorithm; multiple-search genetic algorithm; network systems", "title": "Analyzing the Performance of the Multiple-Searching Genetic Algorithm to Generate Test Cases", "abstract": "Software testing using traditional genetic algorithms (GAs) minimizes the required number of test cases and reduces the execution time. Currently, GAs are adapted to enhance performance when finding optimal solutions. The multiple-searching genetic algorithm (MSGA) has improved upon current GAs and is used to find the optimal multicast routing in network systems. This paper presents an analysis of the optimization of test case generations using the MSGA by defining suitable values of MSGA parameters, including population size, crossover operator, and mutation operator. Moreover, in this study, we compare the performance of the MSGA with a traditional GA and hybrid GA (HGA). The experimental results demonstrate that MSGA reaches the maximum executed branch statements in the lowest execution time and the smallest number of test cases compared to the GA and HGA.", "journal": "APPLIED SCIENCES-BASEL", "category": "Chemistry, Multidisciplinary; Engineering, Multidisciplinary; Materials Science, Multidisciplinary; Physics, Applied", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000602570200001", "keywords": "prefabrication; IFC standard; production schedule; segmentation; genetic algorithm", "title": "Automated Optimization for the Production Scheduling of Prefabricated Elements Based on the Genetic Algorithm and IFC Object Segmentation", "abstract": "Background: With the ever-increasing availability of data and a higher level of automation and simulation, production scheduling in the factory for prefabrication can no longer be seen as an autonomous solution. Concepts such as building information modelling (BIM), graphic techniques, databases, and interface development as well as heightened emphasis on overall-process optimization topics increase the pressure to connect to and interact with interrelated tasks and procedures. Methods: The automated optimization framework detailed in this study intended to generate optimal schedule of prefabricated component production based on the manufacturing process model and genetic algorithm method. An extraction and segmentation approach based on industry foundation classes (IFC) for prefabricated component production is discussed. During this process, the position and geometric information of the prefabricated components are adjusted and output in the extracted IFC file. Then, the production process and the completion time of each process have been examined and simulated with the genetic algorithm. Lastly, the automated optimization solution can be formed by the linking production scheduling database and the computational environment. Results: This shows that the implementation of the automated optimization framework for the production scheduling of the prefabricated elements improves the operability and accuracy of the production process. Conclusions: Based on the integration technique discussed above, the data transmission and integration in the mating application program is achieved by linking the Python-based application, the Structured Query Language (SQL) database and the computational environment. The implementation of the automated optimization framework model enables BIM models to play a better foundational role in patching up the technical gaps between prefabricated building designers and element producers.", "journal": "PROCESSES", "category": "Engineering, Chemical", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000594148000001", "keywords": "Sign direction; Intelligent dynamic exit sign system; Route planning; BIM", "title": "BIM-based automated determination of exit sign direction for intelligent building sign systems", "abstract": "This study proposes an approach named automated determination of exit sign direction (ADESD) to automatically determine the directions of building exit signs. ADESD generates sign directions for a building by four steps: generate an indoor navigation graph network of the building; integrate exit signs into the navigation network; calculate recommended directions of exit signs based on their shortest accessible evacuation routes, and calculate negated directions of exit signs corresponding to hazardous areas. Novel algorithms are proposed in ADESD to generate spatial relationship between exit signs and exits automatically and determine exit sign directions accurately. A case study is used to validate the feasibility of ADESD. The results demonstrate that ADESD can efficiently calculate both the recommended and negated directions of an intelligent dynamic exit sign system to respond to a fire emergency with real-time information.", "journal": "AUTOMATION IN CONSTRUCTION", "category": "Construction & Building Technology; Engineering, Civil", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000577557300018", "keywords": "Pattern recognition; Fault detection and diagnosis; Sensor; Chiller plant; Data mining; Clustering", "title": "Novel pattern recognition-enhanced sensor fault detection and diagnosis for chiller plant", "abstract": "In building application, sensor faults in chilled water system would cause extra electricity consumption or thermal comfort problems. In the computerized building management system, it is important to automatically detect, diagnose and correct different categories of sensor faults. To realise this aim, a novel 2-stage pattern recognition-enhanced sensor fault detection and diagnosis (PRe-SFDD) was formulated. At the first-stage pattern recognition, various featuring patterns were generated through sensor reading datasets from both fault-free and different faulty test cases. At the second-stage pattern recognition, one-day featuring patterns were used to diagnose the sensor faults of positive bias, negative bias, precision degradation and general drift; while 3-day featuring patterns would allow further recognise the drift fault to be positive or negative. Hence, different categories of sensor faults could be automatically detected, diagnosed and corrected through the proposed pattern recognition strategy. For a representative chiller plant, it was found that the successful diagnosis ratio of the 2-stage PRe-SFDD were 97.9%, 100%, 96.4%, 95.4% and 98.1% for positive bias, negative bias, precision degradation, positive drift and negative drift, respectively. In addition, characteristic curves of clustering score values were constructed for correction of the extent of sensor faults. (C) 2020 Elsevier B.V. All rights reserved.", "journal": "ENERGY AND BUILDINGS", "category": "Construction & Building Technology; Energy & Fuels; Engineering, Civil", "annotated_keywords": [], "label": "1", "title_label": "0"}
{"id": "WOS:000331595100034", "keywords": "Traditional Korean houses; Seoul; Urban redevelopment; Historic urban artifacts; Regression analysis", "title": "Unraveling the factors determining the redevelopment of Seoul's historic hanoks", "abstract": "Recent studies found that the number of traditional Korean houses called hanoks-in Seoul has decreased substantially over the last 50 years. Yet very little was known about the specific causes of large-scale demolition and redevelopment of hanoks. Here, based upon newly built parcel-level datasets of all hanoks in Seoul's 1936 boundary, our probit regression models showed that the combined effects of parcel, neighborhood, and urban-scale factors may explain the probability of hanoks' loss between 2002 and 2013. The results indicated that hanoks that were relatively new, large, and previously converted to a different use were more likely to be lost than older, smaller, and single-family residential-use hanoks. Those with desirable environmental qualities, such as a southern orientation and being part of a cluster of hanoks, were more resistant to redevelopment. The induced-development impacts of nearby urban projects were significant but this relationship varied substantially depending on the locations of the affected hanoks. (C) 2013 Elsevier Ltd. All rights reserved.", "journal": "HABITAT INTERNATIONAL", "category": "Development Studies; Environmental Studies; Regional & Urban Planning; Urban Studies", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000342277300008", "keywords": "Image encryption; Chaos; S-box; Substitution; Diffusion", "title": "Chaotic image encryption based on circular substitution box and key stream buffer", "abstract": "A new image encryption algorithm based on spatiotemporal chaotic system is proposed, in which the circular S-box and the key stream buffer are introduced to increase the security. This algorithm is comprised of a substitution process and a diffusion process. In the substitution process, the S-box is considered as a circular sequence with a head pointer, and each image pixel is replaced with an element of S-box according to both the pixel value and the head pointer, while the head pointer varies with the previous substituted pixel. In the diffusion process, the key stream buffer is used to cache the random numbers generated by the chaotic system, and each image pixel is then enciphered by incorporating the previous cipher pixel and a random number dependently chosen from the key stream buffer. A series of experiments and security analysis results demonstrate that this new encryption algorithm is highly secure and more efficient for most of the real image encryption practices. (C) 2014 Elsevier B.V. All rights reserved.", "journal": "SIGNAL PROCESSING-IMAGE COMMUNICATION", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000369066200008", "keywords": "biodiversity management; bird conservation; hierarchical approach; land abandonment; land cover change; MEDFIRE model; multiscale modelling; species distribution models; vegetation dynamics", "title": "Predicting the future effectiveness of protected areas for bird conservation in Mediterranean ecosystems under climate change and novel fire regime scenarios", "abstract": "AimGlobal environmental changes challenge traditional conservation approaches based on the selection of static protected areas due to their limited ability to deal with the dynamic nature of driving forces relevant to biodiversity. The Natura 2000 network (N2000) constitutes a major milestone in biodiversity conservation in Europe, but the degree to which this static network will be able to reach its long-term conservation objectives raises concern. We assessed the changes in the effectiveness of N2000 in a Mediterranean ecosystem between 2000 and 2050 under different combinations of climate and land cover change scenarios. LocationCatalonia, Spain. MethodsPotential distribution changes of several terrestrial bird species of conservation interest included in the European Union's Birds Directive were predicted within an ensemble-forecasting framework that hierarchically integrated climate change and land cover change scenarios. Land cover changes were simulated using a spatially explicit fire-succession model that integrates fire management strategies and vegetation encroachment after the abandonment of cultivated areas as the main drivers of landscape dynamics in Mediterranean ecosystems. ResultsOur results suggest that the amount of suitable habitats for the target species will strongly decrease both inside and outside N2000. However, the effectiveness of N2000 is expected to increase in the next decades because the amount of suitable habitats is predicted to decrease less inside than outside this network. Main conclusionsSuch predictions shed light on the key role that the current N2000may play in the near future and emphasize the need for an integrative conservation perspective wherein agricultural, forest and fire management policies should be considered to effectively preserve key habitats for threatened birds in fire-prone, highly dynamic Mediterranean ecosystems. Results also show the importance of considering landscape dynamics and the synergies between different driving forces when assessing the long-term effectiveness of protected areas for biodiversity conservation.", "journal": "DIVERSITY AND DISTRIBUTIONS", "category": "Biodiversity Conservation; Ecology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000367070600003", "keywords": "Female genital tract; HIV; mucosal immunity; racial disparities; sexually transmitted infections", "title": "Dealing with missing data: An inpainting application to the MICROSCOPE space mission", "abstract": "Missing data are a common problem in experimental and observational physics. They can be caused by various sources, such as an instrument's saturation, a contamination from an external event, or a data loss. In particular, they can have a disastrous effect when one is seeking to characterize a colored-noise-dominated signal in Fourier space, since they create a spectral leakage that can artificially increase the noise. It is therefore important to either take them into account or to correct for them prior to, e.g., a least-square fit of the signal to be characterized. In this paper, we present an application of the inpainting algorithm to mock MICROSCOPE data. Inpainting is based on a sparsity assumption, and has already been used in various astrophysical contexts; MICROSCOPE is a French Space Agency mission (whose launch is expected in 2016) that aims to test the weak equivalence principle down to the 10(-15) level. We then explore the inpainting dependence on the number of gaps and the total fraction of missing values. We show that, in a worst-case scenario, after reconstructing missing values with inpainting a least-square fit may allow us to significantly measure a 1.1 x 10(-15) equivalence principle violation signal, which is sufficiently close to the MICROSCOPE requirements to implement inpainting in the official MICROSCOPE data processing and analysis pipeline. Together with the previously published KARMA method, inpainting will then allow us to independently characterize and cross-check an equivalence principle violation signal detection down to the 10(-15) level.", "journal": "PHYSICAL REVIEW D", "category": "Astronomy & Astrophysics; Physics, Particles & Fields", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000352069700012", "keywords": "M-CSF; HE4; CA 125; Epithelial ovarian cancer; Tumor markers", "title": "What is the next generation therapeutic strategy for castration-resistant prostate cancer", "abstract": "Prostate cancer (PCa) is one of the most common cancers in the world. Since androgen receptor (AR) signal plays key roles in the PCa progression, targeting androgens via the current androgen deprivation therapy (ADT) is the main therapeutic strategy for advanced PCa. However, most patients who receive ADT, including the second generation anti-androgens enzalutamide (also known as MDV3100) may finally develop the castration (or anti-androgen) resistance after 12-24 months treatment. In the manuscript by Asangani et al., the authors demonstrated that targeting the amino-terminal bromodomains of BRD4 could preferentially suppress human castration-resistant prostate cancer (CRPC) cell lines. While further studies are required to understand the full impact of their findings, the innovative approach provides a potential novel epigenetic approach for the concerted blockade of oncogenic drivers in CRPC.", "journal": "ASIAN JOURNAL OF ANDROLOGY", "category": "Andrology; Urology & Nephrology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000358786600103", "keywords": "Texture Synthesis; Fabrication; By-example Modeling", "title": "By-Example Synthesis of Structurally Sound Patterns", "abstract": "Several techniques exist to automatically synthesize a 2D image resembling an input exemplar texture. Most of the approaches optimize a new image so that the color neighborhoods in the output closely match those in the input, across all scales. In this paper we revisit by-example texture synthesis in the context of additive manufacturing. Our goal is to generate not only colors, but also structure along output surfaces: given an exemplar indicating 'solid' and 'empty' pixels, we generate a similar pattern along the output surface. The core challenge is to guarantee that the pattern is not only fully connected, but also structurally sound. To achieve this goal we propose a novel formulation for on-surface by-example texture synthesis that directly works in a voxel shell around the surface. It enables efficient local updates to the pattern, letting our structural optimizer perform changes that improve the overall rigidity of the pattern. We use this technique in an iterative scheme that jointly optimizes for appearance and structural soundness. We consider fabricability constraints and a user-provided description of a force profile that the object has to resist. Our results fully exploit the capabilities of additive manufacturing by letting users design intricate structures along surfaces. The structures are complex, yet they resemble input exemplars, resulting in a modeling tool accessible to casual users.", "journal": "ACM TRANSACTIONS ON GRAPHICS", "category": "Computer Science, Software Engineering", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000373250700005", "keywords": "Peano algebra; labeled graph; stratified graph; morphism of partial algebras; structured path; accepted structured path; inference process", "title": "Systems of knowledge representation based on stratified graphs. Application in Natural Language Generation", "abstract": "The concept of stratified graph introduces some method of knowledge representation (see [Tandareanu, N., Knowledge representation by labeled stratified graphs, Proc. 8th World Multi-Conference on Systemics, Cybernetics and Informatics, 5 (2004), 345-350; Tandareanu, N., Proving the Existence of Labelled Stratified Graphs, An. Univ. Craiova Ser. Mat. Inform., 27 (2000), 81-92]) The inference process developed for this method uses the paths of the stratified graphs, an order between the elementary arcs of a path and some results of universal algebras. The order is defined by considering a structured path instead of a regular path. In this paper we define the concept of system of knowledge representation as a tuple of the following components: a stratified graph g, a partial algebra Y of real objects, an embedding mapping (an injective mapping that embeds the nodes of g into objects of Y) and a set of algorithms such that each of them can combine two objects of Y to get some other object of Y. We define also the concept of inference process performed by a system of knowledge processing in which the interpretation of the symbolic elements is defined by means of natural language constructions. In this manner we obtained a mechanism for texts generation in a natural language (for this approach, Romanian).", "journal": "CARPATHIAN JOURNAL OF MATHEMATICS", "category": "Mathematics, Applied; Mathematics", "annotated_keywords": [], "label": "1", "title_label": "1"}
{"id": "WOS:000372370000034", "keywords": "Adaptive extended Kalman filter (AEKF); fault-tolerant control (FTC); interior permanent magnet synchronous motor (IPMSM); position sensor fault", "title": "Enhanced Fault-Tolerant Control of Interior PMSMs Based on an Adaptive EKF for EV Traction Applications", "abstract": "This paper proposes an enhanced sensor fault-tolerant control (FTC) scheme of an interior permanent magnet synchronous motor (IPMSM) drive for the electric vehicle (EV) traction applications. For a safe and continuous operation of the modern EV, the drive has to acquire robustness features for position sensor failures. Hence, the proposed FTC is based on an adaptive extended Kalman filter (AEKF), which continuously estimates both the states and covariance matrices that describe the statistic characters of the system. Under a position sensor failure, the proposed FTC scheme instantly detects sensor fault and reconfigures the traction system with a virtual sensor to provide an EV with a necessary limp home capability. Unlike the conventional EKF with fixed covariance matrices, the proposed AEKF exhibits the robustness to the system stochastic noises and the transient operating conditions. Simulation on MATLAB/Simulink and experimental results on the IPMSM test bed with a TMS320F28335 DSP under various transient operating conditions are presented to demonstrate the effectiveness and feasibility of the proposed FTC scheme in comparison to the FTC with the conventional EKF. The comparative results indicate that the proposed AEKF more precisely estimates the rotor position with features robust to the position sensor failures than the conventional EKF.", "journal": "IEEE TRANSACTIONS ON POWER ELECTRONICS", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000374089900002", "keywords": "disaster; military; preparedness", "title": "Wikidata as a semantic framework for the Gene Wiki initiative", "abstract": "Open biological data are distributed over many resources making them challenging to integrate, to update and to disseminate quickly. Wikidata is a growing, open community database which can serve this purpose and also provides tight integration with Wikipedia. In order to improve the state of biological data, facilitate data management and dissemination, we imported all human and mouse genes, and all human and mouse proteins into Wikidata. In total, 59 721 human genes and 73 355 mouse genes have been imported from NCBI and 27 306 human proteins and 16 728 mouse proteins have been imported from the Swissprot subset of UniProt. As Wikidata is open and can be edited by anybody, our corpus of imported data serves as the starting point for integration of further data by scientists, the Wikidata community and citizen scientists alike. The first use case for these data is to populate Wikipedia Gene Wiki infoboxes directly from Wikidata with the data integrated above. This enables immediate updates of the Gene Wiki infoboxes as soon as the data in Wikidata are modified. Although GeneWiki pages are currently only on the English language version of Wikipedia, the multilingual nature of Wikidata allows for usage of the data we imported in all 280 different language Wikipedias. Apart from the Gene Wiki infobox use case, a SPARQL endpoint and exporting functionality to several standard formats (e.g. JSON, XML) enable use of the data by scientists. In summary, we created a fully open and extensible data resource for human and mouse molecular biology and biochemistry data. This resource enriches all the Wikipedias with structured information and serves as a new linking hub for the biological semantic web.", "journal": "DATABASE-THE JOURNAL OF BIOLOGICAL DATABASES AND CURATION", "category": "Mathematical & Computational Biology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000383595600149", "keywords": "carboplatin; hypersensitivity reaction; ovarian cancer", "title": "Effect of Prophylactic Extended-Infusion Carboplatin on Incidence of Hypersensitivity Reactions in Patients with Ovarian, Fallopian Tube, or Peritoneal Carcinomas", "abstract": "STUDY OBJECTIVE To determine whether extended-infusion carboplatin, initiated at approximately the eighth cumulative carboplatin cycle and prior to development of carboplatin hypersensitivity, reduces the incidence of carboplatin hypersensitivity reactions in patients with ovarian, fallopian tube, or peritoneal cancer. DESIGN Retrospective chart review. SETTING Large integrated health system. PATIENTS A total of 326 patients with ovarian, fallopian tube, or primary peritoneal cancer who received at least eight cumulative cycles of carboplatin between January 2007 and September 2014 were included. Of these, 161 patients received all doses of carboplatin infused over 30 or 60 minutes (standard-infusion group [total of 1317 carboplatin cycles]), and 165 patients received the 3-hour extended infusion of carboplatin administered at approximately the eighth cumulative cycle and prior to development of a hypersensitivity reaction (extended-infusion group [total of 1527 carboplatin cycles]). MEASUREMENTS AND MAIN RESULTS Baseline characteristics were similar between the groups, except significantly more patients in the extended-infusion group received triple premedication therapy prior to infusion (p< 0.001). Hypersensitivity reactions occurred in 64 patients (40%) who received standard-infusion carboplatin and 40 patients (24.2%) who received extended-infusion carboplatin (p= 0.0027). The median cycle of hypersensitivity reaction development did not differ significantly between the groups: 9 cycles in patients who received standard-infusion versus 11 cycles in patients who received extended-infusion carboplatin (p= 0.06). Through regression analysis, the premedication regimen received prior to carboplatin infusion was the only variable significantly associated with hypersensitivity reactions (odds ratio 0.59, 95% confidence interval 0.36-0.97, p= 0.038). CONCLUSION Patients who received extended-infusion carboplatin experienced a lower incidence of hypersensitivity reactions than patients who received standard-infusion carboplatin, which may be attributed to the triple premedication regimen received more frequently in patients in the extended-infusion group.", "journal": "PHARMACOTHERAPY", "category": "Pharmacology & Pharmacy", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000429901700013", "keywords": "Diabetes; Prediabetes; Epidemiology; Cameroon", "title": "Prevalence of prediabetes and diabetes mellitus among adults residing in Cameroon: A systematic review and meta-analysis", "abstract": "Aims: To summarize current data on the prevalence of prediabetes and diabetes mellitus in Cameroon. Methods: Population-based cross-sectional studies published between January 1, 2000 and April 30, 2017 including apparently healthy adults residing in Cameroon were searched in PubMed, EMBASE, African Journals Online, and African Index Medicus. We used a random-effects model to pool data. Results: All included studies had a low risk of bias. Six studies were conducted in an urban setting only, one in a rural setting only, and five in both settings. The overall prevalence of diabetes mellitus was 5.8% (95% CI 4.1-7.9; 12 studies) in a pooled sample of 37,147 participants. The prevalence of prediabetes was 7.1% (95% CI: 3.0-21.9; 4 studies) in a pooled sample of 5,872 people. In univariable meta-regression analysis, the prevalence of diabetes mellitus increased with age, hypertension, overweight and obesity. There was no difference for sex and settings (rural versus urban). Conclusions: This study reports a relatively high prevalence of diabetes mellitus and prediabetes in Cameroon, with no difference between urban and rural settings and between sexes. The main drivers include increasing age, overweight and obesity. Community-based educational programs are needed to tackle the burden of the disease in the country. (C) 2018 Elsevier B.V. All rights reserved.", "journal": "DIABETES RESEARCH AND CLINICAL PRACTICE", "category": "Endocrinology & Metabolism", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000419402000017", "keywords": "bleeding disorder; bleeding score; diagnosis; hemorrhage; risk assessment", "title": "The ISTH Bleeding Assessment Tool and the risk of future bleeding", "abstract": "Background: The ISTH Bleeding Assessment Tool (ISTH-BAT) is a diagnostic tool used in subjects with suspected inherited bleeding disorders. Aim: To evaluate whether the ISTH-BAT, applied at first work-up in a tertiary-care center, predicts the risk of subsequent bleeding events. Methods: This was an observational cohort study including all consecutive subjects, of either sex and any age, referred between 2011 and 2015 because of a suspected bleeding disorder. The analysis was restricted to those with an ISTH-BAT score of >= 3. Incidence rates (IRs) of major bleeding (MB) and clinically relevant non-major bleeding (CRNMB) events were calculated as the number of events over accrued person-years. The main analysis was performed with Cox regression analysis, assessing an ISTH-BAT score of <= 5 versus a score of > 5, as well as the score as a continuous variable, and various covariates (sex, age, and presence/absence of a final diagnosis). Results: One hundred and thirty-six subjects had a median ISTH-BAT score of 4 (range 318). Eleven subjects (8.1%) had a bleeding event during follow-up (one MB event; 10 CRNMB events). The overall IR of bleeding events per 100 person-years was 3.7 (95% confidence interval [CI] 1.8-6.6). No difference was observed between subjects with an ISTH-BAT score of <= 5 and those with a score of > 5 (hazard ratio [HR] 1.2, 95% CI 0.3-4.6). The results were similar when the ISTH-BAT score was considered as a continuous variable (HR 1.1, 95% CI 0.9-1.4). The IR of bleeding was increased in individuals with a diagnosis of a hemostatic defect (IR of 7.5 per 100 person-years; HR 3.0, 95% CI 0.8-11.8). Conclusions: The ISTH-BAT does not identify patients at increased risk of future bleeding events.", "journal": "JOURNAL OF THROMBOSIS AND HAEMOSTASIS", "category": "Hematology; Peripheral Vascular Disease", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000446390800001", "keywords": "Computation offloading; multiaccess communication; non-orthogonal multiple access (NOMA); power allocation; resource management", "title": "Optimal Offloading in Fog Computing Systems With Non-Orthogonal Multiple Access", "abstract": "Fog computing has recently become a promising method to meet the increasing computation demands from mobile applications in the Internet of Things (IoT). In fog computing, the computation tasks of an IoT device can be offloaded to fog nodes. Due to the limited computation capacity of a fog node, the IoT device may try to offload its tasks to multiple fog nodes. In this paper, to improve the offloading efficiency, downlink non-orthogonal multiple access is applied in fog computing systems such that the IoT device can perform simultaneous offloading to multiple fog nodes. Then, to maximize the long-term average system utility, a task and power allocation problem for computation offloading is formulated subject to task delay and energy cost constraints. By the Lyapunov optimization method, the original problem is transformed to an online optimization problem in each time slot, which is non-convex. Accordingly, we propose an algorithm to solve the non-convex online optimization problem with polynomial complexity.", "journal": "IEEE ACCESS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000424641000057", "keywords": "Three-dimensional model; Semi-implicit model; Shallow-water equation; Wind-driven circulation", "title": "Three-dimensional hydrodynamic model for wind-driven circulation", "abstract": "This paper presents the development of a three-dimensional hydrodynamic model for wind-driven circulation using the moving element method in the vertical discretization. As most three-dimensional models use sigma coordinate transformation in the vertical discretization, the use of the moving element method is an alternative to the latter, with the advantage that there is no need for fixed subdivision of the water column. In the proposed model, the coupling of two-and three-dimensional hydrodynamic model is considered. The shallow-water equations are integrated in the vertical direction, finite elements are employed in the spatial discretization, and finite differences in the time discretization, to resolve the position of the free surface (zeta), and the vertically integrated velocities components. The three-dimensional module is used to compute the velocity profiles. In the three-dimensional module is employed the moving element method in the vertical discretization. The efficiency of the model is demonstrated through the comparison of its results with laboratory experimental results and with another model that uses sigma coordinate transformation in the vertical discretization.", "journal": "JOURNAL OF THE BRAZILIAN SOCIETY OF MECHANICAL SCIENCES AND ENGINEERING", "category": "Engineering, Mechanical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000430068400007", "keywords": "motion estimation; radar signal processing; Doppler radar; computational complexity; radar detection; object detection; transforms; joint parameter estimation method; multiple manoeuvring targets; long-time coherent integration; motion parameter estimation; multiple weak manoeuvring targets; first-order range migration; FRM; second-order RM; SRM; Doppler frequency migration; Doppler spectrum spanning; pulse repetition frequency; first-order keystone transform; FKT; Doppler shifting function; velocity ambiguity factor; computational complexity; signal-to-noise ratio scenarios", "title": "Joint parameter estimation method for multiple manoeuvring targets with high speed", "abstract": "In this study, a joint parameter estimation method is proposed to realise the long-time coherent integration and estimate the motion parameters for multiple weak manoeuvring targets with high speed and acceleration, involving the first-order range migration (FRM), second-order RM (SRM) and Doppler frequency migration. In the proposed method, the effect of the target's Doppler spectrum spanning over two neighbouring pulse repetition frequency bounds on first-order keystone transform (FKT) is first analysed and a Doppler shifting function is constructed to solve this problem. Then, FKT and the velocity ambiguity factor searching are performed to eliminate the FRM. After that, SRM is removed through the phase compensation function related to acceleration. Finally, the Lv's distribution is applied to realise the coherent integration and obtain the estimates of velocity and acceleration. The numerical experiments using both simulated and real data demonstrate that the proposed method can achieve close detection and estimation performance with much lower computational complexity, compared with the generalised radon-Fourier transform. Moreover, it can estimate the motion parameters of the targets without knowing any prior knowledge of targets and obtain good coherent integration performance in low signal-to-noise ratio scenarios.", "journal": "IET RADAR SONAR AND NAVIGATION", "category": "Engineering, Electrical & Electronic; Telecommunications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000451932800001", "keywords": "autonomous damage detection; aggregation-induced emission; microcapsule-based composites; multilayered polymeric coatings; microscale crack depths; visual detection", "title": "Autonomous Damage Detection in Multilayered Coatings via Integrated Aggregation-Induced Emission Luminogens", "abstract": "Detection and assessment of small-scale damage at early stages are essential for polymeric materials to extend lifetime, avoid catastrophic structural failure, and improve cost-efficiency. Previous self-reporting coatings provide visual indication of surface damage but have been limited to a single layer without information on the depth of crack penetration. Here, we present a novel strategy for autonomous indication of damage in multilayered polymeric materials using aggregation-induced emission luminogens (AlEgens). Three different AlEgens are encapsulated and layered into polymeric coatings. When scratches of varying depths penetrate the coating layers, different combinations of AlEgens are activated to visually detect the depth of damage based on the corresponding fluorescent colors. The AIEgen-based detection mechanism makes this system a powerful tool for damage indication in a variety of polymeric coatings.", "journal": "ACS APPLIED MATERIALS & INTERFACES", "category": "Nanoscience & Nanotechnology; Materials Science, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000435511900001", "keywords": "Motion control; manipulation planning", "title": "A Dynamical-System-Based Approach for Controlling Robotic Manipulators During Noncontact/Contact Transitions", "abstract": "Many daily life tasks require precise control when making contact with surfaces. Ensuring a smooth transition from free motion to contact is crucial as incurring a large impact force may lead to unstable contact with the robot bouncing on the surface, i.e., chattering. Stabilizing the forces at contact is not possible as the impact lasts for less than a millisecond, leaving no time for the robot to react to the impact force. We present a strategy in which the robot adapts its dynamic before entering into contact. The speed is modulated so as to align with the surface. We leverage the properties of autonomous dynamical systems for immediate re-planning and handling unforeseen perturbations and exploit local modulations of the dynamics to control for the smooth transitions at contact. We show theoretically and empirically that by using the modulation framework, the robot can stably touch the contact surface, even when the surface's location is uncertain, at a desired location, and finally, leave the surface or stop on the surface at a desired point.", "journal": "IEEE ROBOTICS AND AUTOMATION LETTERS", "category": "Robotics", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000434973300003", "keywords": "chewing; food texture; jaw tracking", "title": "Comparison of physical chewing measures to consumer typed Mouth Behavior", "abstract": "The purpose of this study was to investigate the hypotheses that when presented with foods that could be chewed in different ways, (1) are participants jaw movements and chewing sequence measures correlated with Mouth Behavior (MB) group, as measured by the JBMB typing tool? (2) can MB group membership can be predicted from jaw movement and chewing sequence measures? One hundred subjects (69 female and 31 male, mean age 27 7.7 years) were given four different foods (Mentos, Walkers, Cheetos Puffs, Twix) and video recordings of their jaw movements made. Twenty-nine parameters were calculated on each chewing sequence with 27 also calculated for the first half and second half of chewing sequence. Subjects were assigned to a MB group using the JBMB typing tool which gives four MB groups (Chewers, Crunchers, Smooshers, and Suckers). The differences between individual chewing parameters and MB group were assessed with analysis of variance which showed only small differences in average chewing parameters between the MB groups. By using discriminant analysis, it was possible to partially discriminate between MB groups based on changes in their chewing parameters between foods with different material properties and stages of the chewing. A 19-variable model correctly predicted 68% of the subjects' membership of a MB group. This partially confirms our first hypothesis that when presented with foods that could be chewed in different ways participants will use a chewing sequence and jaw movements that correlate with their MB as measured by the JBMB typing tool. Practical applicationsThe way consumers chew their food has an impact on their texture perception of that food. While there is a wide range of chewing behaviors between consumers, they can be grouped into broad categories to better target both product design and product testing by sensory panel. In this study, consumers who were grouped on their texture preference (MB group) had jaw movements, when chewing a range of foods, which partially reflected group membership. Therefore, while MB group membership could not be predicted from jaw movement measurements, there were similarities in jaw movements within the members of the groups. A better understanding of how jaw movement during chewing relates to consumer sensory perception would aid in new solid product design with controlled textural attributes.", "journal": "JOURNAL OF TEXTURE STUDIES", "category": "Food Science & Technology", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000446221800020", "keywords": "Acute coronary syndrome; CYP2C19 metabolizer; GRACE score; Risk factor; Prognosis", "title": "Combination of the CYP2C19 metabolizer and the GRACE risk score better predicts the long-term major adverse cardiac events in acute coronary syndrome undergoing percutaneous coronary intervention", "abstract": "Introduction: Both Global Registry of Acute Coronary Events (GRACE) risk score and CYP2C19 metabolizer status can independently predict major adverse cardiac events (MACEs) in patients with acute coronary syndrome (ACS) undergoing percutaneous coronary intervention (PCI). We investigated whether their combination could better predict MACE occurrence in patients with ACS undergoing PCI. Materials and methods: This retrospective cohort study included 548 consecutive patients with ACS undergoing PCI. A cumulative MACE curve was calculated using the Kaplan-Meier method. Multivariate Cox regression was used to identify MACE predictors. The predictive value of GRACE risk score alone and CYP2C19 metabolizer status was estimated by the area under the receiver operating characteristic curve (AUC), net reclassification improvement (NRI), and integrated discrimination improvement (IDI). Results: In a median of 28.58 months, 17 patients (3%) were lost to follow-up, and 62 (11.3%) experienced MACEs. Multivariate Cox regression analysis showed that both GRACE score and CYP2C19 metabolizer status were independent MACE predictors (hazard ratio 1.019, 95% CI 1.011-1.027, p < 0.001; hazard ratio 2.383, 95% CI 1.601-3.547, p < 0.001, respectively). Kaplan-Meier analysis showed that CYP2C19 PM increased the MACE risk (log rank test= 10.848, p= 0.004). The GRACE score adjustment by CYP2C19 metabolizer status enhanced the predictive value (AUC increased from 0.682 for GRACE score alone to 0.731 for GRACE score plus CYP2C19 metabolizer). This result was further verified by IDI and NRI. Conclusions: CYP2C19 metabolizer status and GRACE score are readily available predictive approaches for MACEs, and their combination derives a more accurate long-term MACE prediction in clopidogrel-treated patients with ACS undergoing PCI.", "journal": "THROMBOSIS RESEARCH", "category": "Hematology; Peripheral Vascular Disease", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000443079800092", "keywords": "Gaussian mixture; mathematical programming; probability; robust; wide-area power system stabilizer", "title": "A Computationally Efficient Method to Design Probabilistically Robust Wide-Area PSSs for Damping Inter-Area Oscillations in Wind-Integrated Power Systems", "abstract": "This paper proposes an efficient method that tunes wide-area power system stabilizers (WPSSs) to have probabilistic robustness for damping inter-area electromechanical oscillations in power systems with incorporated random wind power. Specifically, the efficiency of this method benefits fromunique consideration and deduction of the analytic forms of cumulative distribution functions (cdfs) of two types of random variables and their derivatives with respect to tunable parameters of the conventionally structured WPSSs, based on approximations of wind power probability density functions by Gaussian mixtures. These cdfs then compose the objective function of an optimization that can rapidly solve for optimal parameters of WPSSs by a sequential quadratic programming algorithm. The optimized WPSSs are probabilistically robust because they enhance the probability of two commonly desired control effects. Simulation studies on a modified IEEE 10-machine and 39-bus system validate the superior efficiency of the proposed tuning method and the excellent performance of the derived WPSSs.", "journal": "IEEE TRANSACTIONS ON POWER SYSTEMS", "category": "Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000437460600022", "keywords": "Software testing; pair-wise testing; weighting; combinatorial interaction testing", "title": "Prediction of the anti-glioma therapeutic effects of temozolomide through in vivo molecular imaging of MMP expression", "abstract": "Currently, there is no effective way to assess the therapeutic response of temozolomide (TMZ) for the glioma. In this study, the human U87MG-fLuc glioma animal models were set up and the antitumor efficacy of TMZ was evaluated using bioluminescence imaging (BLI) and MRI. Then, bioluminescence tomography (BLT) was reconstructed using an adaptive sparsity matching pursuit (ASMP) algorithm. Second, the expression level of the MMP-750 probe was examined with or without TMZ treatment using FMI. Third, the expression of MMP2 and MMP3 was specifically examined after treatment. The results showed that TMZ effectively inhibited glioma growth. The targeted imaging of MMP-750 was decreased during the treatment of glioma with TMZ. Moreover, the MMP2 and MMP3 expression was found to correlate with the inhibition effect of TMZ. Our study indicated that the therapeutic effects of TMZ can be effectively evaluated at an early stage using molecular imaging, and MMP targeting the fluorescence probe could be utilized for the prediction and assessment of the therapeutic effects of TMZ. (C) 2018 Optical Society of America under the terms of the OSA Open Access Publishing Agreement.", "journal": "BIOMEDICAL OPTICS EXPRESS", "category": "Biochemical Research Methods; Optics; Radiology, Nuclear Medicine & Medical Imaging", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000447086000040", "keywords": "Nonlinear viscous damper; Bilinear oil damper; Fluid viscous damper; Maxwell model; Supplemental damping; Passive control; Response modification; Numerical simulation; Full-scale shake table test", "title": "Adaptive numerical method algorithms for nonlinear viscous and bilinear oil damper models subjected to dynamic loading", "abstract": "Adaptive numerical method algorithms are presented for the numerical simulation of the hysteretic behaviour of nonlinear viscous and bilinear oil dampers within a finite element program for nonlinear dynamic analysis of frame structures under earthquake excitations. The adaptive algorithms are applicable for computing high precision solutions for nonlinear viscous and bilinear oil dampers with valve relief that are typically represented mathematically with a nonlinear Maxwell model. The algorithms presented possess excellent convergence characteristics for viscous dampers with a wide range of velocity exponents and axial stiffness properties. The algorithms are implemented in an open source finite element software, and their applicability and computational efficiency is demonstrated through a number of validation examples with data that involve component experimentation as well as the utilization of full-scale shake table tests of a 5-story steel building equipped with nonlinear viscous and bilinear oil dampers.", "journal": "SOIL DYNAMICS AND EARTHQUAKE ENGINEERING", "category": "Engineering, Geological; Geosciences, Multidisciplinary", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000461614000041", "keywords": "Learned turbo message passing (LTMP); affine rank minimization (ARM); matrix completion; low-rank matrix recovery; compressed robust principal component analysis (CRPCA)", "title": "Feasibility of attenuated total reflection-fourier transform infrared (ATR-FTIR) chemical imaging and partial least squares regression (PLSR) to predict protein adhesion on polymeric surfaces", "abstract": "Predicting the degree to which proteins adhere to a polymeric surface is an ongoing challenge in the scientific community to prevent non-specific protein adhesion and drive favourable protein - surface interactions. This work explores the potential of multivariate PLSR modelling in conjunction with Attenuated Total Reflection -Fourier Transform Infrared (ATR-FTIR) chemical imaging to investigate whether experimentally characterised surface chemistry can be used to predict surface protein adhesion. ATR-FTIR spectra were collected on dry and wetted polymeric surfaces, followed by evaluation of adhered fibrinogen on surfaces using the micro bicinchoninic (BCA) protein assay as a reference method. Partial Least Squares Regression (PLSR) models were built using IR spectra as the predictor variable. Overall the models built with 'wetted polymer' IR spectra performed better as compared to the models built using 'dry polymer' IR spectra (average coefficient of determination, R-P(2) 0.998, 0.996 respectively), with the lowest error in prediction (4 +/- 0.6 mu g) for ultra-high molecular weight polyethylene (UHMPE) as a test surface. This indicates the potential of this method to predict the degree to which protein adhesion occurs on polymeric surfaces using experimentally determined surface chemistry.", "journal": "ANALYST", "category": "Chemistry, Analytical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000482564600002", "keywords": "Forward kinematics; Gough-Stewart platform; Homotopy continuation method; Newton-Raphson method; Three-dimensional platform", "title": "Kinematics of the Gough-Stewart Platform by Means of the Newton-Homotopy Method", "abstract": "In this work the forward kinematic problem of the general Gough-Stewart platform is easily formulated and solved combining the homotopy continuation method and the Newton-Raphson method. Unlike most contributions approaching the topic, in this work the Gough-Stewart platform is considered as a parallel manipulator of general geometry, e.g., it is composed of two three-dimensional platforms. The closure kinematic constraints required to approach the forward displacement analysis are formulated based on linear combinations of three unit vectors attached to the moving platform and one vector relating the position of the moving platform as observed from the base. For the robot manipulator of general geometry a non-linear system of twelve non-linear equations are generated and solved by means of the Newton-homotopy continuation method. Numerical examples are provided with the purpose to illustrate the proposed method.", "journal": "IEEE LATIN AMERICA TRANSACTIONS", "category": "Computer Science, Information Systems; Engineering, Electrical & Electronic", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000450660500004", "keywords": "Gradient processes; Dimension reduction; Nearest neighbor Gaussian processes; Spatial confounding", "title": "Comments on: Process modeling for slope and aspect with application to elevation data maps", "abstract": "The authors should be commended on their methodological development of stochastic processes for slope and aspect. Their development of basic distribution theory needed to study these two processes, and the sufficient conditions that ensure independence and non-informative induced priors, provide a thorough contribution to the collection of work on gradient processes. The fully model-based approach for inference for slope and aspect enables the propagation of uncertainty to environmental process models of interest that would use these variables as explanatory variables in a regression. I greatly appreciate the opportunity to comment on this exciting work and offer some additional model considerations and applications. Specifically, I reaffirm the importance of scalability of the methodology to large datasets, offering a few considerations with regard to model specification. Next, I discuss the unique challenges of using the predictive distributions of the slope and aspect processes as input variables in spatial regression models. Finally, I offer possible applications and extensions of this work that might provide innovative insights into environmental processes.", "journal": "TEST", "category": "Statistics & Probability", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000452782800001", "keywords": "conformal field theory; conformal bootstrap; O(N) symmetry; epsilon-expansion", "title": "Critical O(N) model to order epsilon(4 ) from analytic bootstrap", "abstract": "We compute, using the method of large spin perturbation theory, the anomalous dimensions and OPE coefficients of all leading twist operators in the critical O(N) model, to fourth order in the epsilon-expansion. This is done fully within a bootstrap framework, and generalizes a recent result for the CFT-data of the Wilson-Fisher model. The anomalous dimensions we obtain for the O(N) singlet operators agree with the literature values, obtained by diagrammatic techniques, while the anomalous dimensions for operators in other representations, as well as all OPE coefficients, are new. From the results for the OPE coefficients, we derive the epsilon(4) corrections to the central charges C-T and C-J, which are found to be compatible with the known large N expansions. Predictions for the central charge in the strongly coupled 3d model, including the 3d Ising model, are made for various values of N, which compare favourably with numerical results and previous predictions.", "journal": "JOURNAL OF PHYSICS A-MATHEMATICAL AND THEORETICAL", "category": "Physics, Multidisciplinary; Physics, Mathematical", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000457500100016", "keywords": "Bridge; weigh-in-motion; bridge weigh-in-motion; virtual monitoring; orthotropic steel deck; fatigue evaluation", "title": "Virtual Monitoring of orthotropic steel deck using bridge weigh-in-motion algorithm: Case study", "abstract": "This article outlines a Virtual Monitoring approach for fatigue life assessment of orthotropic steel deck bridges. Bridge weigh-in-motion was used to calculate traffic loads which were then used to calculate \"virtual\" strains. Some of these strains were checked through long-term monitoring of dynamic strain data. Field tests, incorporating calibration with pre-weighed trucks and monitoring the response to regular traffic, were conducted at Fochen Bridge, which has an orthotropic steel deck and is located in Foshan City, China. In the calibration tests, a 45-t 3-axle truck ran repeatedly across Lane 2, the middle lane in a 3-lane carriageway. The results show that using an influence surface to weigh vehicles can improve the accuracy of the weights and, by inference, of remaining service life calculations. The most fatigue-prone position was found to be at the cutout in the diaphragms. Results show that many vehicles are overweight-the maximum gross vehicle weight recorded was 148 t, nearly 3.6 times heavier than the fatigue design truck.", "journal": "STRUCTURAL HEALTH MONITORING-AN INTERNATIONAL JOURNAL", "category": "Engineering, Multidisciplinary; Instruments & Instrumentation", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000448962400002", "keywords": "Load recovery; force transducer; D-optimum design; optimal sensor placement; dynamic programming; model reduction", "title": "Dynamic Programming Approach to Load Estimation Using Optimal Sensor Placement and Model Reduction", "abstract": "A time-domain technique for estimating dynamic loads acting on a structure from structural response measured experimentally at a finite number of optimally placed sensors on the structure is presented. The technique relies on an existing solution method based on dynamic programming, which consists of a backward (inverse) time sweeping phase followed by a forward time sweeping phase. The dynamic programming method of load identification, similar to all other inverse methods, suffers from ill-conditioning. Small variations (noise) in response measurements can cause large errors in load estimates. The condition of the inverse problem, and hence the quality of load estimates, depends on the locations of sensors on the structure. There can be a large number of locations on a structure where sensors can potentially be mounted. A D-optimal design algorithm is used to arrive at optimal sensor locations such that the condition of the inverse problem is improved and precise load estimates are obtained. Another major limitation of the dynamic programming technique is that the computation time increases dramatically as the model size increases. To deal with this shortcoming, a technique based on Craig-Bampton model reduction is also proposed in this paper. Numerical results illustrate the effectiveness of the proposed technique in accurately recovering the loads imposed on discrete as well as continuous systems.", "journal": "INTERNATIONAL JOURNAL OF COMPUTATIONAL METHODS", "category": "Engineering, Multidisciplinary; Mathematics, Interdisciplinary Applications", "annotated_keywords": [], "label": "0", "title_label": "0"}
{"id": "WOS:000487151200001", "keywords": "Colorectal cancer; pT1; lymph node; harvest; retrieval; adenocarcinoma", "title": "Factors affecting retrieval of 12 or more lymph nodes in pT1 colorectal cancers", "abstract": "Objective The aim of this study was to identify clinicopathological factors that affect the number of lymph nodes (LNs) (12 or more) retrieved from patients with colorectal cancer (CRC), particularly those with pathologic T1 (pT1) disease. Methods From 429 CRC patients, 75 pT1 cancers were identified and digitally scanned. Binary logistic regression analysis was performed to identify the clinicopathological factors affecting the number of LNs retrieved from all 429 patients and from the subset of patients with pT1 CRC. Results For the 429 patients, the mean number of harvested LNs per specimen was 20 (median, 19). The number of retrieved LNs was independently associated with maximum tumor diameter > 2.3 cm and right-sided tumor location. The mean number of LNs retrieved from the 75 patients with pT1 CRC was 14 (median, 15); retrieval of 12 or more LNs from this group was independently associated with maximum tumor diameter > 14.1 mm. Conclusion The number of LNs retrieved from patients with CRC was associated with maximum tumor diameter and right-sided tumor location. For patients with pT1 CRC, maximum tumor diameter was independently associated with the harvesting of 12 or more LNs.", "journal": "JOURNAL OF INTERNATIONAL MEDICAL RESEARCH", "category": "Medicine, Research & Experimental; Pharmacology & Pharmacy", "annotated_keywords": [], "label": "0", "title_label": "0"}